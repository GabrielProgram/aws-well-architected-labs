---
title: "Level 200: Optimize Amazon EC2 using Amazon CloudWatch and AWS Compute Optimizer"
menutitle: "Optimize Amazon EC2 using Amazon CloudWatch and AWS Compute Optimizer"
date: 2020-11-18T09:00:08-04:00
chapter: false
weight: 1
hidden: false
---
## Author

- **Neha Garg**, Enterprise Solutions Architect.
- **Jang Whan Han**, Well-Architected Geo Solutions Architect.

## Contributor
- **Stephen Salim**, Well-Architected Geo Solutions Architect.
- **Sam Mokhtari**, Sr Sustainability Lead SA Well-Architected.

## Introduction

This lab focuses on how you can improve the efficiency of your workload by using the fewest number of compute resources and achieving a high utilization, specifically focused on rightsizing vCPU, Memory, and EBS volumes based on recommendations that AWS Compute Optimizer analyzed. Also, we will learn how Amazon CloudWatch Logs and Metrics will be able to simply help you measure business metrics as well as calcuate Key performance indicators.

## Goals
At the end of this lab you will:

* Understand how to identify optimization areas using AWS Well-Architected Sustainability Pillar best practices
* Baseline and optimize workloads by identifying sustainability key performance indicators (KPIs) using **Proxy metric for provisioned resource** and **Business metric for outcome**.
* Learn how to use [Amazon CloutWatch metric math](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/using-metric-math.html) feature to calculate **Resources provisioned per unit of work**. 
* Learn how to use [AWS Compute Optimizer](https://aws.amazon.com/aws-cost-management/aws-cost-optimization/right-sizing/) for right sizing to optimize resources of Amazon EC2 Instnace.

## Prerequisites

* The lab is designed to run in your own AWS account.
* Enable [AWS Compute Optimizer](https://aws.amazon.com/compute-optimizer/)
* We will deploy a sample web application and will automatically generate a worklaod to simulate requets from users.

## Costs
* **t4g.xlarge** instance will be deployed as a baseline Amazon EC2 instance. We will change Amazon EC2 instance type from c4.xlarge to **c7g.large** to reduce idle resources.

{{% notice note %}}
**NOTE:** You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the [AWS Free Tier](https://aws.amazon.com/free/).
{{% /notice %}}
* [AWS Pricing](https://aws.amazon.com/pricing/)

## Lab duration
Estimated time required to complete this lab is 40 minutes.

## Workload details (TBD)
AnyCompany (an fictional event management organization) is running a central data warehouse environment on Amazon Redshift in `us-east-1` region, which is used by various departments in the organization for their respective storage, analytical processing, and reporting. The marketing department is the top consumer of the data warehouse, and have data engineers, analysts, and scientists based out of US west coast. The marketing team has implemented their own Amazon Redshift cluster in `us-west-1` (consumer) region, which refreshes nightly using an Amazon Redshift snapshot received from `us-east-1` region (producer) and uploading to the `us-west-1` (consumer) region Amazon Redshift cluster. Since the marketing team analytical processing consumes lots of resources & integrated with their west coast based on-premise hosted downstream applications, they perform their analytical processing in `us-west-1` region, and other departments use `us-east-1` region hosted data warehouse. This requires storing a redundant dataset in `us-west-1` region, and transferring huge amounts of data (via nightly ETL feed) over the network between AWS regions.

This is not a sustainability friendly implementation, and can be optimized using AWS Well-Architected Sustainability Pillar best practices for data patterns. Also, with this approach, the insights generated by the Marketing department are not based on live data.

## Workload optimization for sustainability (TBD)
In this case, optimization areas include:
* Marketing team using only data which is required for their analytical processing, whereas currently they use the full dataset. This reduces storage requirements in `us-west-1`.
* By reducing the amount of data copied between `us-east-1` and `us-west-1` regions, this reduces network traffic.

## Technical solution (TBD)
By introducing Amazon Redshift Data Sharing feature, the marketing department can optimize their implementation for sustainability, avoiding redundant storage & reducing data transfer between AWS regions. Data sharing enables instant, granular, and fast data access across Amazon Redshift clusters without the need to copy or move it. With data sharing, you have live access to data, so that your users can see the most up-to-date and consistent information as it's updated in Amazon Redshift clusters.

## Sustainability improvement process (TBD)
The improvement goals of this lab are to:
* To eliminate waste, low utilization, and idle or unused resources
* To maximize the value from resources consumed

This lab use case focuses on removing unneeded or redundant data, and minimizing data movement across network. For more details, refer to [Sustainability Pillar Whitepaper](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/improvement-process.html) which explains the iterative process that evaluates, prioritizes, tests, and deploys sustainability-focused improvements for cloud workloads.

To evaluate specific improvements, understand the resources provisioned by your workload to complete a unit of work. Evaluate potential improvements, and estimate their potential impact, the cost to implement, and the associated risks. To measure improvements over time, first understand what you have provisioned in AWS and how those resources are being consumed.

Refer to [Sustainability Pillar Whitepaper](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/evaluate-specific-improvements.html) for detailed understanding around evaluating specific improvements. At high level:
* Use Proxy metrics to measure the resources provisioned to achieve business outcomes. (To derive metrics from AWS Cost and Usage reports check out this [Well-Architected Lab](https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/))

  ![Proxy Metrics](/Sustainability/300_optimize_data_pattern_using_redshift_data_sharing/lab-0/images/proxy_metrics_type.png?classes=lab_picture_small)

  For this lab, we will use these proxy metrics:
    * Total data storage used (MB provisioned)
    * Total data transfer over network (MB transferred)
  To find out how much storage is used for `us-west-1` region Redshift cluster, and how much data is transferred over the network between producer (`us-east-1`) and consumer (`us-west-1`) clusters across regions for data replication:

* Select business metrics to quantify the achievement of business outcomes. Your business metrics should reflect the value provided by your workload, for example, the number of simultaneous active users, API calls served, or the number of transactions completed. For this lab, we will use total number of events held (business outcome) as business metric.

* To calculate a sustainability key performance indicator (KPI), we will use the following formula, divide the provisioned resources by the business outcomes achieved to determine the provisioned resources per unit of work:

    ![Sustainability KPI](/Sustainability/300_optimize_data_pattern_using_redshift_data_sharing/lab-0/images/sustainability_kpi2.png?classes=lab_picture_small)

  Our improvement goal is to:
  * Reduce total storage used, and data transfer over the network for all events
  * Reduce per event provisioned resources


{{< prev_next_button link_next_url="./1_prerequisites/" button_next_text="Start Lab" first_step="true" />}}

## Steps:
{{% children  %}}
