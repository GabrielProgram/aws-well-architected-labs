---
title: "Level 300: Optimize Data Pattern using Amazon Redshift Data Sharing"
menutitle: "Optimize Data Pattern using Amazon Redshift Data Sharing"
date: 2020-11-18T09:00:08-04:00
chapter: false
weight: 1
hidden: false
---
## Author

- **Raman Pujani**, Solutions Architect.

## Introduction

This lab focuses on optimizing Data patterns for Sustainability, specifically focused on removing unneeded or redundant data, and minimizing data movement across networks.

## Goals
At the end of this lab you will:

* Understand how to identify optimization areas using AWS Well-Architected Sustainability Pillar best practices
* Baseline and optimize workload by identifying Sustainability Key Performance Indicators (KPIs)
* Learn how to use the Amazon Redshift Data Sharing feature to implement [data management best practices](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/data-patterns.html) described in [AWS Well-Architected Sustainability Pillar](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html)

## Prerequisites

* The lab is designed to run in your own AWS account
* You can launch an Amazon Redshift cluster in the AWS `us-east-1` and AWS `us-west-1` regions (referred to _`us-east-1`_ region, and _`us-west-1`_ region throughout the lab) using [Redshift ra3 nodes](https://aws.amazon.com/redshift/features/ra3/)
  * This lab is written and tested in `us-east-1` and `us-west-1` regions. You may be able to run this in other AWS regions of your choice [where Redshift ra3 nodes are available](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#ra3-regions), but results may vary.

{{% notice note %}}
**NOTE:** You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the [AWS Free Tier](https://aws.amazon.com/free/). Amazon Redshift ra3 nodes are not part of Amazon Redshift Free trial, or AWS Free Tier. When you decide to stop the lab at any point in time, please revisit the [clean up]({{< ref "content/Sustainability/300_Labs/300_optimize_data_pattern_using_redshift_data_sharing/cleanup.md" >}}) instructions at the end so you stop incuring cost (e.g. for storage in Amazon S3).
{{% /notice %}}

## Lab duration
Estimated time required to complete this lab is 90 minutes.

## Workload details
The use case is AnyCompany (an event management organization) running a central data warehouse environment on Amazon Redshift in `us-east-1` region, which is used by various departments in the organization for their respective storage, analytical processing, and reporting. The Marketing department is the top consumer of the data warehouse, and have data engineers, analysts, and scientists based out of US west coast. The Marketing team has implemented their own Redshift cluster in `us-west-1` (consumer) region, which refreshes nightly using a Redshift snapshot received from `us-east-1` region (producer) and uploading to the `us-west-1` (consumer) region Redshift cluster. Since the Marketing team analytical processing consumes lots of resources & integrated with their west coast based on-premise hosted downstream applications, they perform their analytical processing in `us-west-1` region, and other departments use `us-east-1` region hosted data warehouse. This requires storing redundant dataset in `us-west-1` region, and transferring huge amount of data (via nightly ETL feed) over network between AWS regions.

This is not a Sustainability friendly implementation, and can be optimized using AWS Well-Architected Sustainability Pillar best practices for Data patterns. Also, with this approach, the insights generated by the Marketing department are not based on live data.

## Workload optimization for Sustainability
In this case, optimization areas include:
* Marketing team using only dataset which is required for their analytical processing, whereas currently they use full dataset. This reduces storage requirements in west coast. 
* by reducing the amount of data copy between east and west region, this reduces network traffic as well. AWS services help optimize the workload to make sustainability impact.

## Technical solution
By introducing Amazon Redshift Data Sharing feature, Marketing department can optimize their implementation to make Sustainability impact, and avoid redundant storage & reduce data transfer over network between AWS regions. Data sharing enables instant, granular, and fast data access across Amazon Redshift clusters without the need to copy or move it. With data sharing, you have live access to data, so that your users can see the most up-to-date and consistent information as it's updated in Amazon Redshift clusters, and provides cost benefit.

Redshift environment **before** implementing Data Sharing feature: Both, Producer and Consumer database size is 1252 MB
![Before implementing Redshift Data Sharing](/Sustainability/300_optimize_data_pattern_using_redshift_data_sharing/lab-0/images/before_redshift_data_sharing.png?classes=lab_picture_small)


Redshift environment **after** implementing Data Sharing feature (both Producer and Consumer database size is 1252 MB):
![After implementing Redshift Data Sharing](/Sustainability/300_optimize_data_pattern_using_redshift_data_sharing/lab-0/images/after_redshift_data_sharing.png?classes=lab_picture_small)


## Sustainability Improvement Process
The goals of your improvement process can be:
* To eliminate waste, low utilization, and idle or unused resources
* To maximize the value from resources you consume

This lab use case focuses on removing unneeded or redundant date, and minimizing data movement across network. For more details, refer to [Sustainability Pillar Whitepaper](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/improvement-process.html) which explains iterative process that evaluates, prioritizes, tests, and deploys sustainability-focused improvements for cloud workloads.

To evaluate specific improvements, understand the resources provisioned by your workload to complete a unit of work. Evaluate potential improvements, and estimate their potential impact, the cost to implement, and the associated risks. To measure improvements over time, first understand what you have provisioned in AWS and how those resources are being consumed.

Refer to [Sustainability Pillar Whitepaper](https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/evaluate-specific-improvements.html) for detailed understanding around evaluating specific improvements. At high level:
* Use Proxy metrics to measure the resources provisioned to achieve business outcomes. (To learn about Proxy metrics, refer to this [Well-Architected Lab](https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/))

  ![Proxy Metrics](/Sustainability/300_optimize_data_pattern_using_redshift_data_sharing/lab-0/images/proxy_metrics_type.png?classes=lab_picture_small)

  For this lab, we will use below proxy metrics (provisioned resources) to find out how much storage is used for `us-west-1` region Redshift cluster, and how much data is transferred over the network between Producer (`us-east-1`) and Consumer (`us-west-1`) clusters across regions for data replication:
    * Total data storage used (MB provisioned)
    * Total data transfer over network (MB transferred)

* Select business metrics to quantify the achievement of business outcomes. Your business metrics should reflect the value provided by your workload, for example, the number of simultaneous active users, API calls served, or the number of transactions completed. For this lab, we will use total number of events held (business outcome) as business metric.

* To calculate Sustainability Key Performance Indicator (KPI), we will use the following formula, divide the provisioned resources by the business outcomes achieved to determine the provisioned resources per unit of work:

    ![Sustainability KPI](/Sustainability/300_optimize_data_pattern_using_redshift_data_sharing/lab-0/images/sustainability_kpi2.png?classes=lab_picture_small)

  Our improvement goal is to:
  * Reduce total storage used, and data transfer over network for all events
  * Reduce per event provisioned resources


{{< prev_next_button link_next_url="./1_understand_data_sharing/" button_next_text="Start Lab" first_step="true" />}}

## Steps:
{{% children  %}}
