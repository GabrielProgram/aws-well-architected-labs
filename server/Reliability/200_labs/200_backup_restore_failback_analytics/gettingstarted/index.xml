<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Getting Started on AWS Well-Architected Labs</title><link>https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/</link><description>Recent content in Getting Started on AWS Well-Architected Labs</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 29 Jul 2020 13:22:15 -0600</lastBuildDate><atom:link href="https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/index.xml" rel="self" type="application/rss+xml"/><item><title>Prerequisites</title><link>https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/account/</link><pubDate>Tue, 28 Apr 2020 10:02:27 -0600</pubDate><guid>https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/account/</guid><description>Prerequisites To run this workshop, you need an AWS account, and a user identity with access to the following services:
Glue S3 Athena DynamoDB Global Accelerator EC2 and ELB Lambda Kinesis You can use your own account, or an account provided through Event Engine as part of an AWS organized workshop. Using an account provided by Event Engine is the easier path, as you will have full access to all AWS services, and the account will terminate automatically when the event is over.</description></item><item><title>Architecture</title><link>https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/architecture/</link><pubDate>Tue, 28 Apr 2020 10:02:27 -0600</pubDate><guid>https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/architecture/</guid><description>Architecture In this workshop we&amp;rsquo;ll work with a typical Big Data workload with streaming ingest and batch processing.
Ingest Our data source is streaming data coming from external applications. We present a Global Accelerator endpoint for the producers to send data to. The endpoint is serviced in a particular region by a Lambda function behind an Application Load Balancer. The Lambda function relays the data to a Kinesis stream.
Batch Processing Once the data lands in a Kinesis stream, the batch processing flow picks up with a Firehose landing the data in S3.</description></item></channel></rss>