AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves Pricing data
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: pricing
  RolePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  Schedule:
    Type: String
    Default: "cron(30 12 L * ? *)"
    Description: Cloud watch event Schedule to trigger the lambda 
Outputs:
  LambdaRoleARN:
    Description: Role for Lambda execution of lambda data.
    Value:
      Fn::GetAtt:
        - LambdaRole
        - Arn
Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${RolePrefix}Lambda-Role-${CFDataName}"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:ListBucket"
                  - "s3:GetBucketLocation"
                Resource: !Ref  DestinationBucketARN
              - Effect: "Allow"
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                  - "logs:DescribeLogStreams"
                Resource: "arn:aws:logs:*:*:*"
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${CFDataName}-Lambda-Function"
      Description: !Sub "LambdaFunction to retrieve ${CFDataName}"
      Runtime: python3.8
      Architectures: [arm64]
      Code:
        ZipFile: |
          #Author Stephanie Gooch 2021
          # Function to download EC2 pricing
          # Please reachout to costoptimization@amazon.com if there's any comments or suggestions

          import json
          import boto3
          import urllib3
          import urllib.request
          import os
          import logging

          def lambda_handler(event, context):

              s3=boto3.resource('s3')
              http=urllib3.PoolManager()

              url = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.csv'
              s3Bucket =  os.environ["BUCKET_NAME"]
              key = f'{os.environ["DEST_PREFIX"]}/ec2/ec2_prices.csv'

              urllib.request.urlopen(url)   #Provide URL
              s3.meta.client.upload_fileobj(http.request('GET', url, preload_content=False), s3Bucket, key)

              region_data(s3Bucket, s3)

              return {
                  'statusCode': 200,
                  'body': json.dumps('YAY!')
              }
          def region_data(s3Bucket, s3):

            #Create a Source Dictionary That Specifies Bucket Name and Key Name of the Object to Be Copied
            copy_source = {
                'Bucket': 'aws-well-architected-labs',
                'Key': 'Cost/Labs/300_Optimization_Data_Collection/Region/regions.csv'
            }
            
            bucket = s3.Bucket(s3Bucket)
            
            bucket.copy(copy_source, 'pricing/region/regions.csv')
            
            # Printing the Information That the File Is Copied.
            print('Single File is copied')


      Handler: 'index.lambda_handler'
      MemorySize: 2880
      Timeout: 300
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Environment:
        Variables:
          BUCKET_NAME:
            !Ref DestinationBucket
          ACCOUNT_ID: AWS::AccountId
          DEST_PREFIX: !Ref CFDataName
  CloudWatchTrigger:
    Type: AWS::Events::Rule
    Properties:
      Description: Weekly Notification Event for lambda data
      Name: !Sub "${CFDataName}-Weekly-Scheduler"
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      Targets:
        - Arn:
            Fn::GetAtt:
              - LambdaFunction
              - Arn
          Id: WeeklyTriggerForEBSUtilization
  EventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:  
          Fn::GetAtt:
              - LambdaFunction
              - Arn
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !GetAtt CloudWatchTrigger.Arn
  AthenaRegionTable:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: region_names
      QueryString: !Sub
        "CREATE EXTERNAL TABLE `region_names`(
        `region` string, 
        `regionname` string, 
        `endpoint` string, 
        `protocol` string)
      ROW FORMAT DELIMITED 
        FIELDS TERMINATED BY ',' 
      STORED AS INPUTFORMAT 
        'org.apache.hadoop.mapred.TextInputFormat' 
      OUTPUTFORMAT 
        'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
      LOCATION
        's3://${DestinationBucket}/pricing/region'
      TBLPROPERTIES (
        'has_encrypted_data'='false', 
        'skip.header.line.count'='1', 
        'transient_lastDdlTime'='1611498146')"  
  AthenaEC2PricingTable:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: ec2_pricing_table
      QueryString: !Sub
        "CREATE EXTERNAL TABLE `ec2_pricing`(
        `sku` string,
        `offertermcode` string,
        `ratecode` string,
        `termtype` string,
        `pricedescription` string,
        `effectivedate` string,
        `startingrange` string,
        `endingrange` string,
        `unit` string,
        `priceperunit` double,
        `currency` string,
        `relatedto` string,
        `leasecontractlength` string,
        `purchaseoption` string,
        `offeringclass` string,
        `product family` string,
        `servicecode` string,
        `location` string,
        `location type` string,
        `instance type` string,
        `current generation` string,
        `instance family` string,
        `vcpu` string,
        `physical processor` string,
        `clock speed` string,
        `memory` string,
        `storage` string,
        `network performance` string,
        `processor architecture` string,
        `storage media` string,
        `volume type` string,
        `max volume size` string,
        `max iops/volume` string,
        `max iops burst performance` string,
        `max throughput/volume` string,
        `provisioned` string,
        `tenancy` string,
        `ebs optimized` string,
        `operating system` string,
        `license model` string,
        `group` string,
        `group description` string,
        `transfer type` string,
        `from location` string,
        `from location type` string,
        `to location` string,
        `to location type` string,
        `usagetype` string,
        `operation` string,
        `availabilityzone` string,
        `capacitystatus` string,
        `classicnetworkingsupport` string,
        `dedicated ebs throughput` string,
        `ecu` string,
        `elastic graphics type` string,
        `enhanced networking supported` string,
        `from region code` string,
        `gpu` string,
        `gpu memory` string,
        `instance` string,
        `instance capacity - 10xlarge` string,
        `instance capacity - 12xlarge` string,
        `instance capacity - 16xlarge` string,
        `instance capacity - 18xlarge` string,
        `instance capacity - 24xlarge` string,
        `instance capacity - 2xlarge` string,
        `instance capacity - 32xlarge` string,
        `instance capacity - 4xlarge` string,
        `instance capacity - 8xlarge` string,
        `instance capacity - 9xlarge` string,
        `instance capacity - large` string,
        `instance capacity - medium` string,
        `instance capacity - metal` string,
        `instance capacity - xlarge` string,
        `instancesku` string,
        `intel avx2 available` string,
        `intel avx available` string,
        `intel turbo available` string,
        `marketoption` string,
        `normalization size factor` string,
        `physical cores` string,
        `pre installed s/w` string,
        `processor features` string,
        `product type` string,
        `region code` string,
        `resource type` string,
        `servicename` string,
        `snapshotarchivefeetype` string,
        `to region code` string,
        `volume api name` string,
        `vpcnetworkingsupport` string)
        ROW FORMAT SERDE
        'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES (
        'quoteChar'='\"',
        'separatorChar'=',')
        STORED AS INPUTFORMAT
        'org.apache.hadoop.mapred.TextInputFormat'
        OUTPUTFORMAT
        'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        LOCATION
        's3://${DestinationBucket}/${CFDataName}/ec2'
        TBLPROPERTIES (
        'CrawlerSchemaDeserializerVersion'='1.0',
        'CrawlerSchemaSerializerVersion'='1.0',
        'UPDATED_BY_CRAWLER'='Pricing CSV',
        'areColumnsQuoted'='true',
        'averageRecordSize'='1061',
        'classification'='csv',
        'columnsOrdered'='true',
        'compressionType'='none',
        'delimiter'=',',
        'objectCount'='1',
        'recordCount'='2089892',
        'sizeKey'='2217375799',
        'skip.header.line.count'='6',
        'typeOfData'='file')"