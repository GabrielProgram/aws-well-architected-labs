[{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_vpc_flow_logs_analysis_dashboard/1_enable_vpc_flow_logs/","title":"Enable VPC Flow Logs","tags":[],"description":"","content":" Central AWS Account: AWS account which you want to designate for storing VPC Flow Logs centrally. This account will also contain Athena DB, table and QuickSight Dashboard.\nAdditional Accounts: These are other accounts that you own and has VPCs that you wish to enable Flow Logs and have an ability to push it to Central AWS Account\u0026rsquo;s S3 bucket.\nQuickSight: To manage VPC Flow Logs and QuickSight dashboard in central account please make sure you create resources for the central account in the region supported by QuickSight. Refer to this link to see supported regions.\nVPC If you already have VPC and other resources running your AWS account continue with next section \u0026ldquo;Enable VPC Flow Logs\u0026rdquo; otherwise click on below link to deploy VPC and a toy webapp into your account. Click here for instructions how to deploy a VPC to your AWS account: This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you). Wait until the VPC CloudFormation stack status is CREATE_COMPLETE, then continue. This will take about four minutes.\nDownload the CloudFormation template: staticwebapp.yaml You can right-click then choose Save link as; or you can right click and copy the link to use with wget Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next\nFor Stack name use CloudFormationLab\nParameters\nLook over the Parameters and their default values.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nEnable VPC Flow Logs QuickSight dashboard provided in this lab requires all the fields mentioned in the Introduction section are required. If you already have enabled VPC Flow logs with those fields (with CSV format, Hive partition enabled and delivered to S3) then you can skip this section and proceed to \u0026quot;Create Athena resources, Lambda function and CloudWatch rule\u0026quot; section to continue. If you do not have VPC flow logs enabled or existing VPC Flow logs does not have all the required fields then this section will help you in enabling vpc flow logs for existing VPC(s) in your account. Repeat all the steps from this section for each VPC in case you want to enable VPC Flow logs in respective account to visualize them in QuickSight dashboard under central account.\nParquet file format Click here for the instructions for enabling VPC Flow Logs in Parquet format Use aws cli or AWS CloudShell to run below command. This command will create Flow Log in parquet file format with hive-compatible s3 prefixes\nNavigate to CloudShell from AWS Console from the account where your VPC is located. Note: Please make sure you have correct region selected.\nReplace \u0026lt;VPC ID\u0026gt; with VPC id from your account. You can find the VPC ID in console Replace \u0026lt;S3 ARN\u0026gt; with S3 buckets arn from central account. Please specify subfolder in case you are storing logs under it.\ne.g. arn:aws:s3:::my-flow-log-bucket/my-custom-flow-logs/\naws ec2 create-flow-logs \\ --resource-type VPC \\ --resource-ids \u0026lt;VPC ID\u0026gt; \\ --traffic-type ALL \\ --log-destination-type s3 \\ --log-destination \u0026lt;S3 ARN\u0026gt; \\ --destination-options FileFormat=parquet,HiveCompatiblePartitions=True,PerHourPartition=false \\ --log-format '${account-id} ${action} ${az-id} ${bytes} ${dstaddr} ${dstport} ${end} ${flow-direction} ${instance-id} ${interface-id} ${log-status} ${packets} ${pkt-dst-aws-service} ${pkt-dstaddr} ${pkt-src-aws-service} ${pkt-srcaddr} ${protocol} ${region} ${srcaddr} ${srcport} ${start} ${sublocation-id} ${sublocation-type} ${subnet-id} ${tcp-flags} ${traffic-path} ${type} ${version} ${vpc-id}' Once you finish replacing ID, ARN paste the command in CloudShell and run it. You will see below result with FlowLogIds, if it is successful. CSV file format Click here for the instructions for enabling VPC Flow Logs in CSV format Login to your central AWS account.\nRun CloudFormation stack to enable VPC Flow Logs.\nDownload CloudFormation Template: vpc-flow-logs-custom.yaml This CloudFormation template enables VPC Flow Logs in the account you run it. You will need to run it per VPC. From AWS Console navigate to CloudFormation. Then click on Create stack Create stack page:\nIn Create stack page Specify template select Upload a template file. Then Choose File and upload the template vpc-flow-logs-custom.yaml (you have downloaded previously) Click Next Provide name for the stack e.g., \u0026ldquo;vpc-flow-logs-stack\u0026rdquo; and values for the stack parameters and then click Next\nTrafficType (ACCEPT, REJECT, ALL): Type of traffic you wish to record\nACCEPT — The recorded traffic was permitted by the security groups and network ACLs. REJECT — The recorded traffic was not permitted by the security groups or network ACLs. ALL - The recorded traffic that was permitted (ACCEPT) and was not permitted (REJECT) by the security groups or network ACLs. VpcFlowLogsBucketName (Optional): S3 bucket name where VPC flow logs will be stored.\nIf you specify the bucket name then it is assumed that the bucket already exists. If you want to centralize the storage of the logs, then create the bucket before and specify the bucket name here. If you are enabling VPC Flow Logs in additional account then please make sure to modify S3 bucket\u0026rsquo;s policy from the central account to grant access to additional account and provide the name of the central bucket to this parameter.\nIf you leave it blank CloudFormation template will create a bucket for you.\nNote: VpcFlowLogsBucketName - This bucket will be used to gather vpc flow logs for all of your vpcs from one or more accounts. So please make sure this is the central account where you want your VPC flow logs to be collected and QuickSight dashboard to be hosted.\nVpcFlowLogsFilePrefix (Optional): VPC Flow logfile prefix in S3 bucket. See bold text in below example\ne.g., bucket_name/vpc-flow-logs/AWSLogs/aws_account_id/vpcflowlogs/region/year/month/day/\nVpcId: You can find the VPC ID in console In Configure stack options page, add below tags and click on Next\nName=VPCFlowLogs-CFN Purpose=WALabVPCFlowLogs On Review screen verify the inputs you have provided Last click on Create stack As shown below you will see progress of the stack creation under Events tab. Please wait for the stack to complete the execution. Once complete it will show the status CREATE_COMPLETE in green then proceed to the next step. To verify, navigate to VPC service, click on vpc link and then click on Flow Logs tab at the bottom part of the screen. You will see a line with flow logs you just created now. Delete older VPC Flow Logs from S3 bucket (Optional) We recommend you to create a life cycle policy to delete logs older than 90 days or lesser as per your requirement to save cost. All the steps from this section are required to execute one time in central account.\nClick here to see the steps to Delete older VPC Flow Logs from S3 bucket Login to central AWS account if you are not already in that account.\nNavigate to S3 service from console\nClick on S3 bucket where you stored VPC Flow Logs and click on Management link. Click on Create lifecycle rule Enter name for the rule e.g., 90_DAY_DELETE. You can edit the number of days based on your requirement.\nCheck \u0026ldquo;This rule applies to all objects in the bucket\u0026rdquo; Check \u0026ldquo;I acknowledge that this rule will apply to all the objects in the bucket.\u0026rdquo; Under \u0026ldquo;Lifecycle rule actions\u0026rdquo; check Expire current versions of objects Delete expired delete markers or incomplete multipart uploads Enter 90 days for \u0026ldquo;Number of days after object creation\u0026rdquo; and 90 days for \u0026ldquo;Number of days after object becomes previous versions\u0026rdquo; and click on Create rule\nNOTE: You can change the number of days based on your requirement. Once you create the rule, it will appear on Lifecycle Configuration Page Enable VPC Flow Logs in additional accounts and store it in central bucket (Optional) Click here to see the steps to enable VPC Flow logs in additional accounts Before you proceed to enable VPC Flow Logs in additional account, you need to grant permission to access S3 bucket(from central account) for target account’s VPC to push logs. Repeat all the steps from this section for each Account/VPC. Please follow below steps to edit S3 bucket policy in central account:\nNavigate to S3 service in the central account where you have S3 bucket and QuickSight Dashboard for VPC Flow Logs created in first step.\nClick on the vpc flow logs bucket you created earlier and then navigate to permissions tab.\nScroll down to the bucket policy. You will see existing policy like below.\nClick on Edit. In the policy json find resource attribute. Add another line under resource to grant permission to store flow logs from another account you wish to.\ne.g., \u0026quot;arn:aws:s3:::wa-lab-vpc-flow-logs/vpc-flow-logs/AWSLogs/\u0026lt;New account number\u0026gt;/*\u0026quot; Note: Above is an example only. Please change it according to your bucket name, prefix and \u0026lt;New account number\u0026gt; with actual target account number. If Resource attribute of the policy is not an array, then you need to add any additional account in array format (as a comma separated list surrounded in square brackets)\nClick on Save.\nLog out from central account.\nRepeat steps 1 thru 9 from section “Enable VPC Flow Logs” to enable logs in new account for desired VPC.\nLog out from the additional account once you successfully enable flow logs for another VPC.\nLog in to the central account.\nManually add first partition to the external table for the vpc in the new account:\nFollow below instructions (1 thru 4) to make necessary changes in the code:\nALTER TABLE vpc_flow_logs_custom_integration ADD PARTITION (`aws-account-id`='\u0026lt;your new account number\u0026gt;', `aws-service`='vpcflowlogs', `aws-region`='\u0026lt;your region\u0026gt;', year='yyyy', month='mm', day='dd') LOCATION 's3://\u0026lt;VPC-Flow-Logs-Bucket-Name\u0026gt;/\u0026lt;VPC-Flow-Logs-Prefix\u0026gt;/AWSLogs/\u0026lt;your new account number\u0026gt;/vpcflowlogs/\u0026lt;your region\u0026gt;/yyyy/mm/dd'; Replace \u0026lt;your new account number\u0026gt; with your new account number at 2 places\nReplace the yyyy,mm,dd with date for the log file at 2 places. Look into S3 bucket for files created under specific date\nNote: Navigate to S3 service and click on S3 bucket you have created to store VPC Flow Logs, to see the date and region information as shown in example image below. If you do not see any content then you may need to wait until log records are written to the bucket based on 1- or 10-minutes granularity\nReplace \u0026lt;your region\u0026gt; with respective region in S3 bucket for vpc flow logs at 2 places\nIn LOCATION Replace complete S3 url with appropriate path where your VPC logs are stored\n\u0026lt;VPC-Flow-Logs-Bucket-Name\u0026gt; with bucket name where logs are stored \u0026lt;VPC-Flow-Logs-Prefix\u0026gt; with Flow Logs prefix you have used while enabling logs. If you have not provided any prefix at the time of enabling it you can remove it from above path. Example below:\nALTER TABLE vpc_flow_logs_custom_integration ADD PARTITION (`aws-account-id`='0123456789', `aws-service`='vpcflowlogs', `aws-region`='us-east-1', year='2021', month='10', day='27') LOCATION 's3://my-central-vpc-flow-logs/vpc-flow-logs/AWSLogs/0123456789/vpcflowlogs/us-east-1/2021/10/27'; X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/1_deploy_infra/","title":"Deploy the Infrastructure and Application","tags":[],"description":"","content":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a data store on Amazon Relational Database Service (RDS).\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. You will need the AWS credentials, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, of this IAM user for later use in this lab. If you do not have this IAM user\u0026rsquo;s credentials or you wish to create a new IAM user with needed permissions, follow the instructions here to create them Decide which deployment option you will use for this lab. It can be run as single region or multi region (two region) deployment. If you are attending an in-person workshop, use single region\nIn later steps choose the appropriate instructions for the deployment option you you have decided upon.\nsingle region is faster to get up and running multi region enables you to test some additional aspects of cross-regional resilience. If you are attending an in-person workshop, then please continue to Step 2 now.\n1.2 Checking for existing service-linked roles If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nSkip this step and go directly to step 2. Configure Execution Environment .\nIf you are using your own AWS account:\nFollow these steps , and then return here and resume with the following instructions.\n1.3 Deploy infrastructure and run the service If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nSkip this step and go directly to step 2. Configure Execution Environment .\nIf you are using your own AWS account\nClick here for instructions on creating the deployment machine Here you will build a state machine using AWS Step Functions and AWS Lambda that orchestrates the deployment of the multi-tier infrastructure. This is not the service infrastructure itself, but meta-infrastructure we use to build the actual infrastructure.\nLearn more: After the lab see this blog post on how AWS Step Functions and AWS CodePipelines can work together to deploy your infrastructure\nGet the CloudFormation template: Download the appropriate file (You can right-click then choose download; or you can right click and copy the link to use with wget)\nsingle region: download CloudFormation template here multi region: download CloudFormation template here Ensure you have selected the Ohio region. This region is also known as us-east-2, which you will see referenced throughout this lab. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack and select With new resources from the drop-down menu Leave \u0026ldquo;Prepare template\u0026rdquo; setting as-is\n1 - For \u0026ldquo;Template source\u0026rdquo; select \u0026ldquo;Upload a template file\u0026rdquo; 2 - Specify the CloudFormation template you downloaded Click the “Next” button. For \u0026ldquo;Stack name\u0026rdquo; enter: DeployResiliencyWorkshop\nOn the same screen, for \u0026ldquo;Parameters\u0026rdquo; enter the appropriate values:\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor: Leave all the parameters at their default values If you are using your own AWS account: Set the first three parameters using these instructions and leave all other parameters at their default values. You optionally may review the default values of this CloudFormation template here Click the “Next” button.\nOn the \u0026ldquo;Configure stack options\u0026rdquo; page, click “Next” again On the \u0026ldquo;Review DeployResiliencyWorkshop\u0026rdquo; page, scroll to the bottom and tick the checkbox “I acknowledge that AWS CloudFormation might create IAM resources with custome names.” Click the “Create stack” button. This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nThis will take approximately a minute to deploy. When it shows status CREATE_COMPLETE, then the state machine will start deploying the infrastructure and service.\nOnce the \u0026ldquo;deployment machine\u0026rdquo; starts deploying the infrastructure and service it will take approximately the following times to deploy:\nTime until you can start\u0026hellip; Single region Multi region EC2 failure injection test 15-20 min 15-20 min RDS and AZ failure injection tests 20-25 min 40-45 min Multi-region failure injection tests NA 50-55 min Total deployment time 20-25 min 50-55 min To save time, you can move on to Step 2 now while the application is deploying.\n1.4 Monitoring progress of the deployment Click here for instructions on monitoring the progress of the deployment Go to the AWS Step Function console at https://console.aws.amazon.com/states On the Step Functions dashboard, you will see “State Machines”. Click on the one named “DeploymentMachine-random characters.” This will bring up an execution console\nClick on the state machine execution under Executions\nYou can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen.\nYou can also watch the CloudFormation stacks as they are created and transition from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can start the first test (EC2 failure injection testing) when the web tier has been deployed in the Ohio region. Look for the WaitForWebApp step (for single region) or WaitForWebApp1 step (for multi region) to have completed successfully. This will look something like this on the visual workflow.\nAbove screen shot is for single region. for multi region see this diagram instead 1.5 View website deployed as part of this test application Later when the deployment is complete, you will be able to view the website that you deployed The steps to view the website are in Step 3.3 View the website used for the test application for this lab X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available Level 100: Automating Serverless Best Practices with Dashbird "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_manage_workload_risks_with_opscenter/1_deploy_infrastructure/","title":"Deploy infrastructure","tags":[],"description":"","content":"The AWS WA Tool API provides programmatic access to the AWS WA Tool and can be used to manage workloads, retrieve risk information and improvement plans. AWS WA Tool API calls are made from a Lambda function that is invoked periodically using Amazon EventBridge . The API calls retrieve workload details such as number of High Risk Issues (HRIs) and Medium Risk Issues (MRIs) , best practices missing, and improvement plans. Using this information, the Lambda function creates OpsItems within OpsCenter for best practices missing from all workloads in the AWS Region the solution is deployed in. An Amazon DynamoDB table is used to maintain state and ensure duplicate OpsItems are not being created for the same missing best practice within a workload. Setting the status of an OpsItem to Resolved will trigger a notification to an Amazon Simple Notification Service (SNS) topic. SNS invokes a second Lambda function which updates the workload on the AWS WA Tool with the best practice specified in the OpsItem that was resolved. This second function then updates the workload state in DynamoDB.\nYou will use AWS CloudFormation to deploy some of the infrastructure for this lab. The CloudFormation stack that you provision will create the following resources:\nAn AWS Identity and Access Management (IAM) role An SNS Topic An SNS Topic policy A DynamoDB table 1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the workload using AWS CloudFormation Download the risk_management.yaml CloudFormation template\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources (standard)\nFor Prepare template select Template is ready\nFor Template source select Upload a template file Click Choose file and select the CloudFormation template you downloaded in the previous step: risk_management.yaml Click Next\nFor Stack name use WA-risk-management and click Next\nFor Configure stack options click Next\nOn the Review page:\nScroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here . Note: The template creates an IAM role for the Lambda function. These are the minimum permissions necessary for the function to make API calls to AWS services such as DynamoDB, Systems Manager, and the Well-Architected Tool. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - LambdaRole.\nClick Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows (in reverse order) the activities performed by CloudFormation, such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. The stack takes about 2 mins to create all the resources. Periodically refresh the page until you see that the Stack Status is in CREATE_COMPLETE.\nOnce the stack is in CREATE_COMPLETE, visit the Outputs section for the stack and note down the Key and Value for each of the outputs. This information will be used in the lab.\n1.3 Define and document workload state To observe the behavior of this solution, you need one or more workloads defined and documented in the AWS Well-Architected Tool. Refer to the Walkthrough of the Well-Architected Tool to learn how to do this.\nNOTE: Workloads must be defined and documented in the same AWS Region where you are running this lab.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/1_create_stack/","title":"Create Stack","tags":[],"description":"","content":"Creating this CloudFormation stack will configure CloudTrail including a new trail, an S3 bucket, and a CloudWatch Logs group for CloudTrail logs. You can optionally configure AWS Config and Amazon GuardDuty by setting the CloudFormation parameter for each.\nDownload the latest version of the CloudFormation template here: cloudtrail-config-guardduty-securityhub.yaml Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: cloudtrail-config-guardduty-securityhub.yaml Click Next\nFor Stack name use DetectiveControls\nParameters\nLook over the Parameters and their default values.\nUnder General section only enable the service if you have not configured already. CloudTrail is enabled by default, if you have enabled already this will create another trail and S3 bucket.\nCloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to.\nIMPORTANT: Bucket names need to be unique across all AWS buckets, and only contain lowercase letters, numbers, and hyphens.\nConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to.\nGuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nYou have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account!\nYou should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS it will be JSON format.\n"},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/100_labs/100_automating_serverless_best_practices_with_dashbird/1_deploy_blue_car_application/","title":"Deploy The Blue Car Application","tags":[],"description":"","content":"You will start by deploying the example application which allows a customer to order medical assistance based on selecting a map location. The application consists of a public AWS API gateway which connects to a serverless application layer AWS Lambda , which uses Amazon DynamoDB .\nYou will also deploy AWS Amplify to host the static website with CI/CD build-in and Amazon Cognito to manage users.\nOur deployed architecture should reflect the following diagram:\nNote the following:\nAWS Amplify hosts static web resources including HTML, CSS, JavaScript, and image files which are loaded in the user\u0026rsquo;s browser.\nAmazon Cognito provides user management and authentication functions to secure the backend API.\nAmazon DynamoDB provides a persistence layer where data can be stored by the API\u0026rsquo;s Lambda function.\nJavaScript executed in the browser sends and receives data from a public backend API built using Lambda and API Gateway.\nTo deploy the template for the base infrastructure complete the following steps:\n1.1. Application Deployment using Cloud9 as the IDE. For our application deployment, we will use AWS Cloud9 as our IDE, where our prebuilt script will be automatically cloned.\nYou can get the CloudFormation template here. Console: If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. Sign in to the AWS Management Console and open the CloudFormation console at https://console.aws.amazon.com/cloudformation . Select the stack template which you downloaded earlier, and create a stack: For the stack name use cloud9-stack and click the Next button. Click Next button. Scroll down to the bottom of the stack creation page and acknowledge the IAM resources creation by selecting all the check boxes. Then launch the stack. It may take 4-5 minutes to complete this deployment. Click cloud9-stack and go to the Outputs section of the CloudFormation stack. Then, click Cloud9URL to set up your IDE environment. 1.2. Deploy The Blue Car Application. Launch Cloud9 from the AWS Console. The repository where all of the CloudFormation templates are stored will be automatically cloned. Go to aws-well-architected-labs/static/wapartners/100_Automating_Serverless_Best_Practices_with_Dashbird/Code/oncall-health-sample-app directory path and run bash build_script.sh. It may take 20-25 minutes to complete this deployment. cd aws-well-architected-labs/static/wapartners/100_Automating_Serverless_Best_Practices_with_Dashbird/Code/oncall-health-sample-app bash build_script.sh In the CloudFormation console, you will see a new stack called oncall-health-amplify. Click oncall-health-amplify and go to the Outputs section of the CloudFormation stack. Then, click AppURL to access the application. You should see the Blue Car application landing page as show below, then click START HERE. Click Create account to create a new user. Provide a Username, Password, Email, and Phone number, then click CREATE ACCOUNT Retrieve the Confirmation code from your email which you entered previously and click CONFIRM. This step is required because only verified user can call blue car in the application. You should now be able to log into Blue Car Application. The application will default to a map of Melbourne as shown. Click a location on the map and click the Set Pickup button to request a blue ambulance car. END OF SECTION 1\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/1_deploy_the_lab_base_infrastructure/","title":"Deploy the lab base infrastructure","tags":[],"description":"","content":"In this section we will build out our base lab infrastructure. This will consist of a public API gateway which connects to Lambda (application layer). The application layer will connect to RDS for MySQL (database layer) within a Virtual Private Cloud (VPC) . The environment will be deployed to separate private subnets which will allow for segregation of application and network traffic across multiple Availability Zones . We will also deploy an Internet Gateway and NAT gateway along with appropriate routes from both public and private subnets.\nWhen we successfully complete our initial stage template deployment, our deployed workload should reflect the following diagram:\nNote the following:\nThe API Gateway has been provided with a role to allow access to invoke the Lambda function in the private subnet (application layer).\nThe Lambda function has been provided with a role to allow the API Gateway to invoke the Lambda function.\nSecrets Manager has been configured as the master password store which the Lambda function will retrieve to provide access to RDS. This will allow Secrets Manager to be used to encrypt, store and transparently decrypt the password when required.\nThe Security Group associated with Amazon RDS for MySQL will only allow inbound traffic on port 3306 from the specific security group associated with Lambda. This will allow sufficient access for Lambda to connect to Amazon RDS for MySQL.\nNote: For simplicity, we have used North Virginia \u0026lsquo;us-east-1\u0026rsquo; as the default region for this lab. Please ensure all lab interaction is completed from this region.\nTo deploy the template for the base infrastructure complete the following steps:\n1.1. Get the Cloudformation Template. To deploy the second CloudFormation template, you can deploy directly via the console. You can get the template here. Click here for CloudFormation console deployment steps Console: If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. Sign in to the AWS Management Console as an IAM user and open the S3 console at https://console.aws.amazon.com/s3 as shown: Create a bucket with a unique name and select us-east-1(N.Virginia) as the region as shown: Take note of your S3 Bucket name, which we will need later in the lab when we create a Cloudformation stack: Download our 3 Lambda deployment packages: rds-create-table.zip rds-query.zip python-requests-lambda-layer.zip These packages will be used to build the lambda environment later in the lab.\nWhen you have downloaded all packages, upload all 3 to your new bucket with the sames filesnames:\nOpen the CloudFormation console at https://console.aws.amazon.com/cloudformation and select us-east-1(N.Virginia) as your AWS Region: Select the stack template which you downloaded earlier, and create a stack: For the stack name use \u0026lsquo;walab-api\u0026rsquo; and enter the bucket name you just created in the parameters. You can leave all other parameters as default value. When you are ready, click the \u0026lsquo;Next\u0026rsquo; button. Scroll down to the bottom of the stack creation page and acknowledge the IAM resources creation by selecting all the check boxes. Then launch the stack. It may take 9~10 minutes to complete this deployment. Go to the Outputs section of the cloudformation stack, click Cloud9URL to set up your test environment. Run cd walab-scripts to ensure Cloud9 automatically cloned all scripts. Run bash install_package.sh\nthis script will install boto3 and requests 1.2. Confirm Successful Application Deployment. Go to the Outputs section of the cloudformation stack you just deployed and copy APIGatewayURL to make sure if the lab base infrastruture has been successfully deployed. Take a note of APIGatewayURL as we will often use this URL for testing.\nIn Cloud9, execute the script called sendRequest.py with the argument of your APIGatewayURL. python sendRequest.py \u0026#39;APIGatewayURL\u0026#39; Once your command runs successfully, you should be seeing Response code 200 with Response data as shown here:\nEND OF SECTION 1\n"},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/100_labs/100_automating_serverless_best_practices_with_dashbird/","title":"Level 100: Automating Serverless Best Practices with Dashbird","tags":[],"description":"","content":"Authors Stephen Salim, Well-Architected Geo Solutions Architect. Jang Whan Han, Well-Architected Geo Solutions Architect. Introduction Serverless architecture refers to a software design pattern where applications are broken up into individual functions. These functions are specfic in use and can be scaled individually according to architectural requirements. Serverless architecture allows developers to spend less time provisioning, scaling and managing infrastructure, freeing up time to develop value-added business logic. Customers can also benefit from cost efficiency in their architectures as they only pay for what they use.\nAt large scale, serverless architecture experiences exponential growth in the amount of data which must be monitored (logs, metrics, configurations etc). To efficiently utilize serverless architecture at this scale, Observerability is highly important to gather insights and discover performance and cost optimization opportunities.\nDashbird continuously runs multiple best practice checks against their customers serverless workloads, which provides actionable advice on how to improve their applications in alignment with Well-Architected best practices.\nIn this lab, you will create a simple serverless web application using the following AWS services:\nAWS Amplify - Used for Static Web Hosting. AWS Cognito - Used for authentication functions to secure the backend API. Amazon API Gateway - Used for securing REST API. AWS Lambda - Used to run code without provisioning. Amazon DynamoDB - Used for a fully managed NoSQL database. Our lab is divided into several sections as follows:\nDeploy Blue Car application Dashbird Well-Architected Insights Modern Load Test Tear down We have included a single script to assist you with the application deployment. Once the application is deployed and your account is connected to the Dashbird platform, insights will be discovered based on Well-Architected best practice. You can then perform a modern load test which will trigger further insights and demonstrate the value of Observability.\nDesign Principles Enable traceability Test systems at production scale Stop guessing capacity Stop spending money on undifferentiated heavy-lifting Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nSteps: Deploy The Blue Car Application Well-Architected Insights Modern Load Test Tear down "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/","title":"Level 300: Multilayered API Security with Cognito and WAF","tags":[],"description":"","content":"Authors Jang Whan Han, Well-Architected Geo Solutions Architect. Tim Robinson, Well-Architected Geo Solutions Architect. Introduction APIs are used for integration between applications and assist our customers in delivering new digital businesses as public APIs in partner ecosystems. Due to the public nature of these APIs, security is a top concern for all organizations who seek to develop APIs to augment their existing business models. Although API security now benefits from increased awareness and product feature coverage, application leaders must create and implement an effective API security strategy which aligns with their business needs. An example of an effective approach to secure an API is to adopt a Zero Trust strategy which ensures only authorized requests are permitted to access the business layer of your application. Additionally, evaluating trust at multiple layers of the architecture allows multiple checks to be performed as the API data transits through the workload.\nThrough the use of AWS Cognito , it is possible to create user pools which work with your API to obtain an identity access token for the user, which can then be used to enforce authorization controls in your API layer. However, not only can legitimate users potentially expose your organization to high risk, but also attacks can come with valid credential or token. To mitigate this risk, AWS Cognito enables you to configure how long your access token will be valid and the integration of Amazon WAF in conjunction with CloudFront will allow you to add another layer of API security to achieve a strong level of protection.\nIn this lab we will walk you through an example scenario of securing your API at multiple layers. We will gradually tighten the security at each layer, using the following services:\nAmazon API Gateway - Used for securing REST API. AWS Secrets Manager - Used to securely store secrets. Amazon CloudFront - Used to prevent direct access to API as well as to enforce encrypted end-to-end connections to origin. AWS WAF - Used to protect our API by filtering, monitoring, and blocking malicious traffic. Amazon Cognito - Used to enable access control for API Our lab is divided into several sections as follows:\nDeploy the lab base infrastructure. Use secrets securely. Prevent requests from accessing API directly. Application layer defense. Contol access to API. We have included CloudFormation templates for the first few steps to get your started and build out the base lab infrstructure. For the remainder of the lab we will use further templates what will deploy addtional services such as CloudFront, WAF and Cognito to further enhance the security of the workload. The remainder of the lab will then focus on the configuration of these services to create an example API environment which is secured at multiple layers.\nNote: For simplicity, we have used North Virginia \u0026lsquo;us-east-1\u0026rsquo; as the default region for this lab. Please ensure all lab interaction is completed from this region.\nGoals Store and use secrets securely Control traffic at all layers Enforce encryption in transit Reduce attack surface Controlling and managing access to your API Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nSteps: Deploy the lab base infrastructure Use Secrets Securely Prevent requests from accessing API directly Application layer defence Control access to API Tear down "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_8_tag_policies/1_tag_policy/","title":"Create a Tag Policy ","tags":[],"description":"","content":"Tag policies are a type of policy that can help you standardize tags across resources in your Organization\u0026rsquo;s accounts. In a tag policy, you specify tagging rules applicable to resources when they are tagged.\nFor example, a tag policy can specify that when the CostCenter tag is attached to a resource, it must use the case treatment and tag values that the tag policy defines, in order to be considered compliant. A tag policy can also prevent noncompliant tagging operations on specified resources.\nUsing tag policies involves working with AWS Organizations and AWS Resource Groups:\nAWS Organizations - When signed in to the organization\u0026rsquo;s master account, you use Organizations to enable the tag policies feature. You must sign in as an IAM user, assume an IAM role, or sign in as the root user (not recommended) in the organization\u0026rsquo;s master account. Then you can create tag policies and attach them to the organization entities to put those tagging rules in effect.\nAWS Resource Groups - When signed in to an account in your organization, you use Resource Groups to find noncompliant tags on resources in the account. You can correct noncompliant tags in the AWS service where you created the resource.\nThere is a optional python script method to setup basic tag polices and SCPs which can be found here. Create a Tag Policy We will create a policy containing two tagging rules. Both tag rules, environment and business unit, will require specific values and syntax in order to be compliant.\nNavigate to the AWS Organizations service using the navigation bar and select Policies on the left-hand side and click \u0026ldquo;Tag policies\u0026rdquo; Within the Tag policies page select “Create Policy”. (If this is your first-time using tag policies, you may need to “Enable tag policies” prior to clicking create policy). On the Create policy page, under both the Policy name and Policy description enter \u0026ldquo;cost_allocation\u0026rdquo; In the New tag key 1 section, under Tag key, type \u0026ldquo;environment\u0026rdquo;. Under Tag value compliance click the box “Specify allowed values for this tag key, the click “Specify values” Add the following values (each separately): prod, dev, uat, test and click Save Changes Click “Add tag key” to create another tag key. Follow steps 4 – 7 and name this tag key “business_unit”, and add the following values: marketing, research, sales, operations and click Create Policy X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/exportimport_utility/1_configure_env/","title":"Configure Environment","tags":[],"description":"","content":"You must have the AWS SDK for Python (Boto3) installed to run this script. Here is more information about installing and configuring the SDK. Check Python SDK Version You must verify that you are running at least v1.16.38 of the AWS SDK for Python to have all of the components necessary to use the Well-Architected API.\nHow to verify version $ pip3 show boto3 Name: boto3 Version: 1.17.27 Summary: The AWS SDK for Python Home-page: https://github.com/boto/boto3 Author: Amazon Web Services Author-email: None License: Apache License 2.0 Location: /usr/local/lib/python3.9/site-packages Requires: botocore, jmespath, s3transfer If the version number is less than 1.16.38, then you can upgrade boto3 via pip:\npip3 install boto3 --upgrade --user X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/disaster/ec2/","title":"EC2","tags":[],"description":"","content":"Launch an EC2 instance from AMI (Amazon Machine Image) 1.1 Click EC2 to navigate to the dashboard in the N. California (us-west-1) region.\n1.2 Click the AMIs link.\n1.3 Verify the BackupAndRestoreImage has a status of Available.\n1.4 Select BackupAndRestoreImage. Click the Launch button.\n1.5 Select the t2.micro instance type, then click the Next: Configure Instance Details button.\n1.6 Select BackupAndRestore-S3InstanceProfile-xxxx as the IAM Role, then click the Next: Add Storage button.\n1.7 Click the Next: Add tags button.\n1.8 Add Name as the Key and BackupAndRestoreSecondary as the Value, then click the Next: Configure Security Group button.\n1.9 Select Create a new security group and enter backupandrestore-us-west-ec2-SG as the Security group name and Description. Add the rules as shown by clicking the Add Rule button. (Click on image to enlarge) Click the Review and Launch button.\n1.10 Click the Launch button.\n1.11 Select Proceed without a key pair, enable the I acknowledge that without a key pair,\u0026hellip; checkbox, then click the Launch Instances button.\nCopy the Public IPv4 DNS of the instance. You will need this in a later step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/backup-resources/rds/","title":"RDS","tags":[],"description":"","content":"Backup the RDS database 1.1 Click AWS Backup to navigate to the dashboard in the N. Virginia (us-east-1) region.\n1.2 Click the Protected resources link, then click the Create an on-demand backup button.\n1.3 Select RDS as the Resource type, then select unishopappv1dbbackupandrestore as the Database name. Click the Create on-demand backup button.\nIf you see this error, please REPEAT Steps 1.1 through 1.3 above making sure you start from Step 1.1.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/copy-to-secondary/rds/","title":"RDS","tags":[],"description":"","content":"Copy RDS Backup 1.1 Click AWS Backup to navigate to the dashboard in the N. Virginia (us-east-1) region.\n1.2 Click the Backup Vaults link, then click the Default link.\nIf you are using your own AWS account you may want to create a non-default vault for this workshop. this will prevent commingling of workshop backups with other backups in the default vault. Instructions can be found in the service documentation .\n1.3 In the Backups section. Select the backup. Click Copy under the Actions dropdown.\nIf you don\u0026rsquo;t see your backup, check the status of the Backup Job. Click the Jobs link, then click the Backup jobs link. Verify the Status of your backup is Completed.\n1.4 Select US West (N. California) as the Copy to destination, then click the Copy button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/prerequisites/account-setup/","title":"Account Setup","tags":[],"description":"","content":"Using your own AWS account If you are using a personal AWS account, be aware that you will incur costs for the resources deployed in this workshop. After completing the workshop, remember to complete the Cleanup Resources section to remove any unnecessary AWS resources.\nThis workshop takes about 90 minutes to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/prerequisites/account-setup/","title":"Account Setup","tags":[],"description":"","content":"Using your own AWS account If you are using a personal AWS account, be aware that you will incur costs for the resources deployed in this workshop. After completing the workshop, remember to complete the Cleanup Resources section to remove any unnecessary AWS resources.\nThis workshop takes about 60 minutes to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/prerequisites/account-setup/","title":"Account Setup","tags":[],"description":"","content":"Using your own AWS account If you are using a personal AWS account, be aware that you will incur costs for the resources deployed in this workshop. After completing the workshop, remember to complete the Cleanup Resources section to remove any unnecessary AWS resources.\nThis workshop takes about 60 minutes to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/prerequisites/account-setup/","title":"Account Setup","tags":[],"description":"","content":"Using your own AWS account If you are using a personal AWS account, be aware that you will incur costs for the resources deployed in this workshop. After completing the workshop, remember to complete the Cleanup Resources section to remove any unnecessary AWS resources.\nThis workshop takes about 60 minutes to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/prerequisites/","title":"Pre-requisites","tags":[],"description":"","content":"Account setup Using an account provided by instructor at virtual or in-person workshop If you are running this workshop as part of an instructor led workshop, please log into the console using this link and enter the hash provided to you as part of the workshop.\nAll resources for this workshop have been pre-provisioned.\nContinue to the Create Backup Resources section of the workshop.\nUsing your own AWS account Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.\nContinue to the Account Setup section of the workshop.\n"},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/prerequisites/","title":"Pre-requisites","tags":[],"description":"","content":"Account setup Using an account provided by instructor at virtual or in-person workshop If you are running this workshop as part of an instructor led workshop, please log into the console using this link and enter the hash provided to you as part of the workshop.\nAll resources for this workshop have been pre-provisioned.\nContinue to the Verify Website section of the workshop.\nUsing your own AWS account If you are running this workshop in your own account, click Next Step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/prerequisites/","title":"Pre-requisites","tags":[],"description":"","content":"Account setup Using an account provided by instructor at virtual or in-person workshop If you are running this workshop as part of an instructor led workshop, please log into the console using this link and enter the hash provided to you as part of the workshop.\nAll resources for this workshop have been pre-provisioned.\nContinue to the Verify Aurora Write Forwarding section of the workshop.\nUsing your own AWS account If you are running this workshop in your own account, click Next Step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/prerequisites/","title":"Pre-requisites","tags":[],"description":"","content":"Account setup Using an account provided by instructor at virtual or in-person workshop If you are running this workshop as part of an instructor led workshop, please log into the console using this link and enter the hash provided to you as part of the workshop.\nAll resources for this workshop have been pre-provisioned.\nContinue to the CloudFormation Outputs section of the workshop.\nUsing your own AWS account If you are running this workshop in your own account, click Next Step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/prerequisites/account-setup/s3-access/","title":"S3 Access","tags":[],"description":"","content":"Allow Amazon S3 Public Access Our application employs AWS Simple Storage Service (S3) Static website hosting. To make the application available to Internet users, we must disable the AWS account policy that blocks public access.\n1.1 Click S3 to navigate to dashboard.\n1.2 Click the Block Public Access settings for this account link.\n1.3 If you see that Block all public access is On, then click the Edit button.\n1.4 Disable the Block Public Access settings for this account checkbox including children. Click the Save Changes button.\n1.5 Enter confirm and click the Confirm button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/prerequisites/account-setup/s3-access/","title":"S3 Access","tags":[],"description":"","content":"Allow Amazon S3 Public Access Our application employs AWS Simple Storage Service (S3) Static website hosting. To make the application available to Internet users, we must disable the AWS account policy that blocks public access.\n1.1 Navigate to S3 .\n1.2 Click Block Public Access settings for this account.\n1.3 If you see that \u0026ldquo;Block all public access\u0026rdquo; is \u0026ldquo;On,\u0026rdquo; then click on the \u0026ldquo;Edit\u0026rdquo; button to get to the next screen.\n1.4 Uncheck \u0026ldquo;Block all public access,\u0026rdquo; including any child selections. Click the \u0026ldquo;Save Changes\u0026rdquo; button. You will be required to confirm the changes.\n1.5 Enter confirm and Click the Confirm button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/prerequisites/account-setup/s3-access/","title":"S3 Access","tags":[],"description":"","content":"Allow Amazon S3 Public Access Our application employs AWS Simple Storage Service (S3) Static website hosting. To make the application available to Internet users, we must disable the AWS account policy that blocks public access.\n1.1 Navigate to S3 .\n1.2 Click Block Public Access settings for this account.\n1.3 If you see that \u0026ldquo;Block all public access\u0026rdquo; is \u0026ldquo;On,\u0026rdquo; then click on the \u0026ldquo;Edit\u0026rdquo; button to get to the next screen.\n1.4 Uncheck \u0026ldquo;Block all public access,\u0026rdquo; including any child selections. Click the \u0026ldquo;Save Changes\u0026rdquo; button. You will be required to confirm the changes.\n1.5 Enter confirm and Click the Confirm button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/prerequisites/account-setup/s3-access/","title":"S3 Access","tags":[],"description":"","content":"Allow Amazon S3 Public Access Our application employs AWS Simple Storage Service (S3) Static website hosting. To make the application available to Internet users, we must disable the AWS account policy that blocks public access.\n1.1 Click S3 to navigate to the dashboard.\n1.2 Click Block Public Access settings for this account.\n1.3 If you see that \u0026ldquo;Block all public access\u0026rdquo; is \u0026ldquo;On,\u0026rdquo; then click on the \u0026ldquo;Edit\u0026rdquo; button to get to the next screen.\n1.4 Uncheck \u0026ldquo;Block all public access,\u0026rdquo; including any child selections. Click the \u0026ldquo;Save Changes\u0026rdquo; button. You will be required to confirm the changes.\n1.5 Enter confirm and click the Confirm button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/1_protect_privileged_credentials/","title":"Step 1 - Protect privileged credentials","tags":[],"description":"","content":"In this exercise we will use AWS Identity \u0026amp; Access Management (IAM) in the AWS Management Console to configure and enable a virtual multi factor authentication (MFA) device for the root. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials.\nUse your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ On the right side of the navigation bar, click your account name, and click My Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page.\n\\\nClick Activate MFA.\nIn the wizard, click virtual MFA device and then click Continue.\nOn the 'set up virtual MFA device window' click Show QR code. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device).\nThe easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually.\nTo use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. The device starts generating six-digit numbers.\nIn the Manage MFA Device wizard, in the MFA Code 1 box, type the six-digit number that\u0026rsquo;s currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box.\nImportant: Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device.\nClick Assign MFA, and then click Finish. Note the 'success' confirmation and click Close.\nFor more information please read the AWS User Guide:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/export_to_xlsx/1_configure_env/","title":"Configure Environment","tags":[],"description":"","content":"You must have the AWS SDK for Python (Boto3) installed to run this script. Here is more information about installing and configuring the SDK. Check Python SDK Version You must verify that you are running at least v1.16.38 of the AWS SDK for Python to have all of the components necessary to use the Well-Architected API.\nHow to verify version $ pip3 show boto3 Name: boto3 Version: 1.17.27 Summary: The AWS SDK for Python Home-page: https://github.com/boto/boto3 Author: Amazon Web Services Author-email: None License: Apache License 2.0 Location: /usr/local/lib/python3.9/site-packages Requires: botocore, jmespath, s3transfer If the version number is less than 1.16.38, then you can upgrade boto3 via pip:\npip3 install boto3 --upgrade --user Other Python modules required You may need to install the following modules for this script to run:\nBeautifulSoup4 pip install beautifulsoup4 xlsxwriter pip install XlsxWriter X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/copy_wa_review_between_accounts/1_configure_env/","title":"Configure Environment","tags":[],"description":"","content":"You must have the AWS SDK for Python (Boto3) installed to run this script. Here is more information about installing and configuring the SDK. Check Python SDK Version You must verify that you are running at least v1.16.38 of the AWS SDK for Python to have all of the components necessary to use the Well-Architected API.\nHow to verify version $ pip3 show boto3 Name: boto3 Version: 1.17.27 Summary: The AWS SDK for Python Home-page: https://github.com/boto/boto3 Author: Amazon Web Services Author-email: None License: Apache License 2.0 Location: /usr/local/lib/python3.9/site-packages Requires: botocore, jmespath, s3transfer If the version number is less than 1.16.38, then you can upgrade boto3 via pip:\npip3 install boto3 --upgrade --user X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/generate_custom_html_wafr_report/1_configure_env/","title":"Configure Environment","tags":[],"description":"","content":"You must have the AWS SDK for Python (Boto3) installed to run this script. Here is more information about installing and configuring the SDK. Check Python SDK Version You must verify that you are running at least v1.16.38 of the AWS SDK for Python to have all of the components necessary to use the Well-Architected API.\nHow to verify version $ pip3 show boto3 Name: boto3 Version: 1.17.27 Summary: The AWS SDK for Python Home-page: https://github.com/boto/boto3 Author: Amazon Web Services Author-email: None License: Apache License 2.0 Location: /usr/local/lib/python3.9/site-packages Requires: botocore, jmespath, s3transfer If the version number is less than 1.16.38, then you can upgrade boto3 via pip:\npip3 install boto3 --upgrade --user Other Python modules You may need to install the following modules for this script to run:\nBeautifulSoup4 pip install beautifulsoup4 X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/copy_wa_review_between_accounts/","title":"Copy a workload from one account or region to another","tags":[],"description":"","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction The purpose of this lab is to teach you how to use the AWS SDK for Python (Boto3) to copy a Well-Architected review to a new location. The python application in this lab allows you to copy a workload between accounts, regions, or a combination of both.\nPrerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Python 3.9+ AWS SDK for Python (Boto3) installed Costs: There are no costs for copying or creating new WellArchitected Reviews Steps: Configure Environment Python Code Script usage examples X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/1_deploy_cfn/","title":"Deploying the infrastructure","tags":[],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and AWS Lambda functions. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nDeploy CloudFormation Template Download the DeployWACustomLambda.yaml CloudFormation template to your machine. For this lab, you will need to use us-east-2 Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: DeployWACustomLambda.yaml Click Next\nFor Stack name use WALambdaHelpers\nParameters\nLook over the Parameters and their default values.\nNone of these parameters need to be changed, but are available if you wish to try different settings\nCreateWALambdaFunctionName - The name for the new CreateWA lambda function.\nUpdateWAQLambdaFunctionName - The name for the new UpdateWA lambda function.\nLambdaFunctionsBucket – S3 Bucket name that holds the WAToolCFNAPILambda.zip file\nWAToolCFNAPIKey - The filename for the Lambda file, WAToolCFNAPILambda.zip Click Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources. I acknowledge that AWS CloudFormation might create IAM resources with custom names. I acknowledge that AWS CloudFormation might require the following capability: CAPABILITY_AUTO_EXPAND Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nThis template will take between 1-5 minutes to fully deploy.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/","title":"Level 300: Using custom resource in AWS CloudFormation to create and update Well-Architected Reviews","tags":[],"description":"","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction The purpose if this lab is to walk you through deploying a pair of AWS Lambda-backed custom resources for AWS CloudFormation so you can integrate the creation and modification of a Well-Architected workload as part of a CloudFormation deployment. One of the Lambda-backed custom resources is for workload creation and a second for selecting various best practices for questions in a Well-Architected pillar. Finally, you will deploy a sample Lambda/API Gateway application that also creates and updates the Well-Architected Tool.\nThe knowledge you acquire will help you learn how to programmatically access content in the Well-Architected tool in alignment with the AWS Well-Architected Framework. Goals: Learn how to deploy AWS Lambda-backed custom resources to be used for Well-Architected workload deployments Utilize the custom resources in a sample AWS Lambda/API Gateway application. Understand how tagging can be used for both the Well-Architected Tool as well as the sample application Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create new IAM roles, deploy Lambda function, and use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy) Costs The Lambda functions deployed in this lab should fit within the free tier usage assuming you haven\u0026rsquo;t already consumed it. The API Gateway deployed in this lab should fit within the free tier usage assuming you haven\u0026rsquo;t already consumed it. Time to complete The lab should take approximately 30 minutes to complete X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploying the infrastructure Exploring AWS Lambda code How to utilize AWS Lambda-backed custom resources in CloudFormation Deploying Sample Lambda application along with Well-Architected review Explore Well-Architected Review Teardown "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_building_custom_aws_well-architected_reports_with_amazon_athena_and_amazon_quicksight/1_etl_wa_workload_data/","title":"Extract workload data","tags":[],"description":"","content":"To extract the Well-Architected workload data, we\u0026rsquo;ll create an AWS Lambda function to query the Well-Architected API. AWS Lambda makes three calls to the Well-Architected Tool API to obtain workload, lens, and milestone information. It performs some minor transformation of the data to normalize the JSON structure and stores the data in an S3 bucket.\nCreate Lambda function Begin by navigating to the Lambda console and choosing the Create function Choose the option to Author from scratch and then fill in extract-war-reports as the function name, \u0026ldquo;Python 3.6\u0026rdquo; as the runtime. Under Permissions, select the option to \u0026ldquo;Create new role from template(s)\u0026rdquo;, assigning a role name of extract-war-reports_role. Then, choose Create function. Lambda will then create a new function along with a role for executing the function. Now, go ahead and paste the following code into the function editor. This code handles calls to obtain the workload data and storing in Amazon S3. Select Deploy to commit the code changes.\nimport boto3 import json import os import logging logger = logging.getLogger() logger.setLevel(logging.INFO) s3_bucket = os.environ[\u0026#39;S3_BUCKET\u0026#39;] s3_key = os.environ[\u0026#39;S3_KEY\u0026#39;] ################# # Boto3 Clients # ################# wa_client = boto3.client(\u0026#39;wellarchitected\u0026#39;) s3_client = boto3.client(\u0026#39;s3\u0026#39;) ############## # Parameters # ############## # The maximum number of results the API can return in a list workloads call. list_workloads_max_results_maximum = 50 # The maximum number of results the API can return in a list answers call. list_answers_max_results_maximum = 50 # The maximum number of results the API can return in a list milestones call. list_milestone_max_results_maximum = 50 def get_all_workloads(): # Get a list of all workloads list_workloads_result = wa_client.list_workloads(MaxResults=list_workloads_max_results_maximum) logger.info(f\u0026#39;Found {len(list_workloads_result)} Well-Archtected workloads.\u0026#39;) workloads_all = list_workloads_result[\u0026#39;WorkloadSummaries\u0026#39;] while \u0026#39;NextToken\u0026#39; in list_workloads_result: next_token = list_workloads_result[\u0026#39;NextToken\u0026#39;] list_workloads_result = wa_client.list_workloads( MaxResults=list_workloads_max_results_maximum, NextToken=next_token ) workloads_all += list_workloads_result[\u0026#39;WorkloadSummaries\u0026#39;] return (workloads_all) def get_milestones(workload_id): # # Get latest milestone review date milestones = wa_client.list_milestones( WorkloadId=workload_id, MaxResults=list_milestone_max_results_maximum )[\u0026#39;MilestoneSummaries\u0026#39;] # If workload has milestone get them. logger.info(f\u0026#39;Workload {workload_id} has {len(milestones)} milestones.\u0026#39;) if milestones: for milestone in milestones: milestone[\u0026#39;RecordedAt\u0026#39;] = milestone[\u0026#39;RecordedAt\u0026#39;].isoformat() return milestones def get_lens(workload_id): # Which lenses have been activated for this workload # print(workload_id) lens_reviews_result = wa_client.list_lens_reviews( WorkloadId=workload_id )[\u0026#39;LensReviewSummaries\u0026#39;] logger.info(f\u0026#39;Workload {workload_id} has used {len(lens_reviews_result)} lens\u0026#39;) return lens_reviews_result def get_lens_answers(workload_id, lens_reviews): # Loop through each activated lens # TODO - List Answers for each milestone list_answers_result = [] for lens in lens_reviews: lens_name = lens[\u0026#39;LensName\u0026#39;] logger.info(f\u0026#39;Looking at {lens_name} answers for Workload {workload_id}\u0026#39;) # Get All answers for the lens list_answers_reponse = wa_client.list_answers( WorkloadId=workload_id, LensAlias=lens[\u0026#39;LensAlias\u0026#39;], MaxResults=list_answers_max_results_maximum ) # Flatten the answer result to include LensAlias and Milestone Number for answer_result in list_answers_reponse[\u0026#39;AnswerSummaries\u0026#39;]: answer_result[\u0026#39;LensAlias\u0026#39;] = list_answers_reponse[\u0026#39;LensAlias\u0026#39;] # if \u0026#39;MilestoneNumber\u0026#39; in list_answers_reponse: # print(\u0026#34;MILSTONE_DETECTED\u0026#34;) # answer_result[\u0026#39;MilestoneNumber\u0026#39;] = list_answers_reponse[\u0026#39;MilestoneNumber\u0026#39;] # Append Answers from each lens. For reporting. list_answers_result.extend(list_answers_reponse[\u0026#39;AnswerSummaries\u0026#39;]) return list_answers_result def get_lens_summary(workload_id, lens_reviews): # Loop through each activated lens list_lens_review_summary = [] for lens in lens_reviews: lens_name = lens[\u0026#39;LensName\u0026#39;] logger.info(f\u0026#39;Looking at {lens_name} Summary for Workload {workload_id}\u0026#39;) list_lens_review_reponse = wa_client.get_lens_review( WorkloadId=workload_id, LensAlias=lens[\u0026#39;LensAlias\u0026#39;] ) list_lens_review_reponse[\u0026#39;LensReview\u0026#39;][\u0026#39;UpdatedAt\u0026#39;] = list_lens_review_reponse[\u0026#39;LensReview\u0026#39;][\u0026#39;UpdatedAt\u0026#39;].isoformat() list_lens_review_summary.append(list_lens_review_reponse[\u0026#39;LensReview\u0026#39;]) return list_lens_review_summary def lambda_handler(event, context): workloads_all = get_all_workloads() # Generate workload JSON file logger.info(f\u0026#39;Generate JSON object for each workload.\u0026#39;) # print (workloads_all) for workload in workloads_all: # Get workload info from WAR Tool API, workload_id = workload[\u0026#39;WorkloadId\u0026#39;] milestones = get_milestones(workload_id) lens_reviews = get_lens(workload_id) lens_summary_result = get_lens_summary(workload_id, lens_reviews) list_answers_result = get_lens_answers(workload_id, lens_reviews) # Build JSON of workload data workload_report_data = {} workload_report_data[\u0026#39;workload_id\u0026#39;] = workload[\u0026#39;WorkloadId\u0026#39;] workload_report_data[\u0026#39;workload_name\u0026#39;] = workload[\u0026#39;WorkloadName\u0026#39;] workload_report_data[\u0026#39;workload_owner\u0026#39;] = workload[\u0026#39;Owner\u0026#39;] workload_report_data[\u0026#39;workload_lastupdated\u0026#39;] = workload[\u0026#39;UpdatedAt\u0026#39;].isoformat() workload_report_data[\u0026#39;lens_summary\u0026#39;] = lens_summary_result workload_report_data[\u0026#39;milestones\u0026#39;] = milestones # workload_report_data[\u0026#39;report_answers\u0026#39;] = list_answers_result[\u0026#39;AnswerSummaries\u0026#39;] workload_report_data[\u0026#39;report_answers\u0026#39;] = list_answers_result logger.debug(workload_report_data) print(workload_report_data) # Write to S3 file_name = workload_id + \u0026#39;.json\u0026#39; logger.info(f\u0026#39;Writing JSON object to s3://{s3_bucket}/{s3_key}{file_name}.\u0026#39;) s3_client.put_object( Body=str(json.dumps(workload_report_data)), Bucket=s3_bucket, Key=f\u0026#39;{s3_key}{file_name}\u0026#39; ) Environment variables Let\u0026rsquo;s add some environment variables to pass environment-specific settings to the code.\nSelect Edit under Environment variables, and add the following environment variables and select Save:\nKey Value S3_BUCKET S3 bucket name, where you will store the extracted AWS Well-Architected data, e.g. \u0026ldquo;well-architected-reporting-blog\u0026rdquo; S3_KEY path in which you want the extracted Well-Architected workload data to be stored, e.g. \u0026ldquo;WorkloadReports/\u0026rdquo; Your environment variables should now look like this:\nTrigger configuration Let\u0026rsquo;s configure an Amazon Eventbridge (Amazon CloudWatch Events ) schedule to have AWS Lambda poll the Well-Architected Tool API to extract all shared workloads to the AWS WA Tool in your AWS management account. Expand the Designer drop-down list and then select EventBridge (CloudWatch Events) as a trigger. Fill in LambdaExtractWARReportsSchedule as the Rule name. Select Schedule expression, and fill in a suitable expression that meets your requirements, e.g. rate(1 hour) will configure the Lambda function once every hour.\nLambda timeout You should also increase the function timeout to 3 minute to ensure that the function always has time to finish reading all the defined workload data.\nIAM role configuration Finally, navigate to the IAM console and open role \u0026ldquo;extract-war-reports_role \u0026rdquo;. Attach policy \u0026ldquo;WellArchitectedConsoleReadOnlyAccess \u0026quot; in order to grant the permissions necessary for calling Well-Architected APIs. Also attach the following policy to the role, replacing \u0026lt;S3_BUCKET_NAME\u0026gt; with the name of your S3 bucket where you will store the extracted AWS Well-Architected data. This is the same bucket configured as an environment variable.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::\u0026lt;S3_BUCKET_NAME\u0026gt;*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/customizations/guides/1_addingtags/","title":"Adding Tags","tags":[],"description":"","content":"Last Updated March 2022\nIntroduction Now that you have your CID dashboards setup, you may want to view or group some of the visuals by tag. Common use cases include seeing spend by application, identifying opportunity by cost category, or building charge-back mechanisms to business units.\nIf you have cost allocation tags or cost categories setup in your AWS account, you will see those tags in your CUR. They show up as their own column. You can confirm that these tags are preset by going to Athena and click on the table that represent yours CUR file. If you scroll down, you should find your tags and cost categories.\nAdding Tags Follow the steps in this tutorial to add tags to any of your CID dashboards.\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/customizations/guides/2_addingcontrols/","title":"Filtering Visuals by Cost Allocation Tags","tags":[],"description":"","content":"Last Updated March 2022\nIntroduction Now that you\u0026rsquo;ve added your cost allocation tags, follow this video tutorial to learn how to add a control or filter across your entire dashboard so you can see everything grouped by tags.\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/customizations/guides/3_unitmetrics/","title":"SaaS Unit Metrics","tags":[],"description":"","content":"Last Updated March 2022\nIntroduction From time to time, customers will have concerns such as: ‘my AWS bills are going up and I don’t know whether that’s a good thing or a bad thing’. Did the bill go up because they are operating more workloads in the cloud, or is it because they are using AWS inefficiently? … Perhaps, a little bit of both. Regardless of the reason, the most often encountered interpretation is that more spend is bad. That isn’t always the case.\nWhen your AWS invoice is viewed without the appropriate context, it is hard to tell if an increase in spend is a result of delivering more value from your use of the Cloud, or if it is due to inefficient and wasteful resource consumption. Being able to tell the difference is important. You’re now likely asking: ‘well, how do I tell the difference?’ … Simply stated: with unit metrics.\nThis hands-on lab will guide you through the steps required to add your business data to a visualization that represents a “cost-per” unit metric. This will provide context to as to whether changes to your AWS Cloud architecture and operations are improving, maintaining, or eroding gross profit margins.\nSource Some Metrics For this examplem, we will use the number of API calls to our service on a daily basis. To acquire something that resembles real life, we will use these publically available statistics . We are going to use the day the API requests took place to map the data to the existing datasets for cost and usage in your dashboards. Make sure you have the date formatted as a date format in your spreadsheet.\nCreate a CSV file with this data. Go to QuickSight\u0026raquo;Datasets and click New dataset. Click on upload a file. Select the CSV you\u0026rsquo;ve created. Click next then click Edit/Preview data. Make sure that the date field in your dataset is of type date. Change it if it is not. Click Save \u0026amp; Publish and return to the list of your datasets in QuickSight.\nClick on summary_view and select Edit dataset.\nClick on Add data. Select from a dataset. Find your uploaded dataset and click Select. Click on the two pink dots next to your dataset. In the join clauses section below select the Usage_Date field on the left for Summary_View and the Date field from your uploaded dataset on the right. Click Apply. Then click Save \u0026amp; publish. Customize your Dashboard For this guide we will use the Cost Intelligence Dashboard deployed in the earlier part of this 200 lab.\nOpen the Analysis version of your dashboard so we can edit it. Start by adding a new tab on the far right side of the dashboard. Rename it to \u0026ldquo;Unit Metrics\u0026rdquo;. Lets start by creating a per API cost field. On the top right click + Add and then Add calculated field. Call it Cost Per API Call and add syntax to divide your Cost field by the new API Count field you imported. Click save. Let\u0026rsquo;s add a visual that shows us our new Cost per API call day over day. Click + Add and select Add visual. Drag over your new Cost Per API Call field into the new visuals. Let\u0026rsquo;s change this to a line graph that shows day-over-day trends. Click on the Line Chart visual type at the bottom left. Next, add the Usage Date field to your X axis. We now see our per API call unit cost day-over-day. Lets map the number of API calls on top of this to see the correlation. Tough to see it if our AWS spend is small. Let\u0026rsquo;s give the API count its own Y axis. Now we have a visual that shows us the correlation between API counts and the cost per API on that day. But it might be difficult to talk about a cost per API call if its less than $0.01 on average. So how do we adjust the multiplier so we can talk about cost per 10,000 API requests? We will add a control and a parameter in QuickSight to accomplish this. Click on parameters and click the plus sign to create a new parameter. Set it to an int, give it a name, and set the default to 10. On the next selection screen, pick Control. On the next screen, give the control a name (this will be seen in the dashboard), select Dropdown or List for Style, and put in some multipler options. I chose 1 through 1 million by orders of magnitude. Check \u0026ldquo;Hide select all option\u0026hellip;\u0026rdquo;. The control will appear at the top of your dashboard. Click on it, click the three dots, select Move to sheet. Position it at the top or wherever you like. Now we need to tie whatever someone selects here to the actual per API cost value. Create a new calculated field called Adjusted API Count and set it to be {API Count}/${APIcallmultiplier}. The APIcallmultiplier is the name of the parameter you just created. Click save. Next swap the API count field for the new Adjusted API Count calculated field. Finally, edit the Cost Per API Call calculated field we created in step 3 to be Cost/{Adjusted API Count}. Now when you select a multiplier from your drop down or list, the cost per API call amounts in the graph should change by that order of magnitude.\nLets add a few more visuals to get you familiar with what else you can do. Create a new visual, and in the Visual Types section choose KPI indicator.\nIn the Field Wells along the top of the dashboard put the usage_date in as the Trend group, click on the arrow next to it and select Aggregate and choose month. Next, put the Cost Per API call field into the Value box. And finally, to get rid of all those decimal places, select the Cost per API call field in the well, click on the down arrow next to it and select Show as: Currency.\nNow we can see how our cost per API call month-over-month changed from this month to the prior month. Finally, lets add a table where you can dig into the details and see cost per API call per service, per tag, per business unit, per account, per region, etc.\nCreate a new visual and set the visual type to Pivot Table. In the Values field well put Cost Per API Call set to Currency. In columns put usage_date set to aggregate monthly. In rows, put the dimensions you want to group on, for example tags, service, and operation.\nNote the little plus and minus signs next to the values in the columns and rows to the left. You can click on them to zoom in and see more granularity. For example, big a tag value in the first column and click plus, then click plus on the relevant service, then click plus again to see the operations. Now you should be able to see the cost per API per operation, grouped by tag and service. Next Steps Now that you know how to add controls, you might consider adding a control for a start date and end date to give users the ability to set the time being considered across all the visuals.\nExplore QuickSight forecast features to see if you can forecast what your cost per API call will be in the future.\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/1_configure_services/","title":"Configure Services","tags":[],"description":"","content":"Create S3 Bucket Create a single S3 bucket that will contain the journey files for all workloads in that account.\nLog into the console via SSO, go to the S3 service page\nClick Create bucket\nEnter a Bucket name starting with cost (we have used cost-wa-reports, you will need to use a unique bucket name) and click Create bucket: Upload the following object into the bucket. Code/cost_journey.csv You can edit this CSV file to customize your journey for your organization. The definitions used within this file are at the end of this lab in the tear down step.\nYou have now setup the S3 bucket which contains your organizations journey configuration, all the journeys for the workloads.\nCreate the Lambda Function Go to the Lambda Console\nClick Create function\nSelect Author from scratch\nEnter a function name of Cost_W-A_Journey\nSelect a runtime of Python 3.6, this is a specifically required version\nUnder Permissions:\nExecution role: Create a new role from AWS policy templates Role name: extract-wa-reports_role Click Create function: Select the lambda_function.py and paste the following code:\nLambda function code import boto3 import json import os import logging import urllib.parse logger = logging.getLogger() logger.setLevel(logging.INFO) s3_bucket = os.environ['S3_BUCKET'] s3_key = os.environ['S3_KEY'] Image_XSize = os.environ['Image_XSize'] Image_YSize = os.environ['Image_YSize'] ################# # Boto3 Clients # ################# wa_client = boto3.client('wellarchitected') s3_client = boto3.client('s3') ############## # Parameters # ############## # The maximum number of results the API can return in a list workloads call. list_workloads_max_results_maximum = 50 # The maximum number of results the API can return in a list answers call. list_answers_max_results_maximum = 50 # The maximum number of results the API can return in a list milestones call. list_milestone_max_results_maximum = 50 def get_all_workloads(): # Get a list of all workloads list_workloads_result = wa_client.list_workloads(MaxResults=list_workloads_max_results_maximum) logger.info(f'Found {len(list_workloads_result)} Well-Archtected workloads.') workloads_all = list_workloads_result['WorkloadSummaries'] while 'NextToken' in list_workloads_result: next_token = list_workloads_result['NextToken'] list_workloads_result = wa_client.list_workloads( MaxResults=list_workloads_max_results_maximum, NextToken=next_token ) workloads_all += list_workloads_result['WorkloadSummaries'] return (workloads_all) def get_milestones(workload_id): # Get latest milestone review date milestones = wa_client.list_milestones( WorkloadId=workload_id, MaxResults=list_milestone_max_results_maximum )['MilestoneSummaries'] # If workload has milestone get them. logger.info(f'Workload {workload_id} has {len(milestones)} milestones.') if milestones: for milestone in milestones: milestone['RecordedAt'] = milestone['RecordedAt'].isoformat() return milestones def get_lens(workload_id): # Which lenses have been activated for this workload lens_reviews_result = wa_client.list_lens_reviews( WorkloadId=workload_id )['LensReviewSummaries'] # An array to hold the reviews we specifically want, in this case its those that used the 'wellarchitected' lens lens_reviews = [] # Go through each lens review \u0026amp; look for W-A lenses of the right version for lens_review in lens_reviews_result: alias = lens_review['LensAlias'] version = lens_review['LensVersion'] # Add W-A lenses from the right version only to the array if (\u0026quot;wellarchitected\u0026quot; in alias) and (\u0026quot;2020-07-02\u0026quot; in version): lens_reviews.append(lens_review) # return the selected lens reviews logger.info(f'Workload {workload_id} has used {len(lens_reviews)} lens') return lens_reviews def get_lens_answers(workload_id, lens_reviews): # Loop through each activated lens list_answers_result = [] for lens in lens_reviews: lens_name = lens['LensName'] logger.info(f'Looking at {lens_name} answers for Workload {workload_id}') # Get All answers for the lens list_answers_reponse = wa_client.list_answers( WorkloadId=workload_id, LensAlias=lens['LensAlias'], MaxResults=list_answers_max_results_maximum ) # An array to hold the answers that were 'selected' in the review cost_answers = [] # Flatten the answer result to include LensAlias and Milestone Number for answer_result in list_answers_reponse['AnswerSummaries']: pillarid = answer_result['PillarId'] # If its a cost answer/Best practice that was selected, then store it if \u0026quot;costOptimization\u0026quot; in pillarid: # Get the list of selected answers/best practices answers = answer_result['SelectedChoices'] # Go through each selected answer for answer in answers: # Remove answers with '_no', as they are the \u0026quot;none of these\u0026quot; answers if \u0026quot;_no\u0026quot; not in answer: # Add all selected answers to the array cost_answers.append(answer) # Return all selected cost answers/best practices return cost_answers def get_journey(): # Get the cost journey information from S3 response = s3_client.get_object( Bucket=s3_bucket, Key='cost_journey.csv' ) data = [] # Array of dicts/maps, that have all the best practices \u0026amp; their journey attributes from the journey file best_practices = [] # Get the file contents \u0026amp; split it line by line, remove trailing formatting characters data = response['Body'].read().decode('utf-8') lines = data.split('\\r\\n') # Go through each line in the journey file for line in lines: # Only get lines with \u0026quot;cost\u0026quot;, which ignores the header line if \u0026quot;cost\u0026quot; in line: # Break the line up by ',', as its a CSV line_attributes = line.split(',') # Put each element into a dict/map best_practice_attributes = {\u0026quot;id\u0026quot;: line_attributes[0], \u0026quot;name\u0026quot;: line_attributes[1], \u0026quot;risk\u0026quot;: line_attributes[2], \u0026quot;phase\u0026quot;: int(line_attributes[3]), \u0026quot;order\u0026quot;: int(line_attributes[4]), \u0026quot;effort\u0026quot;: int(line_attributes[5]), \u0026quot;duration\u0026quot;: int(line_attributes[6]), \u0026quot;frequency\u0026quot;: line_attributes[7]} # Add the map into an array best_practices.append(best_practice_attributes) # Find out how big each phase is, so we can scale the image accordingly ph1_sum = 0 ph2_sum = 0 ph3_sum = 0 ph4_sum = 0 # The size of each phase is the effort of all best practices + the duration between them # Go through each best practice for best_practice in best_practices: # If its a cost BP, ignore the header or any other non-cost BPs if \u0026quot;cost\u0026quot; in best_practice['id']: # Get the phase of the best prac incase they are not ordered in the file phase = best_practice['phase'] # Add the effort \u0026amp; duration to each phases total if phase == 1: ph1_sum = ph1_sum + best_practice['effort'] + best_practice['duration'] elif phase == 2: ph2_sum = ph2_sum + best_practice['effort'] + best_practice['duration'] elif phase == 3: ph3_sum = ph3_sum + best_practice['effort'] + best_practice['duration'] elif phase == 4: ph4_sum = ph4_sum + best_practice['effort'] + best_practice['duration'] # Put the scale factor of each phase into an array # The phases are staggered, ph1: x=0, ph2 = x=150, ph3 = x=300, ph4 = x=450 # The scale is (total image size - stagger) / phase_size. So that each phase could be scaled to fit the Image_XSize specified phase_scales = [(int(Image_XSize) - 0) / int(ph1_sum), (int(Image_XSize) - 150) / int(ph2_sum), (int(Image_XSize) - 300) / int(ph3_sum), (int(Image_XSize) - 450) / int(ph4_sum)] # Scale all phases by the smallest overall factor - which is the largest phase x_scale_factor = min(phase_scales) # Return the image scale factor \u0026amp; the answers/best practices from the journey file return x_scale_factor, best_practices def draw_journey(scale_factor, best_practices, report_answers): # Current cursor positions, come in \u0026amp; down a little from the top left current_xpos = 10 current_ypos = 10*scale_factor + 10 # Create html file headers \u0026amp; part of the body wa_journey = \u0026quot;\u0026lt;HTML xmlns=\\\u0026quot;http://www.w3.org/1999/xhtml\\\u0026quot;\u0026gt;\\n\u0026lt;HEAD\u0026gt;\\n\u0026lt;meta charset=\\\u0026quot;utf-8\\\u0026quot;\u0026gt;\u0026lt;/meta\u0026gt;\\n\\t\u0026lt;TITLE\u0026gt;Journey\u0026lt;/TITLE\u0026gt;\\n\u0026lt;/HEAD\u0026gt;\\n\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;BODY\u0026gt;\\n\\t\u0026lt;p\u0026gt;Here is the journey\u0026lt;/p\u0026gt;\\n\\t\\t\u0026lt;svg xmlns=\\\u0026quot;http://www.w3.org/2000/svg\\\u0026quot; width=\\\u0026quot;1080\\\u0026quot; height=\\\u0026quot;1920\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\\t\\t\u0026lt;svg width=\\\u0026quot;\u0026quot; + str(Image_XSize) + \u0026quot;\\\u0026quot; height=\\\u0026quot;\u0026quot; + str(Image_YSize) + \u0026quot;\\\u0026quot;\u0026gt;\\n\u0026quot; # loop counter \u0026amp; tracking of the current phase so we can drop down on the next phase x = 1 current_phase = 1 # Go through each best practice while x \u0026lt;= len(best_practices): # Get the best practices in order current_bestprac = dict(list(filter(lambda best_practice: best_practice['order'] == x, best_practices))[0]) # If its a new phase move down \u0026amp; across in the image, otherwise hold steady if current_bestprac['phase'] == current_phase: current_ypos = current_ypos else: # Move across by 150 from the prevoius phase to stagger the start current_xpos = current_phase * 150 # Move down by 10 (maximum effort/diameter) x scale factor, + a buffer of 10px current_ypos = current_ypos + 10*scale_factor + 10 # We're on a new phase current_phase = current_phase + 1 # Move to the center of the next circle, which is adding the duration between and half effort (radius) from previous point current_xpos = current_xpos + (current_bestprac['duration'])/2*scale_factor + (current_bestprac['effort'])/2*scale_factor # Check if it was a selected best practice/answer, if so colour it green if current_bestprac['id'] in report_answers: # print Green wa_journey = wa_journey + \u0026quot;\u0026lt;circle cx=\\\u0026quot;\u0026quot; + str(current_xpos) + \u0026quot;\\\u0026quot; cy=\\\u0026quot;\u0026quot; + str(current_ypos) + \u0026quot;\\\u0026quot; r=\\\u0026quot;\u0026quot; + str(current_bestprac[\u0026quot;effort\u0026quot;]/2 * scale_factor) + \u0026quot;\\\u0026quot; stroke=\\\u0026quot;black\\\u0026quot; stroke-width=\\\u0026quot;\u0026quot; + str(current_bestprac['frequency']) + \u0026quot;\\\u0026quot; fill=\\\u0026quot;green\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;title\u0026gt;\u0026quot; + current_bestprac['name'] + \u0026quot;\u0026lt;/title\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/circle\u0026gt;\\n\u0026quot; # It wasnt selected so print red for high risk or blue otherwise else: # print red if high risk if current_bestprac['risk'] == \u0026quot;High\u0026quot;: wa_journey = wa_journey + \u0026quot;\u0026lt;circle cx=\\\u0026quot;\u0026quot; + str(current_xpos) + \u0026quot;\\\u0026quot; cy=\\\u0026quot;\u0026quot; + str(current_ypos) + \u0026quot;\\\u0026quot; r=\\\u0026quot;\u0026quot; + str(current_bestprac[\u0026quot;effort\u0026quot;]/2 * scale_factor) + \u0026quot;\\\u0026quot; stroke=\\\u0026quot;black\\\u0026quot; stroke-width=\\\u0026quot;\u0026quot; + str(current_bestprac['frequency']) + \u0026quot;\\\u0026quot; fill=\\\u0026quot;red\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;title\u0026gt;\u0026quot; + current_bestprac['name'] + \u0026quot;\u0026lt;/title\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/circle\u0026gt;\\n\u0026quot; # print blue otherwise else: wa_journey = wa_journey + \u0026quot;\u0026lt;circle cx=\\\u0026quot;\u0026quot; + str(current_xpos) + \u0026quot;\\\u0026quot; cy=\\\u0026quot;\u0026quot; + str(current_ypos) + \u0026quot;\\\u0026quot; r=\\\u0026quot;\u0026quot; + str(current_bestprac[\u0026quot;effort\u0026quot;]/2 * scale_factor) + \u0026quot;\\\u0026quot; stroke=\\\u0026quot;black\\\u0026quot; stroke-width=\\\u0026quot;\u0026quot; + str(current_bestprac['frequency']) + \u0026quot;\\\u0026quot; fill=\\\u0026quot;blue\\\u0026quot;\u0026gt;\\n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;title\u0026gt;\u0026quot; + current_bestprac['name'] + \u0026quot;\u0026lt;/title\u0026gt;n\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/circle\u0026gt;\\n\u0026quot; # Move to the end of the current circle, add half the effort current_xpos = current_xpos + (current_bestprac['effort'])/2*scale_factor # Onto the next best practice x = x + 1 # Add the trailing HTML wa_journey = wa_journey + \u0026quot;\u0026lt;script type=\\\u0026quot;text/javascript\\\u0026quot;\u0026gt;\u0026lt;![CDATA[\\n\\t(function() {\\n\\t\\tvar tooltip = document.getElementById('tooltip');\\n\\t\\t})();\\n\\t]\u0026gt;\\n\\n\\t\\tvar triggers = document.getElementsByClassName('tooltip-trigger');\\n\\tfor (var i = 0; i \u0026lt; triggers.length; i++) {\\n\\ttriggers[i].addEventListener('mousemove', showTooltip);\\n\\t\\ttriggers[i].addEventListener('mouseout', hideTooltip);\\n\\t}\\n\\n\\tfunction showTooltip(evt) {\\n\\t\\ttooltip.setAttributeNS(null, \\\u0026quot;visibility\\\u0026quot;, \\\u0026quot;visible\\\u0026quot;);\\n\\t}\\n\\tfunction hideTooltip() {\\n\\t\\ttooltip.setAttributeNS(null, \\\u0026quot;visibility\\\u0026quot;, \\\u0026quot;hidden\\\u0026quot;);\\n\\t}\u0026quot; wa_journey = wa_journey + \u0026quot;\u0026lt;/SVG\u0026gt;\\n\u0026lt;/BODY\u0026gt;\\n\u0026lt;/HTML\u0026gt;\\n\u0026quot; # Return the html text of the journey return wa_journey def lambda_handler(event, context): workloads_all = get_all_workloads() # Generate workload JSON file logger.info(f'Generate JSON object for each workload.') # Text to build the HTML index file of all workloads generated_workloads = \u0026quot;\u0026lt;HTML\u0026gt;\\n\u0026lt;HEAD\u0026gt;\\n\u0026lt;/HEAD\u0026gt;\\n\u0026lt;BODY\u0026gt;\\n\u0026quot; for workload in workloads_all: # Get workload info from WAR Tool API, workload_id = workload['WorkloadId'] workload_name = workload['WorkloadName'] milestones = get_milestones(workload_id) lens_reviews = get_lens(workload_id) if len(lens_reviews) \u0026gt; 0: list_answers_result = get_lens_answers(workload_id, lens_reviews) # Build JSON of workload data workload_report_data = {} # Get the answers from the W-A report workload_report_data['report_answers'] = list_answers_result # Get the scale factor \u0026amp; best practices from the journey file scale_factor, best_practices = get_journey() # Create the Journey image \u0026amp; HTML file te journey_image = draw_journey(scale_factor, best_practices, workload_report_data['report_answers']) # Write to S3 journey_file_name = workload_name + '_' + workload_id + '.html' html_link = \u0026quot;https://\u0026quot; + s3_bucket + '.s3.amazonaws.com/' + s3_key + '/' + urllib.parse.quote_plus(journey_file_name) s3_client.put_object( Body=journey_image, Bucket=s3_bucket, Key=f'{s3_key}/{journey_file_name}' ) # Add the next workload file to the index file generated_workloads = generated_workloads + \u0026quot;\u0026lt;A href=\\\u0026quot;\u0026quot; + html_link + \u0026quot;\\\u0026quot;\u0026gt;\u0026quot; + workload_name + \u0026quot;\u0026lt;/A\u0026gt;\u0026lt;BR\u0026gt;\\n\u0026quot; # Add the closing HTML in the index file generated_workloads = generated_workloads + \u0026quot;\u0026lt;/HTML\u0026gt;\u0026quot; # Write the index file to the S3 bucket file_name = \u0026quot;W-A Workload Journeys.html\u0026quot; s3_client.put_object( Body=generated_workloads, Bucket=s3_bucket, Key=f'{file_name}' ) Above the code click Configuration, select Environment variables Click Edit and add the following variables, then click Save\nImage_XSize - 1440 Image_YSize - 900 S3_BUCKET - (the name of your bucket created previously) S3_KEY - WorkloadReports Select General configuration, click Edit, change the timeout to 1min, click Save You have now created the lambda function, however we need to add permissions before its run\nCreate the IAM Role Modify the IAM role that is used by the Lambda function, to allow access to your S3 bucket and your Well-Architected reviews.\nGo to the IAM Console\nGo to Roles and select the extract-wa-reports_role role\nAdd an inline policy\nModify the policy below replacing \u0026lt;S3_BUCKET_NAME\u0026gt;, and paste it into the json:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::\u0026lt;S3_BUCKET_NAME\u0026gt;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } Click Review policy, enter a name of WAReportAccess click Create policy\nClick Attach policies, and attach the WellArchitectedConsoleReadOnlyAccess\nYou have now added the required permissions and all configuration is complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/1_prerequisite/","title":"Deploy the Infrastructure","tags":[],"description":"","content":"For many organizations, the data that they possess is one of the most valuable assets they have. Backing up data frequently is of vital importance for the long lasting success of any organization. However, a backup of data is only valuable if data can be recovered/restored from the backup. In the cloud, backing up data and testing the restore is easier compared to on-premises datacenters. Automating this process with appropriate notification systems will ensure that an organization\u0026rsquo;s data is backed up frequently, the backups are tested to ensure expected recovery, and appropriate people are notified in case of failures.\nYou will use AWS CloudFormation to provision some resources needed for this lab. As part of this lab, the CloudFormation stack that you provision will create an EC2 Instance, an SNS Topic, and a Lambda Function. You can view the CloudFormation template here for a complete list of all resources that are provisioned. This lab will only work in us-east-1.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the infrastructure using AWS CloudFormation Click here to deploy the stack.\nUnder PARAMETERS:\nSelect an AvailabilityZone to launch the resources in. For LatestAmiId leave the default value. This will automatically retrieve the latest AMI ID for Amazon Linux 2. Specify an email address that you have access to for NotificationEmail. Check the box I acknowledge that AWS CloudFormation might create IAM resources.\nClick CREATE / CREATE STACK.\nMANUAL STEPS\nUse your administrator account to access the CloudFormation console - https://console.aws.amazon.com/cloudformation/ . Click on CREATE STACK. Under PREREQUISITE - PREPARE TEMPLATE, select the option TEMPLATE IS READY. Under SPECIFY TEMPLATE, select the option AMAZON S3 URL, enter the link - https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/200_Testing_Backup_and_Restore_of_Data/backup-lab.yaml and click NEXT. Enter a STACK NAME such as WA-Backup-Lab. Select an AvailabilityZone to launch the resources in. For LatestAmiId leave the default value. This will automatically retrieve the latest AMI ID for Amazon Linux 2. For NotificationEmail, specify an email address that you have access to. Leave default values for the rest of the fields and click NEXT. No changes are needed on the CONFIGURE STACK OPTIONS page, click NEXT. Review the details of the stack, scroll down to CAPABILITIES, and check the box next to I acknowledge that AWS CloudFormation might create IAM resources. Click CREATE STACK. Note: Once stack creation starts, monitor the email address you entered. You should receive an email from SNS with the subject AWS Notification - Subscription Confirmation. Click on the link Confirm subscription to confirm the subscription of your email to the SNS Topic.\nThe stack takes about 2 minutes to create all the resources. Periodically refresh the page until you see that the STACK STATUS is in CREATE_COMPLETE. Once the stack is in CREATE_COMPLETE, visit the OUTPUTS section for the stack and note down the KEY and VALUE for each of the outputs. This information will be used later in the lab.\nYou can view the simple application running on the instance by visiting the URL specified in the outputs. If you get an error that says \u0026ldquo;Connection refused\u0026rdquo;, wait a couple of minutes and try again.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/1_pricing_sources/","title":"Create Pricing Data Sources","tags":[],"description":"","content":"Option 1: Manual Deployment Click here to continue with the manual Setup Create S3 Bucket and Folders Create a single S3 bucket that contains two folders - od_pricedata and sp_pricedata, these will contain the on-demand pricing data and the Savings Plans pricing data.\nLog into the console via SSO, go to the S3 service page: Click Create bucket:\nEnter a Bucket name starting with cost (we have used cost-sptool-pricingfiles, you will need to use a unique bucket name) and click Create bucket: Click on the (bucket name): Click Create folder: Enter a folder name of od_pricedata, click Save: Click Create folder: Enter a folder name of sp_pricedata, click Save: You have now setup the S3 bucket with the two folders that will contain the OnDemand and Savings Plans pricing data. Create IAM Role and Policies Go to the IAM Console\nSelect Policies and Create Policy Edit the following policy and replace (S3 pricing bucket) with your bucket name, on the JSON tab, enter the following policy, click Review policy:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;S3SPTool\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:DeleteObjectVersion\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(S3 pricing bucket)/*\u0026quot; } ] } Policy Name S3PricingLambda, Description Access to S3 for Lambda SPTool function, click Create policy: Select Roles, click Create role: Select Lambda, click Next: Permissions: Select the S3PricingLambda policy, click Next: Tags: Click Next: Review\nRole name SPToolS3Lambda, click Create role: Setup On-Demand Pricing Lambda Function Create the On-Demand Lambda function to get the pricing information, and extract the required parts from it.\nGo to the Lambda service page: Click Create function: Enter the following details:\nSelect: Author from scratch Function name: Cost_SPTool_ODPricing_Download Runtime: Python (Latest) Execution Role: Use an existing role Role name: SPToolS3Lambda Click Create function Copy and paste the following code into the Function code section:\n# Lambda Function Code - SPTool_OD_pricing_Download # Function to download OnDemand pricing # Please reachout to costoptimization@amazon.com if there's any comments or suggestions # Lambda Function Code - SPTool_OD_pricing_Download import json import boto3 import urllib3 import urllib.request import os def lambda_handler(event, context): s3=boto3.resource('s3') http=urllib3.PoolManager() url = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.csv' s3Bucket = \u0026lt;bucket_name\u0026gt; key = 'od_pricedata/ec2_prices.csv' urllib.request.urlopen(url) #Provide URL s3.meta.client.upload_fileobj(http.request('GET', url, preload_content=False), s3Bucket, key) return { 'statusCode': 200, 'body': json.dumps('YAY!') } Edit the pasted code, replacing bucket_name with the name of your bucket: Click Deploy above the code\nScroll down and edit Basic settings:\nMemory: 4096MB Timeout: 2min Click save Scroll to the top and click Test Enter an Event name of Test, click Create: Click Test: The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Setup Savings Plan Pricing Lambda Function Create the Savings Plan Lambda function to get the pricing information, and extract the required parts from it.\nGo to the Lambda service page: Click Create function: Enter the following details:\nSelect: Author from scratch Function name: Cost_SPTool_SPPricing_Download Runtime: Python (Latest) Execution Role: Use an existing role Role name: SPToolS3Lambda Click Create function Copy and paste the following code into the Function code section:\n# Lambda Function Code - SPTool_SP_pricing_Download # Function to download SavingsPlans pricing, get out the required lines \u0026amp; upload it to S3 as a zipped file # It will get each regions pricing file in CSV, find 'Usage' and '1yr', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 import json def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the SavingsPlans pricing index file, so you can get all the region files, which have the pricing in them r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/savingsPlan/v1.0/aws/AWSComputeSavingsPlan/current/region_index.json') # Load the json file into a variable, and parse it sp_regions = r.data sp_regions_json = (json.loads(sp_regions)) # Variable to hold all of the pricing data, its large at over 150MB sp_pricing_data = \u0026quot;\u0026quot; # Cycle through each regions pricing file, to get the data we need for region in sp_regions_json['regions']: # Get the CSV URL url = \u0026quot;https://pricing.us-east-1.amazonaws.com\u0026quot; + region['versionUrl'] url = url.replace('.json', '.csv') # Create a connection \u0026amp; get the regions pricing data CSV file http = urllib3.PoolManager() r = http.request('GET', url) spregion_content = r.data # Split the lines into an array spregion_lines = spregion_content.splitlines() # Go through each of the pricing lines for line in spregion_lines: # If the line has 'Usage' then grab it for pricing data, exclude all others if (str(line).find('Usage') != -1): sp_pricing_data += str(line.decode(\u0026quot;utf-8\u0026quot;)) sp_pricing_data += \u0026quot;\\n\u0026quot; # Compress the text into a local temporary file with gzip.open('/tmp/sp_pricedata.txt.gz', 'wb') as f: f.write(sp_pricing_data.encode()) # Upload the file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/sp_pricedata.txt.gz', 'bucket_name', 'sp_pricedata/sp_pricedata.txt.gz') # Die if you cant get the file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 } Edit the pasted code, replacing bucket_name with the name of your bucket: Click Deploy above the code\nEdit Basic settings below:\nMemory: 10240MB Ephemeral storage: 10240MB Timeout: 10min Click save Scroll to the top and click Test Enter an Event name of Test, click Create: Click Test: The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Go to your S3 bucket and into the sp_pricedata folder and you should see a gz file of non-zero size is in it: CloudWatch Events Setup We will setup a CloudWatch Event to periodically run the Lambda functions, this will update the pricing and include any newly released instances.\nGo to the CloudWatch service page: Click on Events, then click Rules: Click Create rule For the Event Source select Schedule and set the required period, we have selected 5 days, click Add target: Add the SPTool_ODPricing_Download Lambda function, and click Add target: Add the SPTool_SPPricing_Download Lambda function, and click Configure details: Add the name SPTool-pricing, optionally add a description and click Create rule: Prepare the Pricing Data Source - Crawler We will prepare a pricing data source which we will use to join with the CUR. In this example we will take 1 year No Upfront Savings Plans rates and join them to On-Demand pricing. You can modify this part to select 3 year or Partial or All-Upfront rates.\nGo to the Glue Service page: Click Crawlers from the left menu: Click Add crawler: Enter a crawler name of SP_Pricing : Ensure Data stores is the source type, click Next: Click the folder icon to list the S3 folders in your account. Expand the bucket which contains your pricing folders, and select the folder name sp_pricedata, click Select. Click Next: Click Next: Create an IAM role with a name of SPToolPricing, click Next: Leave the frequency as Run on demand, and click Next: Click on Add database: Enter a database name of pricing, and click Create: Click Next: Click Finish: Select the crawler SP_Pricing and click Run crawler: Option 2: Terraform Advanced Deployment Click here to continue with the Terraform Advanced Setup Create the Pricing Sources using Terraform There is an AWS Github Repo which has a module to deploy all the resources needed in this lab. Please deploy using the instructions in the github repo then return to the step below.\nTest Lambda Function Now you have deployed the Terraform then you can test your lambda to get your first set of data in Amazon S3.\nGo to the Lambda service page : Search for your new function called Cost_SPTool_ODPricing_Download and click on it. To test your lambda function click Test\nEnter an Event name of Test, click Create: Click Test: The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Do the same for the Cost_SPTool_SPPricing_Download function\nYou have successfully setup the pricing data source. We have a database of on demand and Savings Plans rates.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/","title":"Level 200: Integration with AWS Compute Optimizer and AWS Trusted Advisor","tags":[],"description":"","content":"Authors Jang Whan Han, Solutions Architect, AWS Well-Architected Phong Le, Geo Solutions Architect, AWS Well-Architected Contributor Ben Mergen, Senior Cost Lead Solutions Architect, AWS Well-Architected Introduction AWS Well-Architected Framework Review is a constructive conversation about architectural decisions. The one of best ways to get the maximum amount of impact from the AWS Well-Architected Framework Review is to complete any pre-review analysis before the review starts and be armed with data that can be shared during the review. Once data is available for the review, not only can the reviwer specifically ask pillar-aligned questions, but also customers can authoritatively answer those questions based on data. This will We are going to focus on cost optimization review with data that supports Rightsize usecase to align your service allocation size to your actual workload demand. This will allows customer to see an immediate cost benefit from data. The purpose of this lab is to walk you through one of many integration examples with Well-Architected Tool to have a data-driven Cost Optimization review using AWS Cloud Financial Management Services . This lab enables you to automatically have cost optimization data that AWS Compute Optimizer and AWS Trusted Advisor analyzed for COST 6 question in Cost Optimization Pillar as soon as you define a workload in Well-Architected Tool.\nThe knowledge you acquire will help you learn how to programmatically access content in the Well-Architected Tool in alignment with the AWS Well-Architected Framework. Goals Identify cost optimization opportunities through a data-driven Well-Architected Framework Review Learn about AWS Cloud Financial Management Services. Evaluate Cost When Selecting Services Select the Correct Resource Type, Size, and Number Select the Best Pricing Model Prerequisites Now that we retrive cost optimization data from AWS Cloud Financial Management Services, there are a couple of prerequisites required.\nAn AWS Account that you are able to use for testing, that is not used for production or other purposes.\nAn Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy ).\nOpt in for AWS Compute Optimizer You will need to opt in for AWS Compute Optimizer if you have not done so. AWS Compute Optimizer analyzes metrics from the past 14 days to generate recommendations.\nAWS Trusted Advisor Amazon Trusted Advisor provides best practices (or checks) in four categories: cost optimization, security, fault tolerance, and performance improvement. This demo will use \u0026ldquo;Low Utilization Amazon EC2 Instances\u0026rdquo; check in Cost Optimization.\nAttach AWS Tags to your existing AWS resources that you would like to review against if you have not done so. We will attach the same AWS Tags to a workload that you will define through Well-Architected Tool later. If there is no AWS Tags attached to your AWS resources, please attach the following AWS Tags to your existing Amazon EC2 Instances as follows:\nKey = workload Value = wademo NOTE: If you attached your own key and value to AWS resources, please use the same key and value when defining a workload later.\nCosts NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nAWS Pricing Time to complete The lab should take approximately 30 minutes to complete Steps Prerequisites Configure Lab Environment Create a Well-Architected Workload with Tags Performing a data-driven review Integrate AWS Compute Optimizer and Trusted Advisor to Another Question Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/","title":"Level 200: Using AWSCLI to Manage WA Reviews","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction The purpose if this lab is to walk you through using the AWS Command Line Interface (AWS CLI) to access the features of the AWS Well-Architected Tool. You will create a workload, review an Operational Excellence question, save the workload, create a milestone, and examine and download the Well-Architected Review report.\nThe knowledge you acquire will help you learn how to programmatically access content in the Well-Architected tool in alignment with the AWS Well-Architected Framework. Goals: Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool. Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Costs: There are no costs for this lab AWS Pricing Time to complete The lab should take approximately 30 minutes to complete Steps: Configure Environment Create a Well-Architected Workload Performing a review Saving a milestone Viewing and downloading the report Optional - Programmatic access via API Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/1_prerequisites/","title":"Prerequisites","tags":[],"description":"","content":"Opt in for AWS Compute Optimizer Opt in for AWS Compute Optimizer if you have not done so. You will need to opt in for AWS Compute Optimizer if you have not done so. AWS Compute Optimizer analyzes metrics from the past 14 days to generate recommendations.\nCost Optimization checks in Amazon Trusted Advisor Amazon Trusted Advisor provides best practices (or checks) in four categories: cost optimization, security, fault tolerance, and performance improvement. This demo will use \u0026ldquo;Low Utilization Amazon EC2 Instances\u0026rdquo; check in Cost Optimization.\nAttach AWS Tags to Amazon EC2 Instances Amazon EC2 Instances must have AWS Tags. If you have not associated AWS Tags with Amazon EC2 Instances with any AWS Tags, tag your Amazon EC2 resources . You can use your own key and value.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/1_configure_env/","title":"Configure Environment","tags":[],"description":"","content":"Install AWS CLI v2 The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.\nLinux This includes:\nAWS CloudShell All native Linux installs Windows Subsystem for Linux (WSL) Verify existing version:\nRun the following command aws --version If the version number is less than 2.1.12 or you get \u0026ldquo;command not found\u0026rdquo; You need to install or upgrade. Follow these steps:\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot; unzip awscliv2.zip sudo ./aws/install --update After typing the commands, you should see the following in your console: For additional troubleshooting, see the detailed installation instructions here MacOS See the detailed MacOS installation instructions here Windows See the detailed Windows installation instructions here X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/1_deploy_workload/","title":"Deploy the workload","tags":[],"description":"","content":"Traditionally most workloads are designed to withstand infrastructure failure by deploying workload components across multiple Availability Zones/Regions, implementing self-healing capabilities such as AutoScaling, etc. While such techniques are effective in ensuring uptime of workload resources, they do not address issues introduced at the workload application level (i.e. a software bug). Leveraging techniques like sharding and shuffle sharding will provide additional reliability to workloads by limiting the blast radius of failures so that only a subset of users are impacted by such failures.\nThe following diagram shows the initial architecture you will deploy. This architecture has no sharding: You will use AWS CloudFormation to provision the resources needed for this lab. The CloudFormation stack that you provision will create an Application Load Balancer, Target Groups, and EC2 instances in a new VPC.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Access the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the workload using AWS CloudFormation Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources (standard)\nFor Prepare template select Template is ready\nFor Template source select Amazon S3 URL In the text box under Amazon S3 URL specify https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/300_Fault_Isolation_with_Shuffle_Sharding/regular.yaml Click Next\nFor Stack name use Shuffle-sharding-lab\nNo changes are required for Parameters. Click Next\nFor Configure stack options click Next\nOn the Review page:\nScroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here . Note: The template creates an IAM role and Instance Profile for EC2. These are the minimum permissions necessary for the instances to be managed by AWS Systems Manager. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - InstanceRole.\nClick Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows (in reverse order) the activities performed by CloudFormation, such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. The stack takes about 5 mins to create all the resources. Periodically refresh the CloudFormation stack events until you see that the Stack Status is in CREATE_COMPLETE. The stack creates the following resources:\nA new VPC, subnets, Internet Gateway, Route tables to host the workload in 8 EC2 instances that host the application An Application Load Balancer, Listener and rules, and Target Groups to route traffic IAM resources (roles, policies) that allow the EC2 instances to be managed by AWS Systems Manager An SSM Document that will be run on the instances CloudWatch Synthetics canaries IAM role and S3 bucket for the canaries Once the stack is in CREATE_COMPLETE, visit the Outputs section for the stack and note down the Key and Value for each of the outputs. This information will be used in the lab.\n1.3 Test the application Now that the application has been deployed, it is time to test it to understand how it works. The sample application used in this lab is a simple web application that returns a message with the Worker that responded to the request. Customers pass in a query string as part of the request to identify themselves. The query string used here is name.\nVisit the Outputs section of the CloudFormation stack created in the previous step. You will see a list of URLs next to customer names.\nOpen one of the customer links in a new browser tab. Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end.\nThe list of EC2 instances in your workload can be viewed in the AWS Console here Open the links for a few other customers and verify that they are able to get responses from different EC2 instances. The different customers are - Alpha, Bravo, Charlie, Delta, Echo, Foxtrot, Golf, and Hotel.\nNote that the URLs all have the same base URL but use a query string to represent different customers. The query string used is name. Refresh the web browser multiple times to verify that all customers are able to receive responses from all EC2 instances (workers) in the back-end.\n1.4 Verify workload availability The lab uses Amazon CloudWatch Synthetics canaries to check availability of the workload for each of the customers. The results are aggregated in a CloudWatch Dashboard so you can see the availability of the workload for every customer in a single location.\nVisit the Outputs section of the CloudFormation stack created as part of this lab and open the AvailabilityDashboard in a new browser window.\nYou will see individual graphs for each customer. The graphs represent successful requests made by the corresponding canary to the sample workload. Currently, the workload is available for all customers.\nKeep the AvailabilityDashboard open as you will revisit it later in the lab.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/1_deploy_vpc/","title":"Deploying the infrastructure","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nDeploy VPC This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use PerfLab-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\nWhen the stack status is CREATE_COMPLETE, you can continue to the next step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/1_deploy_vpc/","title":"Deploying the infrastructure","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nDeploy VPC This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use PerfLab-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\nWhen the stack status is CREATE_COMPLETE, you can continue to the next step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/","title":"Level 200: Optimize Amazon EC2 using Amazon CloudWatch and AWS Compute Optimizer","tags":[],"description":"","content":"Author Neha Garg, Enterprise Solutions Architect. Jang Whan Han, Well-Architected Geo Solutions Architect. Contributor Stephen Salim, Well-Architected Geo Solutions Architect. Introduction This lab focuses on optimizing data patterns for sustainability, specifically focused on removing unneeded or redundant data, and minimizing data movement across networks.\nGoals At the end of this lab you will:\nUnderstand how to identify optimization areas using AWS Well-Architected Sustainability Pillar best practices Baseline and optimize workloads by identifying sustainability key performance indicators (KPIs) Learn how to use the Amazon Redshift Data Sharing feature to implement data management best practices described in AWS Well-Architected Sustainability Pillar Prerequisites The lab is designed to run in your own AWS account You can launch an Amazon Redshift cluster in the AWS us-east-1 and AWS us-west-1 regions (referred to us-east-1 region, and us-west-1 region throughout the lab) using Redshift ra3 nodes This lab is written and tested in us-east-1 and us-west-1 regions. You may be able to run this in other AWS regions of your choice where Redshift ra3 nodes are available , but results may vary. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Amazon Redshift ra3 nodes are not part of Amazon Redshift Free trial, or AWS Free Tier. When you decide to stop the lab at any point in time, please revisit the clean up instructions at the end so you stop incuring cost (e.g. for storage in Amazon S3).\nLab duration Estimated time required to complete this lab is 90 minutes.\nWorkload details AnyCompany (an fictional event management organization) is running a central data warehouse environment on Amazon Redshift in us-east-1 region, which is used by various departments in the organization for their respective storage, analytical processing, and reporting. The marketing department is the top consumer of the data warehouse, and have data engineers, analysts, and scientists based out of US west coast. The marketing team has implemented their own Amazon Redshift cluster in us-west-1 (consumer) region, which refreshes nightly using an Amazon Redshift snapshot received from us-east-1 region (producer) and uploading to the us-west-1 (consumer) region Amazon Redshift cluster. Since the marketing team analytical processing consumes lots of resources \u0026amp; integrated with their west coast based on-premise hosted downstream applications, they perform their analytical processing in us-west-1 region, and other departments use us-east-1 region hosted data warehouse. This requires storing a redundant dataset in us-west-1 region, and transferring huge amounts of data (via nightly ETL feed) over the network between AWS regions.\nThis is not a sustainability friendly implementation, and can be optimized using AWS Well-Architected Sustainability Pillar best practices for data patterns. Also, with this approach, the insights generated by the Marketing department are not based on live data.\nWorkload optimization for sustainability In this case, optimization areas include:\nMarketing team using only data which is required for their analytical processing, whereas currently they use the full dataset. This reduces storage requirements in us-west-1. By reducing the amount of data copied between us-east-1 and us-west-1 regions, this reduces network traffic. Technical solution By introducing Amazon Redshift Data Sharing feature, the marketing department can optimize their implementation for sustainability, avoiding redundant storage \u0026amp; reducing data transfer between AWS regions. Data sharing enables instant, granular, and fast data access across Amazon Redshift clusters without the need to copy or move it. With data sharing, you have live access to data, so that your users can see the most up-to-date and consistent information as it\u0026rsquo;s updated in Amazon Redshift clusters.\nRedshift environment before implementing Data Sharing feature\nBoth, producer and consumer cluster size is 640 MB each - Total storage consumed is 1280 MB: Redshift environment after implementing Data Sharing feature\nProducer cluster size is 640 MB whereas consumer cluster size is 0 MB - Total storage consumed is 640 MB: Sustainability improvement process The improvement goals of this lab are to:\nTo eliminate waste, low utilization, and idle or unused resources To maximize the value from resources consumed This lab use case focuses on removing unneeded or redundant data, and minimizing data movement across network. For more details, refer to Sustainability Pillar Whitepaper which explains the iterative process that evaluates, prioritizes, tests, and deploys sustainability-focused improvements for cloud workloads.\nTo evaluate specific improvements, understand the resources provisioned by your workload to complete a unit of work. Evaluate potential improvements, and estimate their potential impact, the cost to implement, and the associated risks. To measure improvements over time, first understand what you have provisioned in AWS and how those resources are being consumed.\nRefer to Sustainability Pillar Whitepaper for detailed understanding around evaluating specific improvements. At high level:\nUse Proxy metrics to measure the resources provisioned to achieve business outcomes. (To derive metrics from AWS Cost and Usage reports check out this Well-Architected Lab )\nFor this lab, we will use these proxy metrics:\nTotal data storage used (MB provisioned) Total data transfer over network (MB transferred) To find out how much storage is used for us-west-1 region Redshift cluster, and how much data is transferred over the network between producer (us-east-1) and consumer (us-west-1) clusters across regions for data replication: Select business metrics to quantify the achievement of business outcomes. Your business metrics should reflect the value provided by your workload, for example, the number of simultaneous active users, API calls served, or the number of transactions completed. For this lab, we will use total number of events held (business outcome) as business metric.\nTo calculate a sustainability key performance indicator (KPI), we will use the following formula, divide the provisioned resources by the business outcomes achieved to determine the provisioned resources per unit of work:\nOur improvement goal is to:\nReduce total storage used, and data transfer over the network for all events Reduce per event provisioned resources X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Understand Amazon Redshift Data Sharing Prepare Amazon Redshift Producer Cluster Prepare Amazon Redshift Consumer Cluster Baseline Sustainability KPI Enable Amazon Redshift Data Sharing Amazon Redshift Consumer Cluster Validation Review Sustainability KPI Optimization Cleanup "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/","title":"Level 300: Optimize Data Pattern using Amazon Redshift Data Sharing","tags":[],"description":"","content":"Author Raman Pujani, Solutions Architect. Introduction This lab focuses on optimizing data patterns for sustainability, specifically focused on removing unneeded or redundant data, and minimizing data movement across networks.\nGoals At the end of this lab you will:\nUnderstand how to identify optimization areas using AWS Well-Architected Sustainability Pillar best practices Baseline and optimize workloads by identifying sustainability key performance indicators (KPIs) Learn how to use the Amazon Redshift Data Sharing feature to implement data management best practices described in AWS Well-Architected Sustainability Pillar Prerequisites The lab is designed to run in your own AWS account You can launch an Amazon Redshift cluster in the AWS us-east-1 and AWS us-west-1 regions (referred to us-east-1 region, and us-west-1 region throughout the lab) using Redshift ra3 nodes This lab is written and tested in us-east-1 and us-west-1 regions. You may be able to run this in other AWS regions of your choice where Redshift ra3 nodes are available , but results may vary. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Amazon Redshift ra3 nodes are not part of Amazon Redshift Free trial, or AWS Free Tier. When you decide to stop the lab at any point in time, please revisit the clean up instructions at the end so you stop incuring cost (e.g. for storage in Amazon S3).\nLab duration Estimated time required to complete this lab is 90 minutes.\nWorkload details AnyCompany (an fictional event management organization) is running a central data warehouse environment on Amazon Redshift in us-east-1 region, which is used by various departments in the organization for their respective storage, analytical processing, and reporting. The marketing department is the top consumer of the data warehouse, and have data engineers, analysts, and scientists based out of US west coast. The marketing team has implemented their own Amazon Redshift cluster in us-west-1 (consumer) region, which refreshes nightly using an Amazon Redshift snapshot received from us-east-1 region (producer) and uploading to the us-west-1 (consumer) region Amazon Redshift cluster. Since the marketing team analytical processing consumes lots of resources \u0026amp; integrated with their west coast based on-premise hosted downstream applications, they perform their analytical processing in us-west-1 region, and other departments use us-east-1 region hosted data warehouse. This requires storing a redundant dataset in us-west-1 region, and transferring huge amounts of data (via nightly ETL feed) over the network between AWS regions.\nThis is not a sustainability friendly implementation, and can be optimized using AWS Well-Architected Sustainability Pillar best practices for data patterns. Also, with this approach, the insights generated by the Marketing department are not based on live data.\nWorkload optimization for sustainability In this case, optimization areas include:\nMarketing team using only data which is required for their analytical processing, whereas currently they use the full dataset. This reduces storage requirements in us-west-1. By reducing the amount of data copied between us-east-1 and us-west-1 regions, this reduces network traffic. Technical solution By introducing Amazon Redshift Data Sharing feature, the marketing department can optimize their implementation for sustainability, avoiding redundant storage \u0026amp; reducing data transfer between AWS regions. Data sharing enables instant, granular, and fast data access across Amazon Redshift clusters without the need to copy or move it. With data sharing, you have live access to data, so that your users can see the most up-to-date and consistent information as it\u0026rsquo;s updated in Amazon Redshift clusters.\nRedshift environment before implementing Data Sharing feature\nBoth, producer and consumer cluster size is 640 MB each - Total storage consumed is 1280 MB: Redshift environment after implementing Data Sharing feature\nProducer cluster size is 640 MB whereas consumer cluster size is 0 MB - Total storage consumed is 640 MB: Sustainability improvement process The improvement goals of this lab are to:\nTo eliminate waste, low utilization, and idle or unused resources To maximize the value from resources consumed This lab use case focuses on removing unneeded or redundant data, and minimizing data movement across network. For more details, refer to Sustainability Pillar Whitepaper which explains the iterative process that evaluates, prioritizes, tests, and deploys sustainability-focused improvements for cloud workloads.\nTo evaluate specific improvements, understand the resources provisioned by your workload to complete a unit of work. Evaluate potential improvements, and estimate their potential impact, the cost to implement, and the associated risks. To measure improvements over time, first understand what you have provisioned in AWS and how those resources are being consumed.\nRefer to Sustainability Pillar Whitepaper for detailed understanding around evaluating specific improvements. At high level:\nUse Proxy metrics to measure the resources provisioned to achieve business outcomes. (To derive metrics from AWS Cost and Usage reports check out this Well-Architected Lab )\nFor this lab, we will use these proxy metrics:\nTotal data storage used (MB provisioned) Total data transfer over network (MB transferred) To find out how much storage is used for us-west-1 region Redshift cluster, and how much data is transferred over the network between producer (us-east-1) and consumer (us-west-1) clusters across regions for data replication: Select business metrics to quantify the achievement of business outcomes. Your business metrics should reflect the value provided by your workload, for example, the number of simultaneous active users, API calls served, or the number of transactions completed. For this lab, we will use total number of events held (business outcome) as business metric.\nTo calculate a sustainability key performance indicator (KPI), we will use the following formula, divide the provisioned resources by the business outcomes achieved to determine the provisioned resources per unit of work:\nOur improvement goal is to:\nReduce total storage used, and data transfer over the network for all events Reduce per event provisioned resources X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Understand Amazon Redshift Data Sharing Prepare Amazon Redshift Producer Cluster Prepare Amazon Redshift Consumer Cluster Baseline Sustainability KPI Enable Amazon Redshift Data Sharing Amazon Redshift Consumer Cluster Validation Review Sustainability KPI Optimization Cleanup "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/","title":"Level 300: Turning Cost &amp; Usage Reports into Efficiency Reports","tags":[],"description":"","content":"Authors Steffen Grunwald, Principal Solutions Architect. Thomas Attree, Solutions Architect. Introduction Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Video transcript Hello and welcome to this Well Architected Lab, my name is Thomas and I’m a Solutions Architect, here at AWS.\nThe outcomes of this lab are to give you an overview of what proxy metrics are and what you need to think about when selecting them.\nIdentifying proxy metrics and using them from your AWS cost and usage report.\nPreparing this data in a way that can be queried for dashboarding later. And learning how to bring your own data in the form of assumptions.\nNow, you may have seen the shared responsibility for security as part of the Well Architected labs on security. Well, AWS also has a shared responsibility with our customers for sustainability.\nWe think about this responsibility in two ways, sustainability of the cloud, which is the responsibility of AWS and this covers our own operational responsibilities. AWS designs buildings, rooms, servers and takes care of it from construction to recycling AWS purchases the energy and ensure that energy and other resources such as water for cooling are used efficiently. And lastly, AWS Service Teams run managed services and optimise them for sustainability\nThen we have sustainability in the cloud, and this is the customer responsibility. This responsibility is what this lab and others within the well architected labs site will focus on. Sustainability in the cloud pertains to how you design and run your software, architect your infrastructure and maximise utilisation.\nSo how can you optimise for sustainability?\nThe answer is the same as you always have with, especially with the cost and performance efficiency pillars.\nThe well architected pillar outlines an iterative process, similar to how customers have improving for e.g. for cost or performance for decades.\nYou need to be aware of your goals (or set them) and your current performance against your KPIs.\nFirst, you set a goal for your company, then you look at all of the workloads you can optimise,\nThen you prioritise by the workloads which will have the biggest impact, for which you will need data and metrics, and are a good investment in terms of time for optimisation. Next you hypothesise what can you optimise for the workload, can I change the processor to Graviton or can I implement autoscaling? Does this instance need to be on all of the time or can I perform the work as a batch? Or can I re-write components in a more efficient language such as rust?\nThese hypotheses then need to be qualified and prioritised by how long will it take, how much will it cost, what risks are associated with it?\nThen experiment, measure the impact, rollback if the outcome is not desirable or deploy the change to production.\nThen move onto the next hypothesis by iterating through the flywheel again.\nThrough working with customers, we’ve found that they are very good at optimisation. They’ve been optimising inside and outside of the cloud for decades and as long as you can provide a metric to an engineer, they can optimise towards that metric.\nBut what could be a good metric to optimise for sustainability?\nThere are a few that are useful for optimisations and we call those proxy metrics\nThey are aligned broadly around utilisation of storage, network and compute.\nCost can also be considered but can sometimes be a poor proxy metric for sustainability because of differing pricing structures across regions, and doesn’t discriminate between a heavily utilised EC2 instance and an idle EC2 instance for example.\nWhen you select a metric to optimise for, you’ll actually be selecting many as they compete with each other.\nThink about a use case for generating thumbnails for uploaded user content, maybe you could generate this thumbnail at upload time, but it never requested, optimising storage by using less. Or you could generate the thumbnails when requested, duplicating compute work but minimising storage.\nOptimising for these metrics also competes with traditional non-functional requirements, such as availability or data retention policies.\nAn example being using an application deployed on EC2 servers across two availability zones. If one instance were to fail, your application can be served by the EC2 instance in the second AZ, keeping availability high. This however trades off utilisation as the two instances are unlikely to be running at a high load whilst still having capacity to absorb the additional load of the failed instance.\nWhen selecting metrics to optimise for sustainability, you also need to be aware of your non-functional requirements and determine where you can make a trade off.\nWe see customers making architectural changes to optimise for sustainability and considering the tradeoffs I just mentioned.\nAnother example is making use of service features you might not be making use of today, such as using colder tiers of S3 storage.\nFor example, can can you trade off availability, in the case of one zone IA, or trade off object retrieval time with with even colder storage tiers in the glacier family.\nUsing one zone IA optimises for storage and data transfer, as AWS does not need to duplicate the object across availability zones, eliminating the cross AZ traffic, and removing the need to store the object on hardware in additional AZ’s.\nOr you might simply determine that you don’t need that data anymore and you implement mechanisms to either lifecycle data through these tiers, or delete it completely.\nRemember, the greenest energy is the energy you don’t use.\nIn these labs, you will use Amazon Athena to discover proxy metrics from your cost and usage reports.\nYou will then in part 2 add your own assumptions to these metrics, with examples including a preference for a region you determine to have a cleaner energy grid.\nYou’ll then learn in part 3 how to add these assumptions as IaC with the AWS CDK.\nNow it’s time to get started! Enjoy the labs and have fun!\nGoals At the end of this lab you will:\nUnderstand the need for proxy metrics for sustainability and identify candidates Draw these proxy metrics from your AWS Cost \u0026amp; Usage Report (or sample data) and prepare the data ready for dashboarding with e.g. Amazon QuickSight Learn how you can add your own data and combine it with the AWS Cost \u0026amp; Usage Reports Prerequisites The lab is designed to run in your own AWS account. Pick a region where Amazon Athena is available . If you have existing AWS Cost \u0026amp; Usage Report (CUR) data in a bucket, you should run the lab in the region of this bucket. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . When you decide to stop the lab at any point in time, please revisit the clean up instructions at the end so you stop incuring cost (e.g. for storage in Amazon S3).\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Prepare CUR Data Discover CUR data with Amazon Athena Query your Amazon S3 usage by storage class Install the AWS Cost and Usage Queries from the AWS Serverless Application Repository Add your own assumptions (using the Amazon Athena console) Add your own assumptions (using infrastructure as code) Cleanup "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/","title":"Level 300: Autonomous Monitoring Of Cryptographic Activity With KMS","tags":[],"description":"","content":"Authors Tim Robinson, Well-Architected Geo Solutions Architect. Stephen Salim, Well-Architected Geo Solutions Architect. Introduction The ability to provide traceability and automatically react to security events occurring in your workload is vital to maintaining a high security posture within your application. Through the use monitoring key activity metrics, architects are able to detect potentially malicious behaviour at an early stage and respond according to the event. By combining this early detection approach with appropriate alerting, we can create an autonomous feedback loop which ensures that cloud administrators are adequately informed before a serious event takes place.\nIn AWS you can use AWS CloudTrail to capture all API based activity within an AWS account. However, simply capturing these activities is not be sufficient without the ability to contextualize events and create the mechanism to automatically react in an appropriate manner. The integration of Amazon CloudWatch in combination with CloudTrail and our other services, allows customers to produce adequate alerting and visibility to important system events triggered by a key activity metric.\nOne such example of a key activity metric would be Key Management Service (KMS) activity. KMS is integral to most secure architecture designs and responsible for autonomous encryption and decryption activity. Whilst we would expect regular activity which is triggered as a byproduct from user interaction with an architecture, significantly high activity could be an early warning signal that the architecture could be subject to data exfiltration by an interested third party or competitor.\nIn this lab we will walk you through an example scenario of monitoring our KMS service for encryption and decryption activity. We will autonomously detect abormal activity beyond a predefined threshold and respond accordingly, using the following services:\nAWS CloudTrail - Used for capturing API events within the environment. Amazon CloudWatch Log Groups - Used to log our CloudTrail API events. Amazon CloudWatch Metric Filter to create apply filter so we can measure the only the events that matters for us. Amazon CloudWatch Alarms to allow our system to react against pre created events Amazon Simple Notification Service to allow us to send email notification when an event occurs. Our lab is divided into several sections as follows:\nDeploy the lab base infrastructure. Configure the ECS repository and deploy the application stack. Configure the workload logging and alarm. Testing the workload functionality. We have included CloudFormation templates for the first few steps to get your started, and also provide optional templates for the rest of the lab so you can choose between creating the monitoring resources via cloudformation or manually through the console.\nNote: For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. Please ensure all lab interaction is completed from this region.\nGoals Analyze CloudTrail and target specific API events. Integrate CloudTrail events with A CloudWatch Log Group to record the event. Apply appropriate metric filters and alarms to trigger an event. Integrate SNS to respond appropriately to the event. Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. If you want to follow the command line instructions, you should have the AWS command line installed and configured. Please follow this guide In this Lab we will be creating a local docker image, please ensure you have docker installed in your machine, and you are running Docker version 18.09.9 or above. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nSteps: Deploy The Lab Base Infrastructure Configure ECS Respository and Deploy The Application Stack Configure CloudTrail Configure The Workload Logging and Alarm Testing the Workload Functionality Teardown "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/1_create_static_resources_source/","title":"Create Static Resources","tags":[],"description":"","content":"Create Resource To deploy the resource, you need for this lab you have three options. You can deploy using a AWS CloudFormation template or a Terraform module, either allows you to complete the lab in less than half the time as the standard setup. Or, you can deploy manually if you do not have access to deploy using CloudFormation or would like hands one experience going through the steps.\nClick here to continue with the CloudFormation Advanced Setup Create the Organization data collector using a CloudFormation Template Console This section is optional and automates the creation of the AWS organizations data collection using a CloudFormation template. You will require permissions to modify CloudFormation templates, create an IAM role, S3 Bucket, Lambda and create a Glue Crawler. If you do not have the required permissions skip over this section to continue using the standard setup.\nClick Launch CloudFormation Template if you are deploying to your linked account (recommended)\nClick Launch CloudFormation Template if you wish to deploy straight into your management account\nInput the stack name as Organization-data-collector-stack. Then filled in the Parameters. Click Next. DatabaseName - Athena Database name where you table will be created DestinationBucket - The name you would like of the bucket that will be created in to hold org data, you will need to use a with cost at the start, (we have used cost-aws-lab-organisation-bucket) ManagementAccountId - Your Management Account Id where your Optimization is held Tags - List of tags from your Organization you would like to include separated by a comma. Scroll down and click Next Scroll down and tick the box acknowledging that this will create and IAM Role. Click Create stack Wait for the CloudFormation to deploy, this can be seen when it has CREATE_COMPLETE under the stack name. Repeat the above steps in your Management account using the Management.yaml template to deploy an IAM Role to allow the lambda to pull data into the Cost Optimization account. If you cannot deploy a CloudFormation into your management account please see the Create IAM Role and Policies in Management account Step further down this page to create manually.\nNow go back to your linked account and find your deployed CloudFormation template. Select your stack and click on Resources and find the lambda function LambdaOrgData and click on the link to take you to the lambda. Test Lambda Function Now that you have deployed the CloudFormation, you must test your Lambda function to get your first set of data in Amazon S3.\nTo test your lambda function click Test Enter an Event name of Test, click Create:\nClick Test\nThe function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in Organizations file size over time:\nGo to your S3 bucket and into the organisation-data folder and you should see a file of non-zero size: Go to the Glue Service page: Now that you have deployed your CloudFomation, jump to step 11 on Create Glue Crawler on Utilize Organization Data Source page to run your Glue crawler which will create your Athena table.\nClick here to continue with the Terraform Advanced Setup Create the Organization data collector using Terraform There is an AWS Github Repo which has a module to deploy all the resources needed in this lab. Please deploy using the instructions in the github repo then return to the step below.\nTest Lambda Function Now you have deployed the Terraform then you can test your lambda to get your first set of data in Amazon S3.\nGo to the Lambda service page : Search for your new function called Lambda_Org_Data and click on it. To test your lambda function click Test Enter an Event name of Test, click Create:\nClick Test\nThe function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in Organizations file size over time:\nGo to your S3 bucket and into the organisation-data folder and you should see a file of non-zero size is in it: Go to the Glue Service page: Now you have deployed your Terraform jump to step 11 on Create Glue Crawler on Utilize Organization Data Source page to run your crawler to create your athena table.\nClick here to continue with Creating Resources Manually Create the Organization data collector manually Create Amazon S3 Bucket and Folders We’ll create an S3 bucket to store the organizations data to be combined with your cost and usage report. This will hold your organisation data so we can connect it to Athena.\nLogin via SSO to your Cost Optimization account, go into the S3 console: Click Create bucket and create a bucket. You will need to use a unique bucket name with cost at the start, (we have used cost-aws-lab-organisation-bucket). Make a note of this as we will be using it later. Create IAM Role and Policies We’ll create an IAM role and policy for the AWS Lambda function to access the organizations data \u0026amp; write it to S3. This role will be used to get the list of accounts in the Organization and the meta data attached to them such as name and email. This is then placed in our S3 bucket.\nGo to the IAM Console\nSelect Policies and Create policy\nOn the JSON tab the following policy and replace (bucket name) with your bucket name from before and replace (account id) with your Management Account id which manages your Organization. Enter the following policy, click Review policy:\n{ \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[ { \u0026quot;Sid\u0026quot;:\u0026quot;S3Org\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:DeleteObjectVersion\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;arn:aws:s3:::(bucket name)/*\u0026quot; }, { \u0026quot;Sid\u0026quot;:\u0026quot;OrgData\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;:[ \u0026quot;organizations:ListAccountsForParent\u0026quot;, \u0026quot;organizations:ListRoots\u0026quot;, \u0026quot;organizations:ListCreateAccountStatus\u0026quot;, \u0026quot;organizations:ListAccounts\u0026quot;, \u0026quot;organizations:ListTagsForResource\u0026quot;, \u0026quot;organizations:DescribeOrganization\u0026quot;, \u0026quot;organizations:DescribeOrganizationalUnit\u0026quot;, \u0026quot;organizations:DescribeAccount\u0026quot;, \u0026quot;organizations:ListParents\u0026quot;, \u0026quot;organizations:ListOrganizationalUnitsForParent\u0026quot;, \u0026quot;organizations:ListChildren\u0026quot; ], \u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot; }, { \u0026quot;Sid\u0026quot;:\u0026quot;Logs\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:*:*:*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;assume\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::(account id):role/OrganizationLambdaAccessRole\u0026quot; } ] } Fill in the following\nPolicy Name LambdaOrgPolicy Description Access to S3 for Lambda function to collect organization data Click Create policy Select Roles, click Create role Select Lambda, click Next: Permissions: Type lambda into the search and select the LambdaOrgPolicy policy, click Next: Tags Click Next: Review\nRole name LambdaOrgRole, click Create role:\nCreate IAM Role and Policies in Management account As we need to pull the data from the Management account we need to allow our role to do this.\nLog into your Management account\nGo to the IAM Console\nSelect Policies and Create policy. Copy steps 2 - 4 from above to create the below policy called ListOrganizations.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;OrgData\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;organizations:ListAccountsForParent\u0026quot;, \u0026quot;organizations:ListRoots\u0026quot;, \u0026quot;organizations:ListCreateAccountStatus\u0026quot;, \u0026quot;organizations:ListAccounts\u0026quot;, \u0026quot;organizations:ListTagsForResource\u0026quot;, \u0026quot;organizations:DescribeOrganization\u0026quot;, \u0026quot;organizations:DescribeOrganizationalUnit\u0026quot;, \u0026quot;organizations:DescribeAccount\u0026quot;, \u0026quot;organizations:ListParents\u0026quot;, \u0026quot;organizations:ListOrganizationalUnitsForParent\u0026quot;, \u0026quot;organizations:ListChildren\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Select Roles, click Create role Choose Another AWS account, and enter your sub account id which is where we started the lab, Next: Permissions Search for Organizations and select the ListOrganizations policy you just made. Click Next: Tags then click Next: Review\nRole name OrganizationLambdaAccessRole, click Create role:\nSearch for your new role in the roles page and click on the role name. Click on Trusted relationships tab then Edit trusted relationship On the JSON tab the replace the current json with the following policy and replace (sub account id) with your sub account id from before, click Update Trust policy:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(sub account id):role/LambdaOrgRole\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } Now you have completed this section there is a bonus part where you can check the Tags on your AWS Accounts. These can be used in the lambda if you wish.\nClick here to see the Bonus Check AWS Organizations Tags Bonus Check AWS Organizations Tags In the next step we will be setting up a lambda to pull the data from your AWS Organization. If you wish to pull your tags from this data too then follow these steps to see your tags.\nIf you can use the AWS CLI then you can run the below command in your terminal where you have access to your management account to see an individual accounts tags:\naws organizations list-tags-for-resource --resource-id (account id) Bonus Check AWS Organizations Tags from Console From your Amazon console on the top right of your screen click on the drop down and chose My Organization. Open AWS accounts and you will see your Organization and your Organization structure. This holds your Organizational Units and your Accounts. Click on an account name that you wish to see the tags for. At the bottom of the page, you will see the Tags section. Make a note the Tags Keys in the left-hand box which you wish to collect to use with the CUR in the next step. Note, these are case sensitive so we recommend copy and pasting them. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/1_grant_permissions/","title":"Grant permissions to your accounts in your AWS Organization","tags":[],"description":"","content":"Permissions We will need to install 2 IAM roles to ensure DataCollection account can collect information accross all accounts in the AWS Organization.\nOne Role WA-Lambda-Assume-Role-Management-Account for read only access from Data Collection account to the Management account. A second read only role must be installed in each Linked accout of Organization via a StackSet. 1/2 Role for Management Account Some of the data needed for the modules is in the Management account we will now create a read only role to assume into that account to get the data.\nLog into your Management account then click Launch CloudFormation Template Call the Stack OptimizationManagementDataRoleStack\nIn the Parameters section set CostAccountID as the ID of Cost Optimization Data Collection Accoint ( where you plan to deploy the OptimizationDataCollectionStack)\nScroll to the bottom and click Next\nTick the acknowledge boxes and click Create stack.\nYou can see the role that was collected by clicking on Resources and clicking on the hyperlink under Physical ID. 2/2 Read Only roles for Data Collector modules Modules that we will deploy later OptimizationDataCollectionStack allow to collect data from all of the accounts in an AWS Organization. We will use a CloudFormation StackSet to deploy a single read only role to all accounts.\nDownload CloudFormation by clicking here. This will be the foundation of the rest of the lab and we will add to this to build out the modules so please store somewhere safely as there is not designer in StackSets.\nLogin to your Management account and search for Cloud Formation Click on the hamburger icon on the side panel on the left hand side of the screen and select StackSets. If you have not enabled this Click the button Enable trusted access. Once Successful or if you have it enabled already click Create StackSet.\nChoose Template is ready and Upload a template file and upload the optimisation_read_only_role.yaml file you downloaded from above. Click Next.\nCall the Stack OptimizationDataRoleStack. In the Parameters section for CostAccountID use the Account ID that where you will deploy the OptimizationDataCollectionStack. Under available modules section select modules that you need. This CloudFormation StackSet will provision required roles for modules in linked accounts. Detailed description of each module can be found here Leave all as default and Click Next. Select Deploy to organization and choose the region you are currently deploying to. Tick the boxes and click Create stack. This role will now be deployed to all linked accounts.\n(Optional) Read Only roles in Management Account If you wish to also access data in your management account, deploy the same CloudFormation stack as a normal stack in your management account as you did in the Role for Management Account step above.\nTo do this follow these instructions Log into your Management account then click Launch CloudFormation Template Call the Stack OptimizationDataRoleStack. In the Parameters section use the Cost Optimization Account ID that you deployed the OptimizationDataCollectionStack into for CostAccountID. Under available modules section select modules which you you selected in OptimizationDataCollectionStack deployment step. This CloudFormation StackSet will provision required roles for modules in linked accounts. Detailed description of each module can be found here Scroll to the bottom and click Next\nTick the box \u0026lsquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names.\u0026rsquo; and click Create stack. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/1_create_s3/","title":"Create S3 bucket","tags":[],"description":"","content":"Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 .\nOpen the Amazon S3 console at https://console.aws.amazon.com/s3/ . From the console dashboard, choose Create bucket. Enter a Bucket name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Accept default value for Block all public access as CloudFront will serve the content for you from S3. Enable bucket versioning , to keep multiple versions of an object so you can recover an object if you unintentionally modify or delete it. Click Create bucket. "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/","title":"100 Level Foundational Labs","tags":[],"description":"","content":"List of labs available AWS Account Setup and Root User Creating your first Identity and Access Management User, Group, Role CloudFront with S3 Bucket Origin Enable Security Hub Create a Data Bunker Account "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/","title":"200 Level Intermediate Labs","tags":[],"description":"","content":"List of labs available Automated Deployment of Detective Controls Automated Deployment of EC2 Web Application Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Automated Deployment of Web Application Firewall Level 200: Automated IAM User Cleanup Level 200: Basic EC2 Web Application Firewall Protection Level 200: AWS Certificate Manager Request Public Certificate Level 200: CloudFront for Web Application Level 200: CloudFront with WAF Protection Level 200: Remote Configuration, Installation, and Viewing of CloudWatch logs "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_aws_account_and_root_user/1_intro/","title":"Account Settings &amp; Root User Security","tags":[],"description":"","content":"When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account.\nIt is strongly recommended that you only use the root user by exception. Instead, adhere to the best practice of using the root user only to setup identity federation using AWS Single Sign-On or an identity provider configured in IAM. To view the tasks that require root login you need to sign in as the root user, see AWS Tasks That Require Root User .\nIf you don’t have an existing organizational structure with AWS Organizations , AWS Control Tower is the easiest way to get started. For more information see Security Foundations and Identity and Access Management in the AWS Well-Architected security whitepaper.\n1.1 Generate and Review the AWS Account Credential Report Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations:\nOn a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you\u0026rsquo;ve added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account\u0026rsquo;s security configuration, follow these guidelines:\nBe thorough. Look at all aspects of your security configuration, including those you might not use regularly. Don\u0026rsquo;t assume. If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple. To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console:\nSign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html 1.2 Enable a Virtual MFA Device for Your AWS Account Root User You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials.\nIf your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can\u0026rsquo;t sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support .\nUse your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following:\nOption 1: Click Dashboard, and under Security Status, expand Activate MFA on your root user.\nOption 2: On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page.\nClick Manage MFA or Activate MFA, depending on which option you chose in the preceding step.\nIn the wizard, click A virtual MFA device and then click Next Step.\nConfirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes.\nWith the Manage MFA Device wizard still open, open the virtual MFA app on the device.\nIf the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device).\nThe easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually.\nTo use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device\u0026rsquo;s camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important\nMake a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account.\nNote\nThe QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device.\nThe device starts generating six-digit numbers.\nIn the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that\u0026rsquo;s currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box.\nImportant\nSubmit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device.\nClick Next Step, and then click Finish.\nThe device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page .\n1.3 Configure Account Security Challenge Questions Configure account security challenge questions because they are used to verify that you own an AWS account.\nUse your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update. 1.4 Configure Account Alternate Contacts Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable.\nUse your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update. 1.5 Remove Your AWS Account Root User Access Keys You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key.\nCheck in the credential report; if you don\u0026rsquo;t already have an access key for your AWS account, don\u0026rsquo;t create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account\u0026rsquo;s email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone. 1.6 Periodically Change the AWS Account Root User Password You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys .\nTo change the password for the root user:\nUse your AWS account email address and password to sign in to the AWS Management Console as the root user.\nNote\nIf you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password.\nIn the upper right corner of the console, click your account name or number and then click My Account.\nOn the right side of the page, next to the Account Settings section, click Edit.\nOn the Password line choose Click here to change your password.\nChoose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user.\nAWS requires that your password meet these conditions:\nhave a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ \u0026amp; * () \u0026lt;\u0026gt; [] {} | _ + - = symbols not be identical to your AWS account name or email address Note\nAWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password.\nTo protect your password, it\u0026rsquo;s important to follow these best practices:\nChange your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained. 1.7 Configure a Strong Password Policy for Your Users You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users\u0026rsquo; passwords. The IAM password policy does not apply to the AWS root account password.\nTo create or change a password policy:\nSign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy. "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_enable_security_hub/1_intro/","title":"Enable AWS Security Hub","tags":[],"description":"","content":"Table of Contents Getting Started 1. Getting Started The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub.\n1.1 Enable AWS Config AWS Security Hub requires AWS Config to run within your account.\nIf you have not enabled AWS Config, we\u0026rsquo;ll need to enable that now. If it\u0026rsquo;s already enabled in your account, you can skip to the next step. Navigate to the AWS Config console and select 1-click setup and then select Confirm.\nOnce successful, you\u0026rsquo;ll see this Welcome to AWS Config page. 1.2 AWS Security Hub Once you have logged into your AWS account and enabled AWS Config, we need to enable Security Hub. Navigate to the AWS Security Hub console .\nAlternatively, you can just search for Security Hub and select the service. 1.3 Enable AWS Security Hub In the AWS Security Hub service console you can click on the Go to Security Hub orange button to navigate to AWS Security Hub in your account.\nAdditional information is provided regarding Security standards and AWS Integrations. You can read more here . Now select Enable Security Hub.\n1.4 Explore AWS Security Hub NOTE: Because Security Hub is a Regional service, the checks performed for this control only apply to the current Region for the account. It must be enabled separately for each region.\nWith AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers. Once you enable, it may take up to two hours or more to see results from the security checks. You might see this banner below. If you forgot to enable AWS Config, you might see this banner. "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_aws_account_and_root_user/","title":"AWS Account Setup and Root User","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through the introductory steps to configure contacts for your AWS account and secure the root user. This process is not required for accounts you manage with AWS Organizations or AWS Control Tower . For more information see Security Foundations in the AWS Well-Architected security whitepaper . For accounts used for business purposes it is recommended to start with AWS Control Tower. For personal or invidual accounts you can follow the steps in this unofficial video from the author of this lab.\nPrerequisites An AWS account that you are able to use for testing. Root user login. Costs There are no costs for this lab AWS Pricing Steps Account Settings \u0026amp; Root User Security Tear down References \u0026amp; Useful Resources AWS Tasks That Require Root User Credential Report AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/1_deploy_infrastructure/","title":"Deploy the Infrastructure","tags":[],"description":"","content":"Many workloads depend on external resources or services for data or additional capabilities such as 3rd party data providers or service providers, DNS providers, etc. Functionality or outcomes of the workload may be at risk when dependent resources or services become degraded or unreachable.\nMonitoring these dependencies will enable quick action to ensure business continuity is not affected. Setting up alerting and notifications will ensure that appropriate team members are aware of issues and can take action to address the situation.\nThis lab provides examples of how to implement Well-Architected Operational Excellence best practices such as “Implement dependency telemetry”, “Alert when workload outcomes are at risk”, and “Enable push notifications”.\nIn this lab there is an external service (3rd party data provider) that provides data which will be consumed by the workload. This has been emulated in this lab by using an EC2 instance which acts as the 3rd party data provider, and it writes data to an S3 bucket at 50 second intervals. Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket.\nFor this use-case the notification has been configured on the S3 bucket to invoke a lambda function after every write to the bucket, using the S3 PutObject API. The objective of this lab is to create awareness when an external service is experiencing downtime or is otherwise impaired. For this example the assumption is that the 3rd party data provider is experiencing downtime when data is no longer being written to the S3 bucket.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the infrastructure using AWS CloudFormation You will use AWS CloudFormation to provision resources that will emulate the workload described in the use-case. AWS CloudFormation provides you a common language to model and provision AWS and third party application resources by applying Infrastructure as Code in your cloud environment.\nDownload the dependency_monitoring.yaml CloudFormation template (right-click on the link and select \u0026ldquo;Save Link As\u0026hellip;\u0026rdquo;)\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources (standard)\nLeave Prepare template setting as-is since you already have a template ready (dependency_monitoring.yaml)\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: dependency_monitoring.yaml Click Next\nFor Stack name use Dependency-Monitoring-Lab\nParameters\nAvailabilityZone - select one of the availability zones from the list BucketName - enter a name for the S3 bucket that will be created as part of the lab. Amazon S3 bucket names are globally unique, and the namespace is shared by all AWS accounts, so make sure you name the bucket as uniquely as possible. For example - wa-lab-\u0026lt;your last name\u0026gt;-\u0026lt;date\u0026gt;\u0026lt;time\u0026gt;. LatestAmiId - leave the default value here. This will ensure that CloudFormation will retrieve the latest Amazon Linux AMI for the region you are launching the stack in. NotificationEmail - specify an email address that you have access to. This is the email address that notifications related to the dependent service will be sent to. Click Next For Configure stack options click Next\nOn the Review page:\nScroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here . Note: The template creates 2 roles for Lambda as well as a role and instance profile for an EC2 instance. They are the minimum permissions necessary to read and write from an S3 bucket created as part of this lab and create an OpsItem in OpsCenter. These permissions can be reviewed in the CloudFormation template under \u0026ldquo;Resources\u0026rdquo; section - DataReadLambdaRole, OpsItemLambdaRole, and InstanceRole.\nClick Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows (in reverse order) the activities performed by CloudFormation, such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. Once stack creation starts, monitor the email address you entered. You should receive an email from SNS with the subject AWS Notification - Subscription Confirmation. Click on the link Confirm subscription to confirm the subscription of your email to the SNS Topic. This will allow SNS to send email notifications to the email address specified.\nThe stack takes about 3 mins to create all the resources. Periodically refresh the page until you see that the Stack Status is in CREATE_COMPLETE. The stack creates the following resources:\nA new VPC, subnets, Internet Gateway, Route tables to host the workload in An EC2 instance that acts as the 3rd party data provider An S3 bucket and Lambda function that act as the workload IAM resources (roles, policies) that allow different services to access each other An SNS Topic for notifications Once the stack is in CREATE_COMPLETE, visit the Outputs section for the stack and note down the Key and Value for each of the outputs. This information will be used later in the lab.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/","title":"Cost &amp; Usage Report Dashboards","tags":[],"description":"","content":"Last Updated April 2022\nFeedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: cloud-intelligence-dashboards@amazon.com Get Help Ask your questions on re:Post and get answers from our team, other AWS experts, and other customers using the dashboards.\nSubscribe to our YouTube channel to see guides, tutorials, and walkthroughs on all things Cloud Intelligence Dashboards.\nIntroduction The dashboards in this section use data exclusively from the AWS Cost and Usage Report. For explanations on each field and visual found in the Cost Intelligence Dashboard (as well as some FAQs) download and read the CID User Guide The AWS Cost \u0026amp; Usage Report (CUR) contains the most comprehensive set of AWS cost and usage data available, including additional metadata about AWS services, pricing, Reserved Instances, and Savings Plans. The CUR itemizes usage at the account or Organization level by product code, usage type and operation. These costs can be further organized by enabling Cost Allocation tags and Cost Categories. The AWS Cost \u0026amp; Usage Report is available at an hourly, daily, or monthly level of granularity.\nThese dashboards and their content: (a) are for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS content, products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. We recommend validating your data by comparing the aggregate un-grouped Payer and Linked Account spend for a prior month. Customers are responsible for making their own independent assessment of these dashboards and their content.\nGoals Create the Cost Intelligence dashboard\nCreate the CUDOS dashboard\nCreate the KPI dashboard\nCreate Additional Dashboards\nData Transfer Dashboard Trends Dashboard Distribute your dashboards in your organization\nPermissions Permission to access Amazon Athena, AWS Glue, the Amazon S3 bucket where the CUR lives, and Amazon QuickSight via both the console and the Command Line Tool. Permissions for CloudFormation is helpful but not required. You can download and apply to IAM this set of minimal permissions (https://github.com/aws-samples/aws-cudos-framework-deployment/blob/main/assets/minimal_permissions.json ) Detailed permissions are shared in the next step\nCosts A QuickSight Enterprise license starts at $18 per month. Incremental costs associated with AWS Glue, Amazon Athena, and Amazon S3. Estimated total cost for all Dashboards together in a large AWS deployment is $54 per month. Time to complete Approximately 45-60 minutes to onboard all CUR dashboards in this section manually. If using the optional automation steps, setup should take approximately 15-30 minutes to complete.\nSteps: Prerequisites Cost Intelligence Dashboard CUDOS Dashboard KPI Dashboard Additional Dashboards Additional Dashboards include the Data Transfer Dashboard and Trends Dashboard\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/1_pricing_sources/","title":"Create Pricing Data Source","tags":[],"description":"","content":" If you have RHEL usage in your CUR you can skip this step, this step sets up a provided Cost and Usage report with RHEL usage for analysis.\nCreate the pricing data source We will create a data source with approximately 24 hours of usage. This is a sample data source which contains multiple workloads, which is representative of running small web server applications.\nCreate the pricing data Log into the console via SSO.\nGo to the S3 service dashboard\nCreate a bucket, with a name starting with cost-\nGo into the bucket and create 2 folders, before and after: Go into the before folder and upload the following file: Code/BeforeCUR.gz Go into the after folder and upload the following file: Code/AfterCUR.gz You now have your sample usage files ready to be setup.\nSetup Athena Go into the Athena service dashboard\nCreate the costmaster database if it does not exist, copy and paste the following command:\ncreate database if not exists costmaster; Create the before table, modify the location line at the bottom, in the query below by replacing (bucketname) with the name of your bucket, and paste the following query into Athena:\nCreate before table - Athena query CREATE EXTERNAL TABLE if not exists `costmaster.before`( `identity_line_item_id` string, `identity_time_interval` string, `bill_invoice_id` string, `bill_billing_entity` string, `bill_bill_type` string, `bill_payer_account_id` bigint, `bill_billing_period_start_date` string, `bill_billing_period_end_date` string, `line_item_usage_account_id` bigint, `line_item_line_item_type` string, `line_item_usage_start_date` string, `line_item_usage_end_date` string, `line_item_product_code` string, `line_item_usage_type` string, `line_item_operation` string, `line_item_availability_zone` string, `line_item_resource_id` string, `line_item_usage_amount` double, `line_item_normalization_factor` double, `line_item_normalized_usage_amount` double, `line_item_currency_code` string, `line_item_unblended_rate` double, `line_item_unblended_cost` double, `line_item_blended_rate` double, `line_item_blended_cost` string, `line_item_line_item_description` string, `line_item_tax_type` string, `line_item_legal_entity` string, `product_product_name` string, `product_availability` string, `product_capacitystatus` string, `product_clock_speed` string, `product_current_generation` string, `product_database_engine` string, `product_dedicated_ebs_throughput` string, `product_deployment_option` string, `product_description` string, `product_durability` string, `product_ecu` string, `product_edition` string, `product_engine_code` string, `product_enhanced_networking_supported` string, `product_event_type` string, `product_free_query_types` string, `product_from_location` string, `product_from_location_type` string, `product_group` string, `product_group_description` string, `product_instance_family` string, `product_instance_type` string, `product_instance_type_family` string, `product_license_model` string, `product_location` string, `product_location_type` string, `product_max_iops_burst_performance` string, `product_max_iopsvolume` bigint, `product_max_throughputvolume` string, `product_max_volume_size` string, `product_memory` string, `product_message_delivery_frequency` string, `product_message_delivery_order` string, `product_min_volume_size` string, `product_network_performance` string, `product_normalization_size_factor` double, `product_operating_system` string, `product_operation` string, `product_physical_processor` string, `product_pre_installed_sw` string, `product_processor_architecture` string, `product_processor_features` string, `product_product_family` string, `product_queue_type` string, `product_region` string, `product_servicecode` string, `product_servicename` string, `product_sku` string, `product_storage` string, `product_storage_class` string, `product_storage_media` string, `product_subscription_type` string, `product_tenancy` string, `product_to_location` string, `product_to_location_type` string, `product_transfer_type` string, `product_usagetype` string, `product_vcpu` bigint, `product_version` string, `product_volume_type` string, `pricing_lease_contract_length` string, `pricing_offering_class` string, `pricing_purchase_option` string, `pricing_rate_id` bigint, `pricing_public_on_demand_cost` string, `pricing_public_on_demand_rate` double, `pricing_term` string, `pricing_unit` string, `reservation_amortized_upfront_cost_for_usage` double, `reservation_amortized_upfront_fee_for_billing_period` double, `reservation_effective_cost` double, `reservation_end_time` string, `reservation_modification_status` string, `reservation_normalized_units_per_reservation` string, `reservation_number_of_reservations` string, `reservation_recurring_fee_for_usage` double, `reservation_reservation_a_r_n` string, `reservation_start_time` string, `reservation_subscription_id` bigint, `reservation_total_reserved_normalized_units` string, `reservation_total_reserved_units` string, `reservation_units_per_reservation` string, `reservation_unused_amortized_upfront_fee_for_billing_period` double, `reservation_unused_normalized_unit_quantity` double, `reservation_unused_quantity` double, `reservation_unused_recurring_fee` double, `reservation_upfront_value` double, `resource_tags_aws_autoscaling_group_name` string, `resource_tags_aws_created_by` string, `resource_tags_aws_ec2spot_fleet_request_id` string, `resource_tags_user_cost_center` string, `resource_tags_user_department` string, `resource_tags_user_environment` string, `resource_tags_user_name` string, `resource_tags_user_workload` string, `resource_tags_user_workload_type` string, `product_category` string, `resource_tags_user_tag21_nov` string, `resource_tags_user_application` string, `resource_tags_user_tier` string, `product_content_type` string, `product_granularity` string, `product_origin` string, `product_recipient` string, `product_volume_api_name` string, `savings_plan_total_commitment_to_date` double, `savings_plan_savings_plan_a_r_n` string, `savings_plan_savings_plan_rate` double, `savings_plan_used_commitment` double, `savings_plan_savings_plan_effective_cost` double, `savings_plan_amortized_upfront_commitment_for_billing_period` double, `savings_plan_recurring_commitment_for_billing_period` double, `product_instance` string, `product_provisioned` string, `product_request_description` string, `product_request_type` string, `product_fee_code` string, `product_fee_description` string, `year` bigint, `month` bigint) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\u0001' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://(bucketname)/before/' TBLPROPERTIES ( \u0026quot;skip.header.line.count\u0026quot;=\u0026quot;1\u0026quot;) Create the after table, modify the location line at the bottom, in the query below by replacing (bucketname) with the name of your bucket, and paste the following query into Athena: Create after table - Athena query CREATE EXTERNAL TABLE if not exists `costmaster.after`( `identity_line_item_id` string, `identity_time_interval` string, `bill_invoice_id` string, `bill_billing_entity` string, `bill_bill_type` string, `bill_payer_account_id` bigint, `bill_billing_period_start_date` string, `bill_billing_period_end_date` string, `line_item_usage_account_id` bigint, `line_item_line_item_type` string, `line_item_usage_start_date` string, `line_item_usage_end_date` string, `line_item_product_code` string, `line_item_usage_type` string, `line_item_operation` string, `line_item_availability_zone` string, `line_item_resource_id` string, `line_item_usage_amount` double, `line_item_normalization_factor` double, `line_item_normalized_usage_amount` double, `line_item_currency_code` string, `line_item_unblended_rate` double, `line_item_unblended_cost` double, `line_item_blended_rate` double, `line_item_blended_cost` string, `line_item_line_item_description` string, `line_item_tax_type` string, `line_item_legal_entity` string, `product_product_name` string, `product_availability` string, `product_capacitystatus` string, `product_clock_speed` string, `product_current_generation` string, `product_database_engine` string, `product_dedicated_ebs_throughput` string, `product_deployment_option` string, `product_description` string, `product_durability` string, `product_ecu` string, `product_edition` string, `product_engine_code` string, `product_enhanced_networking_supported` string, `product_event_type` string, `product_free_query_types` string, `product_from_location` string, `product_from_location_type` string, `product_group` string, `product_group_description` string, `product_instance_family` string, `product_instance_type` string, `product_instance_type_family` string, `product_license_model` string, `product_location` string, `product_location_type` string, `product_max_iops_burst_performance` string, `product_max_iopsvolume` bigint, `product_max_throughputvolume` string, `product_max_volume_size` string, `product_memory` string, `product_message_delivery_frequency` string, `product_message_delivery_order` string, `product_min_volume_size` string, `product_network_performance` string, `product_normalization_size_factor` double, `product_operating_system` string, `product_operation` string, `product_physical_processor` string, `product_pre_installed_sw` string, `product_processor_architecture` string, `product_processor_features` string, `product_product_family` string, `product_queue_type` string, `product_region` string, `product_servicecode` string, `product_servicename` string, `product_sku` string, `product_storage` string, `product_storage_class` string, `product_storage_media` string, `product_subscription_type` string, `product_tenancy` string, `product_to_location` string, `product_to_location_type` string, `product_transfer_type` string, `product_usagetype` string, `product_vcpu` bigint, `product_version` string, `product_volume_type` string, `pricing_lease_contract_length` string, `pricing_offering_class` string, `pricing_purchase_option` string, `pricing_rate_id` bigint, `pricing_public_on_demand_cost` string, `pricing_public_on_demand_rate` double, `pricing_term` string, `pricing_unit` string, `reservation_amortized_upfront_cost_for_usage` double, `reservation_amortized_upfront_fee_for_billing_period` double, `reservation_effective_cost` double, `reservation_end_time` string, `reservation_modification_status` string, `reservation_normalized_units_per_reservation` string, `reservation_number_of_reservations` string, `reservation_recurring_fee_for_usage` double, `reservation_reservation_a_r_n` string, `reservation_start_time` string, `reservation_subscription_id` bigint, `reservation_total_reserved_normalized_units` string, `reservation_total_reserved_units` string, `reservation_units_per_reservation` string, `reservation_unused_amortized_upfront_fee_for_billing_period` double, `reservation_unused_normalized_unit_quantity` double, `reservation_unused_quantity` double, `reservation_unused_recurring_fee` double, `reservation_upfront_value` double, `resource_tags_aws_autoscaling_group_name` string, `resource_tags_aws_created_by` string, `resource_tags_aws_ec2spot_fleet_request_id` string, `resource_tags_user_cost_center` string, `resource_tags_user_department` string, `resource_tags_user_environment` string, `resource_tags_user_name` string, `resource_tags_user_workload` string, `resource_tags_user_workload_type` string, `product_category` string, `resource_tags_user_tag21_nov` string, `resource_tags_user_application` string, `resource_tags_user_tier` string, `product_content_type` string, `product_granularity` string, `product_origin` string, `product_recipient` string, `product_volume_api_name` string, `savings_plan_total_commitment_to_date` double, `savings_plan_savings_plan_a_r_n` string, `savings_plan_savings_plan_rate` double, `savings_plan_used_commitment` double, `savings_plan_savings_plan_effective_cost` double, `savings_plan_amortized_upfront_commitment_for_billing_period` double, `savings_plan_recurring_commitment_for_billing_period` double, `product_instance` string, `product_provisioned` string, `product_request_description` string, `product_request_type` string, `product_fee_code` string, `product_fee_description` string, `year` bigint, `month` bigint) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\u0001' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://(bucketname)/after/' TBLPROPERTIES ( \u0026quot;skip.header.line.count\u0026quot;=\u0026quot;1\u0026quot;) Test and Verify Confirm the before table is readable, copy and paste the following query into Athena and ensure it returns lines:\nSELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; limit 10; Confirm the after table is readable, copy and paste the following query into Athena and ensure it returns lines:\nSELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;after\u0026quot; limit 10; You have successfully setup the cost and usage data source. We have a database of licensed and unlicensed usage to analyze and verify.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/contributing/01_general/","title":"General","tags":[],"description":"","content":"Initial Steps Check for existing open, and recently merged, pull requests to make sure someone else hasn\u0026rsquo;t addressed the problem already. Open an issue to discuss any significant work or new lab ideas - we would hate for your time to be wasted. Requirements GitHub account Git setup locally on your computer Install Hugo Contribution Etiquette Note: Please focus on the specific change you are contributing. If you make modifications to multiple labs please submit a separate request for each lab.\nCode of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.\nSecurity issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public GitHub issue.\nLicensing See the LICENSE file for our project\u0026rsquo;s licensing. We will ask you to confirm the licensing of your contribution.\nWe may ask you to sign a Contributor License Agreement (CLA) for larger changes.\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/analytics/","title":"Analytics","tags":[],"description":"","content":"These are queries for AWS Services under the Analytics product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon Athena AWS Glue Amazon Kinesis Amazon Elasticsearch Amazon EMR Amazon QuickSight Amazon Athena Query Description This query will provide daily unblended and usage information for Amazon Athena. The output will include detailed information about the resource id. The cost will be summed and in descending order.\nPricing Please refer to the Athena pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_usage_type, line_item_resource_id, product_region, line_item_product_code, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonAthena\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, line_item_usage_start_date, line_item_usage_type, line_item_resource_id, product_region, line_item_product_code ORDER BY sum_line_item_unblended_cost DESC LIMIT 20; Help \u0026amp; Feedback Back to Table of Contents AWS Glue Query Description This query will provide daily unblended and usage information per linked account for AWS Glue. The output will include detailed information about the resource id (Glue Crawler) and API operation. The cost will be summed and in descending order.\nPricing Please refer to the Glue pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_operation, CASE WHEN LOWER(line_item_operation) = \u0026#39;jobrun\u0026#39; THEN SPLIT_PART(line_item_resource_id, \u0026#39;job/\u0026#39;, 2) WHEN LOWER(line_item_operation) = \u0026#39;crawlerrun\u0026#39; THEN SPLIT_PART(line_item_resource_id, \u0026#39;crawler/\u0026#39;, 2) ELSE \u0026#39;N/A\u0026#39; END AS case_line_item_resource_id, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16, 8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = (\u0026#39;AWS Glue\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_operation, line_item_resource_id ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost; Help \u0026amp; Feedback Back to Table of Contents Amazon Kinesis Query Description This query will provide daily unblended and usage information per linked account for each Kinesis product (Amazon Kinesis, Amazon Kinesis Firehose, and Amazon Kinesis Analytics). The output will include detailed information about the resource id (Stream, Delivery Stream, etc\u0026hellip;) and API operation. The cost will be summed and in descending order.\nPricing Please refer to the Kinesis pricing pages:\nAmazon Kinesis Data Streams Pricing Amazon Kinesis Data Firehose Pricing Amazon Kinesis Data Analytics Pricing Amazon Kinesis Video Streams pricing Sample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6) AS split_line_item_resource_id, product_product_name, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16, 8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE ${date_filter} AND product_product_name IN (\u0026#39;Amazon Kinesis\u0026#39;,\u0026#39;Amazon Kinesis Firehose\u0026#39;,\u0026#39;Amazon Kinesis Analytics\u0026#39;,\u0026#39;Amazon Kinesis Video\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, product_product_name ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon Elasticsearch Query Description This query will provide daily unblended and amortized cost as well as usage information per linked account for Amazon Elasticsearch. The output will include detailed information about the resource id (ES Domain), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order. This query includes RI and SP true up which will show any upfront fees to the account that purchased the pricing model.\nPricing Please refer to the Elasticsearch pricing page . Please refer to this blog for Cost Optimization techniques .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6) AS split_line_item_resource_id, product_product_family, product_instance_family, product_instance_type, pricing_term, product_storage_media, product_transfer_type, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN line_item_usage_amount WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN line_item_usage_amount WHEN (line_item_line_item_type = \u0026#39;Usage\u0026#39;) THEN line_item_usage_amount ELSE 0 END) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) AS sum_amortized_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (-savings_plan_amortized_upfront_commitment_for_billing_period) WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (-reservation_amortized_upfront_fee_for_billing_period) ELSE 0 END) AS sum_ri_sp_trueup, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN line_item_unblended_cost WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END) AS sum_ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND product_product_name in (\u0026#39;Amazon Elasticsearch Service\u0026#39; ,\u0026#39;Amazon OpenSearch Service\u0026#39;) AND line_item_product_code = \u0026#39;AmazonES\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6), product_product_family, product_instance_family, product_instance_type, pricing_term, product_storage_media, product_transfer_type ORDER BY day_line_item_usage_start_date, product_product_family, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon EMR Query Description This query will provide daily unblended cost and usage information per linked account for Amazon EMR. The cost will be summed and the cost will be in descending order.\nPricing Please refer to the EMR pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_usage_type ,\u0026#39;:\u0026#39;,2) AS split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic MapReduce\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, line_item_line_item_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost DESC, split_line_item_usage_type; Help \u0026amp; Feedback Back to Table of Contents Amazon QuickSight Query Description This query will provide monthly unblended and usage information per linked account for Amazon QuickSight. The output will include detailed information about the usage type and its usage amount. The cost will be summed and in descending order.\nPricing Please refer to the Amazon QuickSight pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, CASE WHEN LOWER(line_item_usage_type) LIKE \u0026#39;qs-user-enterprise%\u0026#39; THEN \u0026#39;Users - Enterprise\u0026#39; WHEN LOWER(line_item_usage_type) LIKE \u0026#39;qs-user-standard%\u0026#39; THEN \u0026#39;Users - Standard\u0026#39; WHEN LOWER(line_item_usage_type) LIKE \u0026#39;qs-reader-usage%\u0026#39; THEN \u0026#39;Reader Usage\u0026#39; WHEN LOWER(line_item_usage_type) LIKE \u0026#39;%spice\u0026#39; THEN \u0026#39;SPICE\u0026#39; ELSE line_item_usage_type END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon QuickSight\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), 4 -- refers to case_line_item_usage_type ORDER BY month_line_item_usage_start_date, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/1_create_cost_intelligence/","title":"Create Cost Intelligence Dashboard","tags":[],"description":"","content":" This Lab has moved under the Level 200 Cloud Intelligence Dashboards.Click this link to navigate to the updated Lab Authors Alee Whitman, Commercial Architect (AWS OPTICS) Contributors Arun Santhosh, Specialist SA (Amazon QuickSight) Kareem Syed-Mohammed, Senior Product Manager - Technical (Amazon QuickSight) Aaron Edell, Global Head of Business and GTM - EC2 Insights Timur Tulyaganov, AWS Technical Account Manager Yuriy Prykhodko, AWS Technical Account Manager FAQ The FAQ with the added Visual by Visual breakdown for this dashboard is here. Request Template Access Ensure you have requested access to the Cost Intelligence template here. Optional: Advanced Setup using a CloudFormation Template This section is optional and automates the creation of the Cost Intelligence Dashboard using a CloudFormation template. The CloudFormation template allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates, create an IAM role, create an S3 Bucket, and create an Athena Database. If you do not have the required permissions skip over this section to continue using the standard setup.\nCreate the Cost Intelligence Dashboard using a CloudFormation Template Login via SSO in your Cost Optimization account\nClick the Launch CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch CloudFormation Template Enter a Stack name for your template such as Cost-Intelligence-Dashboard-QuickSight Review 1stReadMe parameter to confirm prerequisites before specifying the other parameters Validate your Athena primary workgroup has an output location\nOpen a new tab or window and navigate to the Athena console Select Workgroup: primary Click the bubble next to primary and then select view detail Confirm your Query result location is configured with an S3 bucket path. If configured, continue to step 6. If not configured, continue to setting up by clicking Edit workgroup Add the S3 bucket path you have selected for your Query result location and click save Update your BucketFolderPath with the S3 path where your year partitions of CUR data are stored To validate the correct path for your year partitions of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the S3 console Select the S3 Bucket your CUR is located in Navigate your folders until you find the folder with the year partitions of the CUR Tip: Your yearly partitions folder is located in the folder with your .yml file, monthly folders, and status report Add the identified BucketFolderPath to the CloudFormation parameter making sure to not add trailing / (eg - BucketName/FolderName/\u0026hellip;/FolderName) Tip: copy and paste the S3 URI then remove the leading \u0026lsquo;s3://\u0026rsquo; and the ending \u0026lsquo;/\u0026rsquo; Update your QuickSightUser with your QuickSight username To validate your QuickSight complete the tasks below:\nOpen a new tab or window and navigate to the QuickSight console Click on the profile icon in the top right side of the navigation bar, then select Manage QuickSight Locate your username in the manage users section Select Next at the bottom of Specify stack details and then select Next again on the Configure stack options page\nReview the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack. You will see the stack will start in CREATE_IN_PROGRESS Once complete, the stack will show CREATE_COMPLETE Navigate to Dashboards page in your QuickSight console, click on your Dashboard name Skip to the bottom of the page to step 8 of the Create the Dashboard section to finish setting up your dashboard\nTo create the dashboard using the standard setup move to the Create Athena Views section\nCreate Athena Views The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR). The default dashboard assumes you have both Savings Plans and Reserved Instances, if not you will need to create the alternate views.\nLogin via SSO in your Cost Optimization account, go into the Athena console:\nModify and run the following queries to confirm if you have Savings Plans, and Reserved Instances in your usage. If no lines are returned, you have no Savings Plans or Reserved Instances. Replace (database).(tablename) and run the following:\nSavings Plans:\nselect * from (database).(tablename) where savings_plan_savings_plan_a_r_n not like '' limit 10 Reserved Instances:\nselect * from (database).(tablename) where reservation_reservation_a_r_n not like '' limit 10 Create the Summary view by modifying the following code, and executing it in Athena:\nView1 - Summary Create the EC2_Running_Cost view by modifying the following code, and executing it in Athena:\nView2 - EC2_Running_Cost Create the Compute savings plan eligible spend view by modifying the following code, and executing it in Athena:\nView3 - compute savings plan eligible spend Create the s3 view by modifying the following code, and executing it in Athena:\nView4 - s3 Create the RI SP Mapping view by modifying the following code, and executing it in Athena:\nView5 - RI SP Mapping Create QuickSight Data Sets We will now create the data sets in QuickSight from the Athena views.\nGo to the QuickSight service homepage\nClick Manage data: Click New dataset Click Athena Enter a data source name of Cost_Dashboard and click Create data source: Select the costmaster database, and the summary_view table, click Edit/Preview data: Select SPICE to change your Query mode: Select Save: Select the summary_view Data Set: Click Schedule refresh: Click Create: Enter a schedule, it needs to be refreshed daily, and click Create: Click Cancel to exit: Click the x in the top corner: Repeat steps 3-14, creating data sets with the remaining Athena views. The data source name will be Cost_Dashboard, and select the following views as the table:\ns3_view ec2_running_cost compute_savings_plan_eligible_spend Select summary_view Data Set: Select Edit data set\nSelect Add Data: Select your ri_sp_mapping view and click Select: Select the two circles to open the Join configuration then select Left to change your join type: Select Add a new join clause two times so you have 3 join clauses: Create following 3 join clauses then click Apply:\nri_sp_arn = ri_sp_arn_mapping payer_account_id = payer_account_id_mapping billing_period = billing_period_mapping Select Save Create the Dashboard We will now use the CLI to create the dashboard from the Cost Intelligence Dashboard template, then create an Analysis you can customize and modify in the next step.\nGo to this page to request access to the template. Enter your AWS AccountID and click Submit: Template Access Edit the following command, replacing AccountID and region, then using the CLI list the QuickSight datasets and copy the Name and Arn for the 4 datasets: s3_view, ec2_running_cost, compute_savings_plan_eligible_spend, summary_view:\naws quicksight list-data-sets --aws-account-id (AccountID) --region (region) Get your users Arn by editing the following command, replacing AccountID and region, then using the CLI run the command:\naws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region) Create a local file create-dashboard.json with the text below, replace the values (Account ID) on line 2, (User ARN) one line 7, and each Dataset (ARN) on lines 25, 30, 35, 40:\n{ \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;cost_intelligence_dashboard\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;Cost Intelligence Dashboard\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;summary_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(Summary Dataset ARN)\u0026quot; }, { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;ec2_running_cost\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(ec2_running_cost Dataset ARN)\u0026quot; }, { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;compute_savings_plan_eligible_spend\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(compute_savings_plan_eligible_spend Dataset ARN)\u0026quot; }, { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;s3_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(s3_view Dataset ARN)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/cost-intelligence-dashboard\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; } To create the dashboard from the template, edit then run the following command, replacing (region) and you should receive a 202 response:\naws quicksight create-dashboard --cli-input-json file://create-dashboard.json --region (region) After a few minutes the dashboard will become available in QuickSight under All dashboard, click on the Dashboard name: Click here - if you do not see your dashboard \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Click Share, click Share dashboard:, Click Manage dashboard access: Add the required users, or share with all users, ensure you check Save as for each user, then click the x to close the window: Click Save as: Enter an Analysis name and click Create: You will now have an analysis created from the template that you can edit and modify: Additional Dashboard Resources For opportunities to customize or add additional dashboard insights\nContinue with modules 2-4 in the Level 200: Enterprise Dashboard Create the CUDOS dashboard to get ideas on visuals you want to add to your Cost Intelligence Dashboard. The CUDOS QuickSight dashboard is built on top of the Cost Intelligence Dashboard so you can leverage your existing data sets. "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/compute-optimizer-dashboards/dashboards/1_prerequisites/","title":"Prerequisites","tags":[],"description":"","content":"1/4 Enable AWS Compute Optimizer To get right sizing recommendations you need to Enroll all accounts to Compute Optmizer . You can use free version that provides recommendations based on 14 days of look-back period.\n2/4 Install Compute Optimizer Data Collection Before installing Dashboard please install Compute Optimizer module of Optimization Data Collection lab - this provides an automated way to collect Compute Optimizer recommendations for all accounts in your AWS Organizations and AWS Regions.\n3/4 Prepare Athena If this is the first time you will be using Athena you will need to complete a few setup steps before you are able to create the views needed. If you are already a regular Athena user you can skip these steps and move on to the Enable Quicksight section below.\nTo get Athena warmed up:\nFrom the services list, choose S3\nCreate a new S3 bucket for Athena queries to be logged to. Keep to the same region as the S3 bucket created for your Compute Optimizer data created via Data Collection Lab.\nFrom the services list, choose Athena\nSelect Get Started to enable Athena and start the basic configuration At the top of this screen select Before you run your first query, you need to set up a query result location in Amazon S3.\nValidate your Athena primary workgroup has an output location by\nOpen a new tab or window and navigate to the Athena console Select Workgroup: primary Confirm your Query result location is configured with an S3 bucket path. If not configured, continue to setting up by clicking Edit workgroup Add the S3 bucket path you have selected for your Query result location and click save 4/4 Enable QuickSight QuickSight is the AWS Business Intelligence tool that will allow you to not only view the Standard AWS provided insights into all of your accounts, but will also allow to produce new versions of the Dashboards we provide or create something entirely customized to you. If you are already a regular QuickSight user you can skip these steps.\nLog into your AWS Account and search for QuickSight in the list of Services\nYou will be asked to sign up before you will be able to use it\nAfter pressing the Sign up button you will be presented with 2 options, please ensure you select the Enterprise Edition during this step\nSelect continue and you will need to fill in a series of options in order to finish creating your account.\nEnsure you select the region that is most appropriate based on where your S3 Bucket is located containing your CO report files.\nEnable the Amazon S3 option and select the bucket where your Compute Optimizer data created via Data Collection Lab are located\nClick Finish \u0026amp; wait for the congratulations screen to display\nClick Go to Amazon QuickSight\nCheck you have Amazon QuickSight Enterprise Edition\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/1_prerequistes/","title":"Prerequisites","tags":[],"description":"","content":"Install or Update the AWS CLI CLI commands are required to complete the dashboard setup. If you need to enable your AWS CLI, follow the steps below.\nClick here - to use AWS Cloudshell - Recommended option AWS CloudShell is a browser-based, pre-authenticated shell that you can launch directly from the AWS Management Console. You can run AWS CLI commands against AWS services using your preferred shell (Bash, PowerShell, or Z shell). And you can do this without needing to download or install command line tools. If you are working out of one of these Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), or Europe (Ireland), using AWS CloudShell is the simplest option.\nTo launch AWS CloudShell:\nFrom the AWS Management Console, you can launch AWS CloudShell by choosing the following options available on the navigation bar:\nChoose the AWS CloudShell icon.\nStart typing \u0026ldquo;cloudshell\u0026rdquo; in the Search box and then choose the CloudShell option.\nWhen AWS CloudShell launches in a new browser window for the first time, a welcome panel displays and lists key features. After you close this panel, status updates are provided while the shell configures and forwards your console credentials. When the command prompt displays, the shell is ready for interaction.\nTo choose an AWS Region to work in, go to the Select a Region menu and select a supported AWS Region to work in. (Available Regions are highlighted.)\nClick here - if you are using Docker Prerequisites You must have Docker installed. For installation instructions, see the Docker website To verify your installation of Docker, run the following command and confirm there is an output.\n$ docker --version Docker version 19.03.1 Run the official AWS CLI version 2 Docker image The official AWS CLI version 2 Docker image is hosted on DockerHub in the amazon/aws-cli repository. The first time you use the docker run command, the latest Docker image is downloaded to your computer. Each subsequent use of the docker run command runs from your local copy.\nTo run the AWS CLI version 2 Docker image, use the docker run command.\n$ docker run --rm -it amazon/aws-cli \u0026lt;command\u0026gt; This is how the command functions:\ndocker run --rm --it amazon/aws-cli – The equivalent of the aws executable. Each time you run this command, Docker spins up a container of your downloaded amazon/aws-cli image, and executes your aws command. By default, the Docker image uses the latest version of the AWS CLI version 2.\nFor example, to call the aws --version command in Docker, you run the following.\n$ docker run --rm -it amazon/aws-cli --version aws-cli/2.1.22 Python/3.7.3 Linux/4.9.184-linuxkit botocore/2.0.0dev10 --rm – Specifies to clean up the container after the command exits.\n-it – Specifies to open a pseudo-TTY with stdin. This enables you to provide input to the AWS CLI version 2 while it\u0026rsquo;s running in a container, for example, by using the aws configure and aws help commands. If you are running scripts, -it is not needed. If you are experiencing errors with your scripts, omit -it from your Docker call.\nFor more information about the docker run command, see the Docker reference guide Update to the latest Docker image Because the latest Docker image is downloaded to your computer only the first time you use the docker run command, you need to manually pull an updated image. To manually update to the latest version, we recommend you pull the latest tagged image. Pulling the Docker image downloads the latest version to your computer.\n$ docker pull amazon/aws-cli:latest Click here - if you are using Linux x86 (64-bit) Prerequisites You must be able to extract or \u0026ldquo;unzip\u0026rdquo; the downloaded package. If your operating system doesn\u0026rsquo;t have the built-in unzip command, use an equivalent.\nThe AWS CLI version 2 uses glibc, groff, and less. These are included by default in most major distributions of Linux.\nWe support the AWS CLI version 2 on 64-bit versions of recent distributions of CentOS, Fedora, Ubuntu, Amazon Linux 1, and Amazon Linux 2.\nBecause AWS doesn\u0026rsquo;t maintain third-party repositories, we can’t guarantee that they contain the latest version of the AWS CLI.\nInstall the AWS CLI version 2 on Linux Follow these steps from the command line to install the AWS CLI\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For a list of versions, see the AWS CLI version 2 changelog on GitHub.\nUpdate the AWS CLI version 2 on Linux To update your copy of the AWS CLI version 2, from the Linux command line, follow these steps.\nDownload the installation file in one of the following ways:\nUsing the curl command – The options on the following example command write the downloaded file to the current directory with the local name awscliv2.zip.\nThe -o option specifies the file name that the downloaded package is written to. In this example, the file is written to awscliv2.zip in the current directory.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a list of versions, see the AWS CLI version 2 changelog .\nDownloading from the URL – To download the installer using your browser, use one of the following URLs. You can verify the integrity and authenticity of the installation file after you download it. For more information before you unzip the package, see Verify the integrity and authenticity of the downloaded installer files .\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following link https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip . For a list of versions, see the AWS CLI version 2 changelog on GitHub.\nUnzip the installer. If your Linux distribution doesn\u0026rsquo;t have a built-in unzip command, use an equivalent to install it. The following example command unzips the package and creates a directory named aws under the current directory.\n$ unzip awscliv2.zip To ensure that the update installs in the same location as your current AWS CLI version 2, locate the existing symlink and installation directory.\nUse the which command to find your symlink. This will then display the path to use with the --bin-dir parameter.\n$ which aws /usr/local/bin/aws Use the ls -l command against the value returned above to find the directory that your symlink points to. This will display the path to use with the --install-dir parameter.\n$ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -\u0026gt; /usr/local/aws-cli/v2/current/bin/aws Use your symlink and installer information to construct the install command with the --update parameter.\n$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update Confirm the installation.\n$ aws --version aws-cli/2.1.24 Python/3.7.4 Linux/4.14.133-113.105.amzn2.x86_64 botocore/2.0.0 Click here - if you are using Linux ARM Prerequisites You must be able to extract or \u0026ldquo;unzip\u0026rdquo; the downloaded package. If your operating system doesn\u0026rsquo;t have the built-in unzip command, use an equivalent.\nThe AWS CLI version 2 uses glibc, groff, and less. These are included by default in most major distributions of Linux.\nWe support the AWS CLI version 2 on 64-bit Linux ARM.\nBecause AWS doesn\u0026rsquo;t maintain third-party repositories, we can’t guarantee that they contain the latest version of the AWS CLI.\nInstall the AWS CLI version 2 on Linux ARM For the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Update the AWS CLI version 2 on Linux To update your copy of the AWS CLI version 2, from the Linux command line, follow these steps.\nDownload the installation file in one of the following ways:\nUsing the curl command – The options on the following example command write the downloaded file to the current directory with the local name awscliv2.zip.\nThe -o option specifies the file name that the downloaded package is written to. In this example, the file is written to awscliv2.zip in the current directory.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a list of versions, see the AWS CLI version 2 changelog .\nDownloading from the URL – To download the installer using your browser, use one of the following URLs. You can verify the integrity and authenticity of the installation file after you download it. For more information before you unzip the package, see Verify the integrity and authenticity of the downloaded installer files .\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following link https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip . For a list of versions, see the AWS CLI version 2 changelog on GitHub.\nUnzip the installer. If your Linux distribution doesn\u0026rsquo;t have a built-in unzip command, use an equivalent to install it. The following example command unzips the package and creates a directory named aws under the current directory.\n$ unzip awscliv2.zip To ensure that the update installs in the same location as your current AWS CLI version 2, locate the existing symlink and installation directory.\nUse the which command to find your symlink. This will then display the path to use with the --bin-dir parameter.\n$ which aws /usr/local/bin/aws Use the ls -l command against the value returned above to find the directory that your symlink points to. This will display the path to use with the --install-dir parameter.\n$ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -\u0026gt; /usr/local/aws-cli/v2/current/bin/aws Use your symlink and installer information to construct the install command with the --update parameter.\n$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update Confirm the installation.\n$ aws --version aws-cli/2.1.24 Python/3.7.4 Linux/4.14.133-113.105.amzn2.x86_64 botocore/2.0.0 Click here - if you are using MacOS Prerequisites We support the AWS CLI version 2 on Apple-supported versions of 64-bit macOS.\nBecause AWS doesn\u0026rsquo;t maintain third-party repositories, we can’t guarantee that they contain the latest version of the AWS CLI.\nInstall and update the AWS CLI version 2 using the macOS user interface The following steps show how to install or update to the latest version of the AWS CLI version 2 by using the standard macOS user interface and your browser. If you are updating to the latest version, use the same installation method that you used for your current version.\nIn your browser, download the macOS pkg file:\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/AWSCLIV2.pkg For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.pkg resulting in the following link https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkg . For a list of versions, see the AWS CLI version 2 changelog on GitHub .\nDouble-click the downloaded file to launch the installer.\nFollow the on-screen instructions. You can choose to install the AWS CLI version 2 in the following ways:\nFor all users on the computer (requires sudo)\nYou can install to any folder, or choose the recommended default folder of /usr/local/aws-cli.\nThe installer automatically creates a symlink at /usr/local/bin/aws that links to the main program in the installation folder you chose.\nFor only the current user (doesn\u0026rsquo;t require sudo)\nYou can install to any folder to which you have write permission.\nDue to standard user permissions, after the installer finishes, you must manually create a symlink file in your $PATH that points to the aws and aws_completer programs by using the following commands at the command prompt. If your $PATH includes a folder you can write to, you can run the following command without sudo if you specify that folder as the target\u0026rsquo;s path. If you don\u0026rsquo;t have a writable folder in your $PATH, you must use sudo in the commands to get permissions to write to the specified target folder. The default location for a symlink is /usr/local/bin/.\n$ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws $ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer Install and update the AWS CLI version 2 for all users using the macOS command line You can download, install, and update from the command line. If you are updating to the latest version, use the same installation method that you used in your current version. You can install the AWS CLI version 2.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; $ sudo installer -pkg AWSCLIV2.pkg -target / For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.pkg resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; $ sudo installer -pkg AWSCLIV2.pkg -target / Install and update the AWS CLI version 2 for only the current user using the macOS command line To specify which folder the AWS CLI is installed to, you must create an XML file. This file is an XML-formatted file that looks like the following example. Leave all values as shown, except you must replace the path /Users/myusername in line 9 with the path to the folder you want the AWS CLI version 2 installed to. The folder must already exist, or the command fails. This XML example specifies that the installer installs the AWS CLI in the folder /Users/myusername, where it creates a folder named aws-cli.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;choiceAttribute\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;customLocation\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;attributeSetting\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/Users/myusername\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;choiceIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;default\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/plist\u0026gt; Download the pkg installer using the curl command. The -o option specifies the file name that the downloaded package is written to. In this example, the file is written to AWSCLIV2.pkg in the current folder.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.pkg resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; For a list of versions, see the AWS CLI version 2 changelog on GitHub .\nRun the standard macOS installer program with the following options:\nSpecify the name of the package to install by using the -pkg parameter.\nSpecify a current user only installation by setting the parameter --target CurrentUserHomeDirectory.\nSpecify the path (relative to the current folder) and name of the XML file that you created in the --applyChoiceChangesXML parameter\nThe following example installs the AWS CLI in the folder /Users/myusername/aws-cli.\n$ installer -pkg AWSCLIV2.pkg \\ -target CurrentUserHomeDirectory \\ -applyChoiceChangesXML choices.xml Because standard user permissions typically don\u0026rsquo;t allow writing to folders in your $PATH, the installer in this mode doesn\u0026rsquo;t try to add the symlinks to the aws and aws_completer programs. For the AWS CLI to run correctly, you must manually create the symlinks after the installer finishes. If your $PATH includes a folder you can write to and you specify the folder as the target\u0026rsquo;s path, you can run the following command without sudo. If you don\u0026rsquo;t have a writable folder in your $PATH, you must use sudo for permissions to write to the specified target folder. The default location for a symlink is /usr/local/bin/.\n$ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws $ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer After installation is complete, debug logs are written to /var/log/install.log.\nVerify the installation To verify that the shell can find and run the aws command in your $PATH, use the following commands.\n$ which aws /usr/local/bin/aws $ aws --version aws-cli/2.1.24 Python/3.7.4 Darwin/18.7.0 botocore/2.0.0 Click here - if you are using Windows Prerequisites Before you can install or update the AWS CLI version 2 on Windows, be sure you have the following:\nA 64-bit version of Windows XP or later\nAdmin rights to install software\nInstall or update the AWS CLI version 2 on Windows using the MSI installer Download the AWS CLI MSI installer for Windows (64-bit):\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/AWSCLIV2.msi For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.msi resulting in the following link https://awscli.amazonaws.com/AWSCLIV2-2.0.30.msi . For a list of versions, see the AWS CLI version 2 changelog on GitHub .\nTo update your current installation of AWS CLI version 2 on Windows, download a new installer each time you update to overwrite previous versions. AWS CLI is updated regularly. To see when the latest version was released, see the AWS CLI version 2 changelog on GitHub .\nRun the downloaded MSI installer and follow the on-screen instructions. By default, the AWS CLI installs to C:\\Program Files\\Amazon\\AWSCLIV2.\nTo confirm the installation, open the Start menu, search for cmd to open a command prompt window, and at the command prompt use the aws --version command.\nDon\u0026rsquo;t include the prompt symbol (C:\\\u0026gt;) when you type a command. These are included in program listings to differentiate commands that you type from output returned by the AWS CLI.\nC:\\\u0026gt; aws --version aws-cli/2.1.24 Python/3.7.4 Windows/10 botocore/2.0.0 If Windows is unable to find the program, you might need to close and reopen the command prompt window to refresh the path, or add the installation directory to your PATH environment variable manually.\nFull details on installing, updating, and uninstalling AWS CLI from supported operating systems is available here .\nThe Cloud Intelligence Dashboards support Multi-Payer Cost and Usage Report (CUR) reporting. After using the steps below to setup a CUR in each payer account, visit this section of the lab on combining multiple CURs for the Cloud Intelligence Dashboards before returning here\nPrepare Cost \u0026amp; Usage Report The Cost \u0026amp; Usage Report is the foundation for these dashboards. You must have a Cost \u0026amp; Usage Report created with the following format\nAdditional report details: Include Resource IDs Time Granularity: Hourly Report Versioning: Overwrite existing report Report data integration for: Amazon Athena Compression type: Parquet If you do not have a Cost \u0026amp; Usage Report that meets this criteria follow the steps for preparing your Cost \u0026amp; Usage report setup guide below.\nIt can take up to 24 hours for AWS to start delivering reports to your Amazon S3 bucket. After delivery starts, AWS updates the AWS Cost and Usage Reports files at least once a day.\nClick here see steps for preparing your Cost \u0026amp; Usage report Configure CUR Sign in to the Billing and Cost Management console.\nOn the navigation pane, choose Cost \u0026amp; Usage Reports.\nChoose Create report.\nFor Report name, enter a name for your report.\nUnder Additional report details, select Include resource IDs to include the IDs of each individual resource in the report.\nNote: Including resource IDs will create individual line items for each of your resources. This can increase the size of your Cost and Usage Reports files significantly, based on your AWS usage. For Data refresh settings, select whether you want the AWS Cost and Usage Reports to refresh if AWS applies refunds, credits, or support fees to your account after finalizing your bill. When a report refreshes, a new report is uploaded to Amazon S3.\nChoose Next.\nFor S3 bucket, choose Configure.\nIn the Configure S3 Bucket dialog box, do one of the following:\nSelect an existing bucket from the drop down list and choose Next.\nEnter a bucket name and the Region where you want to create a new bucket and choose Next.\nReview the bucket policy, and select I have confirmed that this policy is correct and choose Save.\nFor Report path prefix, enter the report path prefix that you want prepended to the name of your report.\nFor Time granularity, choose Hourly.\nFor Report versioning, choose Overwrite existing report.\nFor Enable report data integration for, select Amazon Athena.\nNow CUR will be set to Parquet format, this format is mandatory for the workshop completion.\nChoose Next.\nAfter you have reviewed the settings for your report, choose Review and Complete.\nEnable your Cost \u0026amp; Usage Reports in Athena The dashboards use Athena as the QuickSight data source for generating your dashboards. If you do not have your Cost \u0026amp; Usage Report enabled in Athena please click to expand the setup guide below.\nClick here - to follow the steps for setting up Athena for the first time Configure Athena 1. Prepare Athena If this is the first time you will be using Athena you will need to complete a few setup steps before you are able to create the views needed. If you are already a regular Athena user you can skip these steps and move on to step 2 Prepare CUR \u0026amp; Athena Integration\nTo get Athena warmed up:\nFrom the services list, choose S3\nCreate a new S3 bucket for Athena queries to be logged to. Keep to the same region as the S3 bucket created for your Cost \u0026amp; Usage Report.\nFrom the services list, choose Athena\nSelect Get Started to enable Athena and start the basic configuration\nAt the top of this screen select Before you run your first query, you need to set up a query result location in Amazon S3.\nEnter the path of the bucket created for Athena queries, it is recommended that you also select the AutoComplete option NOTE: The trailing “/” in the folder path is required!\n2. Prepare CUR \u0026amp; Athena Integration To streamline and automate integration of your Cost and Usage Reports with Athena, AWS provides an AWS CloudFormation template with several key resources along with the reports you setup for Athena integration. The AWS CloudFormation template includes an AWS Glue crawler, an AWS Glue database, and an AWS Lambda event.\nFrom the services list, choose S3\nNavigate to the S3 bucket where the Cost \u0026amp; Usage Report was saved\nSelect the Object named after the prefix defined when your Cost \u0026amp; Usage Report was created (Step 11 in Prepare Cost \u0026amp; Usage Report )\nSelect the Object named after the Cost \u0026amp; Usage Report\nDownload the crawler-cfn.yml file\nNavigate to the CloudFormation service\nEnsure you are in the same Region as your Cost \u0026amp; Usage Report S3 bucket\nDeploy the CloudFormation template by clicking Create stack - With new resources (standard)\nSelect Upload a template file\nClick Choose file and locate your crawler-cfn.yml file\nClick Next\nEnter a Stack Name to identify this as part of your CUDOS Dashboard setup\nClick Next\nDefine Stack options including tags, permissions and rollback configurations.\nClick Next\nEnable \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; and click Create Stack\nEnable QuickSight QuickSight is the AWS Business Intelligence tool that will allow you to not only view the Standard AWS provided insights into all of your accounts, but will also allow to produce new versions of the Dashboards we provide or create something entirely customized to you. If you are already a regular QuickSight user you can skip these steps and move on to the next step. If not, complete the steps below.\nClick here - to setup QuickSight Enable QuickSight: Log into your AWS Account and search for QuickSight in the list of Services\nYou will be asked to sign up before you will be able to use it\nAfter pressing the Sign up button you will be presented with 2 options, please ensure you select the Enterprise Edition during this step\nSelect continue and you will need to fill in a series of options in order to finish creating your account.\nEnsure you select the region that is most appropriate based on where your S3 Bucket is located containing your Cost \u0026amp; Usage Report file.\nEnable the Amazon S3 option and select the bucket where your Cost \u0026amp; Usage Report is stored, as well as your Athena query bucket\nClick Finish and wait for the congratulations screen to display\nClick Go to Amazon QuickSight\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/trusted-advisor-dashboards/dashboards/1_prerequistes/","title":"Prerequisites","tags":[],"description":"","content":"Install or Update the AWS CLI CLI commands are required to complete the dashboard setup. If you need to enable your AWS CLI, follow the steps below.\nClick here - to use AWS Cloudshell (recommended) AWS CloudShell is a browser-based, pre-authenticated shell that you can launch directly from the AWS Management Console. You can run AWS CLI commands against AWS services using your preferred shell (Bash, PowerShell, or Z shell). And you can do this without needing to download or install command line tools. If you are working out of one of these Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), or Europe (Ireland), using AWS CloudShell is the simplest option.\nTo launch AWS CloudShell:\nFrom the AWS Management Console, you can launch AWS CloudShell by choosing the following options available on the navigation bar:\nChoose the AWS CloudShell icon.\nStart typing \u0026ldquo;cloudshell\u0026rdquo; in the Search box and then choose the CloudShell option.\nWhen AWS CloudShell launches in a new browser window for the first time, a welcome panel displays and lists key features. After you close this panel, status updates are provided while the shell configures and forwards your console credentials. When the command prompt displays, the shell is ready for interaction.\nTo choose an AWS Region to work in, go to the Select a Region menu and select a supported AWS Region to work in. (Available Regions are highlighted.)\nClick here - if you are using Docker Prerequisites You must have Docker installed. For installation instructions, see the Docker website To verify your installation of Docker, run the following command and confirm there is an output.\n$ docker --version Docker version 19.03.1 Run the official AWS CLI version 2 Docker image The official AWS CLI version 2 Docker image is hosted on DockerHub in the amazon/aws-cli repository. The first time you use the docker run command, the latest Docker image is downloaded to your computer. Each subsequent use of the docker run command runs from your local copy.\nTo run the AWS CLI version 2 Docker image, use the docker run command.\n$ docker run --rm -it amazon/aws-cli \u0026lt;command\u0026gt; This is how the command functions:\ndocker run --rm --it amazon/aws-cli – The equivalent of the aws executable. Each time you run this command, Docker spins up a container of your downloaded amazon/aws-cli image, and executes your aws command. By default, the Docker image uses the latest version of the AWS CLI version 2.\nFor example, to call the aws --version command in Docker, you run the following.\n$ docker run --rm -it amazon/aws-cli --version aws-cli/2.1.22 Python/3.7.3 Linux/4.9.184-linuxkit botocore/2.0.0dev10 --rm – Specifies to clean up the container after the command exits.\n-it – Specifies to open a pseudo-TTY with stdin. This enables you to provide input to the AWS CLI version 2 while it\u0026rsquo;s running in a container, for example, by using the aws configure and aws help commands. If you are running scripts, -it is not needed. If you are experiencing errors with your scripts, omit -it from your Docker call.\nFor more information about the docker run command, see the Docker reference guide Update to the latest Docker image Because the latest Docker image is downloaded to your computer only the first time you use the docker run command, you need to manually pull an updated image. To manually update to the latest version, we recommend you pull the latest tagged image. Pulling the Docker image downloads the latest version to your computer.\n$ docker pull amazon/aws-cli:latest Click here - if you are using Linux x86 (64-bit) Prerequisites You must be able to extract or \u0026ldquo;unzip\u0026rdquo; the downloaded package. If your operating system doesn\u0026rsquo;t have the built-in unzip command, use an equivalent.\nThe AWS CLI version 2 uses glibc, groff, and less. These are included by default in most major distributions of Linux.\nWe support the AWS CLI version 2 on 64-bit versions of recent distributions of CentOS, Fedora, Ubuntu, Amazon Linux 1, and Amazon Linux 2.\nBecause AWS doesn\u0026rsquo;t maintain third-party repositories, we can’t guarantee that they contain the latest version of the AWS CLI.\nInstall the AWS CLI version 2 on Linux Follow these steps from the command line to install the AWS CLI\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For a list of versions, see the AWS CLI version 2 changelog on GitHub.\nUpdate the AWS CLI version 2 on Linux To update your copy of the AWS CLI version 2, from the Linux command line, follow these steps.\nDownload the installation file in one of the following ways:\nUsing the curl command – The options on the following example command write the downloaded file to the current directory with the local name awscliv2.zip.\nThe -o option specifies the file name that the downloaded package is written to. In this example, the file is written to awscliv2.zip in the current directory.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a list of versions, see the AWS CLI version 2 changelog .\nDownloading from the URL – To download the installer using your browser, use one of the following URLs. You can verify the integrity and authenticity of the installation file after you download it. For more information before you unzip the package, see Verify the integrity and authenticity of the downloaded installer files .\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following link https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip . For a list of versions, see the AWS CLI version 2 changelog on GitHub.\nUnzip the installer. If your Linux distribution doesn\u0026rsquo;t have a built-in unzip command, use an equivalent to install it. The following example command unzips the package and creates a directory named aws under the current directory.\n$ unzip awscliv2.zip To ensure that the update installs in the same location as your current AWS CLI version 2, locate the existing symlink and installation directory.\nUse the which command to find your symlink. This will then display the path to use with the --bin-dir parameter.\n$ which aws /usr/local/bin/aws Use the ls -l command against the value returned above to find the directory that your symlink points to. This will display the path to use with the --install-dir parameter.\n$ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -\u0026gt; /usr/local/aws-cli/v2/current/bin/aws Use your symlink and installer information to construct the install command with the --update parameter.\n$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update Confirm the installation.\n$ aws --version aws-cli/2.1.24 Python/3.7.4 Linux/4.14.133-113.105.amzn2.x86_64 botocore/2.0.0 Click here - if you are using Linux ARM Prerequisites You must be able to extract or \u0026ldquo;unzip\u0026rdquo; the downloaded package. If your operating system doesn\u0026rsquo;t have the built-in unzip command, use an equivalent.\nThe AWS CLI version 2 uses glibc, groff, and less. These are included by default in most major distributions of Linux.\nWe support the AWS CLI version 2 on 64-bit Linux ARM.\nBecause AWS doesn\u0026rsquo;t maintain third-party repositories, we can’t guarantee that they contain the latest version of the AWS CLI.\nInstall the AWS CLI version 2 on Linux ARM For the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Update the AWS CLI version 2 on Linux To update your copy of the AWS CLI version 2, from the Linux command line, follow these steps.\nDownload the installation file in one of the following ways:\nUsing the curl command – The options on the following example command write the downloaded file to the current directory with the local name awscliv2.zip.\nThe -o option specifies the file name that the downloaded package is written to. In this example, the file is written to awscliv2.zip in the current directory.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; For a list of versions, see the AWS CLI version 2 changelog .\nDownloading from the URL – To download the installer using your browser, use one of the following URLs. You can verify the integrity and authenticity of the installation file after you download it. For more information before you unzip the package, see Verify the integrity and authenticity of the downloaded installer files .\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be awscli-exe-linux-x86_64-2.0.30.zip resulting in the following link https://awscli.amazonaws.com/awscli-exe-linux-aarch64-2.0.30.zip . For a list of versions, see the AWS CLI version 2 changelog on GitHub.\nUnzip the installer. If your Linux distribution doesn\u0026rsquo;t have a built-in unzip command, use an equivalent to install it. The following example command unzips the package and creates a directory named aws under the current directory.\n$ unzip awscliv2.zip To ensure that the update installs in the same location as your current AWS CLI version 2, locate the existing symlink and installation directory.\nUse the which command to find your symlink. This will then display the path to use with the --bin-dir parameter.\n$ which aws /usr/local/bin/aws Use the ls -l command against the value returned above to find the directory that your symlink points to. This will display the path to use with the --install-dir parameter.\n$ ls -l /usr/local/bin/aws lrwxrwxrwx 1 ec2-user ec2-user 49 Oct 22 09:49 /usr/local/bin/aws -\u0026gt; /usr/local/aws-cli/v2/current/bin/aws Use your symlink and installer information to construct the install command with the --update parameter.\n$ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update Confirm the installation.\n$ aws --version aws-cli/2.1.24 Python/3.7.4 Linux/4.14.133-113.105.amzn2.x86_64 botocore/2.0.0 Click here - if you are using MacOS Prerequisites We support the AWS CLI version 2 on Apple-supported versions of 64-bit macOS.\nBecause AWS doesn\u0026rsquo;t maintain third-party repositories, we can’t guarantee that they contain the latest version of the AWS CLI.\nInstall and update the AWS CLI version 2 using the macOS user interface The following steps show how to install or update to the latest version of the AWS CLI version 2 by using the standard macOS user interface and your browser. If you are updating to the latest version, use the same installation method that you used for your current version.\nIn your browser, download the macOS pkg file:\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/AWSCLIV2.pkg For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.pkg resulting in the following link https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkg . For a list of versions, see the AWS CLI version 2 changelog on GitHub .\nDouble-click the downloaded file to launch the installer.\nFollow the on-screen instructions. You can choose to install the AWS CLI version 2 in the following ways:\nFor all users on the computer (requires sudo)\nYou can install to any folder, or choose the recommended default folder of /usr/local/aws-cli.\nThe installer automatically creates a symlink at /usr/local/bin/aws that links to the main program in the installation folder you chose.\nFor only the current user (doesn\u0026rsquo;t require sudo)\nYou can install to any folder to which you have write permission.\nDue to standard user permissions, after the installer finishes, you must manually create a symlink file in your $PATH that points to the aws and aws_completer programs by using the following commands at the command prompt. If your $PATH includes a folder you can write to, you can run the following command without sudo if you specify that folder as the target\u0026rsquo;s path. If you don\u0026rsquo;t have a writable folder in your $PATH, you must use sudo in the commands to get permissions to write to the specified target folder. The default location for a symlink is /usr/local/bin/.\n$ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws $ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer Install and update the AWS CLI version 2 for all users using the macOS command line You can download, install, and update from the command line. If you are updating to the latest version, use the same installation method that you used in your current version. You can install the AWS CLI version 2.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; $ sudo installer -pkg AWSCLIV2.pkg -target / For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.pkg resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; $ sudo installer -pkg AWSCLIV2.pkg -target / Install and update the AWS CLI version 2 for only the current user using the macOS command line To specify which folder the AWS CLI is installed to, you must create an XML file. This file is an XML-formatted file that looks like the following example. Leave all values as shown, except you must replace the path /Users/myusername in line 9 with the path to the folder you want the AWS CLI version 2 installed to. The folder must already exist, or the command fails. This XML example specifies that the installer installs the AWS CLI in the folder /Users/myusername, where it creates a folder named aws-cli.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;array\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;choiceAttribute\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;customLocation\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;attributeSetting\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/Users/myusername\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;choiceIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;default\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/plist\u0026gt; Download the pkg installer using the curl command. The -o option specifies the file name that the downloaded package is written to. In this example, the file is written to AWSCLIV2.pkg in the current folder.\nFor the latest version of the AWS CLI, use the following command block:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; For a specific version of the AWS CLI, append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.pkg resulting in the following command:\n$ curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2-2.0.30.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; For a list of versions, see the AWS CLI version 2 changelog on GitHub .\nRun the standard macOS installer program with the following options:\nSpecify the name of the package to install by using the -pkg parameter.\nSpecify a current user only installation by setting the parameter --target CurrentUserHomeDirectory.\nSpecify the path (relative to the current folder) and name of the XML file that you created in the --applyChoiceChangesXML parameter\nThe following example installs the AWS CLI in the folder /Users/myusername/aws-cli.\n$ installer -pkg AWSCLIV2.pkg \\ -target CurrentUserHomeDirectory \\ -applyChoiceChangesXML choices.xml Because standard user permissions typically don\u0026rsquo;t allow writing to folders in your $PATH, the installer in this mode doesn\u0026rsquo;t try to add the symlinks to the aws and aws_completer programs. For the AWS CLI to run correctly, you must manually create the symlinks after the installer finishes. If your $PATH includes a folder you can write to and you specify the folder as the target\u0026rsquo;s path, you can run the following command without sudo. If you don\u0026rsquo;t have a writable folder in your $PATH, you must use sudo for permissions to write to the specified target folder. The default location for a symlink is /usr/local/bin/.\n$ sudo ln -s /folder/installed/aws-cli/aws /usr/local/bin/aws $ sudo ln -s /folder/installed/aws-cli/aws_completer /usr/local/bin/aws_completer After installation is complete, debug logs are written to /var/log/install.log.\nVerify the installation To verify that the shell can find and run the aws command in your $PATH, use the following commands.\n$ which aws /usr/local/bin/aws $ aws --version aws-cli/2.1.24 Python/3.7.4 Darwin/18.7.0 botocore/2.0.0 Click here - if you are using Windows Prerequisites Before you can install or update the AWS CLI version 2 on Windows, be sure you have the following:\nA 64-bit version of Windows XP or later\nAdmin rights to install software\nInstall or update the AWS CLI version 2 on Windows using the MSI installer Download the AWS CLI MSI installer for Windows (64-bit):\nFor the latest version of the AWS CLI: https://awscli.amazonaws.com/AWSCLIV2.msi For a specific version of the AWS CLI: Append a hyphen and the version number to the filename. For this example the filename for version 2.0.30 would be AWSCLIV2-2.0.30.msi resulting in the following link https://awscli.amazonaws.com/AWSCLIV2-2.0.30.msi . For a list of versions, see the AWS CLI version 2 changelog on GitHub .\nTo update your current installation of AWS CLI version 2 on Windows, download a new installer each time you update to overwrite previous versions. AWS CLI is updated regularly. To see when the latest version was released, see the AWS CLI version 2 changelog on GitHub .\nRun the downloaded MSI installer and follow the on-screen instructions. By default, the AWS CLI installs to C:\\Program Files\\Amazon\\AWSCLIV2.\nTo confirm the installation, open the Start menu, search for cmd to open a command prompt window, and at the command prompt use the aws --version command.\nDon\u0026rsquo;t include the prompt symbol (C:\\\u0026gt;) when you type a command. These are included in program listings to differentiate commands that you type from output returned by the AWS CLI.\nC:\\\u0026gt; aws --version aws-cli/2.1.24 Python/3.7.4 Windows/10 botocore/2.0.0 If Windows is unable to find the program, you might need to close and reopen the command prompt window to refresh the path, or add the installation directory to your PATH environment variable manually.\nFull details on installing, updating and uninstalling AWS CLI from supported operating systems is available here .\nEnable Trusted Advisor Data Collection There are 2 supported data collection methods:\nTrusted Advisor Organizational View - provides an easy way to collect Trusted Advisor checks for all accounts in your AWS Organizations without need to provision any additional resources. Only manual data refresh is supported. Trusted Advisor API via deployment of Optimization Data Collection lab - provides an automated way to collect Trusted Advisor checks for all accounts in your AWS Organizations via deployment of required AWS resources from provided AWS CloudFormation templates. Supports automated data refresh. Please choose preferred data collection method and expand respective section below to proceed:\nTrusted Advisor Organizational View Enable Trusted Advisor Organizational View For the step by step guide please follow the documentation Create S3 bucket. Create an S3 bucket in a QuickSight supported AWS region (for example us-east-1)\nCreate new folder named reports in the created S3 bucket. Trusted Advisor API via deployment of Optimization Data Collection lab Please follow the steps in Optimization Data Collection lab . Once Optimization Data Collection lab completed, please proceed with next steps.\nNOTE: Only Trusted Advisor Data Collection Module is required to be deployed. Consider other modules form the lab as optional Prepare Athena If this is the first time you will be using Athena you will need to complete a few setup steps before you are able to create the views needed. If you are already a regular Athena user you can skip these steps and move on to the Enable Quicksight section below.\nTo get Athena warmed up:\nFrom the services list, choose S3\nCreate a new S3 bucket for Athena queries to be logged to. Keep to the same region as the S3 bucket created for your Trusted Advisor Organizational View reports.\nFrom the services list, choose Athena\nSelect Get Started to enable Athena and start the basic configuration At the top of this screen select Before you run your first query, you need to set up a query result location in Amazon S3.\nEnter the path of the bucket created for Athena queries, it is recommended that you also select the AutoComplete option NOTE: The trailing “/” in the folder path is required!\nConfiguration MUST be performed at the Athena workgroup level.\nEnable QuickSight QuickSight is the AWS Business Intelligence tool that will allow you to not only view the Standard AWS provided insights into all of your accounts, but will also allow to produce new versions of the Dashboards we provide or create something entirely customized to you. If you are already a regular QuickSight user you can skip these steps.\nLog into your AWS Account and search for QuickSight in the list of Services\nYou will be asked to sign up before you will be able to use it\nAfter pressing the Sign up button you will be presented with 2 options, please ensure you select the Enterprise Edition during this step\nSelect continue and you will need to fill in a series of options in order to finish creating your account.\nEnsure you select the region that is most appropriate based on where your S3 Bucket is located containing your TA report files.\nEnable the Amazon S3 option and select the bucket where your Trusted Advisor Organizational View reports are located\nClick Finish \u0026amp; wait for the congratulations screen to display\nClick Go to Amazon QuickSight\nCheck you have Amazon QuickSight Enterprise Edition\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/1_deploy/","title":"Deploying the infrastructure","tags":[],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nDownload the time_test.yaml CloudFormation template to your machine. This lab assumes you will be deploying to the default VPC within your AWS account. If you wish to deploy to a different VPC, just select the subnet that corresponds to your VPC. If you have modified the default VPC or are using a VPC you have created, ensure that the subnet you are deploying the EC2 instances into can communicate with the internet and with AWS Systems Manager. One method for this is to Create a VPC endpoint for SSM Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: time_test.yaml Click Next\nFor Stack name use TimeTest\nParameters\nLook over the Parameters and their default values.\nStack Name – Whatever you want to call the stack for this test\nEC2InstanceSubnetId – The subnet you wish to deploy the 2 EC2 instances into for testing.\nKVMNodeInstanceType – What size KVM Node (which implies it is a Nitro instance) to use for the test\nKeyName – SSH keyname to use for the test (in case you want to ssh into the box to run additional tests)\nLatestAmiId – This will auto-populate with the latest version of the Amazon Linux AMI\nXenNodeInstanceType – Which non-nitro Xen based node to use for the test\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/","title":"Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation","tags":["implement_change"],"description":"Learn to improve reliability of a service by using automation to deploy a reliable cloud infrastructure","content":"Author Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected Introduction This hands-on lab will guide you through the steps to improve reliability of a service by using automation to deploy a reliable cloud infrastructure. When this lab is completed, you will have deployed two CloudFormation templates. The first will deploy an Amazon Virtual Private Cloud (VPC). The second will deploy into your VPC, a reliable 3-tier infrastructure using Amazon EC2 distributed across three Availability Zones. You will then review the features of the deployed infrastructure and learn how they contribute to reliability.\nAWS Well-Architected offers two different CloudFormation labs illustrating Reliability best practices. Choose which lab you prefer (or do both):\nThis lab is a 100 lab where you will do deployment-only using an AWS CloudFormation template. You will deploy a multi-tier reliable architecture. If you would prefer a more advanced lab where you create and modify CloudFormation, please see the 200 level lab Deploy and Update CloudFormation . Because the 200 lab includes modification and update as part of the exercise, it uses a simplified, single-tier, non-reliable architecture. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework best practices for reliability:\nThe architecture of the infrastructure you will deploy is represented by this diagram: Goals By the end of this lab, you will be able to:\nAutomate infrastructure deployment for a workload Understand how the deployed workload infrastructure contributes to reliability of the workload Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\nAn AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create IAM Roles, EC2 instances, S3 buckets, DynamoDb tables, VPCs, Subnets, and Internet Gateways X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy VPC using CloudFormation Deploy Web Application and Infrastructure using CloudFormation Explore the Web Application Explore the CloudFormation Template Tear down this lab Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nThis lab will cost approximately $5.50 per day when deployed The majority of this cost is the charge for NatGateway-Hours Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation Learn to improve reliability of a service by using automation to deploy a reliable cloud infrastructure\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/1_cfn_create_iam_groups_policies/","title":"AWS CloudFormation to Create Groups, Policies and Roles with MFA Enforced","tags":[],"description":"","content":"Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \u0026ldquo;baseline\u0026rdquo; of your AWS account.\n1.1 Create AWS CloudFormation Stack Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create stack. Enter the following Amazon S3 URL: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam. AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security! "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/1_config_waf/","title":"Configure AWS WAF","tags":[],"description":"","content":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront.\nSign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack. Enter the following Amazon S3 URL: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use waf. WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1. WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1. The remainder of the parameters can be left as defaults. At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, click Create stack. After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for CloudFront to use! "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_for_web_application/1_config_cloudfront/","title":"Configure CloudFront - EC2 or Load Balancer","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created.\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. Click Create Distribution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings.\nFor more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/1_iam_access/","title":"Configure IAM access","tags":[],"description":"","content":"NOTE: You will need to sign into the management account with root account credentials to perform this action.\nTo allow access to your billing information without using the root credentials you need to enable IAM access. This allows other users (non-root) to access billing information in the management account. This approach provides individual sign-in information for each user, and you can grant each user only the permissions they need to work with your account. For example, you can grant your financial teams access to the billing information only, and ensure they dont have access to resources in the account.\nLog in to your management account as the root user, Click on the account name in the top right, and click on Account from the menu: Scroll down to IAM User and Role Access to Billing Information, and click Edit: Select Activate IAM Access and click on Update: Confirm that IAM user/role access to billing information is activated: You will now be able to provide access to non-root users to billing information via IAM policies.\nNOTE: Logout as the root user before continuing.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/1_control_tower/","title":"Control Tower","tags":[],"description":"","content":"Leverage AWS ControlTower to create a set of Core AWS accounts and setup additional accounts for shared services such as build tools and individual environments for your workload. If you currently only have one account, create a new AWS account for your Control Tower management account and invite your existing account to join as a legacy AWS account. You can then migrate your workload to new accounts over time.\nControl Tower applies a number of Service Control Policies to all accounts in your AWS Organization. This will prevent modification of AWS CloudTrail trails and AWS Config rule sets in addition to a number of actions on resources matching the pattern \u0026lsquo;*aws-controltower* or \u0026lsquo;*AWSControlTower*\u0026rsquo;. If you are enabling Control Tower in an existing account you can use an AWS Config conformance pack to evaluate how your accounts may be affected by some AWS Control Tower guardrails. See AWS Control Tower Detective Guardrails as an AWS Config Conformance Pack .\nWalk through Understand best practices for your AWS environment and plan your landing zone . If you are building your own landing zone you should mirror the landing zone structure . This structure has a root account, specific accounts for logging and auditing, and allows for you to create an account per workload environment. If you are currently operating in a single account it is best practice to sign up for a new management account to enable Control Tower in and invite the existing account to join as a legacy account. This will allow you to continue to use your existing account as is but still apply baseline security controls and logging to it. If you are currently leveraging AWS Organizations it is best practice to sign up for a new management account if your current management account is used for purposes other than enabling Organizations and sharing identity. The only resources in your management account are those for enabling Control Tower, other guard rails and identity.\n(If required) Sign up for a new management account Enable Control Tower on the management account for your organization\nIf you have an existing organization refer to the documentation on applying Control Tower to existing organizations If you are not leveraging Control Tower, create an AWS Organization in the root account Invite any existing AWS accounts by enrolling an existing account in Control Tower . If you are not using Control Tower then invite an existing account to join your organization For each additional AWS account required use the account factory to create a new account. Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication . If you are not leveraging Control Tower then\nCreate a new account in organizations . Make note of the organizations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account *. Setup a logging account, secure Amazon S3 bucket and turn on your AWS Organization CloudTrail "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/1_create_dataset/","title":"Create a data set","tags":[],"description":"","content":"We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data.\nLog on to the console via SSO, go to the QuickSight service, Enter your email address and click Continue: Click Manage data in the top right: Click New data set: Click Athena: Enter a Data source name, and click Create data source: Select the costmaster database, and then the CUR table you created in Athena, and click Select: Select Directly query your data, and click Visualize: You have now configured a dataset to be able to create visualizations.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/1_create_test_group/","title":"Create a group of users for testing","tags":[],"description":"","content":"This lab requires you to develop a restrictive IAM policy, then apply the policies to a group of users, then login as a user in that group and verify the policy. We will create this test group.\nGo to the IAM service page: Click on Groups, click Create New Group: Set the group name to CostTest and click Next Step: Click Next Step: Click Create Group: Click Users: Click Add user: Configure the user as follows:\nUsername: TestUser1 Access type: AWS Management Console access Console password: Autogenerated password Un-select Require password reset Click Next: Permissions Select the CostTest group, and click Next: Tags: Click Next: Review: Review the details and click Create user: Record the logon link, the User and the Password for later use, click Close: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/1_budget_forecast/","title":"Create and implement an AWS Budget for monthly forecasted cost ","tags":[],"description":"","content":"Budgets allow you to manage cost and usage by providing notifications or restricting actions when a budget exceeds its threshold (actual or forecasted amounts).\nBudgets and notifications are updated when your billing data is updated, which is at least once per day.\nNOTE: You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account.\nCreate a monthly cost budget for your account We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget.\nLog into the console via SSO and open the Billing console. This can be achieved by using the search bar or by selecting My Billing Dashboard from your account dropdown menu. Select Budgets from the left hand menu: Click on Create budget: Ensure Cost budget is selected, and click on Next: To create a cost budget, enter the following details:\nPeriod: Monthly Budget effective date: Recurring Budget Start month: (select current month) Choose how to budget: Fixed Budgeted amount: 1.00 (enter a dollar amount a lot LESS than last months cost) Name: CostBudget1 Other fields: leave as defaults: Once you have entered all the details select Next: To create an alert for our budget select Add an alert threshold: For Alert #1 select:\nThreshold: 100% of budgeted amount Trigger: Forecasted Notification preferences: Input your email address in the Email recipients field Click on Next: Here you can attach actions that can be taken when you budget exceeds its threshold. We will not be attaching any actions for this lab. Select Next to move to the next page: Review the configuration, and click Create budget: You should see the current forecast will exceed the budget (you may need to refresh your browser): You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/1_create_policies/","title":"Create IAM policies","tags":[],"description":"","content":"1.1 Create policy for permission boundary This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements.\nSign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy. On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;EC2RestrictRegion\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [ \u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-1\u0026#34; ] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaRestrictRegion\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [ \u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-1\u0026#34; ] } } } ] } Click Review policy. Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy. 1.2 Create developer IAM restricted policy This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1, and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here.\nCreate a managed policy using the JSON policy below and name of createrole-restrict-region-boundary. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;CreatePolicy\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreatePolicyVersion\u0026#34;, \u0026#34;iam:DeletePolicyVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::123456789012:policy/app1*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CreateRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/app1*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PermissionsBoundary\u0026#34;: \u0026#34;arn:aws:iam::123456789012:policy/restrict-region-boundary\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AttachDetachRolePolicy\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/app1*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnEquals\u0026#34;: { \u0026#34;iam:PolicyARN\u0026#34;: [ \u0026#34;arn:aws:iam::123456789012:policy/*\u0026#34;, \u0026#34;arn:aws:iam::aws:policy/*\u0026#34; ] } } } ] } 1.3 Create developer IAM console access policy This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation.\nCreate a managed policy using the JSON policy below and name of iam-restricted-list-read. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Get\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:ListPolicies\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetPolicyVersion\u0026#34;, \u0026#34;iam:ListRoleTags\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListAttachedRolePolicies\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/1_create_policies/","title":"Create IAM policies","tags":[],"description":"","content":"The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California).\n1.1 Create policy named ec2-list-read This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements.\nSign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy. On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ec2listread\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:Get*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [ \u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-1\u0026#34; ] } } } ] } Click Review policy. Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy. 1.2 Create policy named ec2-create-tags This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance.\nCreate a managed policy using the JSON policy below and name of ec2-create-tags. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ec2createtags\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:CreateAction\u0026#34;: \u0026#34;RunInstances\u0026#34; } } } ] } 1.3 Create policy named ec2-create-tags-existing This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha.\nCreate a managed policy using the JSON policy below and name of ec2-create-tags-existing. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ec2createtagsexisting\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:ResourceTag/Team\u0026#34;: \u0026#34;Alpha\u0026#34; }, \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;aws:TagKeys\u0026#34;: [ \u0026#34;Team\u0026#34;, \u0026#34;Name\u0026#34; ] }, \u0026#34;StringEqualsIfExists\u0026#34;: { \u0026#34;aws:RequestTag/Team\u0026#34;: \u0026#34;Alpha\u0026#34; } } } ] } 1.4 Create policy named ec2-run-instances This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition.\nCreate a managed policy using the JSON policy below and name of ec2-run-instances. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ec2runinstances\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ec2:*:*:instance/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [ \u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-1\u0026#34; ], \u0026#34;aws:RequestTag/Team\u0026#34;: \u0026#34;Alpha\u0026#34; }, \u0026#34;ForAllValues:StringEquals\u0026#34;: { \u0026#34;aws:TagKeys\u0026#34;: [ \u0026#34;Name\u0026#34;, \u0026#34;Team\u0026#34; ] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;ec2runinstancesother\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ec2:*:*:subnet/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*:key-pair/*\u0026#34;, \u0026#34;arn:aws:ec2:*::snapshot/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*:launch-template/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*:volume/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*:security-group/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*:placement-group/*\u0026#34;, \u0026#34;arn:aws:ec2:*:*:network-interface/*\u0026#34;, \u0026#34;arn:aws:ec2:*::image/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:RequestedRegion\u0026#34;: [ \u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-1\u0026#34; ] } } } ] } 1.5 Create policy named ec2-manage-instances This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region.\nCreate a managed policy using the JSON policy below and name of ec2-manage-instances. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ec2manageinstances\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:RebootInstances\u0026#34;, \u0026#34;ec2:TerminateInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:ResourceTag/Team\u0026#34;: \u0026#34;Alpha\u0026#34;, \u0026#34;aws:RequestedRegion\u0026#34;: [ \u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-1\u0026#34; ] } } } ] } "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/1_create_role_acct_2/","title":"Create role for Lambda in account 2","tags":[],"description":"","content":" Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ .\nClick Roles on the left, then create role.\nClick Another AWS account, enter the account id for account 1 (the origin), then click Next: Permissions.\nDo not select any managed policies, click Next: Tags.\nClick Next: Review.\nEnter LambdaS3ListBuckets for the Role name then click Create role.\nFrom the list of roles click the name of LambdaS3ListBuckets.\nCopy the Role ARN and store for use later in this lab.\nClick Add inline policy, then click JSON tab.\nReplace the sample json with the following, then click Review Policy.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;S3ListAllMyBuckets\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListAllMyBuckets\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Name this policy LambdaS3ListBucketsPolicy, then click Create policy.\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/1_cf_stack/","title":"Create the CloudFormation Stack","tags":[],"description":"","content":"This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data.\nWe will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered.\nNOTE: IAM roles will be created, these are used to:\nAdd event notification to existing S3 buckets\nCreate s3 buckets and upload objects\nCreate and run a Glue crawler\nCreate and update a Glue database and tables\nPlease review the CloudFormation template with your security team.\nWe will build the following solution: Log into the console via SSO. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next: Specify the details for the stack and click Next: Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack: You will see the stack will start in CREATE_IN_PROGRESS: Once complete, the stack will show CREATE_COMPLETE: Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn: View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/1_data_sources/","title":"Create the Data Sources","tags":[],"description":"","content":"We first need to create data sources containing the application logs, and the cost and usage reports. In this lab we provide sample files, it is recommended you use these files initially, then use your own files after you are familiar with the requirements and process.\nWe place both logs into S3, crawl them with Glue and then use Athena to confirm a database is created that we can use.\nCopy files into S3 We will create a bucket and folders in S3, then copy the sample application log files, and cost and usage reports into the folders.\nNOTE Please read the steps carefully, as the naming is critical and any mistakes will require you to rebuild the lab or make significant and repetitive changes.\nLog into the AWS console via SSO: Make sure you run everything in a single region\nGo to the S3 console, Create a new S3 Bucket, it can have any name, but make it start with costefficiencylab to make it identifiable.\nCreate a folder in the new bucket with a name: applogfiles_workshop. NOTE: You MUST name the folder applogfiles_workshop Upload the application log file to the folder: Step1_access_log.gz Click here - if using your own log files If you will be using your own application log files, systems manager can be used to run commands across your environment and copy files from multiple servers to S3.\nDepending on your operating system, you can execute CLI on your application servers to copy the application log files to your S3 bucket. The following Linux sample will copy all access logs from the httpd log directory to the s3 bucket created above using the hostname to separate each servers logs:\nHOSTNAME=$(hostname) aws s3 cp --recursive /var/log/httpd/ s3://applogfiles-workshop/$HOSTNAME --exclude \u0026quot;*\u0026quot; --include \u0026quot;access_log*\u0026quot; Create a folder named costusagefiles_workshop, inside the same bucket.\nNOTE: You MUST name the folder costusagefiles_workshop, this will make pasting the code faster.\nCopy the sample file to your bucket into the costusagefiles_workshop folder:\nStep1CUR.gz Create applicaton log file data source with Glue We will create a database with the uploaded application logs, with AWS Glue. For the application log files, we show you how to write a custom classifier, so you can handle any log file format from any application.\nFor our sample application logs, we have supplied Apache web server log files. The in-bulit AWS Glue classifier COMBINEDAPACHELOG will recognize these files, for example, it will read the timestamp as a single string. We will customize the interpreter to break this up into a date column, timestamp column, and timezone column. This will demonstrate how to write a customer classifier. The reference for classifiers is here: https://docs.aws.amazon.com/glue/latest/dg/custom-classifier.html A sample log file line is:\n10.0.1.80 - - [26/Nov/2019:00:00:07 +0000] \u0026quot;GET /health.html HTTP/1.1\u0026quot; 200 55 \u0026quot;-\u0026quot; \u0026quot;ELB-HealthChecker/2.0\u0026quot; The original columns are:\n- Client IP - Ident - Auth - HTTP Timestamp* - Request - Response - Bytes - Referrer - Agent Using the custom classifier, we will make it build the following columns instead:\n- Client IP - Ident - Auth - Date* - Time* - Timezone* - Request - Response - Bytes - Referrer - Agent Go to the Glue console and click Classifiers: Click Add classifier and create it with the following details:\nClassifier name: WebLogs\nClassifier type: Grok\nClassification: Logs\nGrok pattern:\n%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\u0026quot; %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Custom patterns:\nDATE %{MONTHDAY}/%{MONTH}/%{YEAR} Click Create A classifier tells Glue how to interpret the log file lines, and how to create columns. Each column is contained within %{}, and has the pattern, the separator \u0026lsquo;:\u0026rsquo;, and the column name.\nBy using the custom classifier, we have separated the column timestamp into 3 columns of logdate, logtime and tz. You can compare the custom classifier we wrote with the COMBINEDAPACHELOG classifier:\nCustom - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\u0026quot; %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Builtin - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\u0026quot; %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Next we will create a crawler to read the log files, and build a database. Click on Crawlers and click Add crawler: Configure the crawler:\nCrawler name will be ApplicationLogs Expand Tags, description.. next to our Weblogs classifier, cilck Add Click Next: Crawler source type is Data stores, click Next: Click the folder icon and expand your bucket created above, select the radio button next to the applogfiles_workshop. Do NOT select the actual file or bucket, select the folder. Click Select. Click Next\nSelect No to not add another data store, click Next\nCreate an IAM role named AWSGlueServiceRole-CostWebLogs and click Next: Frequency will be run on demand, click Next\nClick Add database, you MUST name it webserverlogs, click Create.\nClick Next: Click Finish\nSelect and Run crawler, this will create a single database and table with our log files. We need to wait until the crawler has finished, this will take 1-2 minutes. Click refresh to check if it has finished.\nWe will confirm the database has built correctly. Click Databases on the left, and click on the database webserverlogs, you may need to click refresh: Click Tables in webserverlogs, and click the table applogfiles_workshop\nYou can see the table is created, the Name, the Location, and the recordCount has a large number of records in it (the number may be different to the image below): Scroll down and you can see the columns, and that they are all string. This will be a small hurdle for non-string columns like bytes if you want to perform a mathematical function on it. We will work around this with Athena in our example.\nClick here - if using your own log files If using your own application files, you may wish to adjust the field types here. This will typically be anything numerical that you would do mathematical operations on, like a sum or average.\nGo to the Athena service console, and select the webserverlogs database\nClick the three dots next to the table applogfiles_workshop, and click Preview table: View the results which will show 10 lines of your log. Note how there are separate columns logdate, logtime and tz that we created. The default classifier would have had a single column of text for the timestamp. Create cost and usage data source with Glue To measure efficiency we need to know the cost of the workload, so we will use the Cost and Usage Report. We will follow the process above to create\nREAD ONLY - If using your own CUR files If you are using your own Cost and Usage Reports, you will need to have them already configured and delivered as per this lab . The rest of this section is not required, as the Cost and Usage data will be correctly setup.\nTo use the files from this lab, follow the steps below:\nGo into the Glue console, click Crawlers, and click Add crawler\nUse the crawler name CostUsage and click Next\nSelect Data stores as the crawler source type, click Next\nClick the folder icon, Select the S3 folder created above costefficiency and select the costusagefiles-workshop folder, make sure you dont select the bucket or file.\nClick Select, then click Next\nSelect No do not another data store, click Next\nCreate an IAM role named AWSGlueServiceRole-Costusage, click Next\nSet the frequency to run on demand, click Next\nCilck Add database, it MUST be named CostUsage, and click Create\nclick Next\nReview and click Finish\nRun the crawler CostUsage, then use Athena to check the database costusage was created and has records in the table costusagefiles_workshop, as per the Application logs database setup above.\nYou now have both the application and cost data sources setup, ready to create an efficiency metric.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_create_a_data_bunker/1_instructions/","title":"Creating data bunker account in console","tags":[],"description":"","content":"1. Create a logging account from the organizations management account Best practice is to have a separate logging account for your data bunker. This account should only be accessible by folks in your security group with a read only role. How you create this account will depend on your organization\u0026rsquo;s policies, the instructions below are guidance on how to do this. If you do not currently have a landing zone setup see the quest Quick Steps to Security Success for a more in-depth discussion.\nLogin to the management account of your AWS Organization If you do not have an account within your organization to store security logs. Navigate to AWS Organizations and select Create Account. Include a cross account access role and note it\u0026rsquo;s name (default is OrganizationAccountAccessRole) - we will modify this later to remove unnecessary access (Optional) If your role does not have permission to assume any role you will also have to add an IAM policy. The AWS administrator policy has this by default, otherwise follow the steps in the AWS Organizations Documentation to grant permissions to access the role Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication Navigate to Settings and take a note of your Organization ID 2. Create the bucket for CloudTrail logs Switch roles into the logging account for your organization Navigate to S3 Press Create Bucket Enter a Bucket name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Accept default value for Block all public access. Enable bucket versioning , to keep multiple versions of an object so you can recover an object if you unintentionally modify or delete it. Click Create bucket. Press the bucket we just create and navigate to the Properties tab (Strongly recommended unless tearing down immediately) Under Object Lock, enable compliance mode and set a retention period. The length of the retention period will depend on your organizational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs. Note: You will be unable to delete files within this window or the bucket if objects still exist in it Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Press Save { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailAclCheck20150319\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[bucket]\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailWrite20150319\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[bucket]/AWSLogs/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailWrite20150319\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[bucket]/AWSLogs/[organization id]/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } } ] } (Optional) Next we will add a life cycle policy to clean up old logs. Navigate to Management (Optional) Add a life cycle rule named Delete old logs, press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days 3. Ensure cross account access is read-only These instructions outline how to modify the cross account access created in step 1 is read-only. As with step 1, this will depend on how your organization\u0026rsquo;s policies. The key is that our security team are not able to modify data in our data bunker. Human access should only be in a break-glass emergency situation.\nNote: Following these steps will prevent OrganizationAccountAccessRole from making further changes to this account. Ensure other services such as Amazon Guard Duty and AWS Security Hub are configured before proceeding. If further changes are needed you will have to reset the root credentials for the security account.\nNavigate to IAM and select Roles Select the organizations account access role for your organization: Note: the default is OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy 4. Turn on CloudTrail from the management account Switch back to the management account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Enter a name for the trail such as OrganizationTrail Select Yes next to Apply trail to my organization Under Storage location, select No for Create new S3 bucket and enter the bucket name of the bucket created in step 2 Verification Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail License Licensed under the Apache 2.0 and MITnoAttr License.\nCopyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\nLicensed under the Apache License, Version 2.0 (the \u0026ldquo;License\u0026rdquo;). You may not use this file except in compliance with the License. A copy of the License is located at\nhttps://aws.amazon.com/apache2.0/ or in the \u0026ldquo;license\u0026rdquo; file accompanying this file. This file is distributed on an \u0026ldquo;AS IS\u0026rdquo; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/1_deploy_infra/","title":"Deploy Infrastructure using a CloudFormation Stack","tags":[],"description":"","content":"This lab illustrates best practices for reliability as described in the AWS Well-Architected Reliability pillar.\nHow do you implement change?\nBest practice: Deploy changes with automation: Deployments and patching are automated to eliminate negative impact. Design principle: Manage change in automation: Changes to your infrastructure should be made using automation. The changes that need to be managed include changes to the automation, which then can be tracked and reviewed. When this lab is completed, you will have deployed and edited a CloudFormation template. Using this template you will deploy a VPC, an S3 bucket and an EC2 instance running a simple web server.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 The CloudFormation template You will begin by deploying a CloudFormation stack that creates a simple VPC as shown in this diagram:\nDownload the simple_stack.yaml CloudFormation template Open this file in a Text Editor Preferably use an editor that is YAML aware like vi/vim, VS Code, or Notepad++ Do NOT use a Word Processor The template is written in a format called YAML , which is commonly used for configuration files. The format of the file is important, especially indents and hyphens. CloudFormation templates can also be written in JSON.\nLook through the file. You will notice several sections:\nThe Parameters section is used to prompt for inputs that can be used elsewhere in the template. The template is asking for several inputs, but also provides default values for each one. Look through these and start to reason about what each one is.\nThe Conditions section is where you can setup if/then-like control of what happens during template deployment. It defines the circumstances under which entities are created or configured.\nThe Resources section is the \u0026ldquo;heart\u0026rdquo; of the template. It is where you define the infrastructure to be deployed. Look at the first resource defined.\nIt is the VPC (Amazon Virtual Private Cloud) It has a logical ID which in this case is SimpleVPC. This logical ID is how we refer to the VPC resource within the CloudFormation template. It has a Type which tells CloudFormation which type of resource to create And it has Properties that define the values used to create the VPC The Outputs section is used to display selective information about resources in the stack.\nThe Metadata section here is used to group and order how the CloudFormation parameters are displayed when you deploy the template using the AWS Console\nCloudFormation tip When editing CloudFormation templates written in YAML, be extra cautious that you maintain the correct number of spaces for each indentation Indents are always in increments of two spaces. You can use your IDE (may require extensions) or a free online linter such as http://www.yamllint.com/ to identify and correct any syntax errors you may have within your YAML file. You will now use this template to launch a CloudFormation stack that will deploy AWS resources in your AWS account.\n1.3 Deploying an AWS CloudFormation stack to create a simple VPC Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: simple_stack.yaml Click Next\nFor Stack name use CloudFormationLab\nParameters\nLook over the Parameters and their default values.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nDeployment will take approximately 30 seconds to deploy.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/1_deploy_cfn_stack/","title":"Deploy the CloudFormation Stack","tags":[],"description":"","content":"This portion of the lab shows you how to deploy an EC2 instance using a CloudFormation template. The CloudFormation template will deploy the following:\nLab VPC: The VPC used in this lab. This VPC contains a single public subnet, each with its own route table. An Internet Gateway and is used to route traffic to the public subnet. EC2 Instance: This is the EC2 instance that hosts the simple Apache web application. You will also be configuring the CloudWatch Logs Agent to work on this instance. IAM Role: An IAM role that allows the instance to send logs and metrics to CloudWatch and allows SSM actions to be performed on the instance. S3 Bucket: The S3 bucket to store log files generated in this lab. Download the CloudFormation template provided in this lab . OPTIONAL: Look through the CloudFormation template and comments to see the resources deployed. More information on templates can be found here . Go to CloudFormation console , click Create Stack, and select With new resources (standard). In the Specify Template menu, choose Upload a template file, then Choose file, and select the security-lab-stack.yaml template you downloaded. In the Specify Stack Details menu: Enter a stack name, such as security-cw-lab. Note the name down, as you will need to re-visit this stack for Outputs later on. Enter your name in the DisplayedName field, this will be the name that appears on your sample website! Enter an S3 bucket name in the BucketName field. Amazon S3 bucket names are globally unique, and the namespace is shared by all AWS accounts, so make sure your bucket is names as uniquely as possible. For example: wa-lab-\u0026lt;your-account-id\u0026gt;-\u0026lt;date\u0026gt;. Do not modify the LatestAmiId field. This uses a public parameter stored in Systems Manager Parameter Store that will automatically use the latest Amazon Machine Image (AMI) for the EC2 instance. No changes are needed on the Configure Stack Options page. Click through Next.\nOn the Review page, check the box in the Capabilities section to allow the creation of an IAM role. This selection gives the CloudFormation template permission to create IAM roles - in particular, the role used to allow the EC2 instance to interact with SSM and CloudWatch. Click Create Stack. You will be taken back to the CloudFormation console, where your stack will be launched.\nOnce the stack shows CREATE COMPLETE, click on the Outputs tab and click on the WebsiteURL, you will be brought to your sample web server. Recap: In this portion of the lab, you deployed a CloudFormation stack to create the base resources needed for this lab.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/1_deploy_infra/","title":"Deploy the Infrastructure","tags":[],"description":"","content":"You will create two Amazon S3 buckets in two different AWS regions. The Ohio region (also known as us-east-2) will be referred to throughout this lab as the east S3 bucket, and Oregon (also known as us-west-2) will be referred to as the west S3 bucket.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the infrastructure in two AWS Regions using an AWS CloudFormation template You will deploy the infrastructure for two Amazon S3 buckets. Since these will be in two different regions, you will need to create an AWS CloudFormation stack in each region. You will use the same CloudFormation template for both regions.\nYou must use the same NamingPrefix parameter for both CloudFormation templates in both regions, or replication will fail!\nDownload the s3_bucket.yaml CloudFormation template 1.2.1 Deploy east S3 bucket It is recommended that you deploy the east s3 bucket in the Ohio region. This region is also known as us-east-2. Use the drop-down to select this region If you choose to use a different region, you will need to ensure future steps are consistent with your region choice. On the AWS Console go to the CloudFormation console Select Stacks Create a CloudFormation stack (with new resources) using the CloudFormation Template file and the Upload a template file option. For Stack name use S3-CRR-lab-east Under Parameters enter a NamingPrefix This will be used to name your S3 buckets Must be string consisting of lowercase letters, numbers, periods (.), and dashes (-) between five and 40 characters This will be part of your Amazon S3 bucket name, which must be unique across all of S3. Record this value in an accessible place \u0026ndash; you will need it again later in the lab. Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack You can go ahead and create the west bucket before this CloudFormation stack completes Troubleshooting: If your CloudFormation stack deployment fails with the error \u0026lt;bucket name\u0026gt; already exists\nYou did not pick a unique enough NamingPrefix Delete the failed stack Start over and choose a more unique NamingPrefix Amazon S3 bucket names share a global name space across all of AWS (including all AWS regions) 1.2.2 Deploy west S3 bucket It is recommended that you deploy the west s3 bucket in the Oregon region for this lab. This region is also known as us-west-2.\nUse the drop-down to select this region If you choose to use a different region, you will need to ensure future steps are consistent with your region choice. On the AWS Console go to the CloudFormation console Select Stacks\nCreate a CloudFormation stack (with new resources) using the same CloudFormation Template file as before, and the Upload a template file option.\nFor Stack name use S3-CRR-lab-west\nUnder Parameters enter a NamingPrefix\nYou must use the same value as you did previously Click Next until the last page\nAt the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names\nClick Create stack\n1.2.3 Get bucket information Go back to the Ohio AWS Region and wait for the CloudFormation stack you created there to complete Click on the Outputs tab and record the Value of the S3 bucket name in an accessible location as east bucket Go to the the Oregon AWS Region and do the same thing, copying that S3 bucket name down as west bucket Go to the Amazon S3 console and verify that both buckets were created. Although S3 buckets are specific to an AWS region, the Amazon S3 console shows all buckets from all AWS Regions The two S3 buckets you will work with begin with \u0026lt;your_naming_prefix\u0026gt;-crrlab Note the regions for the two S3 buckets your created There are also two new logging buckets \u0026ndash; you will not need to do any actions with these. Click on either the east region or west region bucket, and note the following This bucket is empty - We will be adding objects to the bucket soon Click on Properties and note what properties are Enabled Click here to learn why are these properties enabled Versioning is Enabled: For S3 Replication, both source and destination buckets MUST have versioning enabled\nDefault encryption is Enabled: In our exercise we are demonstrating replication of encrypted objects. It is a best practice to encrypt your data at rest.\nObject-level logging is Enabled: This logging will be used later in the lab. It is used to better understand replication operations AWS takes on your behalf.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/1_prerequisite/","title":"Deploy the Infrastructure and Application","tags":[],"description":"","content":"The first step of this lab is to deploy the static web application stack. If you have already run the following two labs (and have not torn down the resources) then you have already deployed the necessary infrastructure. Proceed to next step Configure Execution Environment\nSecurity: Level 200: Automated Deployment of VPC Security: Level 200: Automated Deployment of EC2 Web Application If you have not already deployed the necessary infrastructure, then follow these steps:\nYou will first deploy an Amazon Virtual Private Cloud (VPC) You will then deploy a Static WebApp hosted on Amazon EC2 instances For each of these deployments: If you are comfortable deploying a CloudFormation stack, then use the Express Steps If you require detailed guidance in how to deploy a CloudFormation stack, then use the Guided Steps 1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the VPC infrastructure Choose either the Express Steps or Guided Steps\nIn the AWS Console, choose the AWS region you wish to use - if possible we recommend using us-east-2 (Ohio) Express Steps (Deploy the VPC infrastructure) Download the vpc-alb-app-db.yaml CloudFormation template Deploy the CloudFormation template Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values When the stack status is CREATE_COMPLETE, you can continue to the next step Guided Steps (Deploy the VPC infrastructure) Click here for detailed instructions to deploy the VPC: This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\nWhen the stack status is CREATE_COMPLETE, you can continue to the next step\n1.3 Deploy the EC2s and Static WebApp infrastructure Choose either the Express Steps or Guided Steps\nExpress Steps (Deploy the EC2s and Static WebApp infrastructure) Download the staticwebapp.yaml CloudFormation template Choose the same AWS region as you did for the VPC (if you used our recommendation, this is us-east-2 (Ohio)) Deploy the CloudFormation template Name the stack WebApp1-Static (case sensitive) Leave all CloudFormation Parameters at their default values When the stack status is CREATE_COMPLETE, you can continue to the next step Guided Steps (Deploy the EC2s and Static WebApp infrastructure) Click here for detailed instructions to deploy the WebApp: Download the latest version of the CloudFormation template here: staticwebapp.yaml Choose the same AWS region as you did for the VPC (if you used our recommendation, this is us-east-2 (Ohio)) Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next\nFor Stack name use WebApp1-Static\nParameters\nLook over the Parameters and their default values.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nWebsite URL Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . Wait until WebApp1-Static stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the WebApp1-Static stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service Save this URL - you will need it later X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/1_deploy_app/","title":"Deploy the Infrastructure and Application","tags":[],"description":"","content":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2. It has an Elastic Load Balancer reverse-proxy in front of it, and has a dependency on Amazon DynamoDB.\nNote: The concepts covered by this lab apply whether your service dependency is an AWS resource like Amazon DynamoDB, or an external service called via API. The DynamoDB dependency here acts as a mock for an external service called RecommendationService. The getRecommendation API on this service is a dependency for the web service used in this lab. getRecommendation is actually a get_item call to a DynamoDB table.\n1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the application using an AWS CloudFormation template You will deploy the service infrastructure including simple service code and some sample data.\nIt is recommended that you use the Ohio region. This region is also known as us-east-2, which you will see referenced throughout this lab. If you choose to use a different region, you will need to ensure future steps are consistent with your region choice. 1.2.1 Deploy the VPC infrastructure If you are comfortable deploying a CloudFormation stack, then use the Express Steps If you require detailed guidance in how to deploy a CloudFormation stack, then use the Guided Steps Choose either the Express Steps or Guided Steps\nExpress Steps (Deploy the VPC infrastructure) Download the vpc-alb-app-db.yaml CloudFormation template Make sure you are in AWS region: us-east-2 (Ohio) Deploy the CloudFormation template Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values When the stack status is CREATE_COMPLETE, you can continue to the next step Guided Steps (Deploy the VPC infrastructure) Click here for detailed instructions to deploy the VPC: This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\nWhen the stack status is CREATE_COMPLETE, you can continue to the next step\n1.2.2 Deploy the web app infrastructure and service Wait until the VPC CloudFormation stack status is CREATE_COMPLETE, then continue. This will take about four minutes.\nChoose either the Express Steps or Guided Steps\nExpress Steps (Deploy the EC2s and Static WebApp infrastructure) Download the staticwebapp.yaml CloudFormation template Make sure you are in AWS region: us-east-2 (Ohio) Deploy the CloudFormation template Name the stack HealthCheckLab (case sensitive) Leave all CloudFormation Parameters at their default values When the stack status is CREATE_COMPLETE, you can continue to the next step Guided Steps (Deploy the EC2s and Static WebApp infrastructure) Click here for detailed instructions to deploy the WebApp: Download the latest version of the CloudFormation template here: staticwebapp.yaml Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next\nFor Stack name use HealthCheckLab\nParameters\nLook over the Parameters and their default values.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\n1.3 View the website for web service Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation .\nWait until HealthCheckLab stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the HealthCheckLab stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service Hint: it will start with http://healt-alb and end in \u0026lt;aws region\u0026gt;.elb.amazonaws.com Click the URL and it will bring up the website:\nThe website simulates a recommendation engine making personalized suggestions for classic television shows. You should note the following features:\nArea A shows the personalized recommendation It shows first name of the user and the show that was recommended The workshop simulation is simple. On every request it chooses a user at random, and shows a recommendation statically mapped to that user. The user names, television show names, and this mapping are in a DynamoDB table, which is simulating the RecommendationService Area B shows metadata which is useful to you during the lab The instance_id and availability_zone enable you to see which EC2 server and Availability Zone were used for each request There is one EC2 instance deployed per Availability Zone Refresh the website several times, note that the EC2 instance and Availability Zone change from among the three available This is Elastic Load Balancing (ELB) distributing these stateless requests among the available EC2 server instances across Availability Zones Well-Architected for Reliability: Best practices Use highly available network connectivity for your workload public endpoints: Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases. Implement loosely coupled dependencies: Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility. Deploy the workload to multiple locations: Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/1_deploy_the_lab_base_infrastructure/","title":"Deploy The Lab Base Infrastructure","tags":[],"description":"","content":"In this section, we will build out a Virtual Public Cloud (VPC) , together with public and private subnets across two Availability Zones , Internet Gateway and NAT gateway along with the necessary routes from both public and private subnets.\nThis VPC will become the baseline network architecture within which the application will run. When we successfully complete our initial template deployment, our deployed workload should reflect the following diagram:\nTo deploy the infrastructure template follow the appropriate steps:\n1.1. Get the Cloudformation Template. To deploy the first CloudFormation template, you can either deploy directly from the command line or via the console.\nYou can get the template here. Click here for CloudFormation command-line deployment steps Command Line Deployment: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n1.1.1. Execute Command aws cloudformation create-stack --stack-name pattern3-base \\ --template-body file://pattern1-base.yml \\ --region ap-southeast-2 Note: Please adjust your command-line if you are using profiles within your aws command line as required.\n1.1.2. Confirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation describe-stacks --stack-name pattern3-base Locate the StackStatus and confirm it is set to CREATE_COMPLETE as shown here:\n1.1.3. Take note of this stack output as we will need it for later sections of the lab.\nClick here for CloudFormation console deployment steps Console: If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\nPlease follow this guide for information on how to deploy the cloudformation template. Use pattern1-base as the Stack Name, as this is referenced by other stacks later in the lab. 1.2. Note Cloudformation Template Outputs When the CloudFormation template deployment is completed, note the outputs produced by the newly created stack as these will be required at later points in the lab.\nYou can do this by clicking on the stack name you just created, and select the Outputs Tab as shown in diagram below.\nYou can now proceed to Section 2 of the lab where we will build out the application stack.\nEND OF SECTION 1\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/1_deploy_the_lab_base_infrastructure/","title":"Deploy The Lab Base Infrastructure","tags":[],"description":"","content":"In this section, we will build out a Virtual Public Cloud (VPC) , together with public and private subnets across two Availability Zones , Internet Gateway and NAT gateway along with the necessary routes from both public and private subnets.\nThis VPC will become the baseline network architecture within which the application will run. When we successfully complete our initial stage template deployment, our deployed workload should reflect the following diagram:\nTo deploy the template for the base infrastructure build follow the approptiate steps:\n1.1. Get the Cloudformation Template. To deploy the first CloudFormation template, you can either deploy directly from the command line or via the console.\nYou can get the template here. Click here for CloudFormation command-line deployment steps Command Line Deployment: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n1.1.1. Execute Command aws cloudformation create-stack --stack-name pattern3-base \\ --template-body file://pattern3-base.yml \\ --region ap-southeast-2 Note: Please adjust your command-line if you are using profiles within your aws command line as required.\n1.1.2. Confirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation desribe-stacks --stack-name pattern3-base Locate the StackStatus and confirm it is set to CREATE_COMPLETE as shown here:\n1.1.3. Take note of this stack output as we will need it for later sections of the lab.\nClick here for CloudFormation console deployment steps Console: If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. Use pattern3-base as the Stack Name, as this is referenced by other stacks later in the lab. 1.2. Note Cloudformation Template Outputs When the CloudFormation template deployment is completed, note the outputs produced by the newly created stack as these will be required at later points in the lab.\nYou can do this by clicking on the stack name you just created, and select the Outputs Tab as shown in diagram below.\nYou can now proceed to Section 2 of the lab where we will build out the application stack.\nEND OF SECTION 1\n"},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/1_deploy_base_application_environment/","title":"Deploy the sample application environment","tags":[],"description":"","content":"In this section, you will prepare a sample application. The application is an API hosted inside a docker container, using Amazon Elastic Compute Service (ECS). . The container is accessed via an Application Load Balancer. The API is a private microservice within your Amazon Virtual Private Cloud (VPC) . Communication to the API can only be done privately through routes within the VPC subnet. In our lab example, the business owner has agreed to run the API over HTTP protocol to simplify the implementation.\nThe API has two actions available which encrypt and decrypt information. This is triggered by doing a REST POST call to the /encrypt / /decrypt methods as appropriate.\nThe encrypt action will allow you to pass a secret message along with a \u0026lsquo;Name\u0026rsquo; key as the identifier and it will return a \u0026lsquo;Secret Key Id\u0026rsquo; that you can use later to decrypt your message. The decrypt action allows you to then decrypt the secret message passing along the \u0026lsquo;Name\u0026rsquo; key and \u0026lsquo;Secret Key Id\u0026rsquo; you obtained before to get your secret message. Both actions will make a write and read call to the application database hosted in Amazon Relation Database Service (RDS) , where the encrypted messages are stored.\nThe following step-by-step instructions will provision the application that you will use with your runbooks and playbooks .\nExplore the contents of the CloudFormation script to learn more about the environment and application.\nYou will use this sample application as a sandbox to simulate an application performance issue, start your runbooks and playbooks to autonomously investigate and remediate.\nActions items in this section: You will prepare the Cloud9 workspace launched with a new VPC. You will run the application build script from the Cloud9 console to build the sample application as shown in the diagram below. 1.0 Prepare Cloud9 workspace. In this first step you will provision a CloudFormation stack that builds a Cloud9 workspace along with the VPC for the sample application. This Cloud9 workspace will be used to run the provisioning script of the sample application. You can choose the to deploy stack in one of the regions below.\nClick on the link below to deploy the stack. This will take you to the CloudFormation console in your account. Use walab-ops-base-resources as the stack name, and take the default values for all options.\nus-west-2 : here ap-southeast-2 : here ap-southeast-1 : here Once the template is deployed, wait until the CloudFormation Stack reaches the CREATE_COMPLETE state.\n1.1 Run the build application script. Next, run the build script to build and deploy you application environment from the Cloud9 workspace as follows:\nFrom the main console, access the Cloud9 service.\nClick Your environments section on the left menu, and locate an environment named WellArchitectedOps-walab-ops-base-resources as below, then click Open IDE.\nYour environment will bootstrap the lab repository. You should see a terminal output showing the following output:\nWhen the bootstrap script finishes you will see a folder called aws-well-architected-labs.\nIn the IDE terminal console, change directory to the working folder where the build script is located:\ncd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/scripts/ Copy and paste the command below, replacing sysops@domain.com and owner@domain.com with the email address you would like the application to notify you with. Replace the sysops@domain.com value with email representing system operators team and owner@domain.com with email address representing business owner.\nbash build_application.sh walab-ops-base-resources sysops@domain.com owner@domain.com The build_application.sh script will build and deploy your sample application, along with the architecture that hosts it. The application architecture will have capabilities to notify systems operators and owners, leveraging Amazon Simple Notification Service . You can use the same email address for sysops@domain.com and owner@domain.com if you need to, but ensure that you have both values specified.\nIf you have deployed Amazon ECS before in your account, you may encounter InvalidInput error with message \u0026ldquo;AWSServiceRoleForECS has been taken\u0026rdquo; while running the build_application.sh script. You can safely ignore this message, as the script will continue despite the error.\nThe above command runs the build and provisioning of the application stack. The script should take about 20 mins to finish.\nThe build_application.sh will deploy the application docker image and push it to Amazon ECR . This is used by Amazon ECS. Once the build script completes, another CloudFormation stack containing the application resources (ECS, RDS, ALB, and others) will be deployed.\nIn the CloudFormation console, you should see a new stack being deployed called walab-ops-sample-application. Wait until the stack reaches CREATE_COMPLETE state and proceed to the next step.\n1.2. Confirm the application status. Once the application is successfully deployed, go to your CloudFormation console and locate the stack named walab-ops-sample-application.\nConfirm that the stack is in a \u0026lsquo;CREATE_COMPLETE\u0026rsquo; state.\nRecord the following output details as it will be required later:\nTake note of the DNS value specified under OutputApplicationEndpoint of the Outputs.\nThe screenshot below shows the output from the CloudFormation stack:\nCheck for an email sent to the system operator and owner addresses you\u0026rsquo;ve specified in the build_application.sh script. This email should also be visible in the CloudFormation parameter under in the SystemOpsNotificationEmail and SystemOwnerNotificationEmail.\nClick confirm subscription on the email links to subscribe.\nThere will be 2 emails sent to your address, please ensure to subscribe to both of them.\n1.3. Test the application. In this section, you will be testing the encrypt API action from the deployed application.\nThe application will take a JSON payload with Name as the identifier and Text key as the value of the secret message.\nThe application will encrypt the value under Text key with a designated KMS key and store the encrypted text in the RDS database with Name as the primary key.\nNote: For simplicity purposes the sample application will re-use the same KMS keys for each record generated.\nIn the Cloud9 terminal, run the command below, replacing the ApplicationEndpoint with the OutputApplicationEndpoint from previous step. This command will run curl to send a POST request with the secret message payload {\u0026quot;Name\u0026quot;:\u0026quot;Bob\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;Run your operations as code\u0026quot;} to the API.\nALBEndpoint=\u0026#34;ApplicationEndpoint\u0026#34; curl --header \u0026#34;Content-Type: application/json\u0026#34; --request POST --data \u0026#39;{\u0026#34;Name\u0026#34;:\u0026#34;Bob\u0026#34;,\u0026#34;Text\u0026#34;:\u0026#34;Run your operations as code\u0026#34;}\u0026#39; $ALBEndpoint/encrypt Once you run this command, you should see output as follows:\n{\u0026#34;Message\u0026#34;:\u0026#34;Data encrypted and stored, keep your key save\u0026#34;,\u0026#34;Key\u0026#34;:\u0026#34;EncryptKey\u0026#34;} Take note of the encrypt key value under Key .\nRun the command below, pasting the encrypt key you took note of previously under the Key section to test the decrypt API.\ncurl --header \u0026#34;Content-Type: application/json\u0026#34; --request GET --data \u0026#39;{\u0026#34;Name\u0026#34;:\u0026#34;Bob\u0026#34;,\u0026#34;Key\u0026#34;:\u0026#34;EncryptKey\u0026#34;}\u0026#39; $ALBEndpoint/decrypt Once you run the command you should see the following output:\n{\u0026#34;Text\u0026#34;:\u0026#34;Run your operations as code\u0026#34;} Congratulations! You have now completed the first section of the Lab.\nYou should have a sample application API which we will use for the remainder of the lab.\nClick on Next Step to continue to the next section.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/1_deploy_vpc/","title":"Deploy VPC using CloudFormation","tags":[],"description":"","content":"1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nClick here for instructions to access your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. If you are using your own AWS account: Click here for instructions to use your own AWS account: Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Configure your AWS Region Select the Ohio region. This region is also known as us-east-2, which you will see referenced throughout this lab. AWS offers you the ability to deploy to over 20 regions located across the globe Each region is fully isolated from the others to isolate any issues and achieve high availability, Each region is comprised of multiple Availability Zones, which are fully isolated partitions of our infrastructure (more on this later) 1.3 Deploy the VPC infrastructure This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. Parameters: Parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we use tags, which are key-value pairs, that can help you identify your stacks. Enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, at the bottom of the page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You can click the refresh button to check on the current status. You have now created the VPC stack (well actually CloudFormation did it for you).\nWhen the stack status is CREATE_COMPLETE, you can continue to the next step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_iam_user_cleanup/1_lambda_iam_cleanup/","title":"Deploying IAM Lambda Cleanup with AWS SAM","tags":[],"description":"","content":" Download these two templates (or by cloning this repository):\ncloudformation-iam-user-cleanup.yaml lambda-iam-user-cleanup.py Create an Amazon S3 bucket if you don\u0026rsquo;t already have one, it needs to be in the same AWS region being deployed into.\nNow that you have the S3 bucket created and the files downloaded to your machine. You can start to create your deployment package on the command line with AWS SAM. Make sure you are working in the folder where where you have downloaded the files to.\nRun the following command to prepare your deployment package:\naws cloudformation package --template-file cloudformation-iam-user-cleanup.yaml --output-template-file output-template.yaml --s3-bucket \u0026lt;bucket\u0026gt;\nOnce you have finished preparing the package you can deploy the CloudFormation with AWS SAM:\nNOTE: The template file to use here is the output file from the previous command:\naws cloudformation deploy --template-file output-template.yaml --stack-name IAM-User-Cleanup --capabilities CAPABILITY_IAM --parameter-overrides NotificationEmail=\u0026lt;replace_with_your_email_address\u0026gt;\nOnce you have completed the deployment of your AWS Lambda function, test the function by going to the AWS Lambda function in your AWS account and create a dummy event by selecting test.\nIf your test runs successfully you should receive an email from:\nAWS Notifications no-reply@sns.amazonaws.com with the subject line of: IAM user cleanup from \u0026lt;account_ID\u0026gt;\nand the body of the email will have a status report from the findings. E.g. IAM Users and AWS Access Keys which require a cleanup\nIAM user cleanup successfully ran.\nUser John Doe has not logged in since 2018-04-19 08:36:18+00:00 and needs cleanup\nUser John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 21:32: 00+00:00 and needs cleanup\nUser John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 20:08:00+00:00 and needs cleanup\nPrincipal 012345678901 has permission to sts:AssumeRole against arn:aws:iam::0123456789012:role/Role AWS::IAM::Role. See finding 00000000-0000-0000-0000-000000000000 in IAM Access Analyzer for more information or to archive this finding\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/1_getting_started/","title":"Getting Started","tags":[],"description":"","content":"1.1 Install the AWS CLI Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts.\nInstall the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI:\nInstall jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.\n1.2 Amazon CloudWatch Logs Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data.\nTo list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions. To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/1_cloudwatch_intro/","title":"Getting to know Amazon Cloudwatch","tags":[],"description":"","content":"Introduction In order to run this lab you will need to have at least one EC2 instance running and have AWS Computer Optimizer enabled at your account.\nAs highlighted in the 100 level Rightsizing Recommendations lab , rightsizing should be an ongoing effort at your organization and one of the best practices is to measure utilization and test new resource configuration multiple times so you only need to modify it once. The last situation you want is for a new resource type to be uncapable of handling load or functioning incorrectly.\nIn this lab we will create a custom metric in Amazon CloudWatch and install the CloudWatch agent on one EC2 instance to collect memory utilization. This will help improve the recommendation accuracy of AWS Compute Optimizer. Be aware that custom metrics are not part of the Amazon CloudWatch free tier usage so additional costs will be incurred. For more information read the Amazon CloudWatch pricing page.\nBefore learning how to capture the memory utilization from a specific resource at your account, let\u0026rsquo;s first do a quick overview on Amazon CloudWatch.\nGetting to know Amazon CloudWatch The first step to perform rightsizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. Observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk i/o.\nLog into the AWS console and go to the Amazon CloudWatch service page: Select Dashboards on the left column, then select Automatic dashboards, followed by EC2: Observe the Service Dashboard and all of its different metrics: Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name: Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks: Navigate to the CPU Utilization Average widget and launch the View Metrics detailed page. Using the Graphed metrics session try to answer the following questions:\nWhat is the instance with the lowest CPU Average? What is the instance with the lowest CPU Max? What is the instance with the lowest CPU Min? X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/1_create_bucket/","title":"Identify (or create) S3 bucket in account 2","tags":[],"description":"","content":"This lab is best run using two AWS accounts Identify the AWS account number for account 1 (no dashes) Identify the AWS account number for account 2 (no dashes) If you only have one AWS account, then use the same AWS account number for both account1 and account2\nIn account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Choose an S3 bucket that contains some objects. You will enable the ability to list the objects in this bucket from the other account. If you would rather create a new bucket to use, follow these directions Record the bucketname "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_playbook_with_jupyter-aws_iam/1_prereq/","title":"Install Python &amp; AWS CLI","tags":[],"description":"","content":"1.1 Install Python and Modules Python 3 and a number of Python modules are required.\nPython downloads Installing pip may be required also. After installing Python, install the following packages by executing the following command in your command line or terminal:\npip install boto3 pandas jupyter 1.2 Install the AWS CLI AWS CLI is not directly used for this lab, however it makes configuration of the AWS IAM credentials easier, and is useful for testing and general use.\nInstall AWS CLI: Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows In your command line or terminal run aws configure to configure your credentials. Note the user will require access to the IAM service. A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.\n"},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/1_intro/","title":"Intro","tags":[],"description":"","content":"In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities.\nIn this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities:\nDeployment of Infrastructure Inventory Management Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested.\nCreating Maintenance Windows and Scheduling Automated Operations Activities Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created.\nRemoving Lab Resources X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/1_intro_right_sizing/","title":"Intro to Rightsizing on AWS","tags":[],"description":"","content":"Introduction Rightsizing is the process of matching instance types and sizes to your workload performance and capacity requirements at the lowest possible cost. It’s also the process of looking at deployed instances and identifying opportunities to eliminate or downsize without compromising capacity or other requirements, which results in lower costs.\nRightsizing is an ongoing process and it\u0026rsquo;s the most effective way to control cloud costs. It involves continually analyzing instance performance and usage needs and patterns—and then turning off idle instances and rightsizing instances that are either overprovisioned or poorly matched to the workload. Because your resource needs are always changing, rightsizing must become an ongoing process to continually achieve cost optimization. You can make rightsizing a smooth process by establishing a rightsizing schedule for each team, enforcing tagging for all instances, and taking full advantage of the powerful tools that AWS and others provide to simplify resource monitoring and analysis.\nRightsizing for Amazon EC2 instances Amazon EC2 provides a wide selection of instance types optimized to fit different use cases . Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.\nThe first step to perform rightsizing on EC2 is to monitor your current use and gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peaks. The most common metrics that define instance performance are: vCPU utilization, memory utilization, network utilization, and disk use.\nThis 100 level hands-on lab will give you an overview on Rightsizing recommendations and how to prioritize your EC2 rightsizing efforts. By the end of this lab you should: 1) Enable and use Rightsizing recommendations; 2) Learn how to filter Rightsizing recommendations report and focus only on the less complex high saving cases.\nEC2 Rightsizing Best Practices Start simple: Idle resources, non-critical development/QA, and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by). Rightsize before performing a migration: If you skip rightsizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the “t” family . The best rightsizing starts on day 1: As you perform rightsizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads and upcoming migrations. Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load or functioning incorrectly. Test once and perform multiple rightsizing: Aggregate instances per autoscaling group and tags to scale rightsizing activities. Combine Reserved Instance or Savings Plans strategies with rightsizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings Plans will automatically adjust the commitment for the new environment. Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn’t be part of the instance types being analyzed for rightsizing. Resources AWS Cost Management - Rightsizing List of available CloudWatch metrics for EC2 instances 200 Level Rightsizing with AWS Compute Optimizer X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/1_launch_instance/","title":"Launch Instance","tags":[],"description":"","content":"For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console.\n1.1 Launch Single Linux Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn\u0026rsquo;t cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance:\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role.\n5.2\tIn the new tab that opens, select Create role.\n5.3\tWith AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions.\n5.4\tEnter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account.\n5.5\tEnter a role name, such as ec2-s3-read-only-role, and then click Create role.\n5.6\tBack on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created.\n5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched:\n``` #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + ``` Accept defaults and Choose Next: Add tags.\nClick Next: Configure Security Group.\n7.1 On type SSH, select Source as My IP\n7.2 Click Add Rule, select Type as HTTP and source as Anywhere\nNote that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.3 Select Add Rule to add both SSH and HTTP, and on source, select My IP\n7.4 Click Review and Launch.\nOn the Review Instance Launch page, check the details, and then click Launch.\nIf you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab, click Download Key Pair, and then click Launch Instances.\nThis is the only chance to save the private key file. You\u0026rsquo;ll need to provide the name of your key pair when you launch an instance, and you\u0026rsquo;ll provide the corresponding private key each time you connect to the instance.\nClick View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/1_launch_instance/","title":"Launch Instance","tags":[],"description":"","content":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn\u0026rsquo;t cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance:\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/ .\nFrom the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes:\n5.1 Select Create new IAM role.\n5.2\tIn the new tab that opens, select Create role.\n5.3\tWith AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions.\n5.4\tEnter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account.\n5.5 Enter a role name, such as ec2-s3-read-only-role, and then click Create role.\n5.6\tBack on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created.\n5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched:\n#!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + Accept defaults and click Next: Add tags.\nClick Next: Configure Security Group. 7.1 Accept default option Create a new security group. 7.2 On the line of the first default entry SSH, select Source as My IP. 7.3 Click Add Rule, select Type as HTTP and Source as Anywhere. Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer.\n7.5 Click Review and Launch.\nOn the Review Instance Launch page, check the details, and then click Launch.\nIf you do not have an existing key pair for access instances, a prompt will appear. Click Create New,then type a name such as lab, click Download Key Pair, and then click Launch Instances.\nThis is the only chance to save the private key file. You\u0026rsquo;ll need to provide the name of your key pair when you launch an instance, and you\u0026rsquo;ll provide the corresponding private key each time you connect to the instance.\nClick View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/1_nav_console/","title":"Navigating to the console","tags":[],"description":"","content":"The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool.\nSign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ .\nIf you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool:\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/1_new_acct_secure_root/","title":"New AWS Account Setup and Securing Root User","tags":[],"description":"","content":"Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/1_architecture/","title":"Overview architecture","tags":[],"description":"","content":"\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_certificate_manager_request_public_certificate/1_request_certificate/","title":"Requesting a public certificate using the console","tags":[],"description":"","content":" Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select your prefferred region for regional certificates including Elastic Load Balancing, or US East (N. Virginia) for global services including Amazon CloudFront.\nIf you see a welcome page, click Get started under provision certificates area.\nOn the Request a certificate page, click Request a public certificate, then click Request a certificate.\nType your domain name. You can use a fully qualified domain name (FQDN) such as www.example.com or a bare or apex domain name such as example.com. You can also use an asterisk * as a wildcard in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com, and images.example.com. The wildcard name will appear in the Subject field and the Subject Alternative Name extension of the ACM certificate.\nNote: When you request a wildcard certificate, the asterisk * must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com, and test.example.com, but it cannot protect test.login.example.com. Also note that *.example.com protects only the subdomains of example.com, it does not protect the bare or apex domain example.com. To protect both, see the next step.\nTo add more domain names to the ACM certificate, choose Add another name to this certificate and type another domain name in the text box that opens. This is useful for protecting both a bare or apex domain (like example.com) and its subdomains *.example.com.\nAfter you have typed valid domain names, choose Next.\nBefore ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database and to five common system administration addresses for each domain name. You or an authorized representative must approve one of these email messages. If you use DNS validation, you simply create a CNAME record provided by ACM to your DNS configuration. Choose your option, then click Review.\nNote: If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See Use DNS to Validate Domain Ownership.\nIf the review page correctly contains the information that you provided for your request, choose Confirm and request. The following page shows that your request status is pending validation. You must approve the request either through email link or DNS record.\nImportant: Unless you choose to opt out, your certificate will be automatically recorded in at least two public certificate transparency databases. You cannot currently use the console to opt out. You must use the AWS CLI or the API. For more information, see Opting Out of Certificate Transparency Logging . For general information about transparency logs, see Certificate Transparency Logging .\nYour certificate is now ready to associate with a supported service .\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/1_setup_s3_output/","title":"Setup Output S3 Bucket","tags":[],"description":"","content":"We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We\u0026rsquo;ll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered.\nSo what we\u0026rsquo;ll do is as follows:\nCreate the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the consolve via SSO.\n2 - Go to the S3 console\n3 - Create the output S3 bucket\n4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account.\nDo not put any hyphens \u0026lsquo;-\u0026rsquo; in your folder name, as it will be an Athena table and there are restrictions on naming conventions.\n5 - Go to Permissions, and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowListingOfFolders\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::(account ID):root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::(bucket)\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAllS3ActionsInSubFolder\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::(account ID):root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::(bucket)/(folder)/*\u0026#34; } ] } 6 - Go to the IAM Dashboard\n7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point:\nNOTE: replace (bucket name):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObjectVersionAcl\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::(bucket name)/*\u0026#34; } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL\n9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard\n11 - Create the lambda function S3LinkedPutACL with the following details:\nNode.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard\n13 - Select the Output Bucket, go to Properties, and add an S3 event to trigger on All object create events, and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: 15 - Delete the file and ensure all folders are empty. The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/1_verify_cur/","title":"Verify your CUR files are being delivered","tags":[],"description":"","content":" Prerequisite: You must have configured Cost and Usage Reports in the AWS Account Setup lab. It can take up to 24 hours for AWS to deliver the first report to your Amazon S3 bucket. However if you are doing this Lab as part of an AWS workshop, or do not have substantial or interesting usage, follow these steps at the bottom of this lab to create an Amazon S3 bucket and use sample files.\nWe will verify the CUR files are being delivered, they are in the correct format and the region they are in.\nLog into the console via SSO.\nGet the bucket name which contains your Cost and Usage Report (CUR) files in the management/payer account. Modify the following address and replace (bucket name) with your bucket:\nhttps://s3.console.aws.amazon.com/s3/buckets/(bucket name)/ You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur. Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that you have access to the CUR files, and they are being delivered in the correct format.\nUse Sample Data Click here to use sample CUR data Follow the Amazon Simple Storage Service documentation page to create an S3 bucket Select your new S3 bucket and click the Create folder button, enter cur for your folder name, and click Create folder Click on your new cur folder to enter it, and follow the same process to create further folders until you have a folder structure which resembles (bucket name)/cur/WorkshopCUR/WorkshopCUR/year=2018 Then create three further folders within the year=2018 folder (month=10, month=11, month=12) Download the three parquet files below October 2018 Usage November 2018 Usage December 2018 Usage Upload each months file into the corresponding folder, (so upload October\u0026rsquo;s parquet file to (bucket name)/cur/WorkshopCUR/WorkshopCUR/year=2018/month=10/ X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_with_cloudwatch_dashboards/1_intro/","title":"View Amazon CloudWatch Automatic Dashboards","tags":[],"description":"","content":"1. View Amazon CloudWatch Automatic Dashboards Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues.\nOpen the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar.\nIf you are logging into a brand new AWS account, you will see the default Cloudwatch console such as this: You will need to deploy something into your account to see a Cloudwatch automatic dashboard.\nOnce you have services deployed into your AWS account, Cloudwatch will automatically populate the Overview tab with various metrics such as this: The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. In this example, it is showing that we have an EC2 instance in use and it is marked as OK. The upper right shows alarms in your account, which will contain up to four alarms that are in the ALARM state or it will show those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues.\nBelow these areas is spot for a custom default dashboard that you can create that is named CloudWatch-Default This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. In this example, we do not have a custom default dashboard created. If you wish, you can click the \u0026ldquo;Create a new CloudWatch-Default dashboard\u0026rdquo; to generate a new dashboard and see it displayed in the overview screen. If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. In this example, we see both EC2 metrics as well as EBS volume metrics for the test machines that were created.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/1_ri_report/","title":"View an RI report","tags":[],"description":"","content":"We are going to view the RI reports within AWS Cost Explorer, to understand the potential savings and usage trends.\nLog into the console via SSO, go to the management/payer account and go to the AWS Cost Explorer service page: In the left menu under Reservations select Recommendations: On the right select the filters: Select recommendation type non-EC2 service, RI term 1 year, Payment Option (your preference), Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations\nOn the right select the filter: Based on the past 30 days: View the Purchase Recommendations, if the 7 days recommendation is more than the 30 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI\u0026rsquo;s would be suitable.\nYou now have an overview of your potential savings and your usage trends.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/1_view_invoice/","title":"View your AWS Invoices","tags":[],"description":"","content":"At the end of a billing cycle or at the time you make a purchase and incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period.\nLog into the console via SSO and go to the billing dashboard: Select Payments from the menu on the left: Click on Transactions and then an Invoice ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/1_cost_usage_service/","title":"View your cost and usage by service","tags":[],"description":"","content":"AWS Cost Explorer is a free built in tool that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise.\nLog into the console via SSO and go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer: Click on Reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service: This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily: The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line: This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Purple line): We will remove the Recurring reservation fees. Click on More filters then click Charge Type filter on the right, click the checkbox next to Recurring reservation fee, select Exclude only to remove the data. Then click Apply filters: We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is EC2-Instances: We will remove the EC2 service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to EC2-Instances, select Exclude only, and click Apply filters: EC2-Instances has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/1_view_recommendations/","title":"View your Savings Plan recommendations","tags":[],"description":"","content":"Introduction Savings Plans are a commitment based discount model. By making a commitment of spend you will use for 1 or 3 years, you receive a discount of up to 72%. They offer the same discounts as Reserved Instances, however offer a great deal more flexibility, and do not have the same management overhead.\nIn this workshop we will take you through your recommendations, and help you choose the right savings plan for your future business requirements.\nLog into the console via SSO and open the AWS Cost Management dashboard by searching and selecting Cost Explorer: Click on Overview under Savings Plans on the left menu: You can see a description and some examples of Savings Plans under the What are Savings Plans? heading, and Potential monthly savings at the bottom:: Click on Recommendations on the left menu: You can see the Recommendation parameters at the top. Here you have the ability to:\nView recommendations by the Savings Plans type Choose Recommendation level of Payer if you want recommentions based on all accounts that have Savings Plans discount sharing enabled or Linked account for sperate recommendations for each account in your Organization. Choose the length of the Savings Plans term Select from different Payment options Choose to have recommendations made Based on the past 7, 30 or 60 days work of historic usage data. Under Recomendations you can see your estimated before and after spend. Under Recommended Saving Plans you will see a breakdown the commitment per hour and the percentage of estimated savings, this is an ideal starting point to understand the overall return you can get on your commitment: Click on Payer, Compute Saving Plans, 1-year, No upfront, and 30 days from the options above, and see the changes in the before and after below. Note down the % saving, in this example is 25%: Click on EC2 Instance Savings Plans, 3-year and All upfront, and note the % saving in this example is now 58%: This will typically be the highest and lowest savings you can achieve based on previous analyzed usage. You can vary the options to achieve the discount and features that most suits your needs. You can also combine the options by purchasing multiple savings plans. e.g. Making a commitment for a 1-year with an upfront component, and another commitment with no upfront on a 3-year term.\nWhile the commitment is a full 1 or 3 years, a Savings Plan will typically be paid off much sooner. We will analyze this in the next step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/","title":"100 - Inventory and Patch Management","tags":[],"description":"","content":"Introduction In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities:\nDeployment of Infrastructure Inventory Management Patch Management Goals: Automated deployment of infrastructure Dynamic management of resources Automated patch management Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Intro Setup Deploy an Environment Using Infrastructure as Code Inventory Management using Operations as Code Patch Management Creating Maintenance Windows and Scheduling Automated Operations Activities Creating a Simple Notification Service Topic Removing Lab Resources "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/","title":"200 - Automating operations with Playbooks and Runbooks","tags":[],"description":"","content":"Authors Stephen Salim, Well-Architected Geo Solutions Architect. Contributors Brian Carlson, Well-Architected Operational Excellence Pillar Lead. Jang Whan Han, Well-Architected Geo Solutions Architect. Introduction Manually running your runbooks and playbooks for operational activities has a number of drawbacks:\nActivities are prone to errors \u0026amp; difficult to trace. Manual activities do not allow your operational practice to scale in line with your business requirements. In contrast, implementing automation in these activities has the following benefits:\nImproved reliability by preventing the introduction of errors through manual processes. Increased scalability by allowing non linear resource investment to operate your workload. Increased traceability on your operation through log collection of the automation activity. Improved incident response by reducing idle time and automatically triggering activity based on known events. At a glance, both runbooks and playbooks appear to be similar documents that technical users, can use to perform operational activities. However, there an essential difference between them:\nA playbook documents contain processes that guides you through activities to investigate an issue. For example, gathering applicable information, identifying potential sources of failure, isolating faults, or determining the root cause of issues. Playbooks can follow multiple paths and yield more than one outcome.\nA runbook contains procedures necessary to achieve a specific outcome. For example, creating a user, rolling back configuration, or scaling resource to resolve the issue identified.\nThis hands-on lab will guide you through the steps to automate your operational activities using runbooks and playbooks built with AWS tools.\nWe will show how you can build automated runbooks and playbooks to investigate and remediate application issues using the following AWS services:\nSystems Manager Automation Simple Notification Service Amazon CloudWatch synthetic monitoring Goals: Build and run automated playbooks to support your investigations Build and run automated runbooks to remediate specific faults Enabling traceability of operations activities in your environment Prerequisites: An AWS account that you are able to use for testing. The account should not be used for production purposes. An IAM user in your AWS account with full access to CloudFormation, Amazon ECS, Amazon RDS, Amazon Virtual Private Cloud (VPC), AWS Identity and Access Management (IAM), AWS Cloud9 Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the sample application environment Simulate an Application Issue Build \u0026amp; Run an Investigative Playbook Build \u0026amp; Run Remediation Runbook Teardown "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/","title":"Level 100: AWS Account Setup: Lab Guide","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Last Updated July 2021\nAuthors Nathan Besh, Cost Lead Well-Architected Matt Berk, Sr. Technical Account Manager Timur Tulyaganov, AWS Principal Technical Account Manager Yuriy Prykhodko, AWS Sr. Technical Account Manager Alee Whitman, Sr. Commercial Architect (AWS OPTICS) Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to configure your accounts and get them prepared for Cost Optimization work. It will create and setup an initial account structure, enable access to billing information and create a cost optimization team. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework.\nGoals Implement an account structure Configure billing services Enable detailed cost and usage information Create a cost optimization team Prerequisites Multiple AWS accounts already created (at least three) Permissions required Root user and administrator access to the management and member accounts Costs https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records S3: Storage of CUR and/or monthly report file, refer to S3 pricing https://aws.amazon.com/s3/pricing/ Estimated costs should be \u0026lt;$5 a month for small accounts Time to complete The lab should take approximately 30 minutes to complete Steps: Configure IAM access Create an account structure Configure Cost and Usage reports Enable Single Sign On (SSO) Configure account settings Setup Amazon QuickSight Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Configure Monthly Reports (Optional) Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_with_cloudwatch_dashboards/","title":"Level 100: Monitoring with CloudWatch Dashboards","tags":[],"description":"How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Monitor resources to ensure they are performing as expected Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Costs https://aws.amazon.com/cloudwatch/pricing/ You can create 3 dashboards for up to 50 metrics per month on the free tier Outside of the free tier, it is $3.00 per dashboard per month This lab creates one dashboard, so the maximum cost would be $3.00 per month if you have already consumed the free tier. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: View Amazon CloudWatch Automatic Dashboards Teardown "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/","title":"Level 100: Walkthrough of the Well-Architected Tool","tags":[],"description":"","content":"Authors Rodney Lester, Principal Solutions Architect, Well-Architected, AWS Introduction The purpose of this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report.\nThe knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework Goals: Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool. Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Costs: There are no costs for this lab AWS Pricing Time to complete The lab should take approximately 30 minutes to complete Steps: Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear down this lab X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/","title":"Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3)","tags":["data_backup"],"description":"Improve reliability using automatic asynchronous backup of encrypted data in Amazon S3","content":"Author Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected Introduction This hands-on lab will guide you through the steps to improve reliability of your service and its data using automatic asynchronous backup of encrypted data you store in Amazon S3. Your Amazon S3 data will be securely backed up to a different AWS region.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals By the end of this lab, you will be able to:\nPerform data backup automatically for objects in Amazon S3 buckets Secure and encrypt backups of objects in Amazon S3 Automate disaster recovery (DR) of your objects in Amazon S3 Query CloudTrail logs to improve your understanding of how cross-region replication works for Amazon S3 Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\nAn AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create IAM Polices and Roles, create S3 buckets and bucket policies, get and put objects into S3 buckets, and create and read CloudTrail trails and CloudWatch Log Groups. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the Infrastructure Configure bi-directional cross-region replication (CRR) for S3 buckets Test bi-directional cross-region replication (CRR) Tear down this lab References \u0026amp; useful resources Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nIn this lab you will do three S3 PUTs, store under 1MB of data in two S3 buckets, and transfer under 1MB of data from one region to another. The total of these operations will not exceed $0.01. Under normal conditions there should be no accrued charges for this lab. Please follow the directions for Tear Down to remove all deployed resources when you are done with this lab. "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/","title":"Level 300: Automated Athena CUR Query and E-mail Delivery","tags":[],"description":"","content":"Authors Na Zhang, Sr. Technical Account Manager, AWS Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through deploying an automatic CUR query \u0026amp; E-mail delivery solution using Athena, Lambda, SES and CloudWatch. The Lambda function is triggered by a CloudWatch event, it then runs saved queries in Athena against your CUR file. The queries are grouped into a single report file (xlsx format), and sends report via SES. This solution provides automated reporting to your organization, to both consumers of cloud and financial teams.\nGoals Provide automated financial reports across your organization Prerequisites CUR is enabled and delivered into S3, with Athena integration. Recommend to complete 200_4_Cost_and_Usage_Analysis If your account is in the SES sandbox(default), verify your email addresses in SES to assure you can send or receive emails via verified mail addresses: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html Permissions required Create IAM policies and roles Write and read to/from S3 Buckets Create and modify Lambda functions Create, save and execute Athena queries Verify e-mail address, send mail in SES Costs Variable, dependent on the amount of data scanned and report frequency Approximately \u0026lt;$5 a month for small to medium accounts Time to complete The lab should take approximately 15 minutes to complete Steps: Overview architecture Create S3 Bucket Create an IAM policy and role for Lambda function Configure parameters of function code and upload code to S3 Create a Lambda function Customize query strings and create scheduled CloudWatch event Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/","title":"Level 300: Autonomous Patching With EC2 Image Builder And Systems Manager","tags":[],"description":"","content":"Authors Tim Robinson, Well-Architected Geo Solutions Architect. Stephen Salim, Well-Architected Geo Solutions Architect. Introduction Patching is a vital component to any security strategy which ensures that your compute environments are operating with the latest code revisions available. This in turn means that you are running with the latest security updates for the system, which reduces the potential attack surface of your workload.\nThe majority of compliance frameworks require evidence of patching strategy or some sort. This means that patching needs to be performed on a regular basis. Depending on the criticality of the workload, the operational overhead will need to be managed in a way that poses minimal impact to the workload\u0026rsquo;s availability.\nEnsuring that you have an automated patching solution, will contribute to building a good security posture, while at the same time reducing the operational overhead, together with allowing traceability that can potentially be useful for future compliance audits.\nThere are multiple different approaches available to automate operating system patching using a combination of AWS services.\nOne approach is to utilize a blue/green deployment methodology to build an entirely new Amazon Machine Image (AMI) that contains the latest operating system patch, which can be deployed into the application cluster. This lab will walk you through this approach, utilizing a combination of the following services and features:\nEC2 Image Builder to automate creation of the AMI Systems Manager Automated Document to orchestrate the execution. CloudFormation with AutoScalingReplacingUpdate update policy, to gracefully deploy the newly created AMI into the workload with minimal interruption to the application availability. We will deploy section 1 and 2 of the lab with CloudFormation templates to get your environment built as efficiently as possible. This will allow the base infrastructure and application deployment to be completed quickly so you can focus on the main lab objectives which are covered in sections 3 and 4. In these sections, we will give you the choice of either using additional pre-built templates or manual steps to complete the EC2 Image Builder and Systems Manager Document configuration.\nThe skills you learn from this lab will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nNote: For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. Please ensure all lab interaction is completed from this region.\nGoals EC2 Image Builder configuration experience. Systems Manager Automated Document experience. Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nSteps: Deploy The Lab Base Infrastructure Deploy The Application Infrastructure Deploy The AMI Builder Pipeline Deploy The Build Automation With SSM Teardown "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/","title":"Level 300: IAM Permission Boundaries Delegating Role Creation","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals IAM permission boundaries IAM policy conditions Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Steps: Create IAM policies Create and Test Developer Role Create and Test User Role Knowledge Check Tear down "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/","title":"Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability","tags":["mitigate_failure"],"description":"Improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors","content":"Author Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected Introduction This hands-on lab will guide you through the steps to improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Amazon Builders\u0026rsquo; Library This lab additionally illustrates best practices as described in the Amazon Builders\u0026rsquo; Library article: Implementing health checks Goals After you have completed this lab, you will be able to:\nImplement graceful degradation to transform applicable hard dependencies into soft dependencies Monitor all layers of the workload to detect failures Route traffic only to healthy application instances Configure fail-open and fail-closed behaviors as appropriate in response to detected faults Use AWS services to reduce mean time to recovery (MTTR) Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\nAn AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets and route tables, Security Groups, Internet Gateways, NAT Gateways, Elastic IP Addresses, IAM Roles, instance profiles, AWS Auto Scaling launch configurations, Application Load Balancers, Auto Scaling Groups, DynamoDB tables, SSM Parameters, and EC2 instances. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the Infrastructure and Application Handle failure of service dependencies Implement deep health checks Fail open when appropriate Tear down this lab Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nThis lab will cost approximately $5.50 per day when deployed The majority of this cost is the charge for NatGateway-Hours Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/","title":"Quest: Loft - Introduction to Security","tags":[],"description":"Introduction to AWS security basics, used as the workshop in AWS loft events.","content":"About this Guide This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Steps: New AWS Account Setup and Securing Root User Basic Identity and Access Management User, Group, Role CloudFront with WAF Protection Automated Deployment of Detective Controls "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available 100 - Inventory and Patch Management 100 - Dependency Monitoring "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/","title":"100 Labs","tags":[],"description":"","content":"AWS Well-Architected Performance Efficiency Labs 100 Level Labs: Level 100: Monitoring with CloudWatch Dashboards How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources\nLevel 100: Calculating differences in clock source How various linux clock sources can affect the performance of your application on EC2\nLevel 100: Monitoring Windows EC2 instance with CloudWatch Dashboards How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Windows EC2 instance.\nLevel 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Amazon Linux EC2 instance.\n"},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available Level 100: Walkthrough of the Well-Architected Tool "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available 200 - Automating operations with Playbooks and Runbooks "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3) Improve reliability using automatic asynchronous backup of encrypted data in Amazon S3\nLevel 200: Deploy and Update CloudFormation Improve reliability of a service by using automation to make changes in your cloud infrastructure\nLevel 200: Testing Backup and Restore of Data Create a strategy to backup data sources periodically using AWS Backup, and automate the testing of the restore process\nLevel 200: Testing for Resiliency of EC2 instances Use code to inject faults simulating EC2 failures. These are used as part of Chaos Engineering to test workload resiliency\nLevel 200: Backup and Restore with Failback for Analytics Workload Implement the backup and restore DR pattern with failback capability for an analytics workload\n"},{"uri":"https://wellarchitectedlabs.com/","title":"AWS Well-Architected Labs","tags":[],"description":"","content":"\nIntroduction The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time.\nThis repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.\nPrerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nLabs: The labs are structured around the six pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability Well-Architected Tool Well-Architected Partners Contributing Guide Contributing Click here for the process to perform a pull request to contribute to the labs "},{"uri":"https://wellarchitectedlabs.com/cost/fundamentals/","title":"Fundamentals","tags":[],"description":"","content":"About cost optimization fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles.\nStep 1 - Account Setup Administrative privileges required This first step will set your organization up correctly for Cost Optimization. It will build an account structure, configure the required data sources and settings, and create a Cost Optimization team with the required permissions to perform Cost Optimization activities.\nThis lab requires root access. It is for inital setup so completed only once. If you have a very large or diverse organization with multilpe management/Payer accounts, you should complete it for each management account or cost optimization team that you have. Step 2 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount.\n100 Level Lab: This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 3 - Pricing Models - Savings Plans and Reserved Instances By using the right pricing model for your workload resources, you pay the lowest price for that resource.\n100 Level Lab: This lab will introduce you to working with Savings Plans (SP\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab: This lab will introduce you to working with Reserved Instances (RI\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. Step 4 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption.\n100 Level Lab: This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 5 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights.\n100 Level Lab: This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 6 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur.\n200 Level Lab: This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 7 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available.\n200 Level Lab: This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 8 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards.\n200 Level Lab: This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source. "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/1_cloud_financial_management/","title":"Cloud Financial Management","tags":[],"description":"","content":"Practice Cloud Financial Management Cost-Aware Processes CFM Reporting Goal: Regular reporting on CFM implemented across the business Target: Within 6 months all DevOps teams must report bi-weekly on CFM activities Best Practice: Cost-Aware Processes Measures: Number of reports delivered, number of teams delivering Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization, ensures a thorough \u0026amp; structured approach to CFM Contact/Contributor: natbesh@amazon.com CFM included in business processes Goal: CFM included in all relevant business processes Target: Change control process must include CFM, the impact to workload efficiency, effort, and risk, must be evaluated and documented on all changes to any production workload Best Practice: Cost-Aware Processes Measures: % of changes with complete/full CFM evaluation (exceptions are NOT included) Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization, minimizes fritciont as changes that impact cost/efficiency are known and communicated to all parts of the business Contact/Contributor: natbesh@amazon.com Include CFM into internal training programs Goal: Improve CFM capability company wide Target: CFM training to be incorporated into internal training courses for all roles. Best Practice: Cost-Aware Processes Measures: % of courses with CFM included Good/Bad: Good Why? When does it work well or not?: Ensures employees have the capability to perform CFM in their jobs/roles Contact/Contributor: natbesh@amazon.com New hire training programs Goal: Improve practical CFM capability company wide Target: All new hires to complete the Well-Architected labs fundamentals within their first 3 months Best Practice: Cost-Aware Processes Measures: % of new hires completed the series Good/Bad: Good Why? When does it work well or not?: Ensures employees have the hands on capability to perform CFM in their jobs/roles Contact/Contributor: natbesh@amazon.com Update CFM guidance Goal: Ensure you keep up to date with best practices Target: Review Well-Architected Cost Optimization Whitepaper yearly, and update all training \u0026amp; process inline with any changes Best Practice: Cost-Aware Processes Measures: % of processes updated Good/Bad: Good Why? When does it work well or not?: Ensures new best practices are implemented, and you\u0026rsquo;re at the forefront of CFM Contact/Contributor: natbesh@amazon.com Cost-Aware Culture Perform CFM, accountability Goal: Ensure CFM is a part of everyones job/role Target: All job descriptions must include CFM within 6 months, applies to all builder/devops/technical/finance \u0026amp; relevant management roles Best Practice: Cost Aware Culture Measures: Number of job descriptions updated, % of relevant descriptions with CFM Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization by driving accountability and ensuring all employees are aware CFM is part of their job/role. Ensures accountability can be enforced, as not performing Cost Opt is not performing their job \u0026amp; standard processes can be used as corrective action Contact/Contributor: natbesh@amazon.com Hire CFM Capabilities Goal: Include CFM as a core capability when hiring Target: All job postings to include CFM within 6 months, applies to all builder/devops/technical/finance \u0026amp; relevant management roles Best Practice: Cost Aware Culture Measures: Number of job descriptions updated Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization by hiring CFM capability, also informs the industry \u0026amp; potiential hires that CFM matters to your organization Contact/Contributor: natbesh@amazon.com Goal: Include CFM as a core capability when hiring Target: All job postings to include CFM within 6 months, applies to all builder/devops/technical/finance \u0026amp; relevant management roles Best Practice: Cost Aware Culture Measures: Number of job descriptions updated Good/Bad: Good Why? When does it work well or not?: Actively drives CFM throughout the organization by hiring CFM capability, also informs the industry \u0026amp; potiential hires that CFM matters to your organization Contact/Contributor: natbesh@amazon.com Goal: Ensure CFM capability when hiring Target: As part of the hiring process, all relevant roles (tech, finance \u0026amp; some management) must obtain a pass mark on a CFM test during the phone screen Best Practice: Cost Aware Culture Measures: Number of tests performed, number of candidates passed Good/Bad: Good Why? When does it work well or not?: Ensures that CFM capability is established across your relevant teams, also informs the industry \u0026amp; potiential hires that CFM matters to your organization Contact/Contributor: natbesh@amazon.com Certification Goal: Ensure CFM capability is retained in the organization Target: Maintain a level of 10 certified employees within the organization Best Practice: Cost-Aware Processes Measures: number of certified employees, number of teams that have certified employees Good/Bad: Good Why? When does it work well or not?: Ensures a verified level of capability throughout the organization Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_cost_estimation/1_launch_aws_pricing_calculator/","title":"Launch AWS Pricing Calculator","tags":[],"description":"","content":"Launch AWS Pricing Calculator Using AWS Pricing Calculator you can explore AWS services and create an estimate for the cost of your use cases on AWS. We will use Pricing Calculator to estimate AWS costs for a 3 tier LAMP stack based Web Application.\nUsing the web browser, open AWS Pricing Calculator by clicking on this link or pasting the following URL on the address bar: https://calculator.aws/#/ Click on Create Estimate : X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/quicksight/quicksight/","title":"Share, Update and Customize your Dashboard in QuickSight","tags":[],"description":"","content":"Share Dashboard Click here to continue sharing your dashboard with other QuickSight users in your QuickSight account NOTE: Please note that adding users can have cost implications for QuickSight Secure sharing and distribution of data is a key feature offered by Amazon QuickSight. Consider other groups of users within your organization that would benefit from viewing the dashboard data. After you publish a QuickSight dashboard, you can share it with other users or groups, and choose the level of access to grant them. You can also choose to share with all users in your Amazon QuickSight subscription.\nUsers who are dashboard viewers can view and filter the dashboard data. Any selections to filters, controls, or sorting that users apply while viewing the dashboard exist only while the user is viewing the dashboard, and aren\u0026rsquo;t saved once it\u0026rsquo;s closed. Users who are dashboard owners/co-owners can edit and share the dashboard, and optionally can edit and share the analysis.\nGo to the QuickSight service homepage inside your account. Be sure to select the correct region from the top right user menu or you will not see your expected tables\nFrom the left hand menu, choose Dashboards\nOn the dashboard page, select the dashboard you wish to share.\nSelect Share on the application bar. Select Share dashboard Do one of the following:\nA. Check what permissions already exist by choosing Manage dashboard access. Then choose Add users to return to this screen.\nB. You have the option to share with all the users in your Amazon QuickSight subscription. To do this, select the option Share with all users in this account. When you manage dashboard access through the Managed dashboard permissions screen, you see that the option Share with all users in this account is enabled. The individual users aren\u0026rsquo;t listed in this screen.\nC. To share with an individual user or group, type the user or group into the search box. Then choose the user or group from the list that appears. Only active users and groups appear in the list.\nAfter you have entered all the users that you want to share with, choose ADD and select the permission of Viewer or Co-owner to confirm your choices. You can see the username, email, permission level, user role, and privileges. You can also remove a user by using the delete icon.\nChoose permissions for each user. Note: Users in the Reader role cannot have permissions modified from Viewer, and cannot have Save as privileges.\nViewer Viewers can view, filter, and sort the dashboard data. They can also use any controls or custom actions that are on the dashboard. Any changes they make to the dashboard exist only while they are viewing it, and aren\u0026rsquo;t saved once they close the dashboard.\nCo-owner Co-owners can edit and share the dashboard. You have the option to provide them with the same permissions to the analysis. If you want them to also edit and share the dataset, you can set that up inside the analysis.\nChoose whether to enable a user\u0026rsquo;s privilege to Save as in order to create a new dashboard from a copy of this one. This privilege grants read-only access to the datasets, so the user or group can create new analyses from it. AWS Documentation For These Steps Customize your Dashboard by saving your Dashboard as an Analysis Click here to continue saving your dashboard as a customizable analysis Go to the QuickSight service homepage inside your account. Be sure to select the correct region from the top right user menu or you will not see your expected tables\nFrom the left hand menu, choose Dashboards\nOn the dashboard page, select the dashboard you wish to share.\nSelect Share on the application bar. Select Share dashboard Add the required users, or share with all users, ensure you check Save as for each user. Select Confirm Select **Go back \u0026lt;\u0026lsquo;Your Dashboard\u0026rsquo;\u0026gt; Refresh your web page and you will now see a Save as option in the upper right\nClick Save as: Enter an Analysis name and click Create: You will now have an analysis created from the template that you can edit and modify: "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_vpc_flow_logs_analysis_dashboard/2_create_athena_lambda_cloudwatch_rule/","title":"Create Athena resources, Lambda function and CloudWatch rule","tags":[],"description":"","content":"Now that you have enabled VPC Flow Logs, which will help you understand how your applications are communicating over your VPC network with log records containing the Instance ID, Source and Destination IP addresses, Subnet ID, VPC ID and the type and volume of traffic to list a few. While the raw VPC Flow Logs by themselves provide detailed information about every single network traffic flow, you still need to filter and aggregate them to derive the necessary insights e.g Dropped traffic, Top Source and destination IPs etc. This is where you need an analytical tool such as Athena to query the raw VPC Flow Logs and get you to the required insights and a QuickSight dashaboard which is built on top of Athena table/view.\nAll the steps from this section are required to execute one time in central account.\nLogin to your central AWS account.\nRun CloudFormation stack to create Athena Database, Table, Lambda function and Cloudwatch rule.\nDownload CloudFormation Template:\nCSV file format: vpc_athena_db_table_view_lambda.yaml OR\nParquet file format: vpc_athena_db_table_view_lambda_parquet.yaml This cloudformation template creates\nAthena DataBase, an external table, VPC Flow Logs View: To query and fetch data from S3 bucket for VPC Flow Logs. Lambda function: To create partitions in external Athena table for log records stored every day in S3 bucket. Cloudwatch rule: Invokes lambda function at daily frequency. From AWS Console navigate to CloudFormation. Then click on Create stack Create stack page:\nIn Specify template section, select Upload a template file.\nThen Choose File and upload the appropriate template below (you have downloaded previously)\nCSV file format: vpc_athena_db_table_view_lambda.yaml\nOR\nParquet file format: vpc_athena_db_table_view_lambda_parquet.yaml\nThen Click Next\nIn Specify stack details page provide values to below parameters:\nProvide unique stack name e.g. VPCFlowLogsAthenaLambdaStack-01\n(Please read the description of each parameter carefully)\nAthenaQueryResultBucketArn: The ARN of the Amazon S3 bucket to which Athena query results are stored. e.g. \u0026lsquo;arn:aws:s3:::aws-athena-query-results-us-east-1-XXXXXXXXXXXXXX\u0026rsquo;\nAthenaResultsOutputLocation: URI path of the Amazon S3 bucket where Athena query results are stored.\nHiveCompatibleS3prefix: documentation Adds prefixes of partition keys in s3 object key (Hive-compatible S3 prefix)\nNote: Please select true for Parquet file format and false for CSV format.\nS3BucketRegion: Region of the S3 bucket created in the central account. e.g. us-east-1\nVpcFlowLogsAthenaDatabaseName: Only provide existing database name if it has a table with all the required fields mentioned in the Introduction section otherwise leave it empty so that this template will create new DB.\nVpcFlowLogsAthenaTableName: Only provide existing table name if it has all the required fields mentioned in the Introduction section otherwise leave it empty so that this template will create new table.\nVpcFlowLogsBucketName: Name of the Amazon S3 bucket where vpc flow logs are stored. e.g. my-vpc-flow-logs-bucket\nVpcFlowLogsFilePrefix: The log file prefix in Amazon S3 bucket that comes right after s3 bucket name e.g. vpc-flow-logs\nVpcFlowLogsS3BucketLocation: Please provide complete path without log file name, as shown below\ne.g.\nFor CSV - s3://my-vpc-flow-logs-bucket/vpc-flow-logs/AWSLogs/0123456789/vpcflowlogs/us-east-1/2021/11/01/\nFor Parquet - s3://my-vpc-flow-logs-bucket/vpc-flow-logs-enh-parquet/AWSLogs/\nClick Next Click here to see Parquet Stack parameters Click here to see CSV Stack parameters Add tags Name=VPCFlowLogs-Lambda-Stack and Purpose=WALabVPCFlowLogs. Keep rest of the selections to default values. Click Next Review the Stack and click on I acknowledge that AWS CloudFormation might create IAM resources. checkbox, Click on Create Stack You will see the progress of the stack creation under Events tab as below. Please wait for the stack to complete the execution. Once complete it will show the status CREATE_COMPLETE in green then proceed to the next step. To verify the result navigate to Athena from AWS Console and run below sql query:\nSELECT * FROM vpc_flow_logs_custom_integration limit 10; Athena View for CSV:\nSELECT * FROM vpc_flow_logs_view limit 10; Athena Views for Parquet:\nSELECT * FROM vpc_flow_logs_summary_view limit 10; SELECT * FROM vpc_flow_logs_daily_view limit 10; SELECT * FROM vpc_flow_logs_enhanced_view limit 10; Example screen shot: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/2_configure_env/","title":"Configure Execution Environment","tags":[],"description":"","content":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed.\nYou have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C#, then instructions for these are also provided.\nIn addition to custom scripts, you can also perform failure injection experiments using AWS Fault Injection Simulator (FIS) .\n2.1 Setup AWS CloudShell If you will be using bash, Java, or Python, and are comfortable with Linux, it is highly recommended you use AWS CloudShell for this lab. If you will not be using AWS CloudShell, then skip to Step 2.2 Go to the AWS CloudShell console here If this is your first time running CloudShell, then it will take less than a minute to create the environment. When you see a prompt like [cloudshell-user@ip-10-0-49-48 ~]$, then you can continue\nValidate that credentials are properly setup.\nexecute the command aws sts get-caller-identity If the command succeeds, and the Arn contains assumed-role/TeamRole/MasterKey, then you can continue Adjust font size and theme using the gear icon on the upper right\nExplore the Actions menu (upper-right) - you can upload/download files or create new tabs\nSkip to Step 2.3 2.2 Setup AWS credentials and configuration If you have chosen to use AWS CloudShell or Windows PowerShell, then skip this step\nOtherwise, your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes:\nCredentials - You identified these credentials back in step 1 AWS access key AWS secret access key AWS session token (used in some cases) Configuration\nRegion: us-east-2 Default output: JSON Note: us-east-2 is the Ohio region\nIf you already know how to configure these, please do so now. If you need help, then follow these instructions If you are using PowerShell for this lab, skip this step and continue to Step 2.3 2.3 Set up the programming language environment Choose the appropriate section below for your language\nUsing bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. Or if you wish, you may choose one of the other languages and scripts.\nClick here for instructions if using bash: Prerequisites\nawscli AWS CLI installed\n$ aws --version aws-cli/2.2.15 Python/3.8.8... Version 2.1.12 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed.\n$ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the resiliency bash scripts from GitHub to your execution location (this is AWS CloudShell if you are using that for this lab). You can use the following links to download the scripts:\nbash/fail_instance.sh bash/failover_rds.sh bash/fail_az.sh If using Linux, then from your execution location (AWS CloudShell if you are using that for this lab):\nCopy the link URL (from the links above) Use this command to download the script:wget \u0026lt;url\u0026gt; Set the scripts to be executable.\nchmod u+x fail_instance.sh chmod u+x failover_rds.sh chmod u+x fail_az.sh Click here for instructions if using Python: Check that python 3 is installed. This is already installed with AWS CloudShell or Amazon Linux.\n$ python3 --version Python 3.7.10 Any version is fine The scripts are written in python with boto3. This is already installed with AWS CloudShell or Amazon Linux.\nCheck that boto3 is installed using this command pip3 show boto3 If it not installed, then use your local operating system instructions to install boto3: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation Download the resiliency Python scripts from GitHub to your execution location (this is AWS CloudShell if you are using that for this lab). You can use the following links to download the scripts:\npython/fail_instance.py python/fail_rds.py python/fail_az.py If using Linux, then from your execution location (AWS CloudShell if you are using that for this lab):\nCopy the link URL (from the links above) Use this command to download the script:wget \u0026lt;url\u0026gt; Click here for instructions if using Java: Java and Maven must be installed\n$ mvn -version Apache Maven 3.0.5 (Red Hat 3.0.5-17) Maven home: /usr/share/maven Java version: 1.8.0_302, vendor: Red Hat, Inc. ... If Maven is not installed, or Java is not 1.8 or higher, then install Maven and Java\nFor Amazon Linux and RedHat\n$ sudo yum install maven For Debian, Ubuntu\n$ sudo apt install maven Next choose one of the following options: Option A or Option B.\nNote: If using CloudShell then choose Option A\nOption A:\nDownload the zipfile of the executable\n$ wget https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip ... 2021-11-20 01:20:28 (43.1 MB/s) - ‘javaresiliency.zip’ saved [19825502/19825502] unzip it\n$ unzip javaresiliency.zip Archive: javaresiliency.zip ... inflating: java/app-resiliency-1.0.jar go to the build directory: cd java/appresiliency\nOption B: If you are comfortable with git and are not using CloudShell\nClone the aws-well-architected-labs repo\n$ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into \u0026#39;aws-well-architected-labs\u0026#39;... ... Checking out files: 100% (1935/1935), done. go to the build directory\ncd aws-well-architected-labs/static/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency/ Build: sudo mvn clean package shade:shade\ncd target - this is where your jar files were built and where you can run from the command line\nClick here for instructions if using C#: Download the zipfile of the executable at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line program.\nThe executable is called AppResiliency.exe It is in the win10-x64 folder Later, when it is time to run you your experiments, you should run the executable from a Windows command prompt Avoid potential credentials problems\nCheck to see if you have a file %USERPROFILE%\\AppData\\Local\\AWSToolkit\\RegisteredAccounts.json If so, then this file may override the AWS credentials you need to run the lab You can rename it so that it does not interfere with the lab move %USERPROFILE%\\AppData\\Local\\AWSToolkit\\RegisteredAccounts.json %USERPROFILE%\\AppData\\Local\\AWSToolkit\\RegisteredAccounts-BAK.json and then move it back if you want, after the lab To view or download the source code, see this github repo Click here for instructions if using PowerShell: To install the necessary AWS Tools for Powershell packages, and to setup AWS credentials for PowerShell follow the instructions here Download the resiliency PowerShell scripts from GitHub to a location where you can run them from within PowerShell. You can use the following links to download the scripts:\npowershell/fail_instance.ps1 powershell/failover_rds.ps1 powershell/fail_az.ps1 To download the script using PowerShell:\nCopy the link URL (from the links above) Use this command to download the script:Invoke-WebRequest \u0026quot;\u0026lt;url\u0026gt;\u0026quot; -OutFile \u0026quot;\u0026lt;filename\u0026gt;\u0026quot; Replace \u0026lt;url\u0026gt; and \u0026lt;filename\u0026gt; Keep the quotation marks 2.4 IAM Role for FIS In this lab, some of the experiments will be executed AWS Fault Injection Simulator (FIS) in addition to using custom scripts. FIS needs a service role to inject failures for various components of a workload.\nThis IAM Role has already been created for you as part of the infrastructure deployment\nYou may proceed to the next step.\nIf you would like to view the instructions on how to create the IAM Role for FIS (for your information), then click here: These instructions are here for informational purposes only. You DO NOT need to execute these as this IAM Role was created for you as part of the infrastructure deployment\nNavigate to the AWS Identity and Access Management (IAM) console .\nClick on Policies from the menu on the left and then click Create Policy.\nOn the Create policy wizard, click on the JSON tab and replace the contents with the following policy. Click Next: Tags.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleReadOnly\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ecs:DescribeClusters\u0026#34;, \u0026#34;ecs:ListContainerInstances\u0026#34;, \u0026#34;eks:DescribeNodegroup\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;rds:DescribeDBInstances\u0026#34;, \u0026#34;rds:DescribeDbClusters\u0026#34;, \u0026#34;ssm:ListCommands\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleEC2Actions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:RebootInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:TerminateInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ec2:*:*:instance/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleECSActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecs:UpdateContainerInstancesState\u0026#34;, \u0026#34;ecs:ListContainerInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ecs:*:*:container-instance/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleEKSActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:TerminateInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ec2:*:*:instance/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleFISActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;fis:InjectApiInternalError\u0026#34;, \u0026#34;fis:InjectApiThrottleError\u0026#34;, \u0026#34;fis:InjectApiUnavailableError\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:*:fis:*:*:experiment/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleRDSReboot\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rds:RebootDBInstance\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:rds:*:*:db:*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleRDSFailOver\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rds:FailoverDBCluster\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:rds:*:*:cluster:*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleSSMSendCommand\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:SendCommand\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ec2:*:*:instance/*\u0026#34;, \u0026#34;arn:aws:ssm:*:*:document/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowFISExperimentRoleSSMCancelCommand\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:CancelCommand\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next: Review.\nOn the Review policy page, enter WALab-FIS-policy under Name and click Create policy.\nClick on Roles from the menu on the left and then click Create role.\nFIS is currently not listed in the list of services under use cases. For the time being, Select EC2 and click Next: Permissions.\nUnder Attach permissions policies, enter WALab-FIS-policy and select the WALab-FIS-policy. This is the policy that was created in the previous steps.\nClick Next: Tags.\nClick Next: Review.\nEnter WALab-FIS-role for Role name. Update the description to Allows FIS to call AWS services on your behalf. and click Create role.\nSearch for the newly created role WALab-FIS-role and click on it to view details.\nOn the Trust relationships tab, click Edit trust relationship.\nReplace the existing Policy Document with the following and click Update Trust Policy.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;fis.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } The change should be reflected in the Trust relationships tab,\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/","title":"Level 300: Testing for Resiliency of EC2, RDS, and AZ","tags":["test_resiliency"],"description":"Use code to inject faults simulating EC2, RDS, and Availability Zone failures. These are used as part of Chaos Engineering to test workload resiliency","content":" Your browser doesn't support video, or if you're on GitHub head to [wellarchitectedlabs.com](https://www.wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/) to watch the video. Contributors Rodney Lester, Senior Solutions Architect Manager, AWS Well-Architected Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected Mahanth Jayadeva, Solutions Architect, AWS Well-Architected Michael Haken, Principal Technologist, AWS Adrian Hornsby, Principal Tech Evangelist, AWS Introduction The purpose of this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). It is also a key component of Chaos Engineering, which uses such failure injection to test hypotheses about workload resiliency. One primary capability that AWS provides is the ability to test your systems at a production scale, under load.\nIt is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner.\nIn this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application running on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, which provides you the ability to progress from simpler component failure testing to failure testing under a simulated AWS regional failure.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Illustrate how AWS Fault Injection Simulator (FIS) can implement chaos testing using AWS native tooling and integrations Show how failure injection fits into the context of Chaos Engineering Demonstrate resilience testing of EC2 instances Demonstrate resilience testing of RDS Multi-AZ instances Demonstrate resilience testing using Availability Zones failures Demonstrate resilience testing of application failures. Demonstrate resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. An IAM user or federated credentials into that account that has permissions to create experiment templates and run experiments using FIS. Note: This 300 level lab covers multiple failure injection scenarios. If you would prefer a simpler 200 level lab that demonstrates only EC2 failure injection, then see Level 200: Testing for Resiliency of EC2 instances . This 300 level lab here includes everything in the 200 level lab, plus additional failure simulations.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the Infrastructure and Application Configure Execution Environment Preparation for Failure Injection Test Resiliency Using EC2 Failure Injection Test Resiliency Using RDS Failure Injection Test Resiliency Using Application Failure Injection Test Resiliency Using Availability Zone (AZ) Failure Injection Test Resiliency Using Failure Injection - Optional steps Tear down this lab Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nThis lab will cost approximately $6.50 per day when deployed About half of this cost is the charge for NatGateway-Hours Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab Additional lab resources: Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables. "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_manage_workload_risks_with_opscenter/2_risk_tracking/","title":"Configure risk tracking","tags":[],"description":"","content":"Tracking workload risks After reviewing a workload, the Well-Architected Tool provides information on the number of High Risk Issues (HRI) and Medium Risk Issues (MRI) for the workload. The Well-Architected Tool also provides improvement guidance that can be used to implement Well-Architected best practices and mitigate risks. In this section, you will configure a solution to convert these identified risks into actionable tasks using Systems Manager OpsCenter.\n2.1 Create and configure Lambda function You will create a Lambda function that will iterate over all the workloads defined in the AWS WA Tool in the AWS Region you are using for this lab. The function retrieves best practices missing for each workload and creates OpsItems. The function also maintains workload state in a DynamoDB table to prevent duplicate OpsItems. OpsItems are configured to send notifications to an SNS topic that was created as part of the CloudFormation stack created in the previous section. Click here to view the Lambda function code used for this solution.\nDownload the risk_tracking.zip Lambda function package Navigate to the AWS Lambda console and select Create function. Choose the option to Author from scratch and enter wa-risk-tracking for the function name. Select Python 3.9 as the runtime. Under Permissions, expand Change default execution role. Choose Use an existing role and select wa-risk-tracking-lambda-role from the dropdown. This is the IAM role that was created as part of the CloudFormation stack in the previous section. Click Create function. Lambda provisions a new function which uses the IAM role that was specified. After the function has been created, scroll down to the Code source section and select Upload from and then .zip file. Upload the function package that you selected at the beginning of this section - risk_tracking.zip Scroll down to Runtime settings, click Edit and replace the value for Handler with risk_tracking.lambda_handler. Click Save. On the function overview page, navigate to the Configuration tab and select Environment variables. Click Edit and then click Add to add a new environment variable. For the Key enter sns_topic_arn For the Value enter the value of the SNS Topic obtained from the Outputs section of the CloudFormation stack as described in the Deploy infrastructure section. Click Save Under the Configuration tab, select General configuration and click Edit. Increase the value for Timeout to 1 min and Save During testing, the function was able to create 48 OpsItems in 39 seconds. Adjust the function Memory and Timeout based on the number of risks (HRIs and MRIs), number of best practices missing, and the number of workloads defined in the Well-Architected Tool in the AWS Region you are using for this lab. 2.2 Test risk tracking To test the solution, you can invoke the wa-risk-tracking Lambda function from the Lambda console. On the function details page, click on the Test tab and select New event. Choose hello-world for the template and click Test. This will invoke the Lambda function and the function code will be executed. This can take up to a minute. Wait for the function to finish execution before moving on.\nClick here if your Lambda function failed to execute successfully If the function timed out before completing execution, adjust the function Memory and Timeout based on the number of risks (HRIs and MRIs), number of best practices missing, and the number of workloads defined in the Well-Architected Tool in the AWS Region you are using for this lab.\nNavigate to the Systems Manager console and click on OpsCenter under Operations Management. On the summary page, you will see OpsItems grouped by their source. Under Grouped by source locate Well-Architected and click on the value for Count next to it.\nYou will see a list of OpsItems that have been created based on best practices missing from your workloads. OpsItems have been created with the naming convention - High Risk/Medium Risk - \u0026lt;your workload name from the AWS WA Tool\u0026gt; - \u0026lt;best practice missing\u0026gt;. You might not see OpsItems immediately due to eventual consistency. If the wa-risk-tracking Lambda function executed successfully, wait a few minutes and refresh the console.\nClick on one of the OpsItems to view its details. Under Related resources you will see the ARN of the workload from the AWS WA Tool for which this OpsItem has been created.\nScroll down to the Operational data section and expand it. You will see a variety of information such as the missing best practice, the pillar it is missing from, and the link to the improvement plan for the missing best practice. Copy the link under Improvement Plan and open it in a new browser to see the guidance for implementing this missing best practice in your workload.\nWith this approach you can use information from the AWS WA Tool to turn missing best practices into actionable tasks enabling better co-ordination and tracking of risks across your teams. Individuals working on an OpsItem can set its status to In Progress, or set it to Resolved if that best practice has been implemented.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/2_use_secrets_securely/","title":"Use Secrets Securely","tags":[],"description":"","content":"Passwords remain vulnerable to brute force attack methods even if we store our secrets in a secure way. Because of this we can augment our deployed architecture to limit the lifespan of a password through the use of automatic rotation. An ideal way to approach this task is through the use of AWS Secrets Manager, which can enable you to automatically rotate secrets for other databases or 3rd party services.\nThe second section of the lab will demonstrate how to configure AWS Secrets Manager to securely store your database credentials and automatically rotate them on a schedule for our deployed database. Follow these steps to complete the configuration:\n2.1. Access Secrets Manager from the Console. Go to the Outputs section of the CloudFormation stack you just deployed and click RDSMysqlSecret as shown. This will allow you to view AWS Secrets Manager from the console. Alternatively, you can launch the service from the console main menu in the usual way. The secret name will begin with WARDSInstanceRotationSecret as shown. By default, AWS Secrets Manager uses AWS KMS to encrypt as well as to decrypt your secrets. You can see all current secret values by clicking on the secret name and then selecting Retrieve secret value from the dialog box as shown: As shown below, all sentitive information such as username, password, and RDS endpoint are stored as part of the encrypted secret value. Take this opportunity to record the current password value which we will rotate later in the lab. 2.2. Configure Password Rotation. AWS Secrets Manager can be configured to automatically rotate a password for an Amazon RDS database. When you enable rotation for Amazon RDS, Secrets Manager provides a complete, ready-to-run Lambda rotation function. More information on this topic can be found here. .\nTo enable rotation, choose Edit rotation. Choose Enable automatic rotation and select the rotation interval based on your requirement. We will create a new Lambda function designed for RDS MySQL. Once you define the new AWS Lambda function name, choose Use this secret at the bottom of the dialog box which will select the secret information which will be subjected to rotation as shown.\nNote that the enabling process may take a few minutes so please be patient.\nNow that automated rotation has been enabled, click Rotate secret immediately to test the rotation as shown. Note that intially this step will fail, so remain calm as we will fix this later on! As mentioned previously, rotation will fail. This is because the new Lambda rotation function created by Secrets Manager is not associated with our Lambda Security Group that we explicitly allowed to connect to Amazon RDS for MySQL via port 3306. In order for our newly created rotation to work, your network environment must permit the Lambda rotation function to communicate with your database and the Secrets Manager service. More details on this topic can be found here A simple workaround to correct our misconfigured rotation is to replace the Security Group and Subnet in new our Lambda rotation function. To complete this, access Lambda Function in AWS console and choose SecretManager-rds-mysql-rotation. Note that the function name will be the one you defined at step 2 above as shown. Go to the Configuration section and VPC on the left panel. You can see it\u0026rsquo;s associated with all RDS subnet and security group. Choose Edit and remove all Subnet and Security Groups. Then search for WAprivateLambdaSubnet for Subnets (select 2 for each availability zone) and LambdaSecurityGroup for the Security Group to add them. Confirm that your dialog box looks similar to the screenshot shown below and click Save. Note that this process sometimes takes 2~3 minutes to be completed, so please be patient. Our new Lambda rotation function is now associated with LambdaSecurityGroup allowed to connect to RDS for MySQL. Your screen should look similar to the screenshot below: Now we should attempt to successfully rotate the password again. To do this, go back to AWS Secrets Manager and select Rotate secret immediately to test the rotation. You should now see a message informing you of a successful rotation as shown: If you are experiencing the same failure as previously, check that you have added the correct Security Group and Subnet information and retry. This process sometimes takes a few minutes to complete so please be patient.\nInspecting the password data within Secrets Manager should now show a change to the original password information which you recorded earlier. Let\u0026rsquo;s confirm if the password in the database has been successfully updated based on the automatic rotation. To do this, go back to Cloud9 in the console and execute the script called sendRequest.py with the argument of your APIGatewayURL. You will need to change to the walab-scripts directory to execute the script. Make sure you replace \u0026lsquo;APIGatewayURL\u0026rsquo; with the value you previously used from the Cloudformation stack output. python sendRequest.py \u0026#39;Enter your APIGatewayURL here\u0026#39; Once your command runs successfully, you should be see a 200 Response code with Response data as shown:\nEND OF SECTION 2\n"},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/100_labs/100_automating_serverless_best_practices_with_dashbird/2_well-architected_insights/","title":"Well-Architected Insights","tags":[],"description":"","content":"Well-Architected Insights provides serverless developers with insights and recommendations to continually improve their applications and keep them secure, compliant, optimized, and efficient.\nThe second section of the lab will demonstrate how to automatically create a role in your AWS account, delegating read-only access to various services in your AWS account. Follow these steps to complete the configuration:\n2.1. Account Onboarding to Dashbird. If you are new to Dashbird, sign up for a free account in Dashbird.io . If you already have an account in Dashbird, go to 2.2.\nClick here for Onboarding to Dashbird Click New to Dashbird? Create an account Provide your Company/Organization name and click Next. Select one of the benefits that you are expecting to get out of Dashbird and click Next. Click Add Dashbird CloudFormation stack to AWS to create an IAM role. This will allow Dashbird to collect logs, metrics and permission resource listing under your AWS account. This will redirect you to AWS Console. Scroll down to the bottom of the stack creation page and acknowledge the IAM resources creation by selecting all of the check boxes. Click the Next button. The stack name will be dashbird-delegation by default. Click dashbird-delegation stack in CloudFormation Console and go to Outputs to get the ARN of DashbirdDelegationRole. This will be required at a later stage in the lab. If you want to see the details of IAM role that Dashbird created, go to the IAM Console and search for DashbirdDelegationRole From the Dashbird console, provide the ARN of DashbirdDelegationRole which you noted previously. Click Submit Arn and click Next. Dashbird will scan your AWS account to collect logs, metrics, listing resources. This process may take 4-5 minutes to complete. All done! You should now have end-to-end observability within Dashbird Console. 2.2. Well-Architected Insights From the Dashbird console, you can now access the Well-Architected Insights. From here, you should be able to see the Well-Architected best practices broken down by six pillars of Well-Architected Framework.\nClick Well-Architected Lens on the left panel to see your workload performance against each one of six pillars. END OF SECTION 2\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/2a_cost_intelligence_dashboard/","title":"Cost Intelligence Dashboard","tags":[],"description":"","content":"Authors Alee Whitman, Sr. Commercial Architect (AWS OPTICS) Contributors Arun Santhosh, Specialist SA (Amazon QuickSight) Kareem Syed-Mohammed, Senior Product Manager - Technical (Amazon QuickSight) Aaron Edell, Global Head of Business and GTM - Customer Cloud Intelligence Timur Tulyaganov, AWS Principal Technical Account Manager Yuriy Prykhodko, AWS Sr. Technical Account Manager Aidin Khosrowshahi, AWS Sr. Technical Account Manager Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Deployment Options There are 3 options to deploy the Cost Intelligence Dashboard. If you are unsure what option to select, we recommend using the Manual deployment\nOption 1: Manual Deployment This option is the manual deployment and will walk you through all steps required to create this dashboard without any automation. We recommend this option users new to Athena and QuickSight. Click here to continue with the manual deployment Create Athena Views NOTE: If you\u0026rsquo;ve deployed the CUDOS Dashboard, you will not need to create any new Athena views. All 5 views are the same as the CUDOS Dashboard The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR). The default dashboard assumes you have both Savings Plans and Reserved Instances, if not you will need to create the alternate views.\nLogin via SSO in your Cost Optimization account, go into the Athena console:\nModify and run the following queries to confirm if you have Savings Plans, and Reserved Instances in your usage. If no lines are returned, you have no Savings Plans or Reserved Instances. Replace (database).(tablename) and run the following:\nSavings Plans:\nselect * from (database).(tablename) where savings_plan_savings_plan_a_r_n not like '' limit 10 Reserved Instances:\nselect * from (database).(tablename) where reservation_reservation_a_r_n not like '' limit 10 NOTE: Unless you already have Savings Plans and Reserved Instances both already adopted as your savings options, recreate Athena Views corresponding with your savings profile whenever you onboard a new savings option (like Savings Plans or Reserved Instances) for the first time. Create the account_map view by modifying the following code, and executing it in Athena:\nView0 - account_map Create the Summary view by modifying the following code, and executing it in Athena:\nView1 - Summary View Create the EC2_Running_Cost view by modifying the following code, and executing it in Athena:\nView2 - EC2_Running_Cost Create the Compute savings plan eligible spend view by modifying the following code, and executing it in Athena:\nView3 - compute savings plan eligible spend Create the s3 view by modifying the following code, and executing it in Athena:\nView4 - s3 Create the RI SP Mapping view by modifying the following code, and executing it in Athena:\nView5 - RI SP Mapping NOTE: The Athena Views are updated to reflect any additions in the cost and usage report. If you created your dashboard prior to June 1, 2021 you will want to update to the latest views. Create QuickSight Data Sets Create Datasets Go to the QuickSight service homepage inside your account. Be sure to select the correct region from the top right user menu or you will not see your expected tables\nFrom the left hand menu, choose Datasets\nClick New dataset displayed in the top right corner\nChoose Athena as your Data Source\nEnter a data source name of Cost_Dashboard and click Create data source\nSelect the database which holds the views you created (reference Athena if you’re unsure which one to select), and the summary_view table, then click Edit/Preview data\nSelect SPICE to change your Query mode\nClick Add Data\nSelect Data source\nChoose your Cost_Dashboard view and click Select\nSelect the database which holds the CUR views you created\nChoose your account_map view and click Select Click the two circles to open the Join conﬁguration, then select Left to change your join type\nConfigure the join clause to linked_account_id = account_id, then click Apply\nSelect Save\nSelect the summary_view dataset\nClick Schedule refresh\nClick Create\nEnter a daily schedule, in the appropriate time zone and click Create\nClick Cancel to exit\nClick x to exit\nRepeat steps 3-21, creating data sets with the remaining Athena views. You will reuse your existing Cost_Dashboard data source, and select the following views as the table:\ns3_view\nec2_running_cost\ncompute_savings_plan_eligible_spend\nNOTE: Make sure to reuse the existing Athena data source by scrolling to the bottom of the Data source create/select page when creating a new Dataset instead of creating a new data source When this step is complete, your Datasets tab should have 4 new SPICE Datasets\nSelect the summary_view dataset\nClick Edit Data Set\nClick Add Data\nSelect Data source Choose your Cost_Dashboard view and click Select\nSelect the database which holds the CUR views you created\nChoose your ri_sp_mapping view and click Select\nClick the two circles to open the Join conﬁguration, then select Left to change your join type\nClick Add a new join clause twice so you have 3 join clauses to configure in total. Configure the 3 join clauses as below, then click Apply\nri_sp_arn = ri_sp_arn_mapping payer_account_id = payer_account_id_mapping billing_period = billing_period_mapping Click Save\nNOTE: This completes the QuickSight Data Preparation section. Next up is the Import process to generate the QuickSight Dashboard. Import Dashboard Template We will now use the CLI to create the dashboard from the CUDOS Dashboard template.\nEdit and Run list-users and make a note of your User ARN: aws quicksight list-users --aws-account-id \u0026lt;Account_ID\u0026gt; --namespace default --region \u0026lt;Region\u0026gt; Edit and Run list-data-sets and make a note of the Name and Arn for the 5 Datasets ARNs: aws quicksight list-data-sets --aws-account-id \u0026lt;Account_ID\u0026gt; --region \u0026lt;Region\u0026gt; Create an cid_import.json file using the below sample { \u0026#34;AwsAccountId\u0026#34;: \u0026#34;\u0026lt;Account_ID\u0026gt;\u0026#34;, \u0026#34;DashboardId\u0026#34;: \u0026#34;cost_intelligence_dashboard\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Cost Intelligence Dashboard\u0026#34;, \u0026#34;Permissions\u0026#34;: [ { \u0026#34;Principal\u0026#34;: \u0026#34;\u0026lt;User ARN\u0026gt;\u0026#34;, \u0026#34;Actions\u0026#34;: [ \u0026#34;quicksight:DescribeDashboard\u0026#34;, \u0026#34;quicksight:ListDashboardVersions\u0026#34;, \u0026#34;quicksight:UpdateDashboardPermissions\u0026#34;, \u0026#34;quicksight:QueryDashboard\u0026#34;, \u0026#34;quicksight:UpdateDashboard\u0026#34;, \u0026#34;quicksight:DeleteDashboard\u0026#34;, \u0026#34;quicksight:DescribeDashboardPermissions\u0026#34;, \u0026#34;quicksight:UpdateDashboardPublishedVersion\u0026#34; ] } ], \u0026#34;DashboardPublishOptions\u0026#34;: { \u0026#34;AdHocFilteringOption\u0026#34;: { \u0026#34;AvailabilityStatus\u0026#34;: \u0026#34;DISABLED\u0026#34; } }, \u0026#34;SourceEntity\u0026#34;: { \u0026#34;SourceTemplate\u0026#34;: { \u0026#34;DataSetReferences\u0026#34;: [ { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;summary_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;ec2_running_cost\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;compute_savings_plan_eligible_spend\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;s3_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; } ], \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:quicksight:us-east-1:223485597511:template/Cost_Intelligence_Dashboard\u0026#34; } }, \u0026#34;VersionDescription\u0026#34;: \u0026#34;1\u0026#34; } Update the cid_import.json to match your details by replacing the following placeholders:\nPlaceholder Replace with \u0026lt;Account_ID\u0026gt; AWS Account ID where the dashboard will be deployed \u0026lt;Region\u0026gt; Region Code where the dashboard will be deployed (Example eu-west-1) \u0026lt;User ARN\u0026gt; ARN of your user \u0026lt;DataSetId\u0026gt; Replace with Dataset ID\u0026rsquo;s from the data sets you created in the Preparing Quicksight section NOTE: There are 4 unique Dataset IDs Run the import\naws quicksight create-dashboard --cli-input-json file://cid_import.json --region \u0026lt;Region\u0026gt; --dashboard-id cost_intelligence_dashboard Check the status of your deployment aws quicksight describe-dashboard --dashboard-id cost_intelligence_dashboard --region \u0026lt;Region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; If you encounter no errors, open QuickSight from the AWS Console, and navigate to Dashboards. You should now see Cost Intelligence Dashboard available. This dashboard can be shared with other users, but is otherwise ready for viewing and customizing.\nIf something goes wrong in the dashboard creation step, correct the issue then delete the failed deployment before re-deploying\naws quicksight delete-dashboard --dashboard-id cost_intelligence_dashboard --region \u0026lt;Region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; NOTE: You have successfully created the Cost Intelligence Dashboard. For a detailed description of the dashboard read the FAQ Option 2: Command Line Interface Deployment The CID command line tool is an optional way to create the Cloud Intelligence Dashboards. The command line tool will allow you to complete the deployments in less than half the time as the standard manual setup.\nClick here to continue with the Automation Scripts Deployment Navigate to the Cloud Intelligence Dashboards automation repo and follow the instructions to run the command line tool. You will have the option of deploying the Cost Intelligence Dashboard from the list of supported dashboards.\nOnce complete, visit the account mapping page and follow the steps there to get your account names into the dashboard.\nSchedule dataset refresh in QuickSight. Select the summary_view dataset\nClick Schedule refresh\nClick Create\nEnter a daily schedule, in the appropriate time zone and click Create\nClick Cancel to exit\nClick x to exit\nRepeat these steps with any remaining Athena datasets except customer_all.\nOption 3: CloudFormation Deployment This section is optional and automates the creation of the Cost Intelligence Dashboard and CUDOS Dashboard using CloudFormation templates. The CloudFormation templates allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates and create an IAM role. If you do not have the required permissions use the Manual Deployment.\nClick here to continue with the CloudFormation Deployment NOTE: An IAM role will be created when you create the CloudFormation stack. Please review the CloudFormation template with your security team and switch to the manual setup if required Create the Cost Intelligence Dashboard using a CloudFormation Template Login via SSO in your Cost Optimization account\nClick the Launch CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch CloudFormation Template Enter a Stack name for your template such as Cost-Intelligence-Dashboard-QuickSight Review 1stReadMe parameter to confirm prerequisites before specifying the other parameters Update your AthenaQueryResultsBucket with the Athena results location where your CUR table is To validate your Athena primary workgroup has an output location by\nOpen a new tab or window and navigate to the Athena console Select Workgroup: primary Click the bubble next to primary and then select view detail Confirm your Query result location is configured with an S3 bucket path. If configured, add the location to the AthenaQueryResultsBucket in your CloudFormation Template. If not configured, continue to setting up by clicking Edit workgroup Add the S3 bucket path you have selected for your Query result location and click save Add the location to the AthenaQueryResultsBucket in your CloudFormation Template. Update your BucketFolderPath with the S3 path where your year partitions of CUR data are stored To validate the correct path for your year partitions of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the S3 console Select the S3 Bucket your CUR is located in Navigate your folders until you find the folder with the year partitions of the CUR Tip: Your yearly partitions folder is located in the folder with your .yml file, monthly folders, and status report Add the identified BucketFolderPath to the CloudFormation parameter making sure to not add trailing / (eg - BucketName/FolderName/\u0026hellip;/FolderName) Tip: copy and paste the S3 URI then remove the leading \u0026lsquo;s3://\u0026rsquo; and the ending \u0026lsquo;/\u0026rsquo; Update your CURDatabaseName and CURTableName with the name of the CUR Athena Database and Table To validate the Athena Database and Table of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the Glue console Select the Athena Table your CUR is located in Find your Database CURDatabaseName and Table CURTableName Add the identified CURDatabaseName and CURTableName to the CloudFormation parameter Update your QuickSightUser with your QuickSight username To validate your QuickSight complete the tasks below:\nOpen a new tab or window and navigate to the QuickSight console Find your username in the top right navigation bar Add the identified username to the CloudFormation parameter Update your QuicksightIdentityRegion with your QuickSight region Optional add a Suffix if you want to create multiple instances of the same account. Select Next at the bottom of Specify stack details and then select Next again on the Configure stack options page\nReview the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack. You will see the stack will start in CREATE_IN_PROGRESS NOTE: This step can take 5-15mins Once complete, the stack will show CREATE_COMPLETE Navigate to Dashboards page in your QuickSight console, click on your Cost Intelligence Dashboard name or your CUDOS Dashboard name Set up scheduled SPICE refresh for SPICE Datasets manually or click the Launch Spice Refresh CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch Spice Refresh CloudFormation Template NOTE: You have successfully created the Cost Intelligence Dashboard. For a detailed description of the dashboard read the FAQ Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you can share your dashboard with users or customize your own version of this dashboard\nClick to navigate QuickSight steps Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you can share your dashboard with users or customize your own version of this dashboard.\nClick to navigate QuickSight steps Add your Account Names to your Dashboard Learn how to replace the Accound IDs with the Account Names for each of your linked accounts in AWS Organizations by following these steps.\nSteps for adding account names Update Dashboard Template - Optional Click here to update your dashboard with the latest version If you are tracking our Changelog , you already know that we are always improving the Cloud Intelligence Dashboards.\nTo pull the latest version of the dashboard from the public template please use the following steps.\nOption 1: Command Line Tool Visit the GitHub repository to download and install the CID Command Line Tool and follow the instructions for running the update command.\nOption 2: Manual Update Create a cid_update.json file by removing permissions section from the cid_import.json file. Sample for Cost Intelligence Dashboard cid_update.json file below: { \u0026#34;AwsAccountId\u0026#34;: \u0026#34;\u0026lt;Account_ID\u0026gt;\u0026#34;, \u0026#34;DashboardId\u0026#34;: \u0026#34;cost_intelligence_dashboard\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Cost Intelligence Dashboard\u0026#34;, \u0026#34;DashboardPublishOptions\u0026#34;: { \u0026#34;AdHocFilteringOption\u0026#34;: { \u0026#34;AvailabilityStatus\u0026#34;: \u0026#34;DISABLED\u0026#34; } }, \u0026#34;SourceEntity\u0026#34;: { \u0026#34;SourceTemplate\u0026#34;: { \u0026#34;DataSetReferences\u0026#34;: [ { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;summary_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;ec2_running_cost\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;compute_savings_plan_eligible_spend\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;s3_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; } ], \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:quicksight:us-east-1:223485597511:template/Cost_Intelligence_Dashboard\u0026#34; } } } If needed update the cid_update.json to match your details by replacing the following placeholders:\nPlaceholder Replace with \u0026lt;Account_ID\u0026gt; AWS Account ID where the dashboard will be deployed \u0026lt;Region\u0026gt; Region Code where the dashboard will be deployed (Example eu-west-1) \u0026lt;DatasetID\u0026gt; Replace with Dataset ID\u0026rsquo;s from the datasets you created in the Preparing Quicksight section NOTE: There are 4 unique Dataset IDs Pull the latest published version of the dashboard template. Example for CID Dashboard below:\naws quicksight update-dashboard --cli-input-json file://cid_update.json --region \u0026lt;region\u0026gt; Query the version number of the published dashboard. Example for CID Dashboard below: aws quicksight list-dashboard-versions --region \u0026lt;region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; --dashboard-id cost_intelligence_dashboard Apply the latest pulled changes to the deployed dashboard with this CLI command. Example for CID Dashboard below: aws quicksight update-dashboard-published-version --region \u0026lt;region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; --dashboard-id cost_intelligence_dashboard --version-number \u0026lt;version\u0026gt; NOTE: The update commands were successfully tested in AWS CloudShell (recommended) X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_8_tag_policies/2_attach_tagpolicy/","title":"Attach the Tag Policy to an account","tags":[],"description":"","content":"Attach Tag Policy to an account Click on Tag polices under polices - you should be to see the new policy was just created \u0026ldquo;cost_allocation\u0026rdquo; Click the box next to the cost allocation policy, and under actions select “Attach policy” Select an account to attach the policy to and click Attach policy X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/compute-optimizer-dashboards/dashboards/2_deployment/","title":"COD Deployment","tags":[],"description":"","content":"Deployment Options Currently this dashboard can only be installed via CID tool.\nDeploy via CID tool Please follow instructions here to install cid-cmd tool.\nYou can run deploy command and follow instructions in an interactive mode to install Compute Optimizer Dashboard.\ncid-cmd deploy Or you can provide all parameters in the command line. Please pay attention to the s3 path, it must be the same as in installation of Data Collection Lab.\ncid-cmd -vv deploy \\ --dashboard-id compute-optimizer-dashboard \\ --athena-database optimization_data \\ --view-compute-optimizer-lambda-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_lambda\u0026#39; \\ --view-compute-optimizer-ebs-volume-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_ebs_volume\u0026#39; \\ --view-compute-optimizer-auto-scale-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_auto_scale\u0026#39; \\ --view-compute-optimizer-ec2-instance-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_ec2_instance\u0026#39; Update via CID tool You can update just dashboard or execute a recursive update. Recursive update will also refresh datasets views and table definitions.\nFor dashboard update in interactive mode:\ncid-cmd update You can also provide all parameters in the command line. Please make sure the parameters are the same as on the deployment.\ncid-cmd -vv -yes update --recursive --force \\ --dashboard-id compute-optimizer-dashboard \\ --athena-database optimization_data \\ --view-compute-optimizer-lambda-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_lambda\u0026#39; \\ --view-compute-optimizer-ebs-volume-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_ebs_volume\u0026#39; \\ --view-compute-optimizer-auto-scale-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_auto_scale\u0026#39; \\ --view-compute-optimizer-ec2-instance-lines-s3FolderPath \u0026#39;s3://costoptimizationdata{account_id}/Compute_Optimizer/Compute_Optimizer_ec2_instance\u0026#39; X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/trusted-advisor-dashboards/dashboards/2_create-upload-ta-report/","title":"Create &amp; Upload Trusted Advisor Report","tags":[],"description":"","content":"Create and upload Trusted Advisor Organizational View report There are 2 supported data collection methods:\nTrusted Advisor Organizational View - provides an easy way to collect Trusted Advisor checks for all accounts in your AWS Organizations without need to provision any additional resources. Only manual data refresh is supported. Trusted Advisor API via deployment of Optimization Data Collection lab - provides an automated way to collect Trusted Advisor checks for all accounts in your AWS Organizations via deployment of required AWS resources from provided AWS CloudFormation templates. Supports automated data refresh. Please expand data collection method which you used in prerequisites step to proceed with workshop: Trusted Advisor Organizational View NOTE: At the moment TA Organizational View supports only manual report generation. Periodic refresh is required for the latest trends Create Organizational View report\nFor the step by step guide please follow the documentation Please choose JSON format for report You can select either all accounts and Trusted Advisor checks or filter by particular checks or Organizational Unit (OU). There is no limitation from dashboard deployment point of view Download Organizational View report\nFor step by step guide please follow the documentation Unzip downloaded report\nUpload downloaded report to the reports folder in the S3 bucket\nMake sure you upload unzipped folder to S3 bucket NOTE: You can upload as many folders with reports as you need. Dashboard will use all uploaded data to show trends over time Trusted Advisor API via deployment of Optimization Data Collection lab Please makes sure you\u0026rsquo;ve deployed Optimization Data Collection lab as prerequisite step. Once Optimization Data Collection lab completed, please proceed with next steps. During next steps please provide S3 URI path to ta-data folder in optimization data bucket created in the lab. The path should be similar to s3://costoptimizationdata{account_id}/optics-data-collector/ta-data/\nNOTE: Only Trusted Advisor Data Collection Module is required to be deployed. Consider other modules form the lab as optional X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/2b_cudos_dashboard/","title":"CUDOS Dashboard","tags":[],"description":"","content":"Authors Timur Tulyaganov, AWS Sr. Technical Account Manager Yuriy Prykhodko, AWS Sr. Technical Account Manager If you used the automation scripts in the previous step you should already have the CUDOS Dashboard and can skip to the bottom of this page and click Next Step\nDeployment Options There are 3 options to deploy the CUDOS Dashboard. If you are unsure what option to select, we recommend using the Manual deployment\nOption 1: Manual Deployment This option is the manual deployment and will walk you through all steps required to create this dashboard without any automation. We recommend this option users new to Athena and QuickSight. Click here to continue with the manual deployment Create Athena Views NOTE: If you\u0026rsquo;ve deployed the Cost Intelligence Dashboard, you will only need to create the customer_all view, views 0-5 are the same as the Cost Intelligence Dashboard The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR). The default dashboard assumes you have both Savings Plans and Reserved Instances, if not you will need to create the alternate views.\nLogin via SSO in your Cost Optimization account, go into the Athena console:\nModify and run the following queries to confirm if you have Savings Plans, and Reserved Instances in your usage. If no lines are returned, you have no Savings Plans or Reserved Instances. Replace (database).(tablename) and run the following:\nSavings Plans:\nselect * from (database).(tablename) where savings_plan_savings_plan_a_r_n not like '' limit 10 Reserved Instances:\nselect * from (database).(tablename) where reservation_reservation_a_r_n not like '' limit 10 NOTE: Unless you already have Savings Plans and Reserved Instances both already adopted as your savings options, recreate Athena Views corresponding with your savings profile whenever you onboard a new savings option (like Savings Plans or Reserved Instances) for the first time. Create the account_map view by modifying the following code, and executing it in Athena:\nView0 - account_map Create the Summary view by modifying the following code, and executing it in Athena:\nView1 - Summary View Create the EC2_Running_Cost view by modifying the following code, and executing it in Athena:\nView2 - EC2_Running_Cost Create the Compute savings plan eligible spend view by modifying the following code, and executing it in Athena:\nView3 - compute savings plan eligible spend Create the s3 view by modifying the following code, and executing it in Athena:\nView4 - s3 Create the RI SP Mapping view by modifying the following code, and executing it in Athena:\nView5 - RI SP Mapping Create the customer_all view by modifying the following code, and executing it in Athena:\nView6 - customer_all NOTE: The Athena Views are updated to reflect any additions in the cost and usage report. If you created your dashboard prior to June 1, 2021 you will want to update to the latest views. Create QuickSight Data Sets Create Datasets Go to the QuickSight service homepage inside your account. Be sure to select the correct region from the top right user menu or you will not see your expected tables\nFrom the left hand menu, choose Datasets\nClick New dataset displayed in the top right corner\nChoose Athena as your Data Source\nEnter a data source name of Cost_Dashboard and click Create data source\nSelect the database which holds the views you created (reference Athena if you’re unsure which one to select), and the summary_view table, then click Edit/Preview data\nSelect SPICE to change your Query mode\nClick Add Data\nSelect Data source\nChoose your Cost_Dashboard view and click Select\nSelect the database which holds the CUR views you created Choose your account_map view and click Select Click the two circles to open the Join conﬁguration, then select Left to change your join type\nConfigure the join clause to linked_account_id = account_id, then click Apply\nSelect Save\nSelect the summary_view dataset\nClick Schedule refresh\nClick Create\nEnter a daily schedule, in the appropriate time zone and click Create\nClick Cancel to exit\nClick x to exit\nRepeat steps 3-21, creating data sets with the remaining Athena views. You will reuse your existing Cost_Dashboard data source, and select the following views as the table:\ns3_view\nec2_running_cost\ncompute_savings_plan_eligible_spend\nNOTE: Make sure to reuse the existing Athena data source by scrolling to the bottom of the Data source create/select page when creating a new Dataset instead of creating a new data source When this step is complete, your Datasets tab should have 4 new SPICE Datasets\nSelect the summary_view dataset\nClick Edit Data Set\nClick Add Data\nSelect Data source Choose your Cost_Dashboard view and click Select\nSelect the database which holds the CUR views you created\nChoose your ri_sp_mapping view and click Select\nClick the two circles to open the Join conﬁguration, then select Left to change your join type\nClick Add a new join clause twice so you have 3 join clauses to configure in total. Configure the 3 join clauses as below, then click Apply\nri_sp_arn = ri_sp_arn_mapping payer_account_id = payer_account_id_mapping billing_period = billing_period_mapping Click Save\nClick New dataset displayed in the top right corner of QuickSight\nSelect your existing Cost_Dashboard as your Data Source\nSelect Create dataset Select the database which holds your customer_all view and select that view then click Edit/Preview data Keep the Query mode as Direct query\nRepeat steps 8-15, but configure this join clause in step 14: line_item_usage_account_id = account_id\nNOTE: This completes the QuickSight Data Preparation section. Next up is the Import process to generate the QuickSight Dashboard. Import Dashboard Template We will now use the CLI to create the dashboard from the CUDOS Dashboard template.\nEdit and Run list-users and make a note of your User ARN: aws quicksight list-users --aws-account-id \u0026lt;Account_ID\u0026gt; --namespace default --region \u0026lt;Region\u0026gt; Edit and Run list-data-sets and make a note of the Name and Arn for the 5 Datasets ARNs: aws quicksight list-data-sets --aws-account-id \u0026lt;Account_ID\u0026gt; --region \u0026lt;Region\u0026gt; Create an import.json file using the below sample { \u0026#34;AwsAccountId\u0026#34;: \u0026#34;\u0026lt;Account_ID\u0026gt;\u0026#34;, \u0026#34;DashboardId\u0026#34;: \u0026#34;cudos\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;CUDOS\u0026#34;, \u0026#34;Permissions\u0026#34;: [ { \u0026#34;Principal\u0026#34;: \u0026#34;\u0026lt;User ARN\u0026gt;\u0026#34;, \u0026#34;Actions\u0026#34;: [ \u0026#34;quicksight:DescribeDashboard\u0026#34;, \u0026#34;quicksight:ListDashboardVersions\u0026#34;, \u0026#34;quicksight:UpdateDashboardPermissions\u0026#34;, \u0026#34;quicksight:QueryDashboard\u0026#34;, \u0026#34;quicksight:UpdateDashboard\u0026#34;, \u0026#34;quicksight:DeleteDashboard\u0026#34;, \u0026#34;quicksight:DescribeDashboardPermissions\u0026#34;, \u0026#34;quicksight:UpdateDashboardPublishedVersion\u0026#34; ] } ], \u0026#34;DashboardPublishOptions\u0026#34;: { \u0026#34;AdHocFilteringOption\u0026#34;: { \u0026#34;AvailabilityStatus\u0026#34;: \u0026#34;DISABLED\u0026#34; } }, \u0026#34;SourceEntity\u0026#34;: { \u0026#34;SourceTemplate\u0026#34;: { \u0026#34;DataSetReferences\u0026#34;: [ { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;summary_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;ec2_running_cost\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;compute_savings_plan_eligible_spend\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;s3_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;customer_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; } ], \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:quicksight:us-east-1:223485597511:template/cudos_dashboard_v3\u0026#34; } }, \u0026#34;VersionDescription\u0026#34;: \u0026#34;1\u0026#34; } Update the import.json to match your details by replacing the following placeholders:\nPlaceholder Replace with \u0026lt;Account_ID\u0026gt; AWS Account ID where the dashboard will be deployed \u0026lt;Region\u0026gt; Region Code where the dashboard will be deployed (Example eu-west-1) \u0026lt;User ARN\u0026gt; ARN of your user \u0026lt;DataSetId\u0026gt; Replace with Dataset ID\u0026rsquo;s from the data sets you created in the Preparing Quicksight section NOTE: There are 5 unique Dataset IDs Run the import\naws quicksight create-dashboard --cli-input-json file://import.json --region \u0026lt;Region\u0026gt; --dashboard-id cudos Check the status of your deployment aws quicksight describe-dashboard --dashboard-id cudos --region \u0026lt;Region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; If you encounter no errors, open QuickSight from the AWS Console, and navigate to Dashboards. You should now see CUDOS available. This dashboard can be shared with other users, but is otherwise ready for viewing and customizing.\nIf something goes wrong in the dashboard creation step, correct the issue then delete the failed deployment before re-deploying\naws quicksight delete-dashboard --dashboard-id cudos --region \u0026lt;Region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; Option 2: Command Line Interface Deployment The CID command line tool is an optional way to create the Cloud Intelligence Dashboards. The command line tool will allow you to complete the deployments in less than half the time as the standard manual setup.\nClick here to continue with the Automation Scripts Deployment Navigate to the Cloud Intelligence Dashboards automation repo and follow the instructions to run the command line tool. You will have the option of deploying the CUDOS Dashboard from the list of supported dashboards. Follow the remaining steps if you have not already deployed the Cost Intelligence Dashboard in the previous step .\nOnce complete, visit the account mapping page and follow the steps there to get your account names into the dashboard. Schedule dataset refresh in QuickSight. Select the summary_view dataset\nClick Schedule refresh\nClick Create\nEnter a daily schedule, in the appropriate time zone and click Create\nClick Cancel to exit\nClick x to exit\nRepeat these steps with any remaining Athena datasets except customer_all.\nOption 3: CloudFormation Deployment This section is optional and automates the creation of the Cost Intelligence Dashboard and CUDOS Dashboard using a CloudFormation template. The CloudFormation template allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates and create an IAM role. If you do not have the required permissions use the Manual Deployment.\nClick here to continue with the CloudFormation Deployment NOTE: An IAM role will be created when you create the CloudFormation stack. Please review the CloudFormation template with your security team and switch to the manual setup if required Create the Cost Intelligence Dashboard using a CloudFormation Template Login via SSO in your Cost Optimization account\nClick the Launch CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch CloudFormation Template Enter a Stack name for your template such as Cost-Intelligence-Dashboard-QuickSight Review 1stReadMe parameter to confirm prerequisites before specifying the other parameters Update your AthenaQueryResultsBucket with the Athena results location where your CUR table is To validate your Athena primary workgroup has an output location by\nOpen a new tab or window and navigate to the Athena console Select Workgroup: primary Click the bubble next to primary and then select view detail Confirm your Query result location is configured with an S3 bucket path. If configured, add the location to the AthenaQueryResultsBucket in your CloudFormation Template. If not configured, continue to setting up by clicking Edit workgroup Add the S3 bucket path you have selected for your Query result location and click save Add the location to the AthenaQueryResultsBucket in your CloudFormation Template. Update your BucketFolderPath with the S3 path where your year partitions of CUR data are stored To validate the correct path for your year partitions of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the S3 console Select the S3 Bucket your CUR is located in Navigate your folders until you find the folder with the year partitions of the CUR Tip: Your yearly partitions folder is located in the folder with your .yml file, monthly folders, and status report Add the identified BucketFolderPath to the CloudFormation parameter making sure to not add trailing / (eg - BucketName/FolderName/\u0026hellip;/FolderName) Tip: copy and paste the S3 URI then remove the leading \u0026lsquo;s3://\u0026rsquo; and the ending \u0026lsquo;/\u0026rsquo; Update your CURDatabaseName and CURTableName with the name of the CUR Athena Database and Table To validate the Athena Database and Table of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the Glue console Select the Athena Table your CUR is located in Find your Database CURDatabaseName and Table CURTableName Add the identified CURDatabaseName and CURTableName to the CloudFormation parameter Update your QuickSightUser with your QuickSight username To validate your QuickSight complete the tasks below:\nOpen a new tab or window and navigate to the QuickSight console Find your username in the top right navigation bar Add the identified username to the CloudFormation parameter Update your QuicksightIdentityRegion with your QuickSight region Optional add a Suffix if you want to create multiple instances of the same account. Select Next at the bottom of Specify stack details and then select Next again on the Configure stack options page\nReview the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack. You will see the stack will start in CREATE_IN_PROGRESS NOTE: This step can take 5-15mins Once complete, the stack will show CREATE_COMPLETE Navigate to Dashboards page in your QuickSight console, click on your Cost Intelligence Dashboard name or your CUDOS Dashboard name Set up scheduled SPICE refresh for SPICE Datasets manually or click the Launch Spice Refresh CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch Spice Refresh CloudFormation Template Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you can share your dashboard with users or customize your own version of this dashboard\nClick to navigate QuickSight steps Add your Account Names to your Dashboard Learn how to replace the Accound IDs with the Account Names for each of your linked accounts in AWS Organizations by following these steps.\nSteps for adding account names Update Dashboard Template - Optional Click here to update your dashboard with the latest version If you are tracking our Changelog , you already know that we continue improving Cloud Intelligence Dashboards.\nTo pull the latest version of the dashboard from the public template please use the following steps.\nOption 1: Command Line Tool Visit the GitHub repository to download and install the CID Command Line Tool and follow the instructions for running the update command.\nOption 2: Manual Update Create an update.json file by removing permissions section from the import.json file. Sample for CUDOS Dashboard update.json file below: { \u0026#34;AwsAccountId\u0026#34;: \u0026#34;\u0026lt;Account_ID\u0026gt;\u0026#34;, \u0026#34;DashboardId\u0026#34;: \u0026#34;cudos\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;CUDOS\u0026#34;, \u0026#34;DashboardPublishOptions\u0026#34;: { \u0026#34;AdHocFilteringOption\u0026#34;: { \u0026#34;AvailabilityStatus\u0026#34;: \u0026#34;DISABLED\u0026#34; } }, \u0026#34;SourceEntity\u0026#34;: { \u0026#34;SourceTemplate\u0026#34;: { \u0026#34;DataSetReferences\u0026#34;: [ { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;summary_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;ec2_running_cost\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;compute_savings_plan_eligible_spend\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;s3_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;customer_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; } ], \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:quicksight:us-east-1:223485597511:template/cudos_dashboard_v3\u0026#34; } } } If needed update the update.json to match your details by replacing the following placeholders:\nPlaceholder Replace with \u0026lt;Account_ID\u0026gt; AWS Account ID where the dashboard will be deployed \u0026lt;Region\u0026gt; Region Code where the dashboard will be deployed (Example eu-west-1) \u0026lt;DatasetID\u0026gt; Replace with Dataset ID\u0026rsquo;s from the datasets you created in the Preparing Quicksight section NOTE: There are 5 unique Dataset IDs Pull the latest published version of the dashboard template. Example for CUDOS Dashboard below:\naws quicksight update-dashboard --cli-input-json file://update.json --region \u0026lt;region\u0026gt; Query the version number of the published dashboard. Example for CUDOS Dashboard below: aws quicksight list-dashboard-versions --region \u0026lt;region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; --dashboard-id cudos --query \u0026#39;sort_by(DashboardVersionSummaryList, \u0026amp;VersionNumber)[-1].VersionNumber\u0026#39; Apply the latest pulled changes to the deployed dashboard with this CLI command. Example for CUDOS Dashboard below: aws quicksight update-dashboard-published-version --region \u0026lt;region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; --dashboard-id cudos --version-number \u0026lt;version\u0026gt; X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/exportimport_utility/2_python_code/","title":"Python Code","tags":[],"description":"","content":"exportImportWAFR.py The purpose of this python script is to either generate a JSON file that contains the results of a Well-Architected review or import the contents of a JSON file into the Well-Architected Tool. This would allow you to create a backup copy of your workload properties to use in another system or to interchange with others if the cannot access your AWS environment.\nThis utility was created using the the AWS SDK for Python (Boto3) . This file assumes you have already setup your AWS credential file, and uses the default profile for all interactions.\nThere is error checking for most of the various API calls, but the code should not be considered production ready. Please review before implementing in your environment.\nParameters usage: exportImportWAFR.py [-h] (--exportWorkload | --importWorkload) [-p PROFILE] [-r REGION] [-w WORKLOADID] -f FILENAME [-v] This utility has two options to run: ------------------------------------ 1) Export - Export the contents of a workload from the Well-Architected tool 2) Import - Create a new workload from the JSON export file generated in export optional arguments: -h, --help show this help message and exit --exportWorkload export the workload to a file --importWorkload import the workload from a file -p PROFILE, --profile PROFILE AWS CLI Profile Name -r REGION, --region REGION From Region Name. Example: us-east-1 -w WORKLOADID, --workloadid WORKLOADID Workload Id to use instead of creating a TEMP workload -f FILENAME, --fileName FILENAME FileName to export JSON file to -v, --debug print debug messages to stderr Limitations You can export any version of a Well-Architected review, but you can only import reviews that are the latest version supported by the tool Python Code Link to download the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 #!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; This is a tool to export a WAFR to a JSON file or import from a JSON file into a new review This code is only for use in Well-Architected labs *** NOT FOR PRODUCTION USE *** Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ \u0026#34;\u0026#34;\u0026#34; import json import datetime import logging import sys import argparse import botocore import boto3 import jmespath from pkg_resources import packaging __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] __version__ = \u0026#34;0.1\u0026#34; # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; # Setup Logging logging.basicConfig( level=logging.DEBUG, format=\u0026#39;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, ) logger = logging.getLogger() logging.getLogger(\u0026#39;boto3\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;botocore\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;s3transfer\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;urllib3\u0026#39;).setLevel(logging.CRITICAL) PARSER = argparse.ArgumentParser( formatter_class=argparse.RawDescriptionHelpFormatter, description=\u0026#39;\u0026#39;\u0026#39;\\ This utility has two options to run: ------------------------------------ 1) Export - Export the contents of a workload from the Well-Architected tool 2) Import - Create a new workload from the JSON export file generated in export \u0026#39;\u0026#39;\u0026#39; ) # We need to know if we should import or export, so these are mutually exclusive requireds GROUP = PARSER.add_mutually_exclusive_group(required=True) GROUP.add_argument(\u0026#39;--exportWorkload\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;export the workload to a file\u0026#39;) GROUP.add_argument(\u0026#39;--importWorkload\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;import the workload from a file\u0026#39;) PARSER.add_argument(\u0026#39;-p\u0026#39;,\u0026#39;--profile\u0026#39;, required=False, default=\u0026#34;default\u0026#34;, help=\u0026#39;AWS CLI Profile Name\u0026#39;) PARSER.add_argument(\u0026#39;-r\u0026#39;,\u0026#39;--region\u0026#39;, required=False, default=\u0026#34;us-east-1\u0026#34;, help=\u0026#39;From Region Name. Example: us-east-1\u0026#39;) PARSER.add_argument(\u0026#39;-w\u0026#39;,\u0026#39;--workloadid\u0026#39;, required=False, default=\u0026#34;\u0026#34;, help=\u0026#39;Workload Id to use instead of creating a TEMP workload\u0026#39;) PARSER.add_argument(\u0026#39;-f\u0026#39;,\u0026#39;--fileName\u0026#39;, required=True, default=\u0026#34;./demo.xlsx\u0026#34;, help=\u0026#39;FileName to export JSON file to\u0026#39;) PARSER.add_argument(\u0026#39;-v\u0026#39;,\u0026#39;--debug\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;print debug messages to stderr\u0026#39;) ARGUMENTS = PARSER.parse_args() PROFILE = ARGUMENTS.profile FILENAME = ARGUMENTS.fileName REGION_NAME = ARGUMENTS.region WORKLOADID = ARGUMENTS.workloadid exportWorkload=False importWorkload=False if ARGUMENTS.exportWorkload: exportWorkload=True elif ARGUMENTS.importWorkload: importWorkload=True else: logger.error(\u0026#34;--exportWorkload or --importWorkload is required\u0026#34;) sys.exit() if ARGUMENTS.debug: logger.setLevel(logging.DEBUG) else: logger.setLevel(logging.INFO) # To map our short hand names in the console to the API defined pillars # Example: print(PILLAR_PARSE_MAP[\u0026#39;performance\u0026#39;]) PILLAR_PARSE_MAP = { \u0026#34;operationalExcellence\u0026#34;: \u0026#34;OPS\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;SEC\u0026#34;, \u0026#34;reliability\u0026#34;: \u0026#34;REL\u0026#34;, \u0026#34;performance\u0026#34;: \u0026#34;PERF\u0026#34;, \u0026#34;costOptimization\u0026#34;: \u0026#34;COST\u0026#34; } PILLAR_PROPER_NAME_MAP = { \u0026#34;operationalExcellence\u0026#34;: \u0026#34;Operational Excellence\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;Security\u0026#34;, \u0026#34;reliability\u0026#34;: \u0026#34;Reliability\u0026#34;, \u0026#34;performance\u0026#34;: \u0026#34;Performance Efficiency\u0026#34;, \u0026#34;costOptimization\u0026#34;: \u0026#34;Cost Optimization\u0026#34; } class DateTimeEncoder(json.JSONEncoder): \u0026#34;\u0026#34;\u0026#34;Helper class to convert a datetime item to JSON.\u0026#34;\u0026#34;\u0026#34; def default(self, z): if isinstance(z, datetime.datetime): return str(z) return super().default(z) def CreateNewWorkload( waclient, workloadName, description, reviewOwner, environment, awsRegions, lenses, tags, pillarPriorities, notes=\u0026#34;\u0026#34;, nonAwsRegions=[], architecturalDesign=\u0026#39;\u0026#39;, industryType=\u0026#39;\u0026#39;, industry=\u0026#39;\u0026#39;, accountIds=[]): \u0026#34;\u0026#34;\u0026#34; Create your workload \u0026#34;\u0026#34;\u0026#34; try: response=waclient.create_workload( WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, Lenses=lenses, NonAwsRegions=nonAwsRegions, PillarPriorities=pillarPriorities, ArchitecturalDesign=architecturalDesign, IndustryType=industryType, Industry=industry, Notes=notes, AccountIds=accountIds ) except waclient.exceptions.ConflictException as e: workloadId,workloadARN = FindWorkload(waclient,workloadName) logger.error(\u0026#34;ERROR - The workload name %s already exists as workloadId %s\u0026#34; % (workloadName, workloadId)) userAnswer=input(\u0026#34;Do You Want To Overwrite workload %s? [y/n]\u0026#34; % workloadId) if userAnswer == \u0026#34;y\u0026#34;: logger.info(\u0026#34;Overwriting existing workload\u0026#34;) UpdateWorkload(waclient,workloadId,workloadARN, workloadName,description,reviewOwner,environment,awsRegions,tags) else: logger.error(\u0026#34;Exiting due to duplicate workload and user states they do not want to continue.\u0026#34;) sys.exit(1) return workloadId, workloadARN except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) workloadId = response[\u0026#39;WorkloadId\u0026#39;] workloadARN = response[\u0026#39;WorkloadArn\u0026#39;] return workloadId, workloadARN def UpdateWorkload( waclient, workloadId, workloadARN, workloadName, description, reviewOwner, environment, awsRegions, tags ): \u0026#34;\u0026#34;\u0026#34; Update your workload \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Updating workload properties\u0026#34;) try: waclient.update_workload( WorkloadId=workloadId, WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) if tags: logger.info(\u0026#34;Updating workload tags\u0026#34;) try: waclient.tag_resource(WorkloadArn=workloadARN,Tags=tags) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) else: logger.info(\u0026#34;Found blank tag set, removing any I find\u0026#34;) try: tagresponse = waclient.list_tags_for_resource(WorkloadArn=workloadARN) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) tagkeys = list(tagresponse[\u0026#39;Tags\u0026#39;]) if tagkeys: try: waclient.untag_resource(WorkloadArn=workloadARN,TagKeys=tagkeys) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) else: logger.info(\u0026#34;TO Workload has blank keys as well, no need to update\u0026#34;) def findAllQuestionId( waclient, workloadId, lensAlias ): \u0026#34;\u0026#34;\u0026#34; Find all question ID\u0026#39;s \u0026#34;\u0026#34;\u0026#34; answers = [] # Due to a bug in some lenses, I have to iterate over each pillar in order to # retrieve the correct results. for pillar in PILLAR_PARSE_MAP: logger.debug(\u0026#34;Grabbing answers for %s %s\u0026#34; % (lensAlias, pillar)) # Find a questionID using the questionTitle try: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillar ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) while \u0026#34;NextToken\u0026#34; in response: try: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,PillarId=pillar,NextToken=response[\u0026#34;NextToken\u0026#34;]) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) return answers def FindWorkload( waclient, workloadName ): \u0026#34;\u0026#34;\u0026#34; Finding your WorkloadId and ARN \u0026#34;\u0026#34;\u0026#34; try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] workloadArn = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadArn\u0026#39;] return workloadId, workloadArn def GetWorkload( waclient, workloadId ): \u0026#34;\u0026#34;\u0026#34; Get the Workload JSON \u0026#34;\u0026#34;\u0026#34; try: response=waclient.get_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) sys.exit() # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Workload\u0026#39;], cls=DateTimeEncoder)) workload = response[\u0026#39;Workload\u0026#39;] return workload def associateLens( waclient, workloadId, lens ): \u0026#34;\u0026#34;\u0026#34; Associate the lens from the WorkloadId\u0026#34;\u0026#34;\u0026#34; try: response=waclient.associate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) return response def getAnswerForQuestion( waclient, workloadId, lensAlias, questionId ): \u0026#34;\u0026#34;\u0026#34; Find a answer for a questionId \u0026#34;\u0026#34;\u0026#34; try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;Answer\u0026#39;] return answers def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): \u0026#34;\u0026#34;\u0026#34; Update a answer to a question \u0026#34;\u0026#34;\u0026#34; try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def listAllAnswers( waclient, workloadId, lensAlias ): \u0026#34;\u0026#34;\u0026#34; Get a list of all answers\u0026#34;\u0026#34;\u0026#34; answers = [] allQuestionsForLens = findAllQuestionId(waclient,workloadId,lensAlias) for pillar in PILLAR_PARSE_MAP: jmesquery = \u0026#34;[?PillarId==\u0026#39;\u0026#34;+pillar+\u0026#34;\u0026#39;]\u0026#34; allQuestionsForPillar = jmespath.search(jmesquery, allQuestionsForLens) for answersLoop in allQuestionsForPillar: fullAnswerForQuestion = getAnswerForQuestion(waclient, workloadId, lensAlias, answersLoop[\u0026#39;QuestionId\u0026#39;]) answers.append(fullAnswerForQuestion) return answers def getWorkloadLensReview( waclient, workloadId, lensAlias ): \u0026#34;\u0026#34;\u0026#34; List all lenses currently available\u0026#34;\u0026#34;\u0026#34; try: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) return response[\u0026#39;LensReview\u0026#39;] def main(): \u0026#34;\u0026#34;\u0026#34; Main program run \u0026#34;\u0026#34;\u0026#34; boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) sys.exit() logger.info(\u0026#34;Script version %s\u0026#34; % __version__) logger.info(\u0026#34;Starting Boto %s Session\u0026#34; % boto3.__version__) # Create a new boto3 session SESSION1 = boto3.session.Session(profile_name=PROFILE) # Initiate the well-architected session using the region defined above WACLIENT = SESSION1.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) # This will setup a blank dict we can use to export to a JSON file exportObject = { \u0026#34;workload\u0026#34;: [], \u0026#34;lenses\u0026#34;: [], \u0026#34;lens_review\u0026#34;: [], } if exportWorkload: logger.info(\u0026#34;Exporting workload \u0026#39;%s\u0026#39; to file %s\u0026#34; % (WORKLOADID, FILENAME)) workloadJson = GetWorkload(WACLIENT,WORKLOADID) exportObject[\u0026#39;workload\u0026#39;].append(workloadJson) # Iterate over each lens and copy all of the answers for lens in workloadJson[\u0026#39;Lenses\u0026#39;]: logger.info(\u0026#34;Gathering overall review for lens %s\u0026#34; % lens) lensReview = getWorkloadLensReview(WACLIENT,WORKLOADID,lens) exportObject[\u0026#39;lens_review\u0026#39;].append({lens: lensReview}) logger.info(\u0026#34;Retrieving all answers for lens %s\u0026#34; % lens) answers = listAllAnswers(WACLIENT,WORKLOADID,lens) exportObject[\u0026#39;lenses\u0026#39;].append({lens: answers}) with open(FILENAME, \u0026#39;w\u0026#39;) as outfile: json.dump(exportObject, outfile, indent=4, cls=DateTimeEncoder) logger.info(\u0026#34;Export completed to file %s\u0026#34; % FILENAME) if importWorkload: logger.info(\u0026#34;Creating a new workload from file %s\u0026#34; % FILENAME) with open(FILENAME) as json_file: importObject = json.load(json_file) workloadJson = importObject[\u0026#39;workload\u0026#39;][0] # For each of the optional variables, lets check and see if we have them first: Notes = workloadJson[\u0026#39;Notes\u0026#39;] if \u0026#34;Notes\u0026#34; in workloadJson else \u0026#34;\u0026#34; nonAwsRegions = workloadJson[\u0026#39;NonAwsRegions\u0026#39;] if \u0026#34;NonAwsRegions\u0026#34; in workloadJson else [] architecturalDesign = workloadJson[\u0026#39;ArchitecturalDesign\u0026#39;] if \u0026#34;ArchitecturalDesign\u0026#34; in workloadJson else \u0026#34;\u0026#34; industryType = workloadJson[\u0026#39;IndustryType\u0026#39;] if \u0026#34;IndustryType\u0026#34; in workloadJson else \u0026#34;\u0026#34; industry = workloadJson[\u0026#39;Industry\u0026#39;] if \u0026#34;Industry\u0026#34; in workloadJson else \u0026#34;\u0026#34; accountIds = workloadJson[\u0026#39;AccountIds\u0026#39;] if \u0026#34;AccountIds\u0026#34; in workloadJson else [] tags = workloadJson[\u0026#39;Tags\u0026#39;] if \u0026#34;Tags\u0026#34; in workloadJson else [] # Create the new workload to copy into toWorkloadId,toWorkloadARN = CreateNewWorkload(WACLIENT, (workloadJson[\u0026#39;WorkloadName\u0026#39;]), workloadJson[\u0026#39;Description\u0026#39;], workloadJson[\u0026#39;ReviewOwner\u0026#39;], workloadJson[\u0026#39;Environment\u0026#39;], workloadJson[\u0026#39;AwsRegions\u0026#39;], workloadJson[\u0026#39;Lenses\u0026#39;], tags, workloadJson[\u0026#39;PillarPriorities\u0026#39;], Notes, nonAwsRegions, architecturalDesign, industryType, industry, accountIds ) logger.info(\u0026#34;New workload id: %s (%s)\u0026#34; % (toWorkloadId,toWorkloadARN)) # Iterate over each lens and copy all of the answers for lens in workloadJson[\u0026#39;Lenses\u0026#39;]: # We need to verify the lens version first logger.info(\u0026#34;Verifying lens version before restoring answers\u0026#34;) lensReview = getWorkloadLensReview(WACLIENT,toWorkloadId,lens) importLensVersion = jmespath.search(\u0026#34;[*].\u0026#34;+lens+\u0026#34;.LensVersion\u0026#34;, importObject[\u0026#39;lens_review\u0026#39;])[0] # ************************************************************************ # There is no ability to restore to a specific lens version # in the API at this time, so we just have to error out if # the version has changed. # ************************************************************************ if lensReview[\u0026#39;LensVersion\u0026#39;] != importLensVersion: logger.error(\u0026#34;Version of the lens %s does not match the new workload\u0026#34; % lens) logger.error(\u0026#34;Import Version: %s\u0026#34; % importLensVersion) logger.error(\u0026#34;New Workload Version: %s\u0026#34; % lensReview[\u0026#39;LensVersion\u0026#39;]) logger.error(\u0026#34;You may need to delete the workload %s\u0026#34; % toWorkloadId) sys.exit() else: logger.info(\u0026#34;Versions match (%s)\u0026#34; % importLensVersion) logger.info(\u0026#34;Retrieving all answers for lens %s\u0026#34; % lens) answers = jmespath.search(\u0026#34;[*].\u0026#34;+lens+\u0026#34;[]\u0026#34;, importObject[\u0026#39;lenses\u0026#39;]) associateLens(WACLIENT,toWorkloadId,[lens]) logger.info(\u0026#34;Copying answers into new workload for lens %s\u0026#34; % lens) for answerCopy in answers: notesField = answerCopy[\u0026#39;Notes\u0026#39;] if \u0026#34;Notes\u0026#34; in answerCopy else \u0026#34;\u0026#34; updateAnswersForQuestion(WACLIENT,toWorkloadId,lens,answerCopy[\u0026#39;QuestionId\u0026#39;],answerCopy[\u0026#39;SelectedChoices\u0026#39;],notesField) logger.info(\u0026#34;Copy complete - exiting\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/disaster/rds/","title":"RDS","tags":[],"description":"","content":"Restore RDS Database If you are running this workshop as part of an instructor led workshop, the RDS has already been restored to the N. California (us-west-1) region due to time constraints. Please review the steps in this section so you understand how the restore should work and then continue with the Configure Security Group Section below.\n1.1 Click AWS Backup to navigate to the dashboard in the N. California (us-west-1) region.\n1.2 Click the Backup Vaults link, then click the Default link.\n1.3 In the Backups section. Select the backup. Click Restore under the Actions dropdown.\nIf you don\u0026rsquo;t see your backup, check the status of the Copy Job. Click AWS Backup to navigate to the dashboard in N. Virginia (us-east-1) region. Click the Jobs link, then click the Copy jobs link. Verify the Status of your backup is Completed.\n1.4 In the Settings section, enter backupandrestore-secondary-region as the DB Instance Identifier. Under Network \u0026amp; Security section, select us-west-xx as the Availability zone.\n1.5 Click the Restore backup button.\nConfigure Security Group 2.1 Click VPC to navigate to the dashboard in the N. California (us-west-1) region.\n2.2 Click the Security Groups link, then click the Create security group button.\n2.3 In the Basic details section, enter backupandrestore-us-west-rds-SG as the Security group name and Description.\n2.4 in the Inbound Rules section, click the Add rule button. Select MYSQL/Aurora as the Type. Select Custom as the Source and choose backupandrestore-us-west-ec2-SG. This will allow the communication between your EC2 instance and your RDS instance. Click the Create security group button.\nModify RDS 3.1 Click RDS to navigate to the dashboard in the N. California (us-west-1) region.\n3.2 Click the DB Instances link.\n3.3 Select backupandrestore-secondary-region, then click the Modify button.\nThe database must have a Status of Available.\n3.4 In the Connectivity section, select backupandrestore-us-west-rds-SG as the Security group. Click the Continue button.\n3.5 Select Apply immediately and then click the Modify DB instance button..\n3.6 Click the backupandrestore-secondary-region link.\nCopy the name of the endpoint and port. You will need this in a later step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/backup-resources/","title":"Create Backup Resources","tags":[],"description":"","content":"Now we are going to backup our resources.\nWe will perform the following:\nBackup the RDS database Create an EC2 AMI (Amazon Machine Image) Create a new S3 UI bucket We would create a backup plan for a production application and schedule recurring backups to meet the target RPO.\nHowever, for the workshop, we will create a manual backup.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/backup-resources/ec2/","title":"EC2","tags":[],"description":"","content":"Create an AMI (Amazon Machine Image) If our EC2 instance contained application data, it would be necessary to schedule recurring backups to meet the target RPO. AWS Backup provides this functionality through a dashboard that makes it simple to audit backup and restores activity across AWS services. Since our instance contains no data, only code, we will create a one-time backup. Reoccurring backups are necessary for a production application every time a change occurs to the EC2 instance.\n1.1 Click EC2 to navigate to the dashboard in the N. Virginia (us-east-1) region.\n1.2 Click the Instances (running) link.\n1.2 Select UniShopAppV1EC2BackupAndRestore. Click Create image under the Actions -\u0026gt; Images and Templates dropdown.\n1.3 Enter BackupAndRestoreImage as the Image name, then click the Create Image button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/copy-to-secondary/ec2/","title":"EC2","tags":[],"description":"","content":"Copy the EC2 AMI (Amazon Machine Image) 1.1 Click EC2 to navigate to the dashboard in the N. Virginia (us-east-1) region.\n1.2 Click the AMIs link.\n1.3 Verify the BackupAndRestoreImage has a status of Available.\n1.4 Select BackupAndRestoreImage. Click Copy AMI under the Actions dropdown.\n1.5 Select US-West (N. California) as the Destination region, then click the Copy AMI button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/prerequisites/cfn-outputs/","title":"CloudFormation Outputs","tags":[],"description":"","content":"Saving the Cloudformation Template Output Values Primary Region 1.1 Click CloudFormation Stacks to navigate to the dashboard in the N. Virginia (us-east-1) region.\n1.2 Select Active-Primary.\n1.3 Wait until the stack\u0026rsquo;s status reports CREATE_COMPLETE. Then click the Outputs link and record the values of the APIGURL, WebsiteURL, and WebsiteBucket outputs. You will need these to complete future steps.\nSecondary Region 2.1 Click CloudFormation Stacks to navigate to the dashboard in the N. California (us-west-1) region.\n2.2 Select Passive-Secondary.\n2.3 Wait until the stack\u0026rsquo;s status reports CREATE_COMPLETE. Then click the Outputs link and record the values of the APIGURL, WebsiteURL, and WebsiteBucket outputs. You will need these to complete future steps.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/dynamodb-global/","title":"DynamoDB Global Tables","tags":[],"description":"","content":"When you create a DynamoDB global table, it consists of multiple replica tables (one per AWS Region) that DynamoDB treats as a single unit. Every replica has the same table name and the same primary key schema. When an application writes data to a replica table in one Region, DynamoDB propagates the write to the other replica tables in the other AWS Regions automatically.\nWe are going to configure DynamoDB global tables replicating from AWS Region N. Virginia (us-east-1) to AWS Region N. California (us-west-1).\nIf you are using your own AWS Account, you must wait for the Primary Region stack to successfully be created before moving on to this step.\nDeploying Amazon DynamoDB Global Tables 1.1 Click DynamoDB to navigate to the dashboard in the N. Virginia (us-east-1) regions.\n1.2 Click the Tables link.\n1.3 Click the unishophotstandby link.\n1.4 Click the Global Tables link, click the Create replica button.\n1.5 Select US West (N. California) as the Available replication Regions, then click the Create replica button.\nThis might take a few minutes, feel free to move onto the next step. Just ensure the status is showing Active before Verify Websites step.\nCongratulations! Your DynamoDB Global Tables have been created! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/prerequisites/account-setup/primary-region/","title":"Primary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Create application in primary region N. Virginia (us-east-1) by launching this CloudFormation Template .\n1.2 Click the Next button.\n1.3 Click the Next button.\nLeave LatestAmiId as the default values\n1.4 Click the Next button.\n1.5 Scroll to the bottom of the page and enable the I acknowledge that AWS CloudFormation might create IAM resources with custom names checkbox, then click the Create stack button.\nWait for the stack creation to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/prerequisites/account-setup/primary-region/","title":"Primary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Create application in primary region N. Virginia (us-east-1) by launching CloudFormation Template .\n1.2 Specify stack details.\nLeave IsPrimary, IsPromote, and LatestAmiId as the default values\n1.3 Click Next to continue.\n1.4 Leave the Configure stack options page defaults and click Next to continue.\n1.5 Scroll to the bottom of the page and click the checkbox to acknowledge then click Create stack.\nWait for the stack creation to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/prerequisites/account-setup/primary-region/","title":"Primary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Create application in primary region N. Virginia (us-east-1) by launching CloudFormation Template .\n1.2 Specify stack details.\nLeave IsPrimary, IsPromote, and LatestAmiId as the default values\n1.3 Click Next to continue.\n1.4 Leave the Configure stack options page defaults and click Next to continue.\n1.5 Scroll to the bottom of the page and click the checkbox to acknowledge then click Create stack.\nWait for the stack creation to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/prerequisites/account-setup/primary-region/","title":"Primary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Deploy the application to the primary region N. Virginia (us-east-1) by launching this CloudFormation Template .\n1.2 Specify the stack parameters.\nLeave isPrimary and LatestAmiId as the default values\n1.3 Click Next to continue.\n1.4 Leave the Configure stack options page defaults and click Next to continue.\n1.5 Scroll to the bottom of the page and click the checkbox to acknowledge IAM role creation, then click Create stack.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/verify-aurora-writefwd/","title":"Verify Aurora Write Forwarding","tags":[],"description":"","content":"Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.\nThe Read-Replica Write Forwarding feature\u0026rsquo;s typical latency is under one second from secondary to primary databases. This capability enables low latency global reads across your global presence. In disaster recovery situations, you can promote a secondary region to take full read-write responsibilities in under a minute.\nNow, let us verify Amazon Aurora MySQL Read-Replica Write Forwarding on our Amazon Aurora MySQL Replica instance\n1.1 Navigate to RDS in N. California (us-west-1) region.\n1.2 Next, click into DB Instances.\n1.3 Click the dr-immersionday-secondary-warm link.\n1.4 Click the Configuration link and verify Read replica write forwarding is Enabled.\nCongratulations! You have verified the Amazon Aurora Global Database supports Read-Replica Write Forwarding! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/","title":"Quest: Simplest Security Steps","tags":[],"description":"These are the six things we encourage all customers to do to improve their security in the cloud.","content":"Authors Farhan Farooq, Solutions Architect, WWPS Introduction These are the six things we encourage all customers to do to improve their security in the cloud.\nPrerequisites An AWS account that you are able to use for testing. Steps: Step 1 - Protect privileged credentials Step 2 - Use temporary credentials Step 3 - Replace hardcoded credentials Step 4 - Limit Network Access Step 5 - Apply patches Step 6 - Restrict public storage "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/2_use_temporary_credentials/","title":"Step 2 - Use temporary credentials","tags":[],"description":"","content":"In this exercise we will use AWS IAM Roles to avoid the usage of AWS IAM access keys that may be required by the Amazon ELastic Compute Cloud (EC2) instance to access AWS resources. We will create a Role and assigned it to EC2 instance, instead of hard coding the access keys within the EC2 instance.\nNote: For this lab, it is assumed that EC2 instance is already created with default settings. For instructions to create EC2 Instance please follow the link .\nFrom the AWS console, on the top right corner, click on the drop-down list where your IAM user and Account is mentioned. From the drop-down list, click on My Security Credentials.\nScroll down the page and under the ‘Access keys for CLI, SDK, \u0026amp; API access’ section and note the staus of any active Access key ID. Anyone with access to these long-lived keys can use them to perform actions with the configured permissions. Instead of Access Keys, we will create a role because they provide short term access.\nTo avoid the access key usage we first need to create an IAM role. Click on Roles on the menu on the left side of the console under Access Management.\nClick on Create role.\nClick on AWS service. Then click on EC2 under \u0026lsquo;Choose a use case\u0026rsquo; section. Click Next: Permission.\nIn the Search field type the policy that you want to attach to your EC2 instance and select from the list below i.e., AmazonRekognitionReadOnlyAccess policy.\nClick Next: Tags.\nProvide the optional Key and value to the tag.\nClick Next: Review\nProvide a meaningful name for the Role and optional description. Click Create role.\nYou will notice the newly created role is now appearing in the list of roles.\nGo to services, click EC2.\nOn the dashboard, click on Instances (running). We will create a Role and assign it to the EC2 instance, instead of hard coding the access keys within the EC2 instance.\nSelect your EC2 instance that you want to assigned the role. Click Actions -\u0026gt; Security -\u0026gt; Modify IAM role.\nFrom the drop-down list of IAM roles, select the role that you have created in the previous steps.\nClick Save. Your EC2 instance can now access the required AWS service with minimum privilege.\nWe will now disable the Access Key as it is no longer required.\nFrom the AWS console, on the top right corner, click on the drop-down list where your IAM user and Account is mentioned and click on My Security Credentials.\nScroll down the page and under the \u0026lsquo;Access keys for CLI, SDK, \u0026amp; API access\u0026rsquo; section click on Make Inactive under the Actions column of the mentioned Access Key.\nClick on Deactivate on the confirmation dialogue box.\nYou will notice the message about deactivation of your IAM user Access key.\nFor more information please read the AWS User Guide: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/export_to_xlsx/2_python_code/","title":"Python Code","tags":[],"description":"","content":"exportAnswersToXLSX.py The purpose of this python script is to generate a XLSX template file that contains all of the questions, best practices, and improvement plans to conduct a review. This spreadsheet can be used to prepare for a AWS Well-Architected review by collecting information from teams before the workload review is entered into the tool. This also allow you perform a review offline if you are working in a AWS region that does not support the Well-Architected Tool. In addition to generating the XLSX template, you can also use this script to export the contents of an existing Well-Architected Workload in a spreadsheet. This can be useful for printing and sharing with feedback sources that do not have access to the AWS Well-Architected tool.\nThis utility was created using the the AWS SDK for Python (Boto3) . This file assumes you have already setup your AWS credential file, and uses the default profile for all interactions.\nThere is error checking for most of the various API calls, but the code should not be considered production ready. Please review before implementing in your environment.\nParameters usage: exportAnswersToXLSX.py [-h] [-p PROFILE] [-r REGION] [-w WORKLOADID] [-k] -f FILENAME [-v] This utility has two options to run: ------------------------------------ 1) If you provide a workloadid, this will gather all of the answers across all Well-Architected Lenss and export them to a spreadsheet. 2) If you do not provide a workloadid, the utility will generate a TEMP workload and auto-answer every question. It will then generate a spreadsheet with all of the questions, best practices, and even the improvement plan links for each. optional arguments: -h, --help show this help message and exit -p PROFILE, --profile PROFILE AWS CLI Profile Name -r REGION, --region REGION From Region Name. Example: us-east-1 -w WORKLOADID, --workloadid WORKLOADID Workload Id to use instead of creating a TEMP workload -k, --keeptempworkload If you want to keep the TEMP workload created at the end of the export -f FILENAME, --fileName FILENAME FileName to export XLSX (REQUIRED) -v, --debug print debug messages to stderr Limitations None at this time Python Code Link to download the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 #!/usr/bin/env python3 # This is a tool to export the WA framework answers to a XLSX file # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ import botocore import boto3 import json import datetime import logging import jmespath import xlsxwriter import argparse from pkg_resources import packaging import urllib.request from bs4 import BeautifulSoup, NavigableString, Tag __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] __version__ = \u0026#34;0.1\u0026#34; # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; # Setup Logging logging.basicConfig( level=logging.DEBUG, format=\u0026#39;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, ) logger = logging.getLogger() logging.getLogger(\u0026#39;boto3\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;botocore\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;s3transfer\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;urllib3\u0026#39;).setLevel(logging.CRITICAL) PARSER = argparse.ArgumentParser( formatter_class=argparse.RawDescriptionHelpFormatter, description=\u0026#39;\u0026#39;\u0026#39;\\ This utility has two options to run: ------------------------------------ 1) If you provide a workloadid, this will gather all of the answers across all Well-Architected Lenss and export them to a spreadsheet. 2) If you do not provide a workloadid, the utility will generate a TEMP workload and auto-answer every question. It will then generate a spreadsheet with all of the questions, best practices, and even the improvement plan links for each. \u0026#39;\u0026#39;\u0026#39; ) PARSER.add_argument(\u0026#39;-p\u0026#39;,\u0026#39;--profile\u0026#39;, required=False, default=\u0026#34;default\u0026#34;, help=\u0026#39;AWS CLI Profile Name\u0026#39;) PARSER.add_argument(\u0026#39;-r\u0026#39;,\u0026#39;--region\u0026#39;, required=False, default=\u0026#34;us-east-1\u0026#34;, help=\u0026#39;From Region Name. Example: us-east-1\u0026#39;) PARSER.add_argument(\u0026#39;-w\u0026#39;,\u0026#39;--workloadid\u0026#39;, required=False, default=\u0026#34;\u0026#34;, help=\u0026#39;Workload Id to use instead of creating a TEMP workload\u0026#39;) PARSER.add_argument(\u0026#39;-k\u0026#39;,\u0026#39;--keeptempworkload\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;If you want to keep the TEMP workload created at the end of the export\u0026#39;) PARSER.add_argument(\u0026#39;-f\u0026#39;,\u0026#39;--fileName\u0026#39;, required=True, default=\u0026#34;./demo.xlsx\u0026#34;, help=\u0026#39;FileName to export XLSX\u0026#39;) PARSER.add_argument(\u0026#39;-v\u0026#39;,\u0026#39;--debug\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;print debug messages to stderr\u0026#39;) ARGUMENTS = PARSER.parse_args() PROFILE = ARGUMENTS.profile FILENAME = ARGUMENTS.fileName REGION_NAME = ARGUMENTS.region WORKLOADID = ARGUMENTS.workloadid KEEPTEMP = ARGUMENTS.keeptempworkload if ARGUMENTS.debug: logger.setLevel(logging.DEBUG) else: logger.setLevel(logging.INFO) # To map our short hand names in the console to the API defined pillars # Example: print(PILLAR_PARSE_MAP[\u0026#39;performance\u0026#39;]) PILLAR_PARSE_MAP = { \u0026#34;operationalExcellence\u0026#34;: \u0026#34;OPS\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;SEC\u0026#34;, \u0026#34;reliability\u0026#34;: \u0026#34;REL\u0026#34;, \u0026#34;performance\u0026#34;: \u0026#34;PERF\u0026#34;, \u0026#34;costOptimization\u0026#34;: \u0026#34;COST\u0026#34; } PILLAR_PROPER_NAME_MAP = { \u0026#34;operationalExcellence\u0026#34;: \u0026#34;Operational Excellence\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;Security\u0026#34;, \u0026#34;reliability\u0026#34;: \u0026#34;Reliability\u0026#34;, \u0026#34;performance\u0026#34;: \u0026#34;Performance Efficiency\u0026#34;, \u0026#34;costOptimization\u0026#34;: \u0026#34;Cost Optimization\u0026#34; } # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def CreateNewWorkload( waclient, workloadName, description, reviewOwner, environment, awsRegions, lenses, tags, pillarPriorities, notes=\u0026#34;\u0026#34;, nonAwsRegions=[], architecturalDesign=\u0026#39;\u0026#39;, industryType=\u0026#39;\u0026#39;, industry=\u0026#39;\u0026#39;, accountIds=[] ): # Create your workload try: response=waclient.create_workload( WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, Lenses=lenses, NonAwsRegions=nonAwsRegions, ArchitecturalDesign=architecturalDesign, IndustryType=industryType, Industry=industry, Notes=notes, AccountIds=accountIds ) except waclient.exceptions.ConflictException as e: workloadId,workloadARN = FindWorkload(waclient,workloadName) logger.error(\u0026#34;ERROR - The workload name %s already exists as workloadId %s\u0026#34; % (workloadName, workloadId)) return workloadId, workloadARN except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) workloadId = response[\u0026#39;WorkloadId\u0026#39;] workloadARN = response[\u0026#39;WorkloadArn\u0026#39;] return workloadId, workloadARN def FindWorkload( waclient, workloadName ): # Finding your WorkloadId try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] workloadArn = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadArn\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workloadId, workloadArn def DeleteWorkload( waclient, workloadId ): # Delete the WorkloadId try: response=waclient.delete_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def GetWorkload( waclient, workloadId ): # Get the WorkloadId try: response=waclient.get_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) exit() # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Workload\u0026#39;], cls=DateTimeEncoder)) workload = response[\u0026#39;Workload\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workload def listLens( waclient ): # List all lenses currently available try: response=waclient.list_lenses() except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) lenses = jmespath.search(\u0026#34;LensSummaries[*].LensAlias\u0026#34;, response) return lenses def getCurrentLensVersion( waclient, lensAlias ): # List all lenses currently available try: response=waclient.list_lenses() except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) searchString = \u0026#34;LensSummaries[?LensAlias==`\u0026#34;+lensAlias+\u0026#34;`].LensVersion\u0026#34; lenses = jmespath.search(searchString, response) return lenses[0] def findAllQuestionId( waclient, workloadId, lensAlias ): answers = [] # Due to a bug in some lenses, I have to iterate over each pillar in order to # retrieve the correct results. for pillar in PILLAR_PARSE_MAP: logger.debug(\u0026#34;Grabbing answers for %s %s\u0026#34; % (lensAlias, pillar)) # Find a questionID using the questionTitle try: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillar ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) while \u0026#34;NextToken\u0026#34; in response: try: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,PillarId=pillar,NextToken=response[\u0026#34;NextToken\u0026#34;]) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) return answers def getQuestionDetails( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) qDescription = jmespath.search(\u0026#34;Answer.QuestionDescription\u0026#34;, response) qImprovementPlanUrl = jmespath.search(\u0026#34;Answer.ImprovementPlanUrl\u0026#34;, response) qHelpfulResourceUrl = jmespath.search(\u0026#34;Answer.HelpfulResourceUrl\u0026#34;, response) qNotes = jmespath.search(\u0026#34;Answer.Notes\u0026#34;, response) return qDescription, qImprovementPlanUrl, qHelpfulResourceUrl, qNotes def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): # Update a answer to a question try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def getImprovementPlanItems( waclient, workloadId, lensAlias, QuestionId, PillarId, ImprovementPlanUrl, ChoiceList ): # This will parse the IP Items to gather the links we need response = {} htmlString = \u0026#34;\u0026#34; # unanswered = getUnansweredForQuestion(waclient,workloadId,\u0026#39;wellarchitected\u0026#39;,QuestionId) urlresponse = urllib.request.urlopen(ImprovementPlanUrl) htmlBytes = urlresponse.read() htmlStr = htmlBytes.decode(\u0026#34;utf8\u0026#34;) htmlSplit = htmlStr.split(\u0026#39;\\n\u0026#39;) ipHTMLList = {} for line in htmlSplit: for uq in ChoiceList: if uq in line: parsed = BeautifulSoup(line,features=\u0026#34;html.parser\u0026#34;) ipHTMLList.update({uq: str(parsed.a[\u0026#39;href\u0026#39;])}) return ipHTMLList def getImprovementPlanHTMLDescription( ImprovementPlanUrl, PillarId ): logger.debug(\u0026#34;ImprovementPlanUrl: %s for pillar %s \u0026#34; % (ImprovementPlanUrl,PILLAR_PARSE_MAP[PillarId])) stepRaw = ImprovementPlanUrl.rsplit(\u0026#39;#\u0026#39;)[1] # Grab the number of the step we are referencing # This will work as long as their are less than 99 steps. if len(stepRaw) \u0026lt;= 5: stepNumber = stepRaw[-1] else: stepNumber = stepRaw[-2] #Generate the string for the step number firstItem = \u0026#34;step\u0026#34;+stepNumber secondItem = (\u0026#34;step\u0026#34;+str((int(stepNumber)+1))) logger.debug (\u0026#34;Going from %s to %s\u0026#34; % (firstItem, secondItem)) urlresponse = urllib.request.urlopen(ImprovementPlanUrl) htmlBytes = urlresponse.read() htmlStr = htmlBytes.decode(\u0026#34;utf8\u0026#34;) htmlSplit = htmlStr.split(\u0026#39;\\n\u0026#39;) foundit = 0 ipString = \u0026#34;\u0026#34; questionIdText = \u0026#34;\u0026#34; for i in htmlSplit: if PILLAR_PARSE_MAP[PillarId] in i: bsparse = BeautifulSoup(i,features=\u0026#34;html.parser\u0026#34;) questionIdText = str(bsparse.text).split(\u0026#39;:\u0026#39;)[0].strip() if (secondItem in i) or (\u0026#34;\u0026lt;/div\u0026gt;\u0026#34; in i): foundit = 0 if firstItem in i: foundit = 1 ipString+=i elif foundit: ipString+=i prettyHTML = BeautifulSoup(ipString,features=\u0026#34;html.parser\u0026#34;) # Need to remove all of the \u0026#34;local glossary links\u0026#34; since they point to relative paths for a in prettyHTML.findAll(\u0026#39;a\u0026#39;, \u0026#39;glossref\u0026#39;): a.replaceWithChildren() return prettyHTML, questionIdText def lensTabCreation( WACLIENT, workloadId, lens, workbook, allQuestionsForLens, workloadName=\u0026#34;\u0026#34;, AWSAccountId=\u0026#34;\u0026#34;, workloadDescription=\u0026#34;\u0026#34; ): # Setup some formatting for the workbook bold = workbook.add_format({\u0026#39;bold\u0026#39;: True}) bold_border = workbook.add_format({ \u0026#39;border\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;text_wrap\u0026#39;: True }) bold_border_bold = workbook.add_format({ \u0026#39;border\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;text_wrap\u0026#39;: True, \u0026#39;font_size\u0026#39;: 20, \u0026#39;bold\u0026#39;: True }) heading = workbook.add_format({ \u0026#39;font_size\u0026#39;: 24, \u0026#39;bold\u0026#39;: True }) lineA = workbook.add_format({ \u0026#39;border\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;bg_color\u0026#39;: \u0026#39;#E0EBF6\u0026#39;, \u0026#39;align\u0026#39;: \u0026#39;top\u0026#39;, \u0026#39;text_wrap\u0026#39;: True }) lineB = workbook.add_format({ \u0026#39;border\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;bg_color\u0026#39;: \u0026#39;#E4EFDC\u0026#39;, \u0026#39;align\u0026#39;: \u0026#39;top\u0026#39;, \u0026#39;text_wrap\u0026#39;: True }) lineAnoborder = workbook.add_format({ \u0026#39;border\u0026#39;: 0, \u0026#39;top\u0026#39;: 1, \u0026#39;left\u0026#39;: 1, \u0026#39;right\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;bg_color\u0026#39;: \u0026#39;#E0EBF6\u0026#39;, \u0026#39;align\u0026#39;: \u0026#39;top\u0026#39;, \u0026#39;text_wrap\u0026#39;: True }) lineBnoborder = workbook.add_format({ \u0026#39;border\u0026#39;: 0, \u0026#39;top\u0026#39;: 1, \u0026#39;left\u0026#39;: 1, \u0026#39;right\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;bg_color\u0026#39;: \u0026#39;#E4EFDC\u0026#39;, \u0026#39;align\u0026#39;: \u0026#39;top\u0026#39;, \u0026#39;text_wrap\u0026#39;: True }) lineAhidden = workbook.add_format({ \u0026#39;border\u0026#39;: 0, \u0026#39;left\u0026#39;: 1, \u0026#39;right\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;bg_color\u0026#39;: \u0026#39;#E0EBF6\u0026#39;, \u0026#39;align\u0026#39;: \u0026#39;top\u0026#39;, \u0026#39;text_wrap\u0026#39;: False, \u0026#39;indent\u0026#39;: 100 }) lineBhidden = workbook.add_format({ \u0026#39;border\u0026#39;: 0, \u0026#39;left\u0026#39;: 1, \u0026#39;right\u0026#39;: 1, \u0026#39;border_color\u0026#39;: \u0026#39;black\u0026#39;, \u0026#39;bg_color\u0026#39;: \u0026#39;#E4EFDC\u0026#39;, \u0026#39;align\u0026#39;: \u0026#39;top\u0026#39;, \u0026#39;text_wrap\u0026#39;: False, \u0026#39;indent\u0026#39;: 100 }) sub_heading = workbook.add_format() sub_heading.set_font_size(20) sub_heading.set_bold(True) small_font = workbook.add_format() small_font.set_font_size(9) # Get the current version of Lens logger.debug(\u0026#34;Getting lens version for \u0026#39;\u0026#34;+lens+\u0026#34;\u0026#39;\u0026#34;) versionString = getCurrentLensVersion(WACLIENT,lens) logger.debug(\u0026#34;Adding worksheet using version \u0026#34;+versionString) lensName = lens[0:18] worksheet = workbook.add_worksheet((lensName+\u0026#39; v\u0026#39;+versionString)) # Print in landscape worksheet.set_landscape() # Set to 8.5x11 paper size worksheet.set_paper(1) # Set the column widths worksheet.set_column(\u0026#39;A:A\u0026#39;, 11) worksheet.set_column(\u0026#39;B:B\u0026#39;, 32) worksheet.set_column(\u0026#39;C:C\u0026#39;, 56) worksheet.set_column(\u0026#39;D:D\u0026#39;, 29) worksheet.set_column(\u0026#39;E:E\u0026#39;, 57) worksheet.set_column(\u0026#39;F:F\u0026#39;, 18) worksheet.set_column(\u0026#39;G:G\u0026#39;, 70) # Top of sheet worksheet.merge_range(\u0026#39;A1:G1\u0026#39;, \u0026#39;Workload Overview\u0026#39;, heading) worksheet.merge_range(\u0026#39;A3:B3\u0026#39;, \u0026#39;Workload Name\u0026#39;, bold_border_bold) worksheet.merge_range(\u0026#39;A4:B4\u0026#39;, \u0026#39;AWS Account ID\u0026#39;, bold_border_bold) worksheet.merge_range(\u0026#39;A5:B5\u0026#39;, \u0026#39;Workload Description\u0026#39;, bold_border_bold) # If we are using an existing workload, then display the Name, ID, and Description at the top # or else just make it blank if WORKLOADID: worksheet.write(\u0026#39;C3\u0026#39;, workloadName, bold_border) accountIdParsed = AWSAccountId.split(\u0026#39;:\u0026#39;)[4] worksheet.write(\u0026#39;C4\u0026#39;, accountIdParsed, bold_border) worksheet.write(\u0026#39;C5\u0026#39;, workloadDescription, bold_border) else: worksheet.write(\u0026#39;C3\u0026#39;, \u0026#39;\u0026#39;, bold_border) worksheet.write(\u0026#39;C4\u0026#39;, \u0026#39;\u0026#39;, bold_border) worksheet.write(\u0026#39;C5\u0026#39;, \u0026#39;\u0026#39;, bold_border) worksheet.write(\u0026#39;D3\u0026#39;, \u0026#39;Enter the name of system\u0026#39;, small_font) worksheet.write(\u0026#39;D4\u0026#39;, \u0026#39;Enter 12-degit AWS account ID\u0026#39;, small_font) worksheet.write(\u0026#39;D5\u0026#39;, \u0026#39;Briefly describe system architecture and workload, flow etc.\u0026#39;, small_font) # Subheadings for columns worksheet.write(\u0026#39;A8\u0026#39;, \u0026#39;Pillar\u0026#39;, sub_heading) worksheet.write(\u0026#39;B8\u0026#39;, \u0026#39;Question\u0026#39;, sub_heading) worksheet.write(\u0026#39;C8\u0026#39;, \u0026#39;Explanation\u0026#39;, sub_heading) worksheet.write(\u0026#39;D8\u0026#39;, \u0026#39;Choice (Best Practice)\u0026#39;, sub_heading) worksheet.write(\u0026#39;E8\u0026#39;, \u0026#39;Detail\u0026#39;, sub_heading) worksheet.write(\u0026#39;F8\u0026#39;, \u0026#39;Response\u0026#39;, sub_heading) worksheet.write(\u0026#39;G8\u0026#39;, \u0026#39;Notes (optional)\u0026#39;, sub_heading) # Freeze the top of the sheet worksheet.freeze_panes(8,0) # AutoFilter on the first two columns worksheet.autofilter(\u0026#39;A8:B8\u0026#39;) # Make it easier to print worksheet.repeat_rows(1, 8) worksheet.fit_to_pages(1, 99) # Starting point for pillar questions cellPosition = 8 # Starting cell look with lineA. Will switch back and forth myCell = lineA myCellhidden = lineAhidden myCellnoborder = lineAnoborder for pillar in PILLAR_PARSE_MAP: # This is the question number for each pillar (ex: OPS1, OPS2, etc) qNum = 1 # The query will return all questions for a lens and pillar jmesquery = \u0026#34;[?PillarId==\u0026#39;\u0026#34;+pillar+\u0026#34;\u0026#39;]\u0026#34; allQuestionsForPillar = jmespath.search(jmesquery, allQuestionsForLens) # For each of the possible answers, parse them and put into the Worksheet for answers in allQuestionsForPillar: # List all best practices questionTitle = PILLAR_PARSE_MAP[answers[\u0026#39;PillarId\u0026#39;]]+str(qNum)+\u0026#34; - \u0026#34;+answers[\u0026#39;QuestionTitle\u0026#39;] qDescription, qImprovementPlanUrl, qHelpfulResourceUrl, qNotes = getQuestionDetails(WACLIENT,workloadId,lens,answers[\u0026#39;QuestionId\u0026#39;]) # Some of the questions have extra whitespaces and I need to remove those to fit into the cell qDescription = qDescription.replace(\u0026#39;\\n \u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;\\t\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) qDescription = qDescription.rstrip() qDescription = qDescription.strip() logger.debug(\u0026#34;Working on \u0026#39;\u0026#34;+questionTitle+\u0026#34;\u0026#39;\u0026#34;) logger.debug(\u0026#34;It has answers of: \u0026#34;+json.dumps(answers[\u0026#39;SelectedChoices\u0026#39;])) cellID = cellPosition + 1 # If the question has been answered (which we do for the TEMP workload) we grab the URL and parse for the HTML content if qImprovementPlanUrl: jmesquery = \u0026#34;[?QuestionId==\u0026#39;\u0026#34;+answers[\u0026#39;QuestionId\u0026#39;]+\u0026#34;\u0026#39;].Choices[].ChoiceId\u0026#34; choiceList = jmespath.search(jmesquery, allQuestionsForLens) ipList = getImprovementPlanItems(WACLIENT,workloadId,lens,answers[\u0026#39;QuestionId\u0026#39;],answers[\u0026#39;PillarId\u0026#39;],qImprovementPlanUrl,choiceList) else: ipList = [] startingCellID=cellID # If its the first time through this particular pillar question: # I want to only write the name once, but I need to fill in # each cell with the same data so the autosort works properly # (else it will only show the first best practice) firstTimePillar=True for choices in answers[\u0026#39;Choices\u0026#39;]: # Write the pillar name and question in every cell for autosort, but only show the first one cell = \u0026#39;A\u0026#39;+str(cellID) if firstTimePillar: worksheet.write(cell, PILLAR_PROPER_NAME_MAP[pillar], myCellnoborder) cell = \u0026#39;B\u0026#39;+str(cellID) worksheet.write(cell, questionTitle, myCellnoborder) firstTimePillar=False else: worksheet.write(cell, PILLAR_PROPER_NAME_MAP[pillar], myCellhidden) cell = \u0026#39;B\u0026#39;+str(cellID) worksheet.write(cell, questionTitle, myCellhidden) # Start writing each of the BP\u0026#39;s, details, etc cell = \u0026#39;D\u0026#39;+str(cellID) Title = choices[\u0026#39;Title\u0026#39;].replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;\\t\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) if any(choices[\u0026#39;ChoiceId\u0026#39;] in d for d in ipList): worksheet.write_url(cell, ipList[choices[\u0026#39;ChoiceId\u0026#39;]], myCell, string=Title) ipItemHTML, questionIdText = getImprovementPlanHTMLDescription(ipList[choices[\u0026#39;ChoiceId\u0026#39;]],answers[\u0026#39;PillarId\u0026#39;]) htmlString = ipItemHTML.text htmlString = htmlString.replace(\u0026#39;\\n \u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;\\t\u0026#39;, \u0026#39;\u0026#39;).strip().rstrip() # print(htmlString) worksheet.write_comment(cell, htmlString, {\u0026#39;author\u0026#39;: \u0026#39;Improvement Plan\u0026#39;}) else: worksheet.write(cell,Title,myCell) # Add all Details for each best practice/choice cell = \u0026#39;E\u0026#39;+str(cellID) # Remove all of the extra spaces in the description field Description = choices[\u0026#39;Description\u0026#39;].replace(\u0026#39;\\n \u0026#39;,\u0026#39;\u0026#39;) Description = Description.replace(\u0026#39;\\n \u0026#39;,\u0026#39;\u0026#39;) Description = Description.replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;\\t\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) Description = Description.rstrip() Description = Description.strip() worksheet.write(cell, Description ,myCell) # If this is an existing workload, we will show SELECTED if the have it checked # I would love to use a XLSX checkbox, but this library doesn\u0026#39;t support it cell = \u0026#39;F\u0026#39;+str(cellID) responseText = \u0026#34;\u0026#34; if choices[\u0026#39;ChoiceId\u0026#39;] in answers[\u0026#39;SelectedChoices\u0026#39;]: responseText = \u0026#34;SELECTED\u0026#34; else: responseText = \u0026#34;\u0026#34; worksheet.write(cell, responseText ,myCell) cellID+=1 # We are out of the choice/detail/response loop, so know how many rows were consumed # and we can create the explanation and notes field to span all of them # Explanantion field cellMerge = \u0026#39;C\u0026#39;+str(startingCellID)+\u0026#39;:C\u0026#39;+str(cellID-1) worksheet.merge_range(cellMerge, qDescription,myCell) # Notes field cellMerge = \u0026#39;G\u0026#39;+str(startingCellID)+\u0026#39;:G\u0026#39;+str(cellID-1) if WORKLOADID: worksheet.merge_range(cellMerge, qNotes, myCell) else: worksheet.merge_range(cellMerge, \u0026#34;\u0026#34;, myCell) cellID-=1 # Increase the question number qNum += 1 # Reset the starting cellPosition to the last cellID cellPosition = cellID # Reset the cell formatting to alternate between the two colors if myCell == lineA: myCell = lineB myCellhidden = lineBhidden myCellnoborder = lineBnoborder else: myCell = lineA myCellhidden = lineAhidden myCellnoborder = lineAnoborder def main(): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() logger.info(\u0026#34;Script version %s\u0026#34; % __version__) logger.info(\u0026#34;Starting Boto %s Session\u0026#34; % boto3.__version__) # Create a new boto3 session SESSION1 = boto3.session.Session(profile_name=PROFILE) # Initiate the well-architected session using the region defined above WACLIENT = SESSION1.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) # If this is an existing workload, we need to query for the various workload properties if WORKLOADID: logger.info(\u0026#34;User specified workload id of %s\u0026#34; % WORKLOADID) workloadJson = GetWorkload(WACLIENT,WORKLOADID) LENSES = workloadJson[\u0026#39;Lenses\u0026#39;] logger.info(\u0026#34;Lenses for %s: %s\u0026#34; % (WORKLOADID, json.dumps(LENSES))) WORKLOADNAME = workloadJson[\u0026#39;WorkloadName\u0026#39;] DESCRIPTION = workloadJson[\u0026#39;Description\u0026#39;] REVIEWOWNER = workloadJson[\u0026#39;ReviewOwner\u0026#39;] ENVIRONMENT= workloadJson[\u0026#39;Environment\u0026#39;] AWSREGIONS = workloadJson[\u0026#39;AwsRegions\u0026#39;] workloadId = WORKLOADID workloadARN = workloadJson[\u0026#39;WorkloadArn\u0026#39;] else: # In order to gather all of the questions, you must create a TEMP Workload logger.info(\u0026#34;No workload ID specified, we will create a TEMP workload\u0026#34;) # Grab all lenses that are currently available LENSES = listLens(WACLIENT) logger.info(\u0026#34;Lenses available: \u0026#34;+json.dumps(LENSES)) # Set the needed workload variables before we create it WORKLOADNAME = \u0026#39;TEMP DO NOT USE WORKLOAD\u0026#39; DESCRIPTION = \u0026#39;TEMP DO NOT USE WORKLOAD\u0026#39; REVIEWOWNER = \u0026#39;WA Python Script\u0026#39; ENVIRONMENT= \u0026#39;PRODUCTION\u0026#39; AWSREGIONS = [REGION_NAME] # Creating the TEMP workload logger.info(\u0026#34;Creating a new workload to gather questions and answers\u0026#34;) workloadId, workloadARN = CreateNewWorkload(WACLIENT,WORKLOADNAME,DESCRIPTION,REVIEWOWNER,ENVIRONMENT,AWSREGIONS,LENSES,\u0026#34;[]\u0026#34;,\u0026#34;[]\u0026#34;) # Create an new xlsx file and add a worksheet. logger.info(\u0026#34;Creating xlsx file \u0026#39;\u0026#34;+FILENAME+\u0026#34;\u0026#39;\u0026#34;) workbook = xlsxwriter.Workbook(FILENAME) workbook.set_size(2800, 1600) # Simple hack to get Wellarchitected base framework first (reverse sort) # This will no longer work if we ever have a lens that starts with WB*, X, Y, or Z :) LENSES.sort(reverse=True) # Iterate over each lens that we either have added or is in the workload for lens in LENSES: # Grab all questions for a particular lens allQuestions = findAllQuestionId(WACLIENT,workloadId,lens) if WORKLOADID: # If this is an existing workload, just go ahead and create the Tab and cells logger.debug(\u0026#34;Not answering questions for existing workload\u0026#34;) lensTabCreation(WACLIENT,workloadId,lens,workbook,allQuestions,WORKLOADNAME,workloadARN,DESCRIPTION) else: # If this is the TEMP workload, we need to first gather all of the questionIDs possible jmesquery = \u0026#34;[*].{QuestionId: QuestionId, PillarId: PillarId, Choices: Choices[].ChoiceId}\u0026#34; allQuestionIds = jmespath.search(jmesquery, allQuestions) # Next we answer all of the questions across all lenses in the TEMP workload for question in allQuestionIds: logger.debug(\u0026#34;Answering question %s in the %s lens\u0026#34; % (question[\u0026#39;QuestionId\u0026#39;], lens)) updateAnswersForQuestion(WACLIENT,workloadId,lens,question[\u0026#39;QuestionId\u0026#39;],question[\u0026#39;Choices\u0026#39;],\u0026#39;TEMP WORKLOAD - Added by export script\u0026#39;) # Once the questions have been answered, we go ahead and create the tab for each lensTabCreation(WACLIENT,workloadId,lens,workbook,allQuestions) # Close out the workbook file logger.info(\u0026#34;Closing Workbook File\u0026#34;) workbook.close() # If this is TEMP workload, we may remove it if it has not been set to keep if not WORKLOADID: if not KEEPTEMP: logger.info(\u0026#34;Removing TEMP Workload\u0026#34;) DeleteWorkload(WACLIENT, workloadId) logger.info(\u0026#34;Done\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/generate_custom_html_wafr_report/","title":"Generate a custom WellArchitected Framework HTML Report","tags":[],"description":"","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction The purpose of this lab is to teach you how to use the AWS SDK for Python (Boto3) to copy create a Well-Architected Framework report in HTML. The python application in this lab will also show you how to incorporate the Improvement Plan text relevant for each of the unchecked best practices.\nPrerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Python 3.9+ AWS SDK for Python (Boto3) installed Costs: There are no costs for copying or creating new WellArchitected Reviews Steps: Configure Environment Python Code Script usage examples X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/copy_wa_review_between_accounts/2_python_code/","title":"Python Code","tags":[],"description":"","content":"duplicateWAFR.py The purpose of this python script is to duplicate a Well-Architected Workload review. This can be done within the same account but to a new AWS Region, or it can be done to a different AWS account and/or region. During the copy process, it will generate a new workload in the target account with the same Workload Name, but the workloadId will be unique. If the target workload already exists, the script will prompt the user if they wish to refresh the data from the source workload.\nThis utility was created using the the AWS SDK for Python (Boto3) . This file assumes you have already setup your AWS credential file, and uses the default profile for all interactions.\nThere is error checking for most of the various API calls, but the code should not be considered production ready. Please review before implementing in your environment.\nParameters usage: duplicateWAFR.py [-h] [--fromaccount FROMACCOUNT] [--toaccount TOACCOUNT] --workloadid WORKLOADID [--fromregion FROMREGION] [--toregion TOREGION] optional arguments: -h, --help show this help message and exit --fromaccount FROMACCOUNT AWS CLI Profile Name or will use the default session for the shell --toaccount TOACCOUNT AWS CLI Profile Name or will use the default session for the shell --workloadid WORKLOADID WorkloadID. Example: 1e5d148ab9744e98343cc9c677a34682 --fromregion FROMREGION From Region Name. Example: us-east-1 --toregion TOREGION To Region Name. Example: us-east-2 Limitations The code has been tested against multiple accounts, but it does not include full error handling for all situations. Please limit the use to interactive sessions at this time. Because this is simply copying the data to a new WA review in the target account, the target workloadId will be different. This will not copy any of the Workload Sharing features to the target workload. If a Workload exists in the target account with the same name, the script will just attempt to update the review with the data from the source, not create a new one. Python Code Link to download the code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 #!/usr/bin/env python3 # This is a tool to copy a WA Framework Review from one account to another # It can also be used to copy between regions for the same account # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ import botocore import boto3 import json import datetime import logging import jmespath import base64 import argparse from pkg_resources import packaging __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; # Setup Logging logging.basicConfig( level=logging.DEBUG, format=\u0026#39;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, ) logger = logging.getLogger() logging.getLogger(\u0026#39;boto3\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;botocore\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;s3transfer\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;urllib3\u0026#39;).setLevel(logging.CRITICAL) PARSER = argparse.ArgumentParser() PARSER.add_argument(\u0026#39;--fromaccount\u0026#39;, required=False, default=\u0026#34;\u0026#34;, help=\u0026#39;AWS CLI Profile Name or will use the default session for the shell\u0026#39;) PARSER.add_argument(\u0026#39;--toaccount\u0026#39;, required=False, default=\u0026#34;\u0026#34;, help=\u0026#39;AWS CLI Profile Name or will use the default session for the shell\u0026#39;) PARSER.add_argument(\u0026#39;--workloadid\u0026#39;, required=True, help=\u0026#39;WorkloadID. Example: 1e5d148ab9744e98343cc9c677a34682\u0026#39;) PARSER.add_argument(\u0026#39;--fromregion\u0026#39;, required=False, default=\u0026#34;us-east-1\u0026#34;, help=\u0026#39;From Region Name. Example: us-east-1\u0026#39;) PARSER.add_argument(\u0026#39;--toregion\u0026#39;, required=False, default=\u0026#34;us-east-1\u0026#34;, help=\u0026#39;To Region Name. Example: us-east-2\u0026#39;) ARGUMENTS = PARSER.parse_args() REGION_NAME = ARGUMENTS.fromregion TO_REGION_NAME = ARGUMENTS.toregion FROM_ACCOUNT = ARGUMENTS.fromaccount TO_ACCOUNT = ARGUMENTS.toaccount FROM_WORKLOADID = ARGUMENTS.workloadid # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def CreateNewWorkload( waclient, workloadName, description, reviewOwner, environment, awsRegions, lenses, tags, pillarPriorities, notes=\u0026#34;\u0026#34;, nonAwsRegions=[], architecturalDesign=\u0026#39;\u0026#39;, industryType=\u0026#39;\u0026#39;, industry=\u0026#39;\u0026#39;, accountIds=[] ): # Create your workload try: response=waclient.create_workload( WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, Lenses=lenses, NonAwsRegions=nonAwsRegions, PillarPriorities=pillarPriorities, ArchitecturalDesign=architecturalDesign, IndustryType=industryType, Industry=industry, Notes=notes, AccountIds=accountIds ) except waclient.exceptions.ConflictException as e: workloadId,workloadARN = FindWorkload(waclient,workloadName) logger.error(\u0026#34;ERROR - The workload name %s already exists as workloadId %s\u0026#34; % (workloadName, workloadId)) userAnswer=input(\u0026#34;Do You Want To Overwrite workload %s? [y/n]\u0026#34; % workloadId) if userAnswer == \u0026#34;y\u0026#34;: logger.info(\u0026#34;Overwriting existing workload\u0026#34;) UpdateWorkload(waclient,workloadId,workloadARN, workloadName,description,reviewOwner,environment,awsRegions,lenses,tags) else: logger.error(\u0026#34;Exiting due to duplicate workload and user states they do not want to continue.\u0026#34;) exit(1) return workloadId, workloadARN except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) workloadId = response[\u0026#39;WorkloadId\u0026#39;] workloadARN = response[\u0026#39;WorkloadArn\u0026#39;] return workloadId, workloadARN def UpdateWorkload( waclient, workloadId, workloadARN, workloadName, description, reviewOwner, environment, awsRegions, lenses, tags ): logger.info(\u0026#34;Updating workload properties\u0026#34;) # Create your workload try: waclient.update_workload( WorkloadId=workloadId, WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # Should add updates for the lenses? # Should add the tags as well if tags: logger.info(\u0026#34;Updating workload tags\u0026#34;) try: waclient.tag_resource(WorkloadArn=workloadARN,Tags=tags) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) else: logger.info(\u0026#34;Found blank tag set, removing any I find\u0026#34;) try: tagresponse = waclient.list_tags_for_resource(WorkloadArn=workloadARN) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) tagkeys = list(tagresponse[\u0026#39;Tags\u0026#39;]) if tagkeys: try: waclient.untag_resource(WorkloadArn=workloadARN,TagKeys=tagkeys) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) else: logger.info(\u0026#34;TO Workload has blank keys as well, no need to update\u0026#34;) def FindWorkload( waclient, workloadName ): # Finding your WorkloadId try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] workloadArn = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadArn\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workloadId, workloadArn def DeleteWorkload( waclient, workloadId ): # Delete the WorkloadId try: response=waclient.delete_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def GetWorkload( waclient, workloadId ): # Get the WorkloadId try: response=waclient.get_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) exit() # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Workload\u0026#39;], cls=DateTimeEncoder)) workload = response[\u0026#39;Workload\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workload def disassociateLens( waclient, workloadId, lens ): # Disassociate the lens from the WorkloadId try: response=waclient.disassociate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def associateLens( waclient, workloadId, lens ): # Associate the lens from the WorkloadId try: response=waclient.associate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def listLens( waclient ): # List all lenses currently available try: response=waclient.list_lenses() except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) lenses = jmespath.search(\u0026#34;LensSummaries[*].LensAlias\u0026#34;, response) return lenses def findQuestionId( waclient, workloadId, lensAlias, pillarId, questionTitle ): # Find a questionID using the questionTitle try: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillarId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,PillarId=pillarId,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) jmesquery = \u0026#34;[?starts_with(QuestionTitle, `\u0026#34;+questionTitle+\u0026#34;`) == `true`].QuestionId\u0026#34; questionId = jmespath.search(jmesquery, answers) return questionId[0] def findChoiceId( waclient, workloadId, lensAlias, questionId, choiceTitle, ): # Find a choiceId using the choiceTitle try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) jmesquery = \u0026#34;Answer.Choices[?starts_with(Title, `\u0026#34;+choiceTitle+\u0026#34;`) == `true`].ChoiceId\u0026#34; choiceId = jmespath.search(jmesquery, response) return choiceId[0] def getAnswersForQuestion( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) # print(answers) return answers def getNotesForQuestion( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) # jmesquery = \u0026#34;Answer.Notes\u0026#34; # answers = jmespath.search(jmesquery, response) response = response[\u0026#39;Answer\u0026#39;] answers = response[\u0026#39;Notes\u0026#39;] if \u0026#34;Notes\u0026#34; in response else \u0026#34;\u0026#34; # print(answers) return answers def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): # Update a answer to a question try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def listMilestones( waclient, workloadId ): # Find a milestone for a workloadId try: response=waclient.list_milestones( WorkloadId=workloadId, MaxResults=50 # Need to check why I am having to pass this parameter ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;MilestoneSummaries\u0026#39;], cls=DateTimeEncoder)) milestoneNumber = response[\u0026#39;MilestoneSummaries\u0026#39;] return milestoneNumber def createMilestone( waclient, workloadId, milestoneName ): # Create a new milestone with milestoneName try: response=waclient.create_milestone( WorkloadId=workloadId, MilestoneName=milestoneName ) except waclient.exceptions.ConflictException as e: milestones = listMilestones(waclient,workloadId) jmesquery = \u0026#34;[?starts_with(MilestoneName,`\u0026#34;+milestoneName+\u0026#34;`) == `true`].MilestoneNumber\u0026#34; milestoneNumber = jmespath.search(jmesquery,milestones) logger.error(\u0026#34;ERROR - The milestone name %s already exists as milestone %s\u0026#34; % (milestoneName, milestoneNumber)) return milestoneNumber[0] except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;MilestoneSummaries\u0026#39;], cls=DateTimeEncoder)) milestoneNumber = response[\u0026#39;MilestoneNumber\u0026#39;] return milestoneNumber def getMilestone( waclient, workloadId, milestoneNumber ): # Use get_milestone to return the milestone structure try: response=waclient.get_milestone( WorkloadId=workloadId, MilestoneNumber=milestoneNumber ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Milestone\u0026#39;], cls=DateTimeEncoder)) milestoneResponse = response[\u0026#39;Milestone\u0026#39;] return milestoneResponse def getMilestoneRiskCounts( waclient, workloadId, milestoneNumber ): # Return just the RiskCount for a particular milestoneNumber milestone = getMilestone(waclient,workloadId,milestoneNumber) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(milestone[\u0026#39;Workload\u0026#39;][\u0026#39;RiskCounts\u0026#39;], cls=DateTimeEncoder)) milestoneRiskCounts = milestone[\u0026#39;Workload\u0026#39;][\u0026#39;RiskCounts\u0026#39;] return milestoneRiskCounts def listAllAnswers( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Get a list of all answers try: if milestoneNumber: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: if milestoneNumber: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,MilestoneNumber=milestoneNumber,NextToken=response[\u0026#34;NextToken\u0026#34;]) else: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(answers, cls=DateTimeEncoder)) return answers def getLensReview( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Use get_lens_review to return the lens review structure try: if milestoneNumber: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReview\u0026#39;], cls=DateTimeEncoder)) lensReview = response[\u0026#39;LensReview\u0026#39;] return lensReview def getLensReviewPDFReport( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Use get_lens_review_report to return the lens review PDF in base64 structure try: if milestoneNumber: response=waclient.get_lens_review_report( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.get_lens_review_report( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;], cls=DateTimeEncoder)) lensReviewPDF = response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;] return lensReviewPDF def main(): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 install boto3 --upgrade --user)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() # STEP 1 - Configure environment logger.info(\u0026#34;Starting Boto %s Session\u0026#34; % boto3.__version__) # Create a new boto3 session if FROM_ACCOUNT: SESSION1 = boto3.session.Session(profile_name=FROM_ACCOUNT) else: SESSION1 = boto3.session.Session() if TO_ACCOUNT: SESSION2 = boto3.session.Session(profile_name=TO_ACCOUNT) else: SESSION2 = boto3.session.Session() # Initiate the well-architected session using the region defined above WACLIENT = SESSION1.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) WACLIENT_TO = SESSION2.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=TO_REGION_NAME, ) logger.info(\u0026#34;Copy WorkloadID \u0026#39;%s\u0026#39; from \u0026#39;%s:%s\u0026#39; to \u0026#39;%s:%s\u0026#39;\u0026#34; % (FROM_WORKLOADID,REGION_NAME,FROM_ACCOUNT,TO_REGION_NAME,TO_ACCOUNT)) # Ignoring milestones for now, will add later if interested workloadId = FROM_WORKLOADID # Find out what lenses apply to the from workloadid workloadJson = GetWorkload(WACLIENT,workloadId) WorkloadARN = workloadJson[\u0026#39;WorkloadArn\u0026#39;] # For each of the optional variables, lets check and see if we have them first: Notes = workloadJson[\u0026#39;Notes\u0026#39;] if \u0026#34;Notes\u0026#34; in workloadJson else \u0026#34;\u0026#34; nonAwsRegions = workloadJson[\u0026#39;NonAwsRegions\u0026#39;] if \u0026#34;NonAwsRegions\u0026#34; in workloadJson else [] architecturalDesign = workloadJson[\u0026#39;ArchitecturalDesign\u0026#39;] if \u0026#34;ArchitecturalDesign\u0026#34; in workloadJson else \u0026#34;\u0026#34; industryType = workloadJson[\u0026#39;IndustryType\u0026#39;] if \u0026#34;IndustryType\u0026#34; in workloadJson else \u0026#34;\u0026#34; industry = workloadJson[\u0026#39;Industry\u0026#39;] if \u0026#34;Industry\u0026#34; in workloadJson else \u0026#34;\u0026#34; accountIds = workloadJson[\u0026#39;AccountIds\u0026#39;] if \u0026#34;AccountIds\u0026#34; in workloadJson else [] tagresponse = WACLIENT.list_tags_for_resource(WorkloadArn=WorkloadARN) tags = tagresponse[\u0026#39;Tags\u0026#39;] if \u0026#34;Tags\u0026#34; in tagresponse else [] # Create the new workload to copy into toWorkloadId,toWorkloadARN = CreateNewWorkload(WACLIENT_TO, (workloadJson[\u0026#39;WorkloadName\u0026#39;]), workloadJson[\u0026#39;Description\u0026#39;], workloadJson[\u0026#39;ReviewOwner\u0026#39;], workloadJson[\u0026#39;Environment\u0026#39;], workloadJson[\u0026#39;AwsRegions\u0026#39;], workloadJson[\u0026#39;Lenses\u0026#39;], tags, workloadJson[\u0026#39;PillarPriorities\u0026#39;], Notes, nonAwsRegions, architecturalDesign, industryType, industry, accountIds ) logger.info(\u0026#34;New workload id: %s (%s)\u0026#34; % (toWorkloadId,toWorkloadARN)) # Iterate over each lens and copy all of the answers for lens in workloadJson[\u0026#39;Lenses\u0026#39;]: logger.info(\u0026#34;Retrieving all answers for lens %s\u0026#34; % lens) answers = listAllAnswers(WACLIENT,workloadId,lens) # Ensure the lens is attached to the new workload associateLens(WACLIENT_TO,toWorkloadId,[lens]) logger.info(\u0026#34;Copying answers into new workload for lens %s\u0026#34; % lens) for answerCopy in answers: notesField = \u0026#39;\u0026#39; notesField = getNotesForQuestion(WACLIENT,workloadId,lens,answerCopy[\u0026#39;QuestionId\u0026#39;]) updateAnswersForQuestion(WACLIENT_TO,toWorkloadId,lens,answerCopy[\u0026#39;QuestionId\u0026#39;],answerCopy[\u0026#39;SelectedChoices\u0026#39;],notesField) logger.info(\u0026#34;Copy complete - exiting\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/generate_custom_html_wafr_report/2_python_code/","title":"Python Code","tags":[],"description":"","content":"generateWAFReport.py The purpose of this python script is to generate a HTML file that displays some basic Well-Architected Workload information as well as each best practice that was unchecked for any question that has been answered. As part of the report generation, it will also incorporate the specific improvement plan content and display it in-line with each unchecked best practice.\nThis utility was created using the the AWS SDK for Python (Boto3) . This file assumes you have already setup your AWS credential file, and uses the default profile for all interactions.\nThere is error checking for most of the various API calls, but the code should not be considered production ready. Please review before implementing in your environment.\nParameters usage: generateWAFReport.py [-h] [--profile PROFILE] --workloadid WORKLOADID [--region REGION] optional arguments: -h, --help show this help message and exit --profile PROFILE AWS CLI Profile Name --workloadid WORKLOADID WorkloadID. Example: 1e5d148ab9744e98343cc9c677a34682 --region REGION From Region Name. Example: us-east-1 Limitations The HTML generated is statically defined in the code and not based on a templating language of any kind. The report will only generate for the base wellarchitected framework, it does not support lenses at this time. Python Code Link to download the code #!/usr/bin/env python3 # This is a simple python app for use with the Well-Architected labs # to generate a report that includes Improvement Plans # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ import botocore import boto3 import json import datetime import logging import jmespath import base64 import argparse import webbrowser import tempfile import urllib.request from pkg_resources import packaging from pathlib import Path from bs4 import BeautifulSoup, NavigableString, Tag __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] __version__ = \u0026#34;0.1\u0026#34; # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; # Setup Logging logging.basicConfig( level=logging.DEBUG, format=\u0026#39;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, ) logger = logging.getLogger() logging.getLogger(\u0026#39;boto3\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;botocore\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;s3transfer\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;urllib3\u0026#39;).setLevel(logging.CRITICAL) PARSER = argparse.ArgumentParser() PARSER.add_argument(\u0026#39;--profile\u0026#39;, required=False, default=\u0026#34;default\u0026#34;, help=\u0026#39;AWS CLI Profile Name\u0026#39;) PARSER.add_argument(\u0026#39;--workloadid\u0026#39;, required=True, help=\u0026#39;WorkloadID. Example: 1e5d148ab9744e98343cc9c677a34682\u0026#39;) PARSER.add_argument(\u0026#39;--region\u0026#39;, required=False, default=\u0026#34;us-east-1\u0026#34;, help=\u0026#39;From Region Name. Example: us-east-1\u0026#39;) PARSER.add_argument(\u0026#39;--debug\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;print debug messages to stderr\u0026#39;) ARGUMENTS = PARSER.parse_args() REGION_NAME = ARGUMENTS.region PROFILE = ARGUMENTS.profile WORKLOADID = ARGUMENTS.workloadid if ARGUMENTS.debug: logger.setLevel(logging.DEBUG) else: logger.setLevel(logging.INFO) # To map our short hand names in the console to the API defined pillars # Example: print(PILLAR_PARSE_MAP[\u0026#39;performance\u0026#39;]) PILLAR_PARSE_MAP = { \u0026#34;operationalExcellence\u0026#34;: \u0026#34;OPS\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;SEC\u0026#34;, \u0026#34;reliability\u0026#34;: \u0026#34;REL\u0026#34;, \u0026#34;performance\u0026#34;: \u0026#34;PERF\u0026#34;, \u0026#34;costOptimization\u0026#34;: \u0026#34;COST\u0026#34; } PILLAR_PROPER_NAME_MAP = { \u0026#34;operationalExcellence\u0026#34;: \u0026#34;Operational Excellence\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;Security\u0026#34;, \u0026#34;reliability\u0026#34;: \u0026#34;Reliability\u0026#34;, \u0026#34;performance\u0026#34;: \u0026#34;Performance Efficiency\u0026#34;, \u0026#34;costOptimization\u0026#34;: \u0026#34;Cost Optimization\u0026#34; } # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def FindWorkload( waclient, workloadName ): # Finding your WorkloadId try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workloadId def getAnswersForQuestion( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) # print(answers) return answers def getUnansweredForQuestion( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) jmesquery = \u0026#34;Answer.Choices[].ChoiceId\u0026#34; possibleAnswers = jmespath.search(jmesquery, response) s = set(answers) diff = [x for x in possibleAnswers if x not in s] # print(answers) return diff def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): # Update a answer to a question try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def getImprovementPlanHTMLDescription( ImprovementPlanUrl, PillarId ): logger.debug(\u0026#34;ImprovementPlanUrl: %s for pillar %s \u0026#34; % (ImprovementPlanUrl,PILLAR_PARSE_MAP[PillarId])) stepRaw = ImprovementPlanUrl.rsplit(\u0026#39;#\u0026#39;)[1] if len(stepRaw) \u0026lt;= 5: stepNumber = stepRaw[-1] else: stepNumber = stepRaw[-2] # print(stepNumber) firstItem = \u0026#34;step\u0026#34;+stepNumber secondItem = (\u0026#34;step\u0026#34;+str((int(stepNumber)+1))) logger.debug (\u0026#34;Going from %s to %s\u0026#34; % (firstItem, secondItem)) urlresponse = urllib.request.urlopen(ImprovementPlanUrl) htmlBytes = urlresponse.read() htmlStr = htmlBytes.decode(\u0026#34;utf8\u0026#34;) htmlSplit = htmlStr.split(\u0026#39;\\n\u0026#39;) foundit = 0 ipString = \u0026#34;\u0026#34; questionIdText = \u0026#34;\u0026#34; for i in htmlSplit: if PILLAR_PARSE_MAP[PillarId] in i: bsparse = BeautifulSoup(i,features=\u0026#34;html.parser\u0026#34;) questionIdText = str(bsparse.text).split(\u0026#39;:\u0026#39;)[0].strip() if (secondItem in i) or (\u0026#34;\u0026lt;/div\u0026gt;\u0026#34; in i): foundit = 0 if firstItem in i: foundit = 1 ipString+=i elif foundit: ipString+=i # print(ipString) prettyHTML = BeautifulSoup(ipString,features=\u0026#34;html.parser\u0026#34;) # We need to remove all of the \u0026#34;local glossary links\u0026#34; since they point to relative paths for a in prettyHTML.findAll(\u0026#39;a\u0026#39;, \u0026#39;glossref\u0026#39;): a.replaceWithChildren() return prettyHTML, questionIdText def getImprovementPlanItems( waclient, workloadId, lensAlias, QuestionId, PillarId, ImprovementPlanUrl ): response = {} htmlString = \u0026#34;\u0026#34; unanswered = getUnansweredForQuestion(waclient,workloadId,\u0026#39;wellarchitected\u0026#39;,QuestionId) # print(\u0026#34;Unanswered: \u0026#34;,json.dumps(unanswered)) urlresponse = urllib.request.urlopen(ImprovementPlanUrl) htmlBytes = urlresponse.read() htmlStr = htmlBytes.decode(\u0026#34;utf8\u0026#34;) htmlSplit = htmlStr.split(\u0026#39;\\n\u0026#39;) # print(\u0026#34; \u0026#34;) # htmlString += \u0026#39;Improvement Plan Items:\u0026lt;br\u0026gt;\u0026#39; # htmlString += \u0026#39;\u0026lt;div id=\u0026#34;detect-investigate-events\u0026#34;\u0026gt;\u0026lt;ul\u0026gt;\u0026#39; ipHTMLList = [] # ipHTMLList.append({\u0026#34;ChoiceId\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;ParsedURL\u0026#34;: \u0026#34;http://test.com/#step1\u0026#34;}) for line in htmlSplit: for uq in unanswered: if uq in line: parsed = BeautifulSoup(line,features=\u0026#34;html.parser\u0026#34;) ipHTMLList.append({\u0026#34;ChoiceID\u0026#34;: uq, \u0026#34;ParsedURL\u0026#34;: str(parsed.a[\u0026#39;href\u0026#39;])}) # htmlString += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026#39; + str(parsed.a[\u0026#39;href\u0026#39;]) + \u0026#39;\u0026lt;/li\u0026gt;\u0026#39; # print(line) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response)) # print(ipHTMLList) # exit() # htmlString += \u0026#34;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026#34; # return htmlString return ipHTMLList def listLensReviewImprovements( waclient, workloadId, lensAlias, pillarId, milestoneNumber=\u0026#34;\u0026#34; ): response = {} try: if milestoneNumber: response=waclient.list_lens_review_improvements( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillarId, MilestoneNumber=milestoneNumber ) else: response=waclient.list_lens_review_improvements( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillarId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;], cls=DateTimeEncoder)) return response[\u0026#39;ImprovementSummaries\u0026#39;] def GetWorkload( waclient, workloadId ): # Get the WorkloadId try: response=waclient.get_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Workload\u0026#39;], cls=DateTimeEncoder)) workload = response[\u0026#39;Workload\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workload def generateHTMLHeader(): htmlPage = \u0026#39;\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt;\u0026lt;title\u0026gt;How do you detect and investigate security events? - AWS Well-Architected Framework\u0026lt;/title\u0026gt;\u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;link rel=\u0026#34;script\u0026#34; href=\u0026#34;./nav_shim.js\u0026#34; /\u0026gt;\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://a0.awsstatic.com/main/css/1/style.css\u0026#34; /\u0026gt;\u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/ico\u0026#34; href=\u0026#34;https://a0.awsstatic.com/main/images/site/fav/favicon.ico\u0026#34; /\u0026gt;\u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; type=\u0026#34;image/ico\u0026#34; href=\u0026#34;https://a0.awsstatic.com/main/images/site/fav/favicon.ico\u0026#34; /\u0026gt;\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;https://docs.aws.amazon.com/css/awsdocs.css?v=20170615\u0026#34;\u0026gt;\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://use.fontawesome.com/releases/v5.4.2/css/all.css\u0026#34; integrity=\u0026#34;sha384-/rXc/GQVaYpyDdyxK+ecHPVYJSN9bmVFBvjA/9eOB+pb3F2w2N6fc5qB9Ew5yIns\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; #pre, #main, #nav, #post { width: 80%; margin-left: 10%; margin-right: 10%; float: inherit; } #pre { margin-top: 80px; } #nav { margin-top: 90px; } #main * { font-size: x-large; text-rendering: optimizelegibility; } @media (max-width: 800px) { #main * { font-size: xx-large; } } .collapsible { background-color: #777; color: white; cursor: pointer; padding: 18px; width: 100%; border: none; text-align: left; outline: none; font-size: 15px; } .active, .collapsible:hover { background-color: #555; } p { line-height: 1.5em; } b { font-weight: bold; } .glossref { color: #444444; text-decoration: none; border-bottom: 1px dotted green; } .glossref:link, .glossref:visited { color: #444444; } .glossref:hover { color: green; } .waTable td:first-child { width: 15pc; } .waTable thead \u0026gt; tr { background-color: #EAF3FE; } .waTable td { padding: 5pt; border: 1pt solid black; } .waQuestionTableRef { width: 100%; margin-top: 1pc; padding: 5pt; border: 1pt solid black; background-color: #EEEEEE; } .waQuestionTableRef td { padding: 5pt; } .waQuestionTable { margin-top: 1pc; padding: 5pt; border: 1pt solid black; } .waQuestionTable tr:nth-child(3n+1) { background-color: #EAF3FE; } .stretchtext { margin-left: 1pt; margin-right: 1pt; padding-left: 8pt; padding-right: 8pt; background-color: #EAF3FE; border-radius: 10pt; } .stretchtext span:nth-child(1) { display: none; background-color: #EAF3FE; } .stretchtext :nth-child(1):target { display: inline; } .stretchtext :nth-child(1):target + a:not(target) { display: none; } #nav-breadcrumbs a, #nav-breadcrumbs span { margin: 0 10px } .toc, .toc li { list-style: none; } #walogo { width: 180px; float: right; } :target:before { content:\u0026#34; \u0026#34;; display:block; height:90px; margin:-90px 0 0; } ul.itemizedlist { margin-left: 2.5em } \u0026lt;/style\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\u0026lt;header id=\u0026#34;aws-page-header\u0026#34; class=\u0026#34;awsm m-page-header\u0026#34; role=\u0026#34;banner\u0026#34;\u0026gt;\u0026lt;/header\u0026gt; \u0026#39; # Add the ability to use a header file instead via an passed argument # Here is how I would do do that: # htmlPage = Path(\u0026#39;header_file.html\u0026#39;).read_text() htmlPage += \u0026#39;\u0026lt;div id=\u0026#34;main\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt;\u0026#39; htmlPage += \u0026#34;\u0026lt;h1\u0026gt;Python Well-Architected Report v\u0026#34;+__version__ htmlPage += \u0026#34;\u0026lt;br\u0026gt;\u0026lt;/div\u0026gt;\u0026#34; return htmlPage def generateHTMLTOC(): htmlPage = \u0026#34;\u0026#34; htmlPage += \u0026#39;\u0026lt;div id=\u0026#34;main\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt;\u0026#39; htmlPage += \u0026#39;\u0026lt;h1\u0026gt;Table of Contents\u0026lt;/h1\u0026gt;\u0026#39; htmlPage += \u0026#39;\u0026lt;ul class=\u0026#34;itemizedlist\u0026#34; type=\u0026#34;disc\u0026#34;\u0026gt;\u0026#39; htmlPage += \u0026#34;\u0026lt;br\u0026gt;\u0026#34; for pillar in PILLAR_PARSE_MAP: htmlPage += (\u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;\u0026lt;a href=\u0026#34;#%s\u0026#34;\u0026gt;%s\u0026lt;/a\u0026gt;\u0026lt;/b\u0026gt; \u0026lt;/li\u0026gt;\u0026#39; % (pillar,PILLAR_PROPER_NAME_MAP[pillar])) htmlPage += \u0026#39;\u0026lt;br\u0026gt;\u0026lt;/div\u0026gt;\u0026#39; return htmlPage def getWorkloadProperties( waclient, workloadId ): htmlPage = \u0026#34;\u0026#34; workloadJson = GetWorkload(waclient,workloadId) htmlPage += \u0026#39;\u0026lt;div id=\u0026#34;main\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt;\u0026#39; htmlPage += \u0026#34;\u0026lt;h1\u0026gt;Workload Properties\u0026lt;/h1\u0026gt;\u0026#34; htmlPage += \u0026#39;\u0026lt;ul class=\u0026#34;itemizedlist\u0026#34; type=\u0026#34;disc\u0026#34;\u0026gt;\u0026#39; # Notes = workloadJson[\u0026#39;Notes\u0026#39;] if \u0026#34;Notes\u0026#34; in workloadJson else \u0026#34;\u0026#34; # nonAwsRegions = workloadJson[\u0026#39;NonAwsRegions\u0026#39;] if \u0026#34;NonAwsRegions\u0026#34; in workloadJson else [] # # industry = workloadJson[\u0026#39;Industry\u0026#39;] if \u0026#34;Industry\u0026#34; in workloadJson else \u0026#34;\u0026#34; # accountIds = workloadJson[\u0026#39;AccountIds\u0026#39;] if \u0026#34;AccountIds\u0026#34; in workloadJson else [] htmlPage += \u0026#34;\u0026lt;br\u0026gt;\u0026#34; htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Workload Name:\u0026lt;/b\u0026gt; \u0026#39; + workloadJson[\u0026#39;WorkloadName\u0026#39;] + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;ARN:\u0026lt;/b\u0026gt; \u0026#39; + workloadJson[\u0026#39;WorkloadArn\u0026#39;] + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Description:\u0026lt;/b\u0026gt; \u0026#39; + workloadJson[\u0026#39;Description\u0026#39;] + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Review Owner:\u0026lt;/b\u0026gt; \u0026#39; + workloadJson[\u0026#39;ReviewOwner\u0026#39;] + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Industry Type:\u0026lt;/b\u0026gt; \u0026#39; + workloadJson[\u0026#39;IndustryType\u0026#39;] + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; if \u0026#34;IndustryType\u0026#34; in workloadJson else \u0026#34;\u0026#34; # Environment # AwsRegions # NonAwsRegions # htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Account IDs:\u0026lt;/b\u0026gt; \u0026#39; + workloadJson[\u0026#39;AccountIds\u0026#39;] + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; if \u0026#34;AccountIds\u0026#34; in workloadJson else [] htmlPage += \u0026#39;\u0026lt;li class=\u0026#34;listitem\u0026#34;\u0026gt;\u0026lt;b\u0026gt;\u0026lt;a href=\u0026#34;\u0026#39;+workloadJson[\u0026#39;ArchitecturalDesign\u0026#39;]+\u0026#39;\u0026#34;\u0026gt;Architectural Design\u0026lt;/a\u0026gt;\u0026lt;/b\u0026gt; \u0026#39;+ \u0026#34;\u0026lt;/li\u0026gt;\u0026#34; if \u0026#34;ArchitecturalDesign\u0026#34; in workloadJson else \u0026#34;\u0026#34; htmlPage += \u0026#34;\u0026lt;br\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026#34; return htmlPage def getPillarReport( waclient, workloadId, lensAlias, pillarId ): htmlPage = \u0026#34;\u0026#34; fullResponse = listLensReviewImprovements(waclient,workloadId,\u0026#34;wellarchitected\u0026#34;,pillarId) for answeredQuestion in fullResponse: htmlString = \u0026#34;\u0026#34; headerString = \u0026#34;\u0026#34; ipList = getImprovementPlanItems(waclient,workloadId,\u0026#34;wellarchitected\u0026#34;,answeredQuestion[\u0026#39;QuestionId\u0026#39;],answeredQuestion[\u0026#39;PillarId\u0026#39;],answeredQuestion[\u0026#39;ImprovementPlanUrl\u0026#39;]) for showChoices in ipList: ipItemHTML, questionIdText = getImprovementPlanHTMLDescription(showChoices[\u0026#39;ParsedURL\u0026#39;],answeredQuestion[\u0026#39;PillarId\u0026#39;]) headerString = \u0026#34;\u0026lt;h2\u0026gt;\u0026lt;b\u0026gt;\u0026#34;+questionIdText+\u0026#34; - \u0026#34;+answeredQuestion[\u0026#39;QuestionTitle\u0026#39;]+\u0026#34;\u0026lt;/h2\u0026gt;\u0026#34; # headerString = \u0026#39;\u0026lt;div class=\u0026#34;wrap-collabsible\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;collapsible\u0026#34; class=\u0026#34;toggle\u0026#34; type=\u0026#34;checkbox\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;collapsible\u0026#34; class=\u0026#34;lbl-toggle\u0026#34;\u0026gt;\u0026#39; # headerString += questionIdText+\u0026#34; - \u0026#34;+answeredQuestion[\u0026#39;QuestionTitle\u0026#39;] # headerString += \u0026#39;\u0026lt;/label\u0026gt;\u0026lt;div class=\u0026#34;collapsible-content\u0026#34;\u0026gt;\u0026lt;div class=\u0026#34;content-inner\u0026#34;\u0026gt;\u0026lt;p\u0026gt;\u0026#39; headerString += \u0026#34;\u0026lt;h2\u0026gt;\u0026lt;b\u0026gt;Current Risk: \u0026#34;+answeredQuestion[\u0026#39;Risk\u0026#39;]+\u0026#34;\u0026lt;/h2\u0026gt;\u0026#34; htmlString += ipItemHTML.prettify() # htmlString += \u0026#34;\u0026lt;/p\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026#34; # print(htmlString) # print(showChoices[\u0026#39;ChoiceID\u0026#39;],) # exit() htmlPage += headerString htmlPage += htmlString # htmlPage += \u0026#39;\u0026lt;/p\u0026gt;\u0026lt;/div\u0026gt;\u0026#39; # htmlPage += \u0026#39;-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\u0026#39; return htmlPage def getPillarSummary( waclient, workloadId, lensAlias, pillarId ): htmlPage = \u0026#34;\u0026#34; return htmlPage def main(): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() # STEP 1 - Configure environment # WORKLOADID = ARGUMENTS.workloadid logger.info(\u0026#34;Starting Boto %s Session\u0026#34; % boto3.__version__) # Create a new boto3 session SESSION1 = boto3.session.Session(profile_name=PROFILE) # Initiate the well-architected session using the region defined above WACLIENT = SESSION1.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) htmlPage = generateHTMLHeader() htmlPage += getWorkloadProperties(WACLIENT,WORKLOADID) # TODO - This currently will only do the base framework. # If people are interested, I can add an enumeration over the # Lenses and gather the same report for them. # htmlPage += \u0026#34;\u0026lt;h1\u0026gt;Improvement Plan\u0026lt;/h1\u0026gt;\u0026#34; htmlPage += generateHTMLTOC() htmlPage += \u0026#39;\u0026lt;div id=\u0026#34;main\u0026#34; role=\u0026#34;main\u0026#34;\u0026gt;\u0026#39; for pillar in PILLAR_PARSE_MAP: # htmlPage += \u0026#34;\u0026lt;h1 id=\u0026#34;+pillar+Improvement Plan\u0026lt;/h1\u0026gt;\u0026#34; htmlPage += (\u0026#39;\u0026lt;h1 id=\u0026#34;%s\u0026#34;\u0026gt;%s Improvement Plans\u0026lt;/h1\u0026gt;\u0026#39; % (pillar, PILLAR_PROPER_NAME_MAP[pillar])) htmlPage += getPillarSummary(WACLIENT,WORKLOADID,\u0026#34;wellarchitected\u0026#34;,pillar) htmlPage += getPillarReport(WACLIENT,WORKLOADID,\u0026#34;wellarchitected\u0026#34;,pillar) # Close out the HTML for the page htmlPage += \u0026#34;\u0026lt;/body\u0026gt;\u0026#34; htmlPretty = BeautifulSoup(htmlPage,features=\u0026#34;html.parser\u0026#34;) htmlPage = htmlPretty.prettify() # Open the file in a browser # Might want to make this an argument in the future with tempfile.NamedTemporaryFile(\u0026#39;w\u0026#39;, delete=False, suffix=\u0026#39;.html\u0026#39;) as f: url = \u0026#39;file://\u0026#39; + f.name logger.info(\u0026#34;Creating HTML file %s \u0026#34; % f.name) f.write(htmlPage) logger.info(\u0026#34;Opening HTML URL (%s) in default WebBrowser\u0026#34; % url) webbrowser.open(url) if __name__ == \u0026#34;__main__\u0026#34;: main() X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/2_explore_lambda_code/","title":"Exploring AWS Lambda code","tags":[],"description":"","content":"Overview There are two AWS Lambda functions that you deployed in the previous step. Both of them utilize the AWS SDK for Python (Boto3) library along with the Lambda Powertools Python via a Lambda layer to perform the Well-Architected Tool API access.\nDeployed AWS Lambda functions Click on each link to understand how each Lambda function works.\nCreateWAWorkloadLambda Overview Python Code Example Lambda Event IAM Role UpdateWAQuestionLambda Overview Python Code Example Lambda Event IAM Role CreateWAWorkloadLambda.py Overview This Lambda function will create or update the workload and is idempotent based on the WorkloadName. The parameters for the workload are:\nWorkloadName - The name of the workload. The name must be unique within an account within a Region. Spaces and capitalization are ignored when checking for uniqueness. WorkloadDesc - The description for the workload. WorkloadOwner - The review owner of the workload. The name, email address, or identifier for the primary group or individual that owns the workload review process. WorkloadEnv - The environment for the workload. Valid Values: PRODUCTION | PREPRODUCTION WorkloadRegion - The list of AWS Regions associated with the workload, for example, us-east-2, or ca-central-1. Maximum number of 50 items. WorkloadLenses - The list of lenses associated with the workload. Each lens is identified by its LensAlias. Tags - The tags to be associated with the workload. Maximum number of 50 items. Python Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 import botocore import boto3 import json import datetime from aws_lambda_powertools import Logger import jmespath import cfnresponse from pkg_resources import packaging __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; logger = Logger() # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def CreateNewWorkload( waclient, workloadName, description, reviewOwner, environment, awsRegions, lenses, tags ): # Create your workload try: waclient.create_workload( WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, Lenses=lenses, Tags=tags ) except waclient.exceptions.ConflictException as e: workloadId,workloadARN = FindWorkload(waclient,workloadName) logger.warning(\u0026#34;WARNING - The workload name %s already exists as workloadId %s\u0026#34; % (workloadName, workloadId)) UpdateWorkload(waclient,workloadId,workloadARN, workloadName,description,reviewOwner,environment,awsRegions,lenses,tags) # Maybe we should \u0026#34;update\u0026#34; the above variables? except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def FindWorkload( waclient, workloadName ): # Finding your WorkloadId try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] workloadARN = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadArn\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workloadId, workloadARN def UpdateWorkload( waclient, workloadId, workloadARN, workloadName, description, reviewOwner, environment, awsRegions, lenses, tags ): logger.info(\u0026#34;Updating workload properties\u0026#34;) # Create your workload try: waclient.update_workload( WorkloadId=workloadId, WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # Should add updates for the lenses? # Should add the tags as well logger.info(\u0026#34;Updating workload tags\u0026#34;) try: waclient.tag_resource(WorkloadArn=workloadARN,Tags=tags) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def lambda_handler(event, context): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() responseData = {} # print(json.dumps(event)) try: WORKLOADNAME = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadName\u0026#39;] DESCRIPTION = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadDesc\u0026#39;] REVIEWOWNER = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadOwner\u0026#39;] ENVIRONMENT= event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadEnv\u0026#39;] AWSREGIONS = [event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadRegion\u0026#39;]] LENSES = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadLenses\u0026#39;] TAGS = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;Tags\u0026#39;] SERVICETOKEN = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;ServiceToken\u0026#39;] except: responseData[\u0026#39;Error\u0026#39;] = \u0026#34;ERROR LOADING RESOURCE PROPERTIES\u0026#34; cfnresponse.send(event, context, cfnresponse.FAILED, responseData, \u0026#39;createWAWorkloadHelperFunction\u0026#39;) exit() IncomingARN = SERVICETOKEN.split(\u0026#34;:\u0026#34;) REGION_NAME = IncomingARN[3] logger.info(\u0026#34;Starting Boto %s Session in %s\u0026#34; % (boto3.__version__, REGION_NAME)) # Create a new boto3 session SESSION = boto3.session.Session() # Initiate the well-architected session using the region defined above WACLIENT = SESSION.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) logger.info(\u0026#34;Creating a new workload\u0026#34;) CreateNewWorkload(WACLIENT,WORKLOADNAME,DESCRIPTION,REVIEWOWNER,ENVIRONMENT,AWSREGIONS,LENSES,TAGS) logger.info(\u0026#34;Finding your WorkloadId\u0026#34;) workloadId,workloadARN = FindWorkload(WACLIENT,WORKLOADNAME) logger.info(\u0026#34;New workload created with id %s\u0026#34; % workloadId) responseData[\u0026#39;WorkloadId\u0026#39;] = workloadId responseData[\u0026#39;WorkloadARN\u0026#39;] = workloadARN logger.info(\u0026#34;Response will be %s\u0026#34; % responseData) cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData) Example incoming Lambda event This is an Lambda test event you can use to see if the Lambda function works as expected:\n{ \u0026#34;RequestType\u0026#34;: \u0026#34;Create\u0026#34;, \u0026#34;ResponseURL\u0026#34;: \u0026#34;http://pre-signed-S3-url-for-response\u0026#34;, \u0026#34;StackId\u0026#34;: \u0026#34;arn:aws:cloudformation:us-east-1:123456789012:stack/MyStack/guid\u0026#34;, \u0026#34;RequestId\u0026#34;: \u0026#34;unique id for this create request\u0026#34;, \u0026#34;ResourceType\u0026#34;: \u0026#34;Custom::CreateNewWAFRFunction\u0026#34;, \u0026#34;LogicalResourceId\u0026#34;: \u0026#34;CreateNewWAFRFunction\u0026#34;, \u0026#34;ResourceProperties\u0026#34;: { \u0026#34;ServiceToken\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:CreateNewWAFRFunction\u0026#34;, \u0026#34;WorkloadName\u0026#34;: \u0026#34;Lambda WA Workload Test\u0026#34;, \u0026#34;WorkloadOwner\u0026#34;: \u0026#34;Lambda Script\u0026#34;, \u0026#34;WorkloadDesc\u0026#34;: \u0026#34;Test Lambda WA Workload\u0026#34;, \u0026#34;WorkloadRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;WorkloadLenses\u0026#34;: [ \u0026#34;wellarchitected\u0026#34;, \u0026#34;serverless\u0026#34; ], \u0026#34;WorkloadEnv\u0026#34;: \u0026#34;PRODUCTION\u0026#34;, \u0026#34;Tags\u0026#34;: { \u0026#34;TestTag3\u0026#34;: \u0026#34;TestTagValue4\u0026#34;, \u0026#34;TestTag\u0026#34;: \u0026#34;TestTagValue\u0026#34; } } } IAM Privileges Using the concept of least privileges for each AWS Lambda function, we create an IAM role for this function that only allows certain access to the Well-Architected Tool (CreateWorkload and UpdateWorkload specifically) as well as the ability to create log file entries (lines 29-34).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 CreateWAlambdaIAMRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Action: - sts:AssumeRole Effect: Allow Principal: Service: - lambda.amazonaws.com Policies: - PolicyDocument: Version: 2012-10-17 Statement: - Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents Effect: Allow Resource: - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${CreateWALambdaFunctionName}:* PolicyName: lambda - PolicyDocument: Version: 2012-10-17 Statement: - Action: - wellarchitected:CreateWorkload - wellarchitected:GetWorkload - wellarchitected:List* - wellarchitected:TagResource - wellarchitected:UntagResource - wellarchitected:UpdateWorkload Effect: Allow Resource: \u0026#39;*\u0026#39; PolicyName: watool UpdateWAQuestionLambda.py Overview This Lambda function will update the answer to a specific question in a workload review. The parameters for the workload are:\nWorkloadId - The ID assigned to the workload. This ID is unique within an AWS Region. Lens - The alias of the lens, for example, wellarchitected or serverless. Pillar - The ID used to identify a pillar, for example, security. QuestionAnswers - An array of pillar Questions and associated best practices you wish to mark as selected. Python Code import botocore import boto3 import json import datetime from aws_lambda_powertools import Logger import jmespath import cfnresponse from pkg_resources import packaging __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; logger = Logger() # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def findQuestionId( waclient, workloadId, lensAlias, pillarId, questionTitle ): # Find a questionID using the questionTitle try: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillarId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,PillarId=pillarId,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) jmesquery = \u0026#34;[?starts_with(QuestionTitle, `\u0026#34;+questionTitle+\u0026#34;`) == `true`].QuestionId\u0026#34; questionId = jmespath.search(jmesquery, answers) return questionId[0] def findChoiceId( waclient, workloadId, lensAlias, questionId, choiceTitle, ): # Find a choiceId using the choiceTitle try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) jmesquery = \u0026#34;Answer.Choices[?starts_with(Title, `\u0026#34;+choiceTitle+\u0026#34;`) == `true`].ChoiceId\u0026#34; choiceId = jmespath.search(jmesquery, response) return choiceId[0] def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): # Update a answer to a question try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def lambda_handler(event, context): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() responseData = {} print(json.dumps(event)) try: WORKLOADID = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;WorkloadId\u0026#39;] PILLAR = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;Pillar\u0026#39;] LENS = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;Lens\u0026#39;] QUESTIONANSWERS = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;QuestionAnswers\u0026#39;] SERVICETOKEN = event[\u0026#39;ResourceProperties\u0026#39;][\u0026#39;ServiceToken\u0026#39;] except: responseData[\u0026#39;Error\u0026#39;] = \u0026#34;ERROR LOADING RESOURCE PROPERTIES\u0026#34; cfnresponse.send(event, context, cfnresponse.FAILED, responseData, \u0026#39;createWAWorkloadHelperFunction\u0026#39;) exit() IncomingARN = SERVICETOKEN.split(\u0026#34;:\u0026#34;) REGION_NAME = IncomingARN[3] logger.info(\u0026#34;Starting Boto %s Session in %s\u0026#34; % (boto3.__version__, REGION_NAME)) # Create a new boto3 session SESSION = boto3.session.Session() # Initiate the well-architected session using the region defined above WACLIENT = SESSION.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) for qaList in QUESTIONANSWERS: for question, answerList in qaList.items(): print(question, answerList) # First we must find the questionID questionId = findQuestionId(WACLIENT,WORKLOADID,LENS,PILLAR,question) logger.info(\u0026#34;Found QuestionID of \u0026#39;%s\u0026#39; for the question text of \u0026#39;%s\u0026#39;\u0026#34; % (questionId, question)) choiceSet = [] # Now we build the choice selection based on the answers provided for answers in answerList: choiceSet.append(findChoiceId(WACLIENT,WORKLOADID,LENS,questionId,answers)) logger.info(\u0026#34;All choices we will select for questionId of %s is %s\u0026#34; % (questionId, choiceSet)) # Update the answer for the question updateAnswersForQuestion(WACLIENT,WORKLOADID,LENS,questionId,choiceSet,\u0026#39;Added by Python\u0026#39;) # exit() cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData, \u0026#39;createWAWorkloadHelperFunction\u0026#39;) Example incoming Lambda event This is an Lambda test event you can use to see if the Lambda function works as expected:\n{ \u0026#34;RequestType\u0026#34;: \u0026#34;Create\u0026#34;, \u0026#34;ServiceToken\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:UpdateWAQFunction\u0026#34;, \u0026#34;ResponseURL\u0026#34;: \u0026#34;http://pre-signed-S3-url-for-response\u0026#34;, \u0026#34;StackId\u0026#34;: \u0026#34;arn:aws:cloudformation:us-east-1:123456789012:stack/MyStack/guid\u0026#34;, \u0026#34;RequestId\u0026#34;: \u0026#34;unique id for this create request\u0026#34;, \u0026#34;LogicalResourceId\u0026#34;: \u0026#34;UpdateWAQFunction\u0026#34;, \u0026#34;ResourceType\u0026#34;: \u0026#34;Custom::UpdateWAQFunction\u0026#34;, \u0026#34;ResourceProperties\u0026#34;: { \u0026#34;ServiceToken\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:UpdateWAQFunction\u0026#34;, \u0026#34;QuestionAnswers\u0026#34;: [ { \u0026#34;How do you determine what your priorities are\u0026#34;: [ \u0026#34;Evaluate governance requirements\u0026#34;, \u0026#34;Evaluate compliance requirements\u0026#34; ] }, { \u0026#34;How do you reduce defects, ease remediation, and improve flow into production\u0026#34;: [ \u0026#34;Use version control\u0026#34;, \u0026#34;Perform patch management\u0026#34;, \u0026#34;Use multiple environments\u0026#34; ] } ], \u0026#34;Pillar\u0026#34;: \u0026#34;operationalExcellence\u0026#34;, \u0026#34;Lens\u0026#34;: \u0026#34;wellarchitected\u0026#34;, \u0026#34;WorkloadId\u0026#34;: \u0026#34;d1a1d1a1d1a1d1a1d1a1d1a1d1a1d1a1\u0026#34; } } IAM Privileges Using the concept of least privileges for each AWS Lambda function, we create an IAM role for this function that only allows certain access to the Well-Architected Tool (UpdateAnswer specifically) as well as the ability to create log file entries (lines 29-34).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 UpdateWAQlambdaIAMRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Action: - sts:AssumeRole Effect: Allow Principal: Service: - lambda.amazonaws.com Policies: - PolicyDocument: Version: 2012-10-17 Statement: - Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents Effect: Allow Resource: - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${UpdateWAQLambdaFunctionName}:* PolicyName: lambda - PolicyDocument: Version: 2012-10-17 Statement: - Action: - wellarchitected:GetAnswer - wellarchitected:GetWorkload - wellarchitected:List* - wellarchitected:TagResource - wellarchitected:UntagResource - wellarchitected:UpdateAnswer Effect: Allow Resource: \u0026#39;*\u0026#39; PolicyName: watool X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_building_custom_aws_well-architected_reports_with_amazon_athena_and_amazon_quicksight/2_catalog_data/","title":"Catalog the workload data","tags":[],"description":"","content":"AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. AWS Glue provides crawlers to determine the schema and stores the metadata in the .\nCreate the Crawler Open the AWS Glue console, and from the left navigation pane, choose Crawlers. Select Add crawler and name the crawler well-architected-reporting, select Next. Select Next to accept the defaults for Specify crawler source type. Add the S3 path of the where you will store the extracted AWS Well-Architected data e.g. s3://well-architected-reporting-blog. Select Select No and then Next to on the Add another data store step. Select Create an IAM role and provide a name, e.g. well-architected-reporting , select Next. Select Run on demand as the schedule frequency. Select Next. Next select Add database, and fill-in a name e.g. war-reports. Select Create and then Next. Review the configuration and select Finish to create the Crawler. Run the Crawler Find the crawler that was just created, select it, and then choose Run Crawler. Wait for the crawler to complete running, which should take approximately one minute. From the left navigation pane, choose Databases. Find the database that was created during the Crawler creation, select it and choose View Tables. In the Name field, you should see \u0026ldquo;workloadreports\u0026rdquo;. Select this and examine the metadata that was discovered during the crawler run, as shown in Figure 6. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/2_create_journey/","title":"Create Journey","tags":[],"description":"","content":"Create the Journey We will now run the Lambda function and create the Well-Architected Cost Journeys. We will manually run the function\nGo to the Lambda service page, select the Cost_W-A_Journey function\nScroll down to the Code source, click Deploy, click Test\nSet the event name to CreateJourney, click Create\nClick Test, You will see a null response and there should be some log messages in the lambda console Go to the s3 console and select the bucket you configured, you will see a W-A Workload Journeys.html file which contains the index of all the workload journeys. The WorkloadReports/ folder contains each workload journey. Open up the files in a web browser and view the image: You have successfully created your Well-Architected Cost Journeys. You can configure the S3 bucket to serve web pages to easily distribute the journeys across your organization.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_enable_security_hub/2_cleanup/","title":"Tear Down","tags":[],"description":"","content":"The following instructions will disable AWS Config and Security Hub.\nDisable AWS Config:\nOpen the AWS Config console at https://console.aws.amazon.com/config/ . Choose Settings in the navigation pane. Select Edit. Uncheck Enable Recording. Select Save. Additional details here Disable Security Hub:\nOpen the AWS Security Hub console at https://console.aws.amazon.com/securityhub/ . In the navigation pane, choose Settings. On the Settings page, choose General. Under Disable AWS Security Hub, choose Disable AWS Security Hub. Then choose Disabled AWS Security Hub again. Additional details here Delete the S3 bucket:\nOpen the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket that was created for AWS Config (i.e. \u0026ldquo;config-bucket-12345\u0026rdquo;), then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting. "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/2_configure_backup_plan/","title":"Create Backup Plan","tags":[],"description":"","content":"A well thought out backup strategy is key to an organization\u0026rsquo;s success and is determined by a variety of factors. The biggest factors influencing a backup strategy is the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) set for the workload. RTO and RPO are determined based on the criticality of the workload to the business, the SLAs that have been agreed upon, and the cost associated with achieving the RTO and RPO. RTO and RPO should be specific to each workload and not set for the entire organization/infrastructure.\nIn this lab, you will create a backup strategy by leveraging AWS Backup, a fully managed backup service that can automatically backup data from various data sources such as EC2 Instances, EBS Volumes, RDS Databases, and more. You can view a complete list of supported services here .\nSign in to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#backupplan .\nChoose CREATE BACKUP PLAN.\nSelect the option to BUILD A NEW PLAN.\nSpecify a Backup plan name such as BACKUP-LAB.\nUnder BACKUP RULE CONFIGURATION, enter a RULE NAME such as BACKUP-LAB-RULE.\nTo set a SCHEDULE for the backup, you can specify the FREQUENCY at which backups are taken. You can enter frequency as every 12 hours, Daily, Weekly, or Monthly. Alternatively, you can specify a custom CRON EXPRESSION for your backup frequency. For this exercise, select the FREQUENCY as DAILY.\nOnce frequency has been established, you will need to specify a backup window - a period of time during which data is being backed up from your data sources. Keep in mind that creating backups could cause you data sources to be temporarily unavailable depending on the underlying configuration of those resources. It is best to create backups during scheduled downtimes/maintenance windows when user impact is minimal. For this exercise, select Use backup window defaults - recommended. The default backup window is set to start at 5 AM UTC time and last 8 hours. If you need to schedule backups at a different time, you can customize the backup window.\nYou can set lifecycle policies for your backups to transition them to cold storage or to expire them after a period of time to reduce costs and operational overhead. This is currently supported for backups of EFS only. For this exercise, set the values for Transition to cold storage and Expire both to NEVER.\nUnder BACKUP VAULT, click on CREATE NEW BACKUP VAULT. It is recommended to use different Backup Vaults for different workloads.\nOn the pop-up screen, specify a BACKUP VAULT NAME such as BACKUP-LAB-VAULT.\nYou can choose to encrypt your backups for additional security by specifying a KMS key. You can choose the default key created and managed by AWS Backup or specify your own custom key. For this exercise, select the default key (default) aws/backup.\nClick CREATE BACKUP VAULT.\nAdditionally, you can choose to have your backups automatically copied to a different AWS Region.\nYou can add tags to your recovery points to help identify them.\nClick CREATE PLAN.\nOnce the backup plan and the backup rule has been created, you can specify resources to back up. You can select individual resources to be backed up, or specify a tag (key-value) associated with the resource. AWS Backup will execute backup jobs on all resources that match the tags specified.\nClick on BACKUP PLANS from the menu on the left side of the screen.\nSelect the backup plan BACKUP-LAB that you just created.\nScroll down to the section titled RESOURCE ASSIGNMENTS and click on ASSIGN RESOURCES.\nSpecify a RESOURCE ASSIGNMENT NAME such as BACKUP-RESOURCES to help identify the resources that are being backed up.\nLeave the DEFAULT ROLE selected for IAM ROLE. If a role does not already exist, the AWS Backup service will create one with the necessary permissions.\nUnder ASSIGN RESOURCES, you can specify resources to be backed up individually by specifying the RESOURCE TYPE and RESOURCE ID, or select TAGS and enter the TAG KEY and the TAG VALUE. For this lab, select TAGS as the value for ASSIGN BY, and enter workload as the KEY and myapp as the VALUE. This tag and value was created by the CloudFormation stack. Remember that tags are case sensitive and ensure that the values you enter are all in lower case.\nClick on ASSIGN RESOURCES.\nYou have successfully created a backup plan for your data sources, and all supported resources with the tags workload=myapp will be backed up automatically, at the frequency specified. In case of a disaster, these backups can be used to recover data to ensure business continuity. Since the entire process is automated, it will save considerable operational overhead for your Operations teams.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/2_usage_source/","title":"Create the Usage Data Source","tags":[],"description":"","content":" If AWS updates pricing table with new column, values might get out of sync. Please contact costoptimization@amazon.com if you encounter any problems\nWe will combine the pricing information with our Cost and Usage Report (CUR). This will give us a usage data source which contains a summary of your usage at an hourly level, with multiple pricing dimensions.\nCreate OD Table Go to the Athena service page: Copy the following query to Athean:\nCREATE EXTERNAL TABLE `od_pricedata`( `sku` string, `offertermcode` string, `ratecode` string, `termtype` string, `pricedescription` string, `effectivedate` string, `startingrange` string, `endingrange` string, `unit` string, `priceperunit` double, `currency` string, `leasecontractlength` string, `purchaseoption` string, `offeringclass` string, `product family` string, `servicecode` string, `location` string, `location type` string, `instance type` string, `current generation` string, `instance family` string, `vcpu` string, `physical processor` string, `clock speed` string, `memory` string, `storage` string, `network performance` string, `processor architecture` string, `storage media` string, `volume type` string, `max volume size` string, `max iops/volume` string, `max iops burst performance` string, `max throughput/volume` string, `provisioned` string, `tenancy` string, `ebs optimized` string, `operating system` string, `license model` string, `group` string, `group description` string, `transfer type` string, `from location` string, `from location type` string, `to location` string, `to location type` string, `usagetype` string, `operation` string, `availabilityzone` string, `capacitystatus` string, `classicnetworkingsupport` string, `dedicated ebs throughput` string, `ecu` string, `elastic graphics type` string, `enhanced networking supported` string, `from region code` string, `gpu` string, `gpu memory` string, `instance` string, `instance capacity - 10xlarge` string, `instance capacity - 12xlarge` string, `instance capacity - 16xlarge` string, `instance capacity - 18xlarge` string, `instance capacity - 24xlarge` string, `instance capacity - 2xlarge` string, `instance capacity - 32xlarge` string, `instance capacity - 4xlarge` string, `instance capacity - 8xlarge` string, `instance capacity - 9xlarge` string, `instance capacity - large` string, `instance capacity - medium` string, `instance capacity - metal` string, `instance capacity - xlarge` string, `instancesku` string, `intel avx2 available` string, `intel avx available` string, `intel turbo available` string, `marketoption` string, `normalization size factor` string, `physical cores` string, `pre installed s/w` string, `processor features` string, `product type` string, `region code` string, `resource type` string, `servicename` string, `snapshotarchivefeetype` string, `to region code` string, `volume api name` string, `vpcnetworkingsupport` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( 'quoteChar'='\\\u0026quot;', 'separatorChar'=',') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3://bucketname/od_pricedata/' TBLPROPERTIES ( 'CrawlerSchemaDeserializerVersion'='1.0', 'CrawlerSchemaSerializerVersion'='1.0', 'UPDATED_BY_CRAWLER'='Pricing CSV', 'areColumnsQuoted'='true', 'averageRecordSize'='1061', 'classification'='csv', 'columnsOrdered'='true', 'compressionType'='none', 'delimiter'=',', 'objectCount'='1', 'recordCount'='2089892', 'sizeKey'='2217375799', 'skip.header.line.count'='6', 'typeOfData'='file') Change the bucketname to your bucket name and click Run: Create View Run the following query to create a single pricing data source, combining the OD and SP pricing:\nCREATE VIEW pricing.pricing AS SELECT sp.location AS Region, sp.discountedoperation AS OS, od.\u0026quot;Instance Type\u0026quot; InstanceType, od.Tenancy Tenancy, od.priceperunit ODRate, sp.discountedrate AS SPRate FROM pricing.sp_pricedata sp JOIN pricing.od_pricedata od ON ((sp.discountedusagetype = od.usageType) AND (sp.discountedoperation = od.operation)) WHERE od.priceperunit IS NOT NULL AND sp.location NOT LIKE '%Any%' AND sp.purchaseoption LIKE 'No Upfront' AND sp.leasecontractlength = 1 and od.TermType = 'OnDemand' group by 1,2,3,4,5,6 Next we\u0026rsquo;ll join the CUR file with that pricing source as a view. Edit the following query, replace costmaster.costmasterfile with your existing database name and tablename of your CUR, then run the rollowing query:\nCREATE VIEW costmaster.SP_Usage AS SELECT costmaster.line_item_usage_account_id, costmaster.line_item_usage_start_date, to_unixtime(costmaster.line_item_usage_start_date) AS EpochTime, costmaster.product_instance_type, costmaster.product_location, costmaster.product_operating_system, costmaster.product_tenancy, SUM(costmaster.line_item_unblended_cost) AS ODPrice, SUM(costmaster.line_item_unblended_cost*(cast(pr.SPRate AS double)/cast(pr.ODRate AS double))) SPPrice, abs(SUM(cast(pr.SPRate AS double)) - SUM (cast(pr.ODRate AS double))) / SUM(cast(pr.ODRate AS double))*100 AS DiscountRate, SUM(costmaster.line_item_usage_amount) AS InstanceCount FROM costmaster.costmasterfile costmaster JOIN pricing.pricing pr ON (costmaster.product_location = pr.Region) AND (costmaster.line_item_operation = pr.OS) AND (costmaster.product_instance_type = pr.InstanceType) AND (costmaster.product_tenancy = pr.Tenancy) WHERE costmaster.line_item_product_code LIKE '%EC2%' AND costmaster.product_instance_type NOT LIKE '' AND costmaster.product_operating_system NOT LIKE 'NA' AND costmaster.line_item_unblended_cost \u0026gt; 0 AND costmaster.line_item_line_item_type like 'Usage' GROUP BY costmaster.line_item_usage_account_id, costmaster.line_item_usage_start_date, costmaster.product_instance_type, costmaster.product_location, costmaster.product_operating_system, costmaster.product_tenancy ORDER BY costmaster.line_item_usage_start_date ASC, DiscountRate DESC The code above will capture ONLY on-demand usage, as defined by costmaster.line_item_line_item_type like \u0026lsquo;Usage\u0026rsquo;. You can remove this to include Savings Plan usage, to see total commitment you should have, instead of additional commitment required.\nVerify the data source is setup by editing the following query, replace costmaster. with the name of the database and run the following query:\nSELECT * FROM costmaster.sp_usage limit 10; You now have your usage data source setup with your pricing dimensions. You can modify the queries above to add or remove any columns you want in the view, which can later be used to visualize the data, for example tags.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/2_configure_env/","title":"Configure Lab Environment","tags":[],"description":"","content":"Overview In this section we will deploy our base lab infrastructure using AWS Serverless Application Model (AWS SAM) in AWS Cloud9 environment. This will consist of a public Amazon API Gateway which connects to AWS Lambda that puts items in AWS DynamoDB. We will also create a rule in Amazon EventBridge and another AWS Lambda that will retrieve data related to cost optimzation from AWS Compute Optimizer and AWS Trusted Advisor .\nWhen we successfully complete our initial stage template deployment, our deployed workload should reflect the following diagram:\nNote the following:\nAmazon API Gateway has been provided with a role to invoke AWS Lambda function with mapping table that contains AWS Trusted Advisor Cheeck IDs and Qustion ID of questions in Well-Architected Tool.\nAWS Lambda function has been provided with a role to put items in AWS DynamoDB.\nIn Well-Architected Tool, a reviewer will define a workload which is a collection of resources and applications that delivers business value.\nDefining a workload in Well-Architected Tool generates an event called CreateWorkload that Amazon EventBridge receives, which will invoke another AWS Lambda function.\nThis AWS Lambda function collects finding, reason, recommended instance type for the rightsizing from AWS Compute Optimizer. It also gathers the details of \u0026ldquo;Low Utilization Amazon EC2 Instances\u0026rdquo; check from AWS Trusted Advisor such as Estimated Monthly Savings and Average CPU Utilization.\nThe AWS Lambda function also will be able to retrieve Qustion ID of questions in Well-Architected Tool associated with Cheeck ID of AWS Trusted Advisor as Qustion ID is a required parameter to update notes.\nThe AWS Lambda function eventually update data points related to rightsizing into notes in Well-Architected Tool so that the reviewer can have a data-driven cost optimization review with customers.\nNote: Please select the region in which your EC2 Instances that you would like to run cost optimization review against are running.\nTo deploy the template for the base infrastructure complete the following steps:\n1.1. Get the CloudFormation Template and deploy Cloud9. You can get the CloudFormation template here. The first CloudFormation template will deploy AWS Cloud9 and you can create CloudFormation Stack directly via the AWS console.\nClick here for CloudFormation console deployment steps Console: If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. Open the CloudFormation console at https://console.aws.amazon.com/cloudformation and select the region in which your existing Amazon EC2 Instances running. Select the stack template which you downloaded earlier, and create a stack. Click Choose file to upload section1-base.yaml and click Next. For the stack name use any stack name you can identify and click Next. Skip stack options and and click Next. Scroll down to the bottom of the stack creation page and acknowledge the IAM resources creation by selecting all the check boxes. Then launch the stack. It may take 1~2 minutes to complete Cloud9 deployment. Click cloud9-stack and go to the Outputs section of the CloudFormation stack. Then, click Cloud9URL to set up your IDE environment. 1.2. Application Deployment using SAM(AWS Serverless Application Model). In Cloud9, repo will be autimatically cloned and go to a working directory called integration to execute sam build. The sam build command processes your AWS SAM template file, application code, and any applicable language-specific files and dependencies. ** janghan, change aws-well-architected-labs-1 to aws-well-architected-labs before this lab is published **\ncd /home/ec2-user/environment/aws-well-architected-labs-1/static/watool/200_Integration_with_AWS_Compute_Optimizer_and_AWS_Trusted_Advisor/Code/integration sam build Deploy an AWS SAM application using sam deploy \u0026ndash;guided. sam deploy --guided Answer y for LambdaPutDynamoDB may not have authorization defined, Is this ok? In Outputs, take a note of APIGWUrl. Now we are going to update AWS DynamoDB table with a sample mapping table in json file through API Gateway. This mapping table has Qustion ID of Well-Architected question associated with AWS Trusted Advisor check ID and AWS Lambda function will retrive Qustion ID to update findings related to cost optimization into notes in Well-Architected Tool. Replace APIGWUrl with your APIGWUrl that you copied from Outputs. curl --header \u0026#34;Content-Type: application/json\u0026#34; -d @mappings/wa-mapping.json -v POST {APIGWUrl} Confirm that UnprocessedItems appear to be empty, which means you successfully put items into AWS DynamoDB. In AWS DynamoDB console, click wa-mapping you just deployed and click Explore table items. There are 2 Qustion IDs of Well-Architected questions and 2 AWS Trusted Advisor checks. END OF SECTION 1\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/2_create_workload/","title":"Create a Well-Architected Workload","tags":[],"description":"","content":"Overview Well-Architected Reviews are conducted per workload. A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Workloads vary in levels of architectural complexity, from static websites to architectures with multiple data stores and many components.\nCreating a workload We will start with creating a Well-Architected workload to use throughout this lab. Using the create-workload API , you can create a new Well-Architected workload: aws wellarchitected create-workload --workload-name \u0026#34;WA Lab Test Workload\u0026#34; --description \u0026#34;Test Workload for WA Lab\u0026#34; --review-owner \u0026#34;John Smith\u0026#34; --environment \u0026#34;PRODUCTION\u0026#34; --aws-regions \u0026#34;us-east-1\u0026#34; --lenses \u0026#34;wellarchitected\u0026#34; \u0026#34;serverless\u0026#34; The following are the required parameters for the command: workload-name - This is a uniquie identifier for the workload. Must be between 3 and 100 characters. description - A brief description of the workload to document its scope and intended purpose. Must be between 3 and 250 characters. review-owner - The name, email address, or identifier for the primary individual or group that owns the review process. Must be between 3 and 255 characters. environment - The environment in which your workload runs. This must either be PRODUCTION or PREPRODUCTION aws-regions - The aws-regions in which your workload runs (us-east-1, etc). lenses - The list of lenses associated with the workload. All workloads must include the \u0026ldquo;wellarchitected\u0026rdquo; lens as a base, but can include additional lenses. For this lab, we are also including the serverless lens. Using the list-lenses API you can get a list of lenses: aws wellarchitected list-lenses Once the command is run, you should get a response that contains the workload json structure. This will include the following items: WorkloadId - The ID assigned to the workload. This ID is unique within an AWS Region. WorkloadArn - The ARN for the workload. Finding your WorkloadId Assume you created a new workload, but you did not write down the WorkloadId to use in subsequent API calls. Using the list-workloads API , you can find the WorkloadId by using a search for the workload name prefix: aws wellarchitected list-workloads --workload-name-prefix \u0026quot;WA Lab\u0026quot; You should get back a response that includes the WorkloadId along with other information about the workload that starts with \u0026ldquo;WA Lab\u0026rdquo; If you want to only return the WorkloadId, you can use the AWS CLI query parameter to query for the value: aws wellarchitected list-workloads --workload-name-prefix \u0026quot;WA Lab\u0026quot; --query 'WorkloadSummaries[].WorkloadId' --output text Using WorkloadId to remove and add lenses In the first step, we added the serverless lens to our new workload. Next, we will remove and then re-add this lens to the workload. Make sure you have the WorkloadId from the previous step and replace WorkloadId with it Using the get-workload API , lets check which lenses are associated with our workload. aws wellarchitected get-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --query 'Workload.Lenses[]' You should see serverless listed as a lens. Using the disassociate-lenses API we will remove the serverless lens. aws wellarchitected disassociate-lenses --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-aliases \u0026quot;serverless\u0026quot; When you use disassociate-lenses, it will be destructive and irreversible to any questions you have answered if you have not saved a milestone. Saving a milestone is recommended before you use the disassociate-lenses API call.\nYou will not get a response to this command, but using the get-workload API you can verify that the lens was removed. aws wellarchitected get-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; You should see a response such as this, showing that you no longer have serverless listed. Using the associate-lenses API we can add the serverless lens back into the workload. aws wellarchitected associate-lenses --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; --lens-aliases \u0026quot;serverless\u0026quot; Again, you will not see a response to this command, but we can verify that it was added by doing another get-workload aws wellarchitected get-workload --workload-id \u0026quot;\u0026lt;WorkloadId\u0026gt;\u0026quot; X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available Level 200: Integration with AWS Compute Optimizer and AWS Trusted Advisor Level 200: Using AWSCLI to Manage WA Reviews Level 200: Manage Workload Risks with OpsCenter "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/2_impact_of_failures/","title":"Scope of Impact of failures","tags":[],"description":"","content":"Break the application In the scenario used in the lab, the application has a known issue which is triggered by passing a \u0026ldquo;bad\u0026rdquo; query string. If such a request is received, the EC2 instance that handles the request will become unresponsive and the application will crash on the instance. The \u0026ldquo;bad\u0026rdquo; query string that triggers this is bug with a value of true. The development team is aware of this bug and are working on a fix, however, the issue exists today and customers might accidentally or intentionally trigger it. This is referred to as a \u0026ldquo;poison pill\u0026rdquo;, a bug or issue which when introduced into a system could compromise the functionality of the system.\nImagine a situation where a customer accidentally triggers the bug in the application that causes it to shutdown on the instance where the request was received.\nThis can be done by including the query-string \u0026amp;bug=true. The modified URL should look like this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\u0026bug=true (but using your own URL from the CloudFormation stack Outputs) Request the page for customer Alpha using your modified URL You should see an Internal Server Error response on the browser indicating that the application has stopped working as expected on the instance that processed this request\nAt this point, there are 7 healthy instances still available so other customers are not impacted. You can verify this:\nOpen another browser tab and specify the URL for a different customer (obtained from the CloudFormation stack Outputs) and without the bug query string. Try with at least two other customers, you may try them all if you want to, but it is not necessary. Refresh the browser for requests from these other customers. You will see that responses are being returned only from 7 EC2 instances instead of 8. Note: If you see a response that says \u0026ldquo;This site can\u0026rsquo;t be reached\u0026rdquo;, please make sure you are using the URL obtained from the outputs section of the CloudFormation stack and not the sample URL provided in this lab guide.\nCustomer Alpha, not aware of this bug in the application, will retry the request.\nReturn to the first browser tab (the one with the \u0026ldquo;buggy request\u0026rdquo; from Alpha) Refresh the page once with customer Alpha\u0026rsquo;s request with the bug=true query string. This new request is then routed to one of the 7 remaining healthy instances. The bug is triggered again and another instance goes down leaving only 6 healthy instances.\nThis can be verified in the second browser tab Try sending requests from one of the other customers without including the query string bug=true and see that responses come from only 6 EC2 instances. This process continues with customer Alpha retrying requests until all instances are unhealthy.\nReturn to the first browser tab Keep refreshing the page as customer Alpha with the query string bug=true. You will eventually see the response change to “502 Bad Gateway” because there are no healthy instances to handle requests. You can verify this by sending requests from other customers (without including the query string bug=true), you should see a 502 Bad Gateway response received for all requests from all customers. This is what is known as a \u0026ldquo;retry storm\u0026rdquo;, where a customer is unknowingly making bad requests and retrying the request every time it fails because they are not aware of the bug within the application. This lab will cover how to use sharding to mitigate the impact of retry storms. After the lab, also see the AWS Well-Architected best practice Control and limit retry calls as another practice to limit problems caused by retry storms\nIn this situation, a buggy request made by one customer has taken down all instances on the backend resulting in complete downtime and all customers are now affected. This is a widespread scope of impact with 100% of customers affected.\nVerify workload availability You can look at the AvailabilityDashboard to see the impact of the failure introduced by customer Alpha across all customers.\nSwitch to the tab that has the AvailabilityDashboard opened. (You can also retrieve the URL from the CloudFormation stack Outputs).\nYou can see that the introduction of the poison-pill and subsequent retries by customer Alpha has impacted all other customers as the canaries for these customers are unable to make successful requests to the workload.\nNOTE: You might have to wait a few minutes for the dashboard to get updated. Fix the application As previously mentioned, the development team is aware of this bug within the application and are working on a fix, however, the fix will not be rolled out for several weeks/months. They have been able to identify the root cause of the issue and provided a temporary manual fix for it. Whenever this issue is encountered, the Operations team executes the temporary fix to bring the application back up again. They have codified this process into a Systems Manager Document and use Systems Manager to implement the fix on their fleet if outages occur. The Systems Manager Document restarts the application on the selected instances.\nGo to the Outputs section of the CloudFormation stack and open the link for “SSMDocument”. This will take you to the Systems Manager console.\nClick on Run command which will open a new tab on the browser\nScroll down to the Targets section and select Specify instance tags\nEnter Workload for the tag key and WALab-shuffle-sharding for the tag value. Click Add.\nScroll down to the Output options section and uncheck the box next to Enable an S3 bucket. This will prevent Systems Manager from writing log files based on the command execution to S3.\nClick on Run\nYou should see the command execution succeed in a few seconds\nOnce the command has finished execution, you can go back to the application and test it to verify it is working as expected by using one of the customer URLs obtained from the CloudFormation stack Outputs.\nMake sure that the query-string bug is not included in the request. Refresh the page a few times to make sure all the instances are up and running and you are able to get responses from them. Test with a few different customer URLs to see that functionality has returned to all customers. Review the AvailabilityDashboard to make sure canary requests are succeeding and normal functionality has returned to all customers. You should see that SuccessPercent has returned to 100 for all customers.\nNOTE: You might have to wait a few minutes for the dashboard to get updated. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/2_deploy_instance/","title":"Deploying an instance","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nIn the upper right portion of the console, select the region you wish to deploy the lab into. For this example we will be using us-west-2 Go to this link: https://console.aws.amazon.com/ec2/v2/home?#KeyPairs Type \u0026ldquo;wapetestlab\u0026rdquo; in the name field For File format select \u0026ldquo;pem\u0026rdquo; Click on \u0026ldquo;Create Key Pair\u0026rdquo; When your browser prompts, save the file in a location on your local computer. You will need this later if you wish to login to the machine directly via RDP. \u0026ndash;\u0026gt; Deploy CloudFormation Template Download the LinuxMachineDeploy.yaml CloudFormation template to your machine. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: LinuxMachineDeploy.yaml Click Next\nFor Stack name use LinuxMachineDeploy\nParameters\nLook over the Parameters and their default values.\nNone of these parameters need to be changed, but are available if you wish to try different settings\nStack name – Use LinuxMachineDeploy (case sensitive)\nCloudWatchNameSpace - The CloudWatch NameSpace to use instead of the default\nInstanceAMI – This will auto-populate with the latest version of the Amazon Linux 2 AMI\nInstanceType - Instance Type, defaults to t3.large but can use any size supported by Linux in the region you have chosen\nMetricAggregationInterval - How often should the CloudWatch Agent send data into CloudWatch.\nMetricCollectionInterval - How often should the CloudWatch Agent collect information from the Operating System.\nPrimaryNodeLabel - The additional label assigned to the EC2 instance to use for searching within CloudWatch Explorer. This is done by adding an extra Tag to the EC2 instance.\nVPCImportName - The name of the stack you created in the previous step that will be used to launch the instance into.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nThis template will take between 2-5 minutes to fully deploy using a t3.large. A smaller instance size may take longer.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/2_deploy_instance/","title":"Deploying an instance","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and IAM Instance Profiles. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nIn the upper right portion of the console, select the region you wish to deploy the lab into. For this example we will be using us-west-2 Go to this link: https://console.aws.amazon.com/ec2/v2/home?#KeyPairs Type \u0026ldquo;wapetestlab\u0026rdquo; in the name field For File format select \u0026ldquo;pem\u0026rdquo; Click on \u0026ldquo;Create Key Pair\u0026rdquo; When your browser prompts, save the file in a location on your local computer. You will need this later if you wish to login to the machine directly via RDP. \u0026ndash;\u0026gt; Deploy CloudFormation Template Download the WindowsMachineDeploy.yaml CloudFormation template to your machine. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: WindowsMachineDeploy.yaml Click Next\nFor Stack name use WindowsMachineDeploy\nParameters\nLook over the Parameters and their default values.\nStack name – Use WindowsMachineDeploy (case sensitive)\nVPCImportName - The VPC that we will deploy to. This will default to the name of the VPC you created in the previous step.\nInstanceAMI – This will auto-populate with the latest version of the Windows 2019 Base AMI\nInstanceType - Instance Type, defaults to t3.large but can use any size supported by Windows in the region you have chosen\nMetricAggregationInterval - How often should the Windows CloudWatch Agent send data into CloudWatch. No need to change this. MetricCollectionInterval - How often should the CloudWatch Agent collect information from the Operating System. No need to change this. PrimaryNodeLabel - The additional label assigned to the EC2 instance to use for searching within CloudWatch Explorer Click Next For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nThis template will take between 10-15 minutes to fully deploy using a t3.large. A smaller instance size may take longer.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/1-1_prepare_cur_data/","title":"Prepare CUR Data","tags":[],"description":"","content":"Lab 1.1 In this lab you will provide AWS Cost \u0026amp; Usage Report (CUR) data in one of your Amazon S3 buckets. In later labs we will query that data to calculate proxy metrics for sustainability.\nThere are several options to provide AWS Cost \u0026amp; Usage Report data:\nOption A: use existing CUR reports from your AWS account. This option makes sense, if you already have configured CUR in your account. Option B: create a new CUR report in your AWS account. This option makes sense, if your account has significant resource and you are willing to suspend the lab for at least 24 hours to wait for CUR data collection. Option C: copy CUR sample data to your bucket. Option A - Existing CUR reports If you have cost usage reporting currently enabled in your account, please check whether the setup of the report is appropriate for this lab. Expand me for Option A steps Go to the AWS Cost \u0026amp; Usage Reports in your Billing Console Choose your existing report Check that the CUR report has been created: In the Parquet format Time granularity set to Hourly Report content contains Resource IDs Option B - Setup new CUR reports in your AWS account If you do not have cost usage reporting currently enabled in your account, follow these steps to configure reports appropriately for this lab. Be aware, if you are setting up CUR reports for the first time, you will need to wait 24 hours for data collection and report generation before continuing with the lab. Expand me for Option B steps Go to the AWS Cost \u0026amp; Usage Reports in your Billing Console Click Create Report Set a Report name, e.g. proxy-metrics-lab Select Include resource IDs Click Next For S3 bucket, click Configure. Choose an existing bucket or create a new bucket in a region in which you will also run the Amazon Athena queries later. 7. Review and accept the bucket policy (for newly created bucket) 8. Set a Report path prefix, e.g. cur-data/hourly. AWS Cost \u0026amp; Usage Report require a prefix on creation, please set a prefix here. 9. Select Enable report data integration for Amazon Athena 10. Click Next 11. Click Review and complete\nNow you need to wait until the CUR data is delivered to your bucket. After delivery starts, AWS updates the AWS Cost and Usage Reports files at least once a day.\nOption C - Using Sample CUR data If you do not already have CUR reports available for your account, and are not in a position to wait for data delivery from Option B, you may use sample report data to continue with the lab. Expand me for Option C steps Create a new bucket in the Amazon S3 console , with the default settings. Choose a region in which you will also run the Amazon Athena queries later: Download and put the .parquet files from the aws-well-architected-labs repository to your bucket to the corresponding prefixes in your bucket: s3://\u0026lt;your-bucket-name\u0026gt;/cur-data/hourly/proxy-metrics-lab/year=2018/month=12/Dec2018-WorkshopCUR-00001.snappy.parquet s3://\u0026lt;your-bucket-name\u0026gt;/cur-data/hourly/proxy-metrics-lab/year=2018/month=11/Nov2018-WorkshopCUR-00001.snappy.parquet s3://\u0026lt;your-bucket-name\u0026gt;/cur-data/hourly/proxy-metrics-lab/year=2018/month=10/Oct2018-WorkshopCUR-00001.snappy.parquet Congratulations, you now have AWS Cost \u0026amp; Usage Report data in an Amazon S3 bucket which we can query with Athena in the next step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/1_understand_data_sharing/","title":"Understand Amazon Redshift Data Sharing","tags":[],"description":"","content":"Lab 1 Let’s first understand some data sharing terms \u0026amp; concepts - for details, refer to the Amazon Redshift User Guide :\nA datashare is the unit of sharing data in Amazon Redshift. Use datashares to share data in the same AWS account or different AWS accounts.\nData producers (also known as data sharing producers or datashare producers) are clusters that you want to share data from.\nData consumers (also known as data sharing consumers or datashare consumers) are clusters that receive datashares from producer clusters.\nDatashare objects are objects from specific databases on a cluster that producer cluster administrators can add to datashares to be shared with data consumers.\nCluster namespaces are identifiers that identify Amazon Redshift clusters. A namespace globally unique identifier (GUID) is automatically created during Amazon Redshift cluster creation and attached to the cluster. You can see the namespace of an Amazon Redshift cluster on the cluster details page on the Amazon Redshift console.\nAWS accounts can be consumers for datashares and are each represented by a 12-digit AWS account ID.\nAlso, review Data sharing considerations in Amazon Redshift .\nNow, let\u0026rsquo;s start by creating an Amazon Redshift environment in AWS Account.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/1_understand_data_sharing/","title":"Understand Amazon Redshift Data Sharing","tags":[],"description":"","content":"Lab 1 Let’s first understand some data sharing terms \u0026amp; concepts - for details, refer to the Amazon Redshift User Guide :\nA datashare is the unit of sharing data in Amazon Redshift. Use datashares to share data in the same AWS account or different AWS accounts.\nData producers (also known as data sharing producers or datashare producers) are clusters that you want to share data from.\nData consumers (also known as data sharing consumers or datashare consumers) are clusters that receive datashares from producer clusters.\nDatashare objects are objects from specific databases on a cluster that producer cluster administrators can add to datashares to be shared with data consumers.\nCluster namespaces are identifiers that identify Amazon Redshift clusters. A namespace globally unique identifier (GUID) is automatically created during Amazon Redshift cluster creation and attached to the cluster. You can see the namespace of an Amazon Redshift cluster on the cluster details page on the Amazon Redshift console.\nAWS accounts can be consumers for datashares and are each represented by a 12-digit AWS account ID.\nAlso, review Data sharing considerations in Amazon Redshift .\nNow, let\u0026rsquo;s start by creating an Amazon Redshift environment in AWS Account.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/","title":"200 Level Labs","tags":[],"description":"","content":"List of labs available Level 200: Optimize Amazon EC2 using Amazon CloudWatch and AWS Compute Optimizer "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/2_create_automation_resources_source/","title":"Create Automation Resources","tags":[],"description":"","content":"Setup an AWS Lambda function to retrieve AWS Organizations information Create the On-Demand AWS Lambda function to get the AWS Organizations information, and extract the required parts from it then write to our bucket in Amazon S3.\nReturn to your Sub account for the rest of this lab. Go to the Lambda service page : Click Create function: Enter the following details:\nSelect: Author from scratch Function name: Lambda_Org_Data Runtime: Python (Latest) Open Change default execution role Execution Role: Use an existing role Role name: LambdaOrgRole Click Create function\nCopy and paste the code below into the Function code section and change (account id) to your Management Account ID on line 30 and (Region) to the Region on line 82 you are deploying in. Or, if you wish to deploy in the management account here is the link to the Code . You will only change the (Region) in the management account version.\n#!/usr/bin/env python3 #Gets org data, grouped by ous and tags from managment accounts in json import argparse import boto3 from botocore.exceptions import ClientError from botocore.client import Config import os import datetime import json def myconverter(o): if isinstance(o, datetime.datetime): return o.__str__() def list_tags(client, resource_id): tags = [] paginator = client.get_paginator(\u0026quot;list_tags_for_resource\u0026quot;) response_iterator = paginator.paginate(ResourceId=resource_id) for response in response_iterator: tags.extend(response['Tags']) return tags def lambda_handler(event, context): sts_connection = boto3.client('sts') acct_b = sts_connection.assume_role( RoleArn=\u0026quot;arn:aws:iam::(account id):role/OrganizationLambdaAccessRole\u0026quot;, RoleSessionName=\u0026quot;cross_acct_lambda\u0026quot; ) ACCESS_KEY = acct_b['Credentials']['AccessKeyId'] SECRET_KEY = acct_b['Credentials']['SecretAccessKey'] SESSION_TOKEN = acct_b['Credentials']['SessionToken'] # create service client using the assumed role credentials client = boto3.client( \u0026quot;organizations\u0026quot;, region_name=\u0026quot;us-east-1\u0026quot;, #Using the Organizations client to get the data. This MUST be us-east-1 regardless of region you have the Lamda in aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, aws_session_token=SESSION_TOKEN, ) root_id = client.list_roots()['Roots'][0]['Id'] ou_id_list = get_ou_ids(root_id, client) with open('/tmp/ou-org.json', 'w') as f: # Saving in the temporay folder in the lambda for ou in ou_id_list.keys(): account_data(f, ou, ou_id_list[ou][0], client) s3_upload('ou-org') with open('/tmp/acc-org.json', 'w') as f: # Saving in the temporay folder in the lambda account_data(f, root_id, root_id, client) s3_upload('acc-org') def account_data(f, parent, parent_name, client): tags_check = os.environ[\u0026quot;TAGS\u0026quot;] account_id_list = get_acc_ids(parent, client) for account_id in account_id_list: response = client.describe_account(AccountId=account_id) account = response[\u0026quot;Account\u0026quot;] if tags_check != '': tags_list = list_tags(client, account[\u0026quot;Id\u0026quot;]) #gets the lists of tags for this account for tag in os.environ.get(\u0026quot;TAGS\u0026quot;).split(\u0026quot;,\u0026quot;): #looking at tags in the enviroment variables split by a space for org_tag in tags_list: if tag == org_tag['Key']: #if the tag found on the account is the same as the current one in the environent varibles, add it to the data value = org_tag['Value'] kv = {tag : value} account.update(kv) account.update({'Parent' : parent_name}) data = json.dumps(account, default = myconverter) #converts datetime to be able to placed in json f.write(data) f.write('\\n') def s3_upload(file_name): bucket = os.environ[\u0026quot;BUCKET_NAME\u0026quot;] #Using environment variables below the Lambda will use your S3 bucket try: s3 = boto3.client('s3', '(Region)', config=Config(s3={'addressing_style': 'path'})) s3.upload_file( f'/tmp/{file_name}.json', bucket, f\u0026quot;organisation-data/{file_name}.json\u0026quot;) #uploading the file with the data to s3 print(f\u0026quot;{file_name}org data in s3\u0026quot;) except Exception as e: print(e) def ou_loop(parent_id, test, client): print(parent_id) paginator = client.get_paginator('list_children') iterator = paginator.paginate( ParentId=parent_id, ChildType='ORGANIZATIONAL_UNIT') for page in iterator: for ou in page['Children']: test.append(ou['Id']) ou_loop(ou['Id'], test, client) return test def get_ou_ids(parent_id, client): full_result = {} test = [] ous = ou_loop(parent_id, test, client) print(ous) for ou in ous: ou_info = client.describe_organizational_unit(OrganizationalUnitId=ou) full_result[ou]=[] full_result[ou].append(ou_info['OrganizationalUnit']['Name']) return full_result def get_acc_ids(parent_id, client): full_result = [] paginator = client.get_paginator('list_accounts_for_parent') iterator = paginator.paginate(ParentId=parent_id) for page in iterator: for acc in page['Accounts']: print(acc['Id']) full_result.append(acc['Id']) return full_result Under Configuration -\u0026gt; General Configuration edit Basic settings below:\nMemory: 512MB Timeout: 2min Click Save Scroll down to Environment variable and click Edit Add S3 Bucket environment variable:\nIn Key paste ‘BUCKET_NAME’ In Value paste your S3 Bucket name where the Organizations data should be saved If you wish to pull tags from your accounts as well, Add environment variable and add the below. If you don\u0026rsquo;t skip this:\nIn Key paste TAGS In Value paste list of tags from your Organization you would like to include separated by a comma Click Save\nScroll to the function code and click Deploy. Then Click Test. Enter an Event name of Test, click Create: Click Test\nThe function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in Organizations file size over time:\nGo to your S3 bucket and into the organisation-data folder and you should see a file of non-zero size is in it: Amazon CloudWatch Events Setup We will setup an Amazon CloudWatch Event to periodically run the Lambda functions, this will update the Organizations and include any newly created accounts.\nGo to the CloudWatch service page: Click on Events, then click Rules: Click Create rule For the Event Source\nSelect Schedule and set the required period Select 7 days Add the Lambda_Org_Data Lambda function Click Configure details\nAdd the name Lambda_Org_Data, optionally add a description and click Create rule: You have now created your lambda function to gather your organization data and place it into the S3 Bucket we made earlier. Using CloudWatch this will now run every 7 days updating the data.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/2_deploy_main_resources/","title":"Deploy core infrastructure for data retrieval","tags":[],"description":"","content":"Create Main Resources The first step is to create a set of reusable resources and respective data collection modules.\nClick Launch CloudFormation template if you are deploying to your Cost Optimization linked account (recommended) Or if you wish to keep this on your local machine please copy CloudFormation template locally and deploy with your preferred method of choice:\nClick Next. Call the stack OptimizationDataCollectionStack and fill Deployment parameters. The Role mentioned in Multi Account Role Name parameter will be deployed in the next step. Select the Code bucket for the region you are deploying in and fill Management Account Id. You have the option to change the name of your access roles, if you do please make the same changes in the Role for Management Account and the Read Only roles for Data Collector deployments.\nUnder available modules section select modules which you would like to deploy. Detailed description of each module can be found here Click Next and Next again When selecting Compute Optimizer module provide additionally a comma separted list of regions where need to collect Compute Optimizer data. Make sure you have a right to deploy S3 buckets in these regions.\nTick the boxes and click Create stack. Wait until your CloudFormation has a status of CREATE_COMPLETE. Troubleshooting Troubleshooting If you see the an issue with stack creation please check the status of nested stacks, including StackSets created by Compute Optimizer module. In case of rollback these StackSets will disappear.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_basic_identity_and_access_management_user_group_role/","title":"Creating your first Identity and Access Management User, Group, Role","tags":[],"description":"","content":"Last Updated: February 2021\nAuthor: Ben Potter, Security Lead, Well-Architected\nThis lab has been retired. It is strongly recommended you centralize your identities instead of using IAM Users. If you have more than a single test account for personal use, use AWS Single Sign-On or an identity provider configured in IAM, instead of IAM users. IAM users should not have access keys, for Command Line Interface (CLI) you should instead assume a role, or use integration with AWS Single Sign-on making it easy to get short term credentials for CLI use without needing to store long lived credentials. Use separate accounts for development/test and production, If you don’t have an existing organizational structure with AWS Organizations , AWS Control Tower is the easiest way to get started. For more information see Security Foundations and Identity and Access Management in the AWS Well-Architected security whitepaper.\n"},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/2_understand_metrics/","title":"Understanding Metrics","tags":[],"description":"","content":"Workloads can and should be designed to emit a variety of metrics such as application metrics (number of requests, error codes), infrastructure metrics (CPU, memory, disk usage), etc. These metrics should be tied back to KPI(s) that are used to measure a certain aspect of the business. Understanding different workload metrics will allow you to select the right metric to monitor, and understand the availability or status of your workload and its dependencies.\nIn this case, a Lambda function gets invoked after every data write using the S3 PutObject API call. Understanding metrics related to the Lambda function will provide insight on what the right metrics are to determine the health of the dependent service i.e. the external data write service.\n\u0026ldquo;AWS Lambda automatically monitors Lambda functions on your behalf and reports metrics through Amazon CloudWatch. To help you monitor your code as it executes, Lambda automatically tracks the number of requests, the execution duration per request, and the number of requests that result in an error. It also publishes the associated CloudWatch metrics and you can leverage these metrics to set CloudWatch custom alarms to enable automated responses to changes in metrics.\u0026rdquo; - https://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html AWS Lambda provides default metrics on CloudWatch across three categories:\nInvocation Metrics Performance Metrics Concurrency Metrics For the use-case in this lab, invocation metrics provided by Lambda will be used. The two key invocation metrics to focus on here are Invocations and Errors.\nMonitoring the Errors metric will provide visibility into function errors. Function errors include exceptions thrown by your code and exceptions thrown by the Lambda runtime. The runtime returns errors for issues such as timeouts and configuration errors. This is a valuable metric to identify issues with the function code, or configuration, as well as certain issues related to the external service.\nFor example, if the Lambda function is expecting the file written by the external service to be in a certain format, and if the written file does not match this format, it could result in a Lambda function error. Creating an alarm on Errors will provide visibility into the function execution.\nThere might be a situation where the external service is running at reduced capacity, some capabilities may be impaired, or it could be experiencing a complete outage with no data writes to S3. If no new files are being written to S3 monitoring for Lambda function errors will not suffice.\nIf the external service is no longer writing files to S3 at 50 second intervals, there would be no S3 notification, and subsequently, no Lambda invocations. As the Lambda function was never invoked, the Errors metric will show no change. A different approach is necessary.\nThe SLA with the external service expects new files to be written at 50 second intervals. The expected Lambda invocation rate is a minimum of 1 invocation per minute as a results of the S3 notifications. Now that a baseline has been established as to what the Invocations metric should look like, you can create an alarm to alert when the metric deviates from the baseline.\nGo to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Metrics\nIn the search bar under All metrics, enter the name of the data read function - WA-Lab-DataReadFunction and press enter\nIn the metric breakdown, select Lambda \u0026gt; By Resource and you will see a list of Lambda metrics available\nCheck the box for the metric Invocations and observe the graph, you will see all the function invocations\nChange the range by clicking on the custom option and selecting 15 Minutes\nChange the period to 1 Minute by going to the Graphed metrics tab\nYou should now see a graph that has a data point at 1 minute intervals representing a Lambda function invocation. Each invocation represents a write to the S3 bucket. By monitoring the number and frequency of lambda invocations, you are monitoring the availability of the external service. If the number of invocations drops below the expected value, there may be an issue with the external service.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/","title":"100 - Dependency Monitoring","tags":[],"description":"","content":"Author: Mahanth Jayadeva, Solutions Architect, Well-Architected\nIntroduction In this lab, you will become familiar with dependency monitoring and how to apply it to gain insights on resources that your workload depends on. You will learn how to create alarms and notifications to determine when a response is required.\nIt is important to design and configure your workload to emit information about the status (for example, reachability or response time) of resources it depends on. Examples of external dependencies can include, external databases, DNS, and network connectivity. By monitoring resources that your workload is dependent on, you will be able to quickly take action and ensure business continuity even when the dependent service is experiencing issues or downtime.\nIn this lab, you will create a CloudWatch alarm to monitor a dependency for a workload, and automate notifications so that your teams are aware of a potential impact to your workload due to a failing/degraded external dependency.\nThe skills you learn will help you define a dependency monitoring strategy in alignment with the AWS Well-Architected Framework Goals: Create alarms to monitor external dependencies Alert relevant stakeholders when outcomes are at risk due to a failed external dependency Learn how to automate this process Best Practices Covered: Implement dependency telemetry: Design and configure your workload to emit information about the status of resources it depends on. Examples of these are external databases, DNS, and network connectivity. Use this information to determine when a response is required.\nAlert when workload outcomes are at risk: Raise an alert when workload outcomes are at risk so that you can respond appropriately if required.\nEnable push notifications: Communicate directly with your users (for example, with email or SMS) when the services they use are impacted, and when the services return to normal operating conditions, to enable users to take appropriate action.\nRequirements An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps Deploy the Infrastructure Understanding Metrics Create an Alarm Test Fail Condition Bonus Content Tear down this lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/2_analyze_understand/","title":"Analyze and Understand Licensing","tags":[],"description":"","content":"Analyze and understand licensing costs in your workload We will analyze the CUR file for any software that includes licensing. First we will see what columns could give us the information we need. Then we look for the amount we are spending on items containing licenses, and how much we are spending on the actual licenses by comparing to a similar non-licensed option.\nWe take this information and decide if the effort required to make the change will be less than what we save, and if the required functionality is still met.\nThe queries below are for the provided data set created previously. If you use your own CUR you will need to modify these queries accordingly.\nAnalyze CUR columns The first step is to understand what information we may have that can show us costs associated with licenses.\nGo to the Athena Console:\nSelect the costmaster database, you should see the before and after tables\nWe want to see all the columns available, and some sample data, so paste the following command and click Run query:\nSELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; limit 10; You can see each column and 10 rows of data below. There are over 100 columns - take a minute or two and slowly scroll through them and view the data.\nThe supplied datasource has multiple workloads in it, we want to ensure we are only looking at costs for our ordering workload. There is a column at the far right resource_tags_user_application, this contains the tags we put on our resources. Lets limit our search to costs tagged with ordering. Run the query:\nSELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' limit 30 Now we will look for EC2 Operating System licenses. Look through the data and see if you can find a column which indicates the operating system used.\nThere are multiple columns we can use, look at the columns line_item_line_item_description and product_operating_system. Note: columns are close to alphabetical order.\nWe will focus on line_item_line_item_description, you can see Linux, Windows and RHEL. We will get the costs of RHEL licensing and compare it to a similar operating system, AWS Linux. To get a sample of the RHEL costs, paste the following query and click Run query:\nSELECT * FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' limit 30 We will now get the costs for each hour for running RHEL. We dont need 100+ columns, so we\u0026rsquo;ll focus on just the following columns: line_item_usage_start_date, linte_item_line_item_description, line_item_unblended_cost. We will sum the unblended_cost column, and group it by usage_start_date, this will give us cost per hour. The order by line orders the output by date. Run the following query:\nSELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see that the cost per hour of running the RHEL instances was 0.8128000000000001 per hour. To get the pricing for a non-licensed operating system we will need the instance size and location. We will use the line_item_usage_type and line_item_availability_zone columns to get this, run the following query:\nSELECT line_item_usage_type, line_item_availability_zone, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_type, line_item_availability_zone limit 30 Look at the columns: line_item_usage_type, line_item_availability_zone, you will see the values BoxUsage:t3.medium and us-east-1a/b:\nWe see that we are running t3.medium RHEL instances in us-east-1. Open the pricing pages in a new tab: https://aws.amazon.com/ec2/pricing/on-demand/ Pricing is correct as of August 2020, we may have had a price drop since then and the prices below may be higher than what is currently in the console. Please disregard and proceed through the lab.\nEnsure the Linux tab is selected, and the region is US East (N. Virginia). The pricing for a t3.medium with AWS Linux is: $0.0416 per hour. Now click the RHEL tab:\nThe pricing will change, the price for RHEL is: $0.1016.\nLets see exactly how many hours of usage we were running in total (line_item_usage_amount), and the price. In the Athena console paste the following query:\nSELECT line_item_line_item_description, sum(line_item_usage_amount) usage_hours, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_line_item_description You can see we consumed 176 instance hours of licensed RHEL ($0.1016 an hour), for a total cost of $17.8815999\u0026hellip;.\nOur sample CUR is less than a month, to get monthly costs - lets see how many instances we are running with RHEL. We will get the resource IDs and how long they ran for with the following query. In the Athena Console paste the following:\nSELECT distinct line_item_resource_id, sum(line_item_usage_amount) as Hours_ran FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_resource_id We are running 8 instances of RHEL at a cost of $0.1016 per hour. If we changed these to AWS Linux at a cost of $0.0416 per hour, we would save $0.06 each hour, per instance.\n8 Instances at saving of $0.06 per hour, for an average of 730 hours each month, is a monthly saving of $350.4.\nAs with all optimization opportunities, weigh up the savings gained from making the change, against the effort required to change. Is $350 each month worth making the change? How long before you pay off your effort? How long will the workload run? What if your instance usage increases or decreases - does the answer change?\nUnderstand the costs of running licensed software in your workload There can be associated costs with running licensed software. Additional resources may be mandated by the software in addition to a base configuration. We will continue our example by looking at operating systems.\nWe will now see if additional resources are required to run RHEL compared to AWS Linux, and the cost of these additional resources.\nIf you do not have the additional privileges the EC2 console you will have to read through first 10 steps below.\nGo into the EC2 console, and click Launch Instance, then Launch instance\nSelect the Amazon Linux 2 AMI\nSelect a t3.medium instance size, and click Next: Configure Instance Details\nClick Next: Add Storage\nYou can see that the default configuration is for 8GiB GP2 of EBS storage. Click Cancel. From the EC2 Console click Launch Instance, then Launch instance\nSelect the Red Hat Enterprise Linux AMI:\nSelect a t3.medium instance and click Next: Configure Instance Details:\nClick Next: Add Storage\nYou will see that there is 10GiB GP2 of storage required for RHEL as a default. Click Cancel. Confirm how much storage we are using and the cost:\nSELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc limit 5 Each hour we are consuming 0.111111 for a cost of $0.011111.\nYou need an additional 2Gb of GP2 storage per instance. Go to the EBS pricing page here: https://aws.amazon.com/ebs/pricing/ The price in US East(N. Virginia) is $0.1 per GB-month for SSD gp2 volumes. We will now calculate the storage savings across our workload.\nLets see how many instances we are running with RHEL. We will get the resource IDs and how long they ran for with the following query. In the Athena Console paste the following:\nSELECT distinct line_item_resource_id, sum(line_item_usage_amount) as Hours_ran FROM \u0026quot;costmaster\u0026quot;.\u0026quot;before\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_resource_id We are running 8 instances, which each require an additional 2GB GP2. This is 16GB-month of storage (at $0.10 per GB-month), which is an additional cost of $1.60 each month on our workload.\nWe have total additional monthly costs due to licensing of $350.4 (licenses) + $1.60 (storage) = $352\nWe have total monthly savings due to licensing. Lets think long term, whats the impact if our workload grows over time? whats the saving?\nSimulate the change and validate We have performed the change in our environment and created new Cost and Usage Reports. We will now analyze the after table which contains these changes.\nLets see the amount and cost of our new AWS Linux instances, and the price. In the Athena console paste the following query:\nSELECT distinct line_item_resource_id, line_item_line_item_description, sum(line_item_usage_amount) as Hours_ran, sum(line_item_unblended_cost) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;after\u0026quot; where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%t3.medium%' and resource_tags_user_tier like 'front' group by line_item_resource_id, line_item_line_item_description You can see we ran 8 Linux instances in a sample of 23hrs at a cost of $0.09568 for each instance.\nThe total cost for 8 instances over 730 hours ($0.9568 / 23 * 730 * 8), would be $242.944. Our original cost was $593.344, and we predicted a saving of $350.4, our final cost of $242.944 verifies this prediction.\nLets analyze the storage with the following query:\nSELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \u0026quot;costmaster\u0026quot;.\u0026quot;after\u0026quot; where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc limit 5 You can see there was an hourly amount of 0.088888 and an hourly cost of $0.008888, which is 8/10 of the original amount of 0.111111 and cost of 0.011111, which validates changing to 8Gb volumes from 10Gb volumes. This validates our $1.60 saving prediction.\nIn this step you discovered your Licensing costs, and associated additional resource costs. You analyzed them to make sure it was worth the effort to make a change, then verified the savings after making (simulating) the change.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/contributing/02_reportingbugs/","title":"Reporting Bugs &amp; Feature Requests","tags":[],"description":"","content":"Please use the GitHub issue tracker to report bugs or suggest features.\nWhen filing an issue, please check existing open or recently closed issues to make sure somebody else hasn\u0026rsquo;t already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\nA label with the Well-Architected framework pillar (i.e. COST) A reproducible test case or series of steps The version of our code being used Any modifications you have made relevant to the bug Anything unusual about your environment or deployment "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/application_integration/","title":"Application Integration","tags":[],"description":"","content":"These are queries for AWS Services under the Application Integration product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon MQ Amazon SES Amazon SNS Amazon SQS Top 20 Daily Unblended Costs Amazon SQS By Product Family Amazon MQ Query Description This query will provide daily unblended and amortized cost as well as usage information per linked account for Amazon MQ. The output will include detailed information about the resource id (broker), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon MQ pricing page .\nThis query will not run against CUR data that does not have any Amazon MQ usage.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, product_broker_engine, line_item_usage_type, product_product_family, pricing_unit, pricing_term, SPLIT_PART(line_item_usage_type, \u0026#39;:\u0026#39;, 2) AS split_line_item_usage_type, SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 7) AS split_line_item_resource_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_operation, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon MQ\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, product_broker_engine, product_product_family, pricing_unit, pricing_term, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, line_item_resource_id, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC, split_line_item_usage_type; Help \u0026amp; Feedback Back to Table of Contents Amazon SES Query Description This query will provide daily unblended and usage information per linked account for Amazon SES. The output will include detailed information about the product family (Sending Attachments, Data Transfer, etc\u0026hellip;) and usage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon SES pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_product_family, CASE WHEN line_item_usage_type LIKE \u0026#39;%%DataTransfer-In-Bytes%%\u0026#39; THEN \u0026#39;Data Transfer GB (IN) \u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%DataTransfer-Out-Bytes%%\u0026#39; THEN \u0026#39;Data Transfer GB (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%AttachmentsSize-Bytes%%\u0026#39; THEN \u0026#39;Attachments GB\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%Recipients\u0026#39; THEN \u0026#39;Recipients\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%Recipients-EC2\u0026#39; THEN \u0026#39;Recipients\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%Recipients-MailboxSim\u0026#39; THEN \u0026#39;Recipients (MailboxSimulator)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%Message%%\u0026#39; THEN \u0026#39;Messages\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%ReceivedChunk%%\u0026#39; THEN \u0026#39;Received Chunk\u0026#39; ELSE \u0026#39;Others\u0026#39; END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Simple Email Service\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_product_family, 5 --refers to case_line_item_usage_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon SNS Query Description This query will provide daily unblended cost and usage information per linked account for Amazon SNS. The output will include detailed information about the product family, API Operation, and usage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon SNS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, CONCAT(product_product_family,\u0026#39; - \u0026#39;,line_item_operation) AS concat_product_product_family, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Simple Notification Service\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), CONCAT(product_product_family,\u0026#39; - \u0026#39;,line_item_operation) ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon SQS Top 20 Daily Unblended Costs Query Description This query will provide the top 20 daily unblended costs as well as usage information for a specified linked account for Amazon SQS. The output will include detailed information about the resource id (queue), usage type, and API operation. The cost will be summed and in descending order. This is helpful for tracking down spikes in cost for SQS usage. Cost Explorer will provide you all of this information except the resource ID. This allows your investigation to be targeted to a time range, linked account, API operation, and resource that is generating the usage.\nPricing Please refer to the Amazon SQS pricing page . Please refer to the Reducing Amazon SQS costs page and Enabling client-side buffering and request batching for Cost Optimization suggestions.\nSample Output Download SQL File Link to Code Copy Query SELECT line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_usage_type, line_item_operation, line_item_resource_id, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AWSQueueService\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, line_item_operation, line_item_resource_id ORDER BY sum_line_item_unblended_cost DESC LIMIT 20; Help \u0026amp; Feedback Back to Table of Contents Amazon SQS By Product Family Query Description This query will provide daily unblended cost and usage information for Amazon SQS, grouped by account and operation. The operation is also grouped using product_product_family, which results in values such as \u0026ldquo;Data Transfer - Receive\u0026rdquo; and \u0026ldquo;API Request - SendMessageBatch\u0026rdquo;. Output is ordered by date, then by cost (descending). This can be used to identify specific API operations driving the most daily cost, which allows for targeted investigation into optimization opportunities.\nPricing Please refer to the Amazon SQS pricing page . Please refer to the Reducing Amazon SQS costs page and Enabling client-side buffering and request batching for Cost Optimization suggestions.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, CONCAT(product_product_family,\u0026#39; - \u0026#39;,line_item_operation) AS concat_product_product_family, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter}} AND line_item_product_code = \u0026#39;AWSQueueService\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), CONCAT(product_product_family,\u0026#39; - \u0026#39;,line_item_operation) ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/2_modify_cost_intelligence/","title":"Modify Cost Intelligence Dashboard","tags":[],"description":"","content":" This Lab has moved under the Level 200 Cloud Intelligence Dashboards.Click this link to navigate to the updated Lab Authors Alee Whitman, Commercial Architect (AWS) Add Account Mapping Data This section is optional and shows how you can add your Business Unit or Enterprise Account mapping data to your dashboard using Account Id as the identifier.\nThis example will show you how to replace the AccountID, with a name that is meaningful to your organization.\nCreate an account mapping file locally, you can use the sample here as a starting point: mapping_document.csv In QuickSIght, select the summary_view Data Set\nSelect Edit data set\nSelect Add Data: Select upload a file Navigate to your mapping data file and click Open\nConfirm the mappings are correct, click Next: Select the two circles to open the Join configuration then select Left to change your join type: Create following join clause then click Apply:\nlinked_account_id = (Your linked Account Id field name) Scroll down in the field list, and confirm the new fields have the correct data types. The Account ID must be String:\nSelect Save\nRepeat steps 2-11, creating mapping joins for your remaining QuickSight data sets:\ns3_view ec2_running_cost compute_savings_plan_eligible_spend Go to the Cost Intelligence Analysis\nEdit the calculated field Account: Change the formula from {linked_account_id} to {Account Name} You can now select a visual, select the Account field, and you will see the account names in your visuals, instead of the Account number: Customize your Summary View Cost value This section is optional and shows how you can customize the analysis from using Amortized Cost to using Unblended/Invoiced Cost.\nFrom the QuickSight Analysis dashboard, click on the Cost Intelligence Analysis Edit the calculated field Cost: Change the formula from {Cost_Amortized} to {Cost_Unblended}: If you have usage with upfront charges - such as partial or full upfront Savings Plans or Reserved Instances, you will notice changes in the cost values of your dashboards.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_vpc/1_create_vpc_stack/","title":"Create VPC Stack","tags":[],"description":"","content":" This step will create the VPC and all components using the example CloudFormation template.\nDownload the latest version of the CloudFormation template here: vpc-alb-app-db.yaml Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: vpc-alb-app-db.yaml Click Next\nFor Stack name use WebApp1-VPC\nParameters\nLook over the Parameters and their default values.\nLeave all parameters as their default values unless you are experimenting.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nNow that you have a new VPC, check out 200_Automated_Deployment_of_EC2_Web_Application to deploy an example web application inside it.\n"},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/","title":"Level 100: Calculating differences in clock source","tags":[],"description":"How various linux clock sources can affect the performance of your application on EC2","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction This hands-on lab will teach you the fundamentals of how various linux clock sources can affect the performance of your application on EC2. AWS has introduced new capabilities within our Nitro system on certain instance types, which takes advantage of better clock timing as well as offloading many other hypervisor related tasks.\nIn this lab, you will deploy three distinct EC2 instances running Amazon Linux, each configured with a different instance type and all with SSM enabled. You will also deploy a set of SSM documents, which will be used to enable and disable various clock timing changes to the machines. Lastly, a set of test scripts will be deployed which allow you to see the differences as the clock changes are made.\nThe skills you learn will help you learn the various clock sources available in EC2, as well as ways to test those changes for your applications in alignment with the AWS Well-Architected Framework.\nFor a list of Nitro-based instances currently available, head to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances for the latest list.\nGoals Deploy EC2 instances on both Nitro and non-nitro backed hypervisors Deploy SSM documents to allow for modifications to the Linux clock source Learn how SSM documents can be used to apply configuration changes to your Linux instances, such as changing the clock source Learn how to use SSM session manager to gain shell access to run your own test against the various instance types Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create an IAM Role, instance profiles, and EC2 instances. An IAM user or federated credentials into that account that has permissions to access AWS System Manager Costs https://aws.amazon.com/ec2/pricing/on-demand/ This lab will create 2 EC2 instances in the default VPC The cost per hour to run this lab would be $0.196 ($4.704/day) if you accept the default parameters in us-east-1 Refer to the link above for pricing if you deploy in a different region There is no additional cost for the AWS Systems Manager run commands used during the lab. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploying the infrastructure Testing Performance of both nodes before clock changes Changing clock type on the Xen based EC2 instance Teardown "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/2_testing_before/","title":"Testing Performance of both nodes before clock changes","tags":[],"description":"","content":"Now that we have two EC2 machines, one Xen based EC2 and one Nitro/KVM based EC2, we can run a simple test to see the speed in which is can return time of day. This test program was installed on each machine under /tmp/time_test.py. The program will simply request the time of day from a local c-library one million times.\nThe time_test.py code #!/usr/bin/python3 import time _gettimeofday = None def gettimeofday(): import ctypes global _gettimeofday if not _gettimeofday: _gettimeofday = ctypes.cdll.LoadLibrary(\u0026#34;libc.so.6\u0026#34;).gettimeofday class timeval(ctypes.Structure): _fields_ = \\ [ (\u0026#34;tv_sec\u0026#34;, ctypes.c_long), (\u0026#34;tv_usec\u0026#34;, ctypes.c_long) ] tv = timeval() _gettimeofday(ctypes.byref(tv), None) return float(tv.tv_sec) + (float(tv.tv_usec) / 1000000) start_time = time.time() for x in range(0,1000000): gettimeofday() print(\u0026#34;--- %s seconds ---\u0026#34; % (time.time() - start_time)) print(\u0026#34;Done\u0026#34;) If you wish to bypass using the pre-defined AWS System Manager documents below, you can also run this script interactively on each EC2 node using AWS Systems Manager Session Manager .\nOpen the AWS Console (https://console.aws.amazon.com ) In the “Find Services” search bar, type “Systems Manager” and press enter Under “Instances \u0026amp; Nodes” click on “Run Command” and on the left side of the screen again click on “Run a command” In the search bar under Command Document, select “Owner” from the pulldown and then select “Owned by me”. You should see 2 SSM documents that have been created for you by the Cloudformation Template. Click on the one with “runTestScriptdocument” in the name. Scroll down and under “Targets”, select the checkbox next to the two instances that were created from the template. The should be labeled “XenTimeInstanceTest” and “KVMTimeInstanceTest” Scroll down and uncheck the box that says “Enable writing to an S3 bucket” and then click the “Run” button at the bottom of the screen. You should see the command running on both nodes as “In Progress”. Click the refresh button every few seconds until both boxes show “Success” in their status column. It will take longer to complete for one node to run than the other. The total time for both to complete should be about 2 minutes if you use the default EC2 instance types.\nYou can now click on each Instance ID to see the output of the command that was run. Just click the pulldown for “Step 1 - Output” and you should see the following. As you noticed, it tells you at the top of the output how long the script took to run in total, as well as the syscalls used during its execution. In this first example, we can see it ran in 16 seconds and the stat call is the most frequently used. In this second instance, we can see that it took 110 seconds and the gettimeofday call used over 99% of the time during its run. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/2_assume_roles_iam_user/","title":"Assume Roles from an IAM user","tags":[],"description":"","content":"We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user.\n2.1 Use Restricted Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don\u0026rsquo;t sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour.\nImportant\nThe permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com .\nIn the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias. Alternatively you can paste the link in your browser that you recorded earlier.\nClick Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again.\nOn the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator.\n(Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color.\nClick Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you.\nTip\nThe last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.\nYou are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role\u0026rsquo;s Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.\n2.2 Use Restricted Administrator Role in Command Line Interface (CLI) Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/2_basic_iam/","title":"Basic Identity and Access Management User, Group, Role","tags":[],"description":"","content":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nWalkthrough Basic Identity and Access Management User, Group, Role "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/2_identity/","title":"Centralize Identities","tags":[],"description":"","content":"Every user must leverage unique credentials so we can trace actions within your accounts. Setup your identity structure in the management account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give permission to perform IAM actions as they can create their own permissions.\nControl Tower sets up your landing zone to leverage AWS Single Sign-On as a central place for your users to log on and access AWS accounts. In this step we will federate that access to your existing identity store.\nWalk through In your existing AWS account perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Configure AWS SSO to federate identity . If you are not using SSO you can still federate Identity leveraging a SAML provider and then use cross account access roles to access the accounts that we setup in step 1. "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/2_config_cloudfront/","title":"Configure Amazon CloudFront","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created.\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation.\nYou have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/2_config_waf/","title":"Configure AWS WAF","tags":[],"description":"","content":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront.\nSign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack. Enter the following Amazon S3 URL: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use waf. WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1. WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1. The remainder of the parameters can be left as defaults. At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, click Create stack. After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for CloudFront to use! "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/2_configure_replication/","title":"Configure bi-directional cross-region replication (CRR) for S3 buckets","tags":[],"description":"","content":"Amazon S3 replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can copy objects between different AWS Regions or within the same Region. You will setup bi-directional replication between S3 buckets in two different regions, owned by the same AWS account.\nReplication is configured via rules. There is no rule for bi-directional replication. You will however setup a rule to replicate from the S3 bucket in the east AWS region to the west bucket, and you will setup a second rule to replicate going the opposite direction. These two rules will enable bi-directional replication across AWS regions.\n2.1 Setup rule #1 to replicate objects from east bucket to west bucket Go to the Amazon S3 console Click on the name of the east bucket\nif you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Click on the Management tab (Step A in screenshot)\nClick Replication (Step B in screenshot)\nClick + Add Rule (Step C in screenshot)\nFor Set source select Entire bucket\nFor Replication criteria leave Replicate objects encrypted with AWS KMS not selected\nOur objects are encrypted using server-side encryption However since you used SSE-S3 encryption, you do not need to select this option and do not need to provide a KMS key SSE-S3 uses KMS keys, but these managed by Amazon S3 for the user For more detail see What Does Amazon S3 Replicate? Click Next\nFor Destination bucket leave Buckets in this account selected, and select the name of the west bucket from the drop-down\nIf you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Troubleshooting: If you get an error saying The bucket doesn’t have versioning enabled then you have chosen the wrong bucket. Double check the bucket name. Click Next\nFor IAM Role select \u0026lt;your-naming-prefix\u0026gt;-S3-Replication-Role-us-east-2 from the search results box\n(If you chose a different region as your east region, then look for that region at the end of the IAM role name) For Rule name enter east to west\nLeave Status set to enabled\nClick Next\nReview the configuration\nClick Save\nThe screen should say Replication configuration updated successfully. and display the Source, Destination, and Permissions of your replication rule\n2.2 Test replication rule #1 - replicate object from east bucket to west bucket To test this rule you will upload an object into the east bucket and observe that it is replicated into the west bucket. For this step you will need a test object:\nThis is a file that you will upload into the east S3 bucket. It should not be too big, as this will increase the time to upload it from your computer. If you do not have a file to use, you can download this file . Right-click and Save image as\u0026hellip; Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner\nClick on the name of the east bucket\nif you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Click on ⬆ Upload\nUpload the file you will use as an object\nDrag and drop the file or click Add files Click Upload (note there is a Next button, but you do not need to click it) When the file is finished uploading, click on the filename\nIt will look like the left side of the screenshot below If Replication status is PENDING, wait and refresh until it says COMPLETED which should be just a few seconds. At the top of the console click on Amazon S3 and then click on the name of the west bucket\nIf you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Click on the filename of the file that you just uploaded to the other bucket (yes, it is here now too!)\nIt will look like the right side of the screenshot below Note the following in from the object details:\nReplication status: Note the different values for the source (east) and destination (west) S3 buckets. The value REPLICA in the west bucket is part of the solution how the system recognizes it should not replicate this object back again to the east bucket, which would cause an infinite loop. Server-side encryption: The object was encrypted in the source (east) bucket, and remains encrypted in the destination (west) bucket. 2.3 Setup rule #2 to replicate objects from west bucket to east bucket After setting up the second rule, you will have completed configuration of bi-directional replication between our two Amazon S3 buckets.\nGo to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner Click on the name of the west bucket if you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Click on the Management tab Click Replication Click + Add Rule For Set source select Entire bucket For Replication criteria leave Replicate objects encrypted with AWS KMS not selected Our objects are encrypted using server-side encryption However since you used SSE-S3 encryption, you do not need to select this option and do not need to provide a KMS key SSE-S3 uses KMS keys, but these managed by Amazon S3 for the user For more detail see What Does Amazon S3 Replicate? Click Next For Destination bucket leave Buckets in this account selected, and select the name of the east bucket from the drop-down If you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Troubleshooting: If you get an error saying The bucket doesn’t have versioning enabled then you have chosen the wrong bucket. Double check the bucket name. Click Next For IAM Role select \u0026lt;your-naming-prefix\u0026gt;-S3-Replication-Role-us-west-2 from the search results box (If you chose a different region as your west region, then look for that region at the end of the IAM role name) For Rule name enter west to east Leave Status set to enabled Click Next Review the configuration Click Save The screen should say Replication configuration updated successfully. and display the Source, Destination, and Permissions of your replication rule\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/2_configure_ecs_repository_and_deploy_application_stack/","title":"Configure ECS Respository and Deploy The Application Stack","tags":[],"description":"","content":"2.0. Introduction. In this section, we are going to prepare our sample application. We will package this as a docker image and push to a repository.\nAs we mentioned in our introduction, our sample application will be running in a docker container which we will deploy using Amazon ECS .\nIn preparation for the deployment, we will need to package our application as a docker image and push it into ECR . When this is completed, we will use the image which we placed in ECR to build our application cluster.\nFor more information on how ECS works, please refer to this guide .\nWhen our application stack is completed, our architecture will look like this:\nMove through the sections below to complete the repository configuration and application stack deployment:\n2.1. Configure the ECS Container Repository. Our initial sample application preparation will require running several docker commands to create a local image in your computer which we will push into Amazon ECR. The following diagram shows the image creation process:\nTo make this process simple for you, we have created a basic application and script to build the container for you.\nComplete the instructions as follows to download the :\n2.1.1. Our environment used to push the container to the repository will need to be running Docker version 18.09.9 or above. To achieve this on a laptop such as a mac involves installing not only Docker itself, but also the Docker Machine and VirtualBox. Because of this, we will use AWS Cloud9 IDE in the console which has all of the dependencies pre-configured for you.\n2.1.2. From the main console, launch the Cloud9 service.\nWhen you get to the welcome screen, select Create Environment as shown here:\n2.1.3. Now we will enter naming details for the environment. To do this enter the following into the name environment dialog box:\nName: pattern1-environment Description: re:Invent 2020 pattern1 environment!\nWhen you are ready, click on Next Step to continue as shown:\n2.1.4. On the Configure Settings dialog box, leave defaults and click Next Step\n2.1.5. On the Review dialog box, click Create Environment\nThe Cloud9 IDE environment will now build, integrating the AWS command line and all docker components that we require to build out our lab.\nThis step can take a few minutes, so please be patient.\n2.1.6. Once our environment is built, you will be greeted with a command prompt to your environment.\nWe will use this to build our application for upload to the repository.\nFirstly we will need to download the files which contain all of the application dependencies. To do this, run the following command within the Cloud9 IDE:\ncurl -o sample-app.zip https://d3h9zoi3eqyz7s.cloudfront.net/Security/sample_app.zip The command should show the file download as follows:\n2.1.7. When you have downloaded the application, unzip it as follows:\nunzip sample-app.zip 2.1.8. Now we will build our application and upload to the repository. We have built a script to help you with this process, which will query the previous CloudFormation stack which you created for the necessary repository information, build an image and then upload to the new repository.\nExecute the script with the argument of pattern1-base as follows:\ncd app/ ./build-container.sh pattern1-base Once your command runs successfully, you should be seeing the image being pushed to ECR and URI marked as shown here:\nTake note of the ECS Image URI produced at the end of the script as we will require it later. This is highlighted in the screenshot above.\n2.1.9. Confirm that the ECR repository exists in the ECR console. To do this, launch ECR in your AWS Console.\nYou can then follow this guide to check to your repository as shown:\n2.2. Deploy The Application Stack Now that we have pushed the docker image into our Amazon ECR repository, we will now deploy it within Amazon ECS .\nOur sample application is configured as follows:\nOur application is built using nodejs express ( You can find the source code under app/app.js file of the github repository ) The service will expose a REST API wth /encrypt and /decrypt action. The /encrypt will take an input of a JSON payload with key and value as below '{\u0026quot;Name\u0026quot;:\u0026quot;Andy Jassy\u0026quot;,\u0026quot;Text\u0026quot;:\u0026quot;Welcome To ReInvent 2020!\u0026quot;}' The Name Key will be the identifier that we will use to store the encrypted value of Text Value. The application will then call the KMS Encrypt API and encrypt it again using a KMS key that we designate. (For simplicity, in this mock app we will be using the same KMS key for every Name you put in, ideally you want to use individual key for each name) The encrypted value of Text key will then be stored in an RDS database, and the app will return a Encryption Key value that the user will have to pass on to decrypt the Text later The decrypt API will do the reverse, taking the Encryption Key you pass to decrypt the text {\u0026quot;Text\u0026quot;:\u0026quot;Welcome To ReInvent 2020!\u0026quot;} Note: In this section we will be deploying a CloudFormation Stack which will launch an ECS cluster. If this is the first time you are working with the ECS service, you will need to deploy a service linked role which will be able to assume the IAM role to perform the required activities within your account. To do this, run the following from the command line using appropriate profile flags: aws iam create-service-linked-role --aws-service-name ecs.amazonaws.com Download the application template from here and deploy according to your preference below.\nClick here for CloudFormation command-line deployment steps Command Line: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\n2.2.1. Execute below command to create the application stack. Ensure that you pass the ECR Image URI you noted at the end of section 1.2 as follows:\naws cloudformation create-stack --stack-name pattern1-app \\ --template-body file://pattern1-app.yml \\ --parameters ParameterKey=BaselineVpcStack,ParameterValue=pattern1-base \\ ParameterKey=ECRImageURI,ParameterValue=\u0026lt;ECR Image URI\u0026gt; \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-2 Note: Our example below shows sample arguments passed into the command for your reference:\naws cloudformation create-stack --stack-name pattern1-app \\ --template-body file://pattern1-app.yml \\ --parameters ParameterKey=BaselineVpcStack,ParameterValue=pattern1-base \\ ParameterKey=ECRImageURI,ParameterValue=111111111111.dkr.ecr.ap-southeast-2.amazonaws.com/pattern1appcontainerrepository-cu9vft86ml5e:latest \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-2 Click here for CloudFormation console deployment steps Console: If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\n2.2.1. Follow this guide for information on how to deploy the cloudformation template via the console.\n2.2.2. Enter the following details into the stack details:\nUse pattern1-app as the Stack Name. Use pattern1-base as the BaselineVpcStack. Use the URI which you recorded in the application build as the ECRImageURI An example would be as follows:\nWhen you are ready, click next to continue.\n2.2.3. On the Configure Stack Options click Next\n2.2.4. On the Review pattern1-app click Create Stack.\nNote Dont forget to tick the Capabilities acknowledgement at the bottom of the screen.\n2.3. Confirm Stack Status. 2.3.1. Once the command deployed successfully, go to your Cloudformation console to locate the stack named pattern1-app.\n2.3.2. Confirm that the stack is in a \u0026lsquo;CREATE_COMPLETE\u0026rsquo; state.\n2.3.3. Record the following output details as they will be required later:\nTake note of this stack name Take note of the DNS value specified under OutputPattern1ApplicationEndpoint of the Outputs. Take note of the ECS Task Role Arn value specified under OutputPattern1ECSTaskRole of the Outputs. Take note of the OutputPattern1ECSTaskRole. The following diagram shows the output from the cloudformation stack:\n2.4. Test the Application launched. In this part of the Lab, we will be testing the encrypt API of the sample application we just deployed. Our application will basically take a JSON payload with Name and Text key, and it will encrypt the value under text key with a designated KMS key. Once the text is encrypted, it will store the encrypted text in the RDS database with the Name as the primary key.\nNote: For simplicity our sample application is not generating individual KMS keys for each record generated. Should you wish to deploy this pattern to production, we recommend that you use a separate KMS key for each record.\nFrom your Cloud9 terminal, replace the \u0026lt; Application Endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint from previous step.\nALBURL=\u0026#34;\u0026lt; Application Endpoint URL \u0026gt;\u0026#34; curl --header \u0026#34;Content-Type: application/json\u0026#34; --request POST --data \u0026#39;{\u0026#34;Name\u0026#34;:\u0026#34;Andy Jassy\u0026#34;,\u0026#34;Text\u0026#34;:\u0026#34;Welcome to ReInvent 2020!\u0026#34;}\u0026#39; $ALBURL/encrypt Once you\u0026rsquo;ve executed this you should see an output similar to this:\n{\u0026#34;Message\u0026#34;:\u0026#34;Data encrypted and stored, keep your key save\u0026#34;,\u0026#34;Key\u0026#34;:\u0026#34;\u0026lt;encrypt key (take note) \u0026gt;\u0026#34;} Take note of the encrypt key value under Key from your output as we will need it for decryption later in the lab.\nThis completes section 2 of the lab. Proceed to section 3 where we will be configuring CloudTrail.\nEND OF SECTION 2\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/2_configure_env/","title":"Configure Execution Environment","tags":[],"description":"","content":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed.\nYou have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided.\n2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes\nCredentials\nAWS access key AWS secret access key AWS session token (used in some cases) Configuration\nRegion: us-east-2 (or region where you deployed your WebApp) Default output: JSON Note: us-east-2 is the Ohio region\nIf you already know how to configure these, please do so now and proceed to the next step 2.2 Set up the bash environment If you need help then follow the instructions in either Option A or Option B below Option A - AWS CLI This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option B\nTo see if the AWS CLI is installed:\n$ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option B Run aws configure and provide the following values:\n$ aws configure AWS Access Key ID [*************xxxx]: \u0026lt;Your AWS Access Key ID\u0026gt; AWS Secret Access Key [**************xxxx]: \u0026lt;Your AWS Secret Access Key\u0026gt; Default region name: [us-east-2]: us-east-2 (or your chosen region) Default output format [None]: json Option B - Manually creating credential files If you already did Option A, then skip this\ncreate a .aws directory under your home directory\nmkdir ~/.aws Change directory to there\ncd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials. In this file you should have the following text.\n[default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; Create a text file (no extension) named config. In this file you should have the following text:\n[default] region = us-east-2 (or your chosen region) output = json 2.2 Set up the bash environment Click here for instructions if using bash: Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section .\nPrerequisites\nawscli AWS CLI installed\nIf you already installed AWS CLI as part of the AWS credentials and configuration setup, you can skip this and proceed to installing jq\n$ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine\nIf you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed.\n$ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the fail_instance.sh script from the resiliency bash scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script:\nbash/fail_instance.sh Set the script to be executable.\nchmod u+x fail_instance.sh 2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) Choose the appropriate section below for your language\nClick here for instructions if using Python: The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the fail_instance.py from the resiliency Python scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script:\npython/fail_instance.py Click here for instructions if using Java: The command line utility in Java requires Java 8 SE.\n$ java -version openjdk version \u0026quot;1.8.0_222\u0026quot; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7.\nFor Amazon Linux and RedHat\n$ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu\n$ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B\nOption A: If you are comfortable with git\nClone the aws-well-architected-labs repo\n$ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory\ncd static/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B:\nDownload the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade\ncd target - this is where your jar files were built and where you can run from the command line\nClick here for instructions if using C#: Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.\nClick here for instructions if using PowerShell: If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the fail_instance.sh script from the resiliency PowerShell scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script:\npowershell/fail_instance.sh If your PowerShell script is refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/2_account_structure/","title":"Create an account structure","tags":[],"description":"","content":"NOTE: Do NOT do this step if you already have an organization and consolidated billing setup.\nYou will create an AWS Organization, and join two or more accounts to the management account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a management account that is used for security and administration, with access provided for limited billing tasks. A dedicated member account will be created for the Cost Optimization team or function, and another (or multiple) member account/s created to contain workload resources.\nYou will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you join a member account to a management account, it will contain all billing information for that member account. Member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data before joining accounts to a management account.\nCreate an AWS Organization You will create an AWS Organization with the management account.\nLogin to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations: Click on Create organization: You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to.\nJoin member accounts You will now join other accounts to your organization. You need to create and join an account that will be used to perform Cost Optimization work, as well as other member accounts used to run workloads.\nFrom the AWS Organizations console click on Add account: Click on Invite an existing AWS Account: Enter in the Email or account ID, enter in any relevant Notes and click Send invitation: You will then have an open request: Log in to your member account, and go to AWS Organizations: You will see an invitation in the menu, click on Invitations: Verify the details in the request (they are hidden here), and click on Accept: You are shown that the account is now part of your organization: The member account will receive an email showing success: The management account will also receive email notification of success: Repeat the steps above for each additional member account in your organization.\nEnable Service Control Policies We will enable Service control policies, which offer central control over the maximum permissions available - for cost governance, and Tag policies which assist to standardize tags across your organization.\nFrom the AWS Organizations console in the management account click on Policies: By default both policies are disabled, Click on Service control policies: Click on Enable service control policies: You will see Service control policies are enabled, click Policies: We will enable tag policies, Click Tag policies: Click Enable tag policies: You will see Tag policies are enabled, click Policies: You will now see that both policies are enabled: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/2_ec2_restrict_region/","title":"Create an IAM Policy to restrict service usage by region","tags":[],"description":"","content":"To manage costs you need to manage and control your usage. AWS offers multiple regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer.\nWe will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.\nCreate the Region restrictive IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy: Click the JSON tab: Copy and paste the policy into the console: IAM Policy { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:*\u0026quot;, \u0026quot;rds:*\u0026quot;, \u0026quot;s3:*\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: {\u0026quot;StringEquals\u0026quot;: {\u0026quot;aws:RequestedRegion\u0026quot;: \u0026quot;us-east-1\u0026quot;}} } ] } Click Review policy: Create the policy with the following details:\nName: RegionRestrict Description: EC2, RDS, S3 access in us-east-1 only Click Create policy: You have successfully created an IAM policy to restrict usage by region.\nApply the policy to your test group Select Groups from the left menu: Click on the CostTest group (created previously): Select the Permissions tab: Click Attach Policy: Click Policy Type and select Customer Managed: Select the checkbox next to Region_Restrict (created above) and click Attach Policy: You have successfully attached the policy to the CostTest group.\nLog out from the console\nVerify the policy is in effect Logon to the console as the TestUser1 user, go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California): Try to launch an instance by clicking Launch Instance, select Launch Instance: Click on Select next to the Amazon Linux 2 AMI, You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you cannot launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia):\nChange the region by clicking the current region, and selecting US East (N.Virginia): Now attempt to launch an instance, choose the Amazon Linux 2 AMI, leave 64-bit (x86) selected, click Select: Scroll down and select a c5.large, and click Review and Launch: Take note of the security group created (as you need to delete it), Click Launch: Select Proceed without a key pair, and click I acknowledge.. checkbox, and click Launch Instances: You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions, then Instance State, then Terminate: Confirm the instance ID is correct, click Yes, Terminate: Log out of the console as TestUser1.\nYou have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. You have also successfully launched a c5 Instance Type.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/2_create_iamrole/","title":"Create an IAM Role to use with Amazon CloudWatch Agent","tags":[],"description":"","content":"Access to AWS resources requires permissions. You will now create an IAM role to grant permissions that the agent needs to write metrics to CloudWatch. Amazon created two new default policies called CloudWatchAgentServerPolicy and CloudWatchAgentAdminPolicy only for that purpose.\nTo create the IAM role first you will need to sign in to the AWS Management Console and open the IAM console In the navigation pane on the left, choose Roles and then Create role. Under Choose the service that will use this role, choose EC2 Allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions. In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy, click Next: Tags: Add tags (optional) for this policy, click Next: Review. Confirm that CloudWatchAgentServerPolicy appears next to Policies. In Role name, enter a name for the role, such as CloudWatchAgentServerRole. Optionally give it a description. Then click Create role. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/2_budget_ec2actual/","title":"Create and implement an AWS Budget for EC2 actual cost","tags":[],"description":"","content":"We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount.\nClick Create budget: Select Cost budget, and click Next: Enter the following details:\nPeriod: Monthly Budget effective date: Recurring budget Start month: (current month) Choose how to budget: Fixed Budgeted amount: 1.00 (enter a dollar amount a lot LESS than last months cost) Other fields: leave a defaults Create a filter to only include EC2 instances in the budget:\nScrool down and under Filters select the Add filter button: Select the filter options:\nUnder Dimension select Service from the dropdown Under Values select EC2-Instances (Elastic Compute Cloud - Compute) Select the Apply filter button: Finish the budet details:\nRemove Upfront reservation fees by selecting the X to the right of the name Budget name: EC2_actual Select the Next button: To create an alert for our new EC2_actual budget select Add an alert threshold: For Alert #1 select:\nThreshold: 100% of budgeted amount Trigger: Actual Notification preferences: Input your email address in the Email recipients field Click on Next: Here you can attach actions that can be taken when you budget exceeds its threshold. We will not be attaching any actions for this lab. Select Next to move to the next page: Review the configuration, and click Create budget: You can see the current amount exceeds the new EC2_actual budget (you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for services other than EC2.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/2_developer_role/","title":"Create and Test Developer Role","tags":[],"description":"","content":"2.1 Create Developer Role Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced:\nSign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions. We enforce MFA here as it is a best practice. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags. For this lab we will not use IAM tags, click Next: Review. Enter the name of developer-restricted-iam for the Role name and click Create role. Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test! 2.2. Test Developer Role Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role.\nSign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_aliasthen click Switch Role. Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user.\nTip\nThe last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.\nYou are now using the developer role with the granted permissions, stay logged in using the role for the next section.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/2_create_waf_rules/","title":"Create AWS WAF Rules","tags":[],"description":"","content":"2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer.\nSign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional. WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg. WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg. The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won\u0026rsquo;t add any tags or other options. Click Next. Review the information for the stack. When you\u0026rsquo;re satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use! "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/2_create_role/","title":"Create Role","tags":[],"description":"","content":"Create a role for EC2 administrators, and attach the managed policies previously created.\nSign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role. Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions. We enforce MFA here as it is a best practice. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags, ec2-create-tags-existing, ec2-list-read, ec2-manage-instances, ec2-run-instances. and then click Next: Tags.\nFor this lab we will not use IAM tags, click Next: Review. Enter the name of ec2-admin-team-alpha for the Role name and click Create role. Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test! "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/2_create_role/","title":"Create role for Lambda in account 1","tags":[],"description":"","content":" In account 1 sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ Click Roles on the left, then create role\nAWS service will be pre-selected, select Lambda, then click Next: Permissions\nDo not select any managed policies, click Next: Tags\nClick Next: Review\nEnter Lambda-List-S3-Role for the Role name then click Create role\nFrom the list of roles click the name of Lambda-List-S3-Role\nClick Add inline policy, then click JSON tab\nReplace the sample json with the following\nReplace account1 with the AWS Account number (no dashes) of account 1\nReplace bucketname with the S3 bucket name from account 2\nThen click Review Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;S3ListBucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::bucketname\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsstreamevent\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-List-S3*/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsgroup\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Name this policy Lambda-List-S3-Policy, then click Create policy\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/2_create_role_acct_1/","title":"Create role for Lambda in account 1","tags":[],"description":"","content":" Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ .\nClick Roles on the left, then create role.\nAWS service will be pre-selected, select Lambda, then click Next: Permissions.\nDo not select any managed policies, click Next: Tags.\nClick Next: Review.\nEnter Lambda-Assume-Roles for the Role name then click Create role.\nFrom the list of roles click the name of Lambda-Assume-Roles.\nCopy the Role ARN and store for use later in this lab.\nClick Add inline policy, then click JSON tab.\nReplace the sample json with the following, replacing account1 and account2 with your respective account id\u0026rsquo;s, us-east-1 region with the region you are using, then click Review Policy.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;stsassumerole\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::account2:role/LambdaS3ListBuckets\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;aws:UserAgent\u0026quot;: \u0026quot;*AWS_Lambda_python*\u0026quot; } } }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsstreamevent\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-Assume-Roles*/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;logsgroup\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Name this policy Lambda-Assume-Roles-Policy, then click Create policy.\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/2_create_bucket/","title":"Create S3 Bucket","tags":[],"description":"","content":"The first step is to create an S3 bucket which will hold the lambda code and also used for storage of the reports. NOTE: the bucket must be in the same region as the Lambda function, it is advised to use a single region for all resources within this lab.\nThis bucket will store the reports and Athena CUR query results. These will not be deleted, to enable historical reporting, so delete these periodically if you do not require them.\nLogin via SSO, go to the s3 dashboard and create an S3 bucket in the required region: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/2_efficiency_data/","title":"Create the efficiency data source","tags":[],"description":"","content":"We will now build the efficiency data source, by combining the application logs with the cost data. When using your own application logs, you need to look through the logs and discover what the application is doing, and capture the log messages that indicate its various outputs, and what could consume resources of the system - as this will potentially indicate cost and usage of the system. Things to look for:\nSuccessful requests: Valid requests that produce an output Unsuccessful requests: Requests that require processing and resources, but dont produce an output Errors: Do not procduce an output, but consume application resources Types of requests: Different requests may require different resourcing, and costs Data transfer: Similar to types of requests, the data going into a system may indicate processing requirements or cost Review application logs We will use Athena to analyze the application logs, and discover the relevant data and fields.\nGo into Athena console\nRun the following query to see a sample of all data available:\nSELECT * FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; limit 10; We can see interesting fields could be: request, response, bytes, as these indicate requests to the workload and could indicate the amount of processing the workload performs.\nRun the following query to see the different types of requests:\nSELECT distinct request, count(*) FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; group by request order by count(*) desc limit 1000; We can see there are: health checks, errors, image_file requests, index.html requests. Successful requests should make up most of the work and cost, however errors may also consume resources and costs.\nRun the following query to see the different types of responses:\nSELECT distinct response, count(*) FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; group by response limit 100; We can see most are 200 - success, but there are a lot of 400 series which are client errors. So there could be considerable load from errors on the workload.\nData transfer also contributes to cost, so lets look at bytes. A large total amount of bytes may come from small numbers of large byte requests, or large numbers of small byte requests. Lets look at the distribution and run the following query:\nSELECT distinct bytes, count(*) FROM \u0026quot;webserverlogs\u0026quot;.\u0026quot;applogfiles_workshop\u0026quot; group by bytes order by count(*) desc limit 100; We have both lots of small requests (55, 91 bytes) and some large sized requests also.\nWe will choose the following fields for our efficiency data:\nRequest Response Bytes Review the Cost and Usage Reports We already know how to analyse the Cost and Usage reports, so lets use Athena to discover the relevant data and fields.\nGo into Athena console\nRun the following query to see a sample of all data available:\nSELECT * FROM \u0026quot;costusage\u0026quot;.\u0026quot;costusagefiles_workshop\u0026quot; limit 10; We know we need the unblended cost, the usage date and time, and also ensure that it is only costs for this specific workload. We tagged our resources, so include the tag: user application = ordering\nRun the following query to get a sample of our cost data for the application:\nSELECT line_item_usage_start_date, sum(try_cast(line_item_unblended_cost as double)) as cost FROM \u0026quot;costusage\u0026quot;.\u0026quot;costusagefiles_workshop\u0026quot; where resource_tags_user_application like 'ordering' group by line_item_usage_start_date limit 10 We now have our workload hourly costs, so lets combine that with our application logs and create an efficiency table.\nCreate the efficiency data source We will combine the application logs and the hourly cost data with a view, to get an efficiency data source. First we\u0026rsquo;ll create an hourly cost data set, then combine this with the application logs in another view.\nRun the following query in Athena to create the hourly cost view:\ncreate view costusage.hourlycost as SELECT line_item_usage_start_date, sum(try_cast(line_item_unblended_cost as double)) as cost FROM \u0026quot;costusage\u0026quot;.\u0026quot;costusagefiles_workshop\u0026quot; where resource_tags_user_application like 'ordering' group by line_item_usage_start_date Lets confirm its setup correctly \u0026amp; sample it, run the following query:\nselect * from costusage.hourlycost We can see the workload cost for every hour\nWe will combine the hourly cost table and the application log table using a union. This will basically copy the lines together in a single table. However, the columns wont match between the tables, so we will add NULL values where required. We will also divide the bytes by 1048576 to get a more readable MBytes value. Copy the following query into Athena to create the efficiency table:\ncreate view costusage.efficiency AS SELECT date_parse(concat(logdate, ' ', logtime), '%d/%b/%Y %H:%i:%S') as Datetime, request, response, try_cast(bytes as double)/1048576 as MBytes, NULL AS Cost from webserverlogs.applogfiles_workshop union select date_parse(line_item_usage_start_date, '%Y-%m-%d %H:%i:%s') as Datetime, NULL AS request, NULL AS response, NULL AS MBytes, Cost from costusage.hourlycost Lets check our new efficiency table. Run the following query:\nSELECT * FROM \u0026quot;costusage\u0026quot;.\u0026quot;efficiency\u0026quot; order by datetime asc limit 100; We have our efficiency data source: The first line is from our cost table, note the NULL values for requets, response and MBytes. The remaining lines will be from our application logs, and contain the data we need to measure efficiency.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/2_create_visualizations/","title":"Create visualizations","tags":[],"description":"","content":"We will now start to visualize our costs and usage, and create a dashboard.\nCost by account and product The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product.\nSelect line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id, which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual: Click on the down arrows under format visual, change:\nY-Axis label: Linked Account ID X-Axis label: Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars: (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending: The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue, and select Exclude AWSGlue: You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter: Elasticity The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances – so we’ll make one with a calculated field.\nClick Add in the top left corner, then select Add calculated field: Copy and paste this formula into the Formula box:\nifelse(split({line_item_usage_type},\u0026#39;:\u0026#39;,1) = \u0026#39;SpotUsage\u0026#39;,\u0026#39;Spot\u0026#39;,ifelse(right(split({product_usagetype},\u0026#39;:\u0026#39;,1), 8) = \u0026#39;BoxUsage\u0026#39;,{pricing_term},\u0026#39;other\u0026#39;)) Description:\nIfelse(, , ) If statement evaluated and returns if true, otherwise Right(, ) Returns the right most characters from a string Split(, , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic: If the first part of ‘lineitem/usagetype’ is ‘SpotUsage’ then PurchaseOption = ‘Spot’, otherwise check part of ‘product/usagetype’ is ‘BoxUsage’, if it is then PurchaseOption = ‘pricing/term’, otherwise PurchaseOption = ‘other’.\nEnter a Calculated field name of PurchaseOption, and click Create: The new field will appear in the list of fields in the data source\nClick Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly, expand the field wells wih the arrows at the top right, click the *down arrow next to line_item_usage_start_date, click the arrow next to Aggregate: Day, and click Hour: Click and drag PurchaseOption to the Color field: Now we will filter out other, click Filter on the left, and click Create One\u0026hellip;: Select PurchaseOption: Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list, enter other and click the +, change the Current list to Exclude: Click Apply: Select the empty line, and right click and select exclude: Update the title to Usage Elasticity, and you now have your elasticity graph, showing hourly usage by purchase option: In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points)\nLine charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next.\nWe will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add, and click Add calculated field: Copy and paste the following formula:\nsplit({line_item_usage_type},\u0026#39;:\u0026#39;,2) Name the field InstanceType, click Create: Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption: Now we’ll focus only on ondemand. Click on the blue line \u0026amp; select Focus only on OnDemand: You can see it automatically added a filter on the left, now click InstanceType: It will now only show hourly usage of OnDemand instances: You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter: This is also useful to work within the limitations of the number of data points on visuals. Remove the color field \u0026amp; enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made. Cost by line item description The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we’ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis.\nClick Add and select Add visual: Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date: The data source for our workshop is 3 months of data, so we’ll narrow that down with a filter to make it faster. Click on Filter and click Create one… Select bill_billing_period_start_date: Click on the filter name, bill_billing_period_start_date: Select a Relative dates filter, by Months and select This month, then click Apply: Click Visualize: Drag line_item_line_item_description to the Color field well, to add it to the visualization: You may have a visualization similar to below, which doesn’t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types: You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible.\nDashboard Complete Your dashboard is now complete, you should have a similar dashboard to below: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_ec2_web_application/1_create_web_stack/","title":"Create Web Stack","tags":[],"description":"","content":"Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name.\nChoose the version of the CloudFormation template and download to your computer, or by cloning this repository: wordpress.yaml to create a WordPress site, including an RDS database. staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack, then With new resources (standard). Click Upload a template file and then click Choose file. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next. Enter the following details: Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next. In this lab, we won\u0026rsquo;t add any tags, permissions or advanced options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you\u0026rsquo;re satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack. After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now created the WordPress stack (well actually CloudFormation did it for you).\nIn the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created. After you have played and explored with your web application, don\u0026rsquo;t forget to tear it down to save cost. Further Considerations Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice (that is supported by WordPress) would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Use EBS encryption by default to encrypting the EBS volumes for the web instances. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront to help protect the application. Create an automated process for patching the AMI\u0026rsquo;s and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack. "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/2_create_workload/","title":"Creating a workload","tags":[],"description":"","content":"Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about.\nClick the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Review owner: Your name Environment: Select \u0026ldquo;Pre-production\u0026rdquo; Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Next button: This will land you at the Apply lenses page. In this interface, the \u0026ldquo;AWS Well-Architected Framework\u0026rdquo; is always selected as it is not technically a Lens. We are not using any lens, so click on the Define Workload button: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/2_deploy_the_application_infrastructure/","title":"Deploy The Application Infrastructure","tags":[],"description":"","content":"The second section of the lab will build out the sample application stack what will run in the VPC which was build in section 1.\nThis application stack will comprise of the following :\nApplication Load Balancer (ALB) . Autoscaling Group along with it\u0026rsquo;s Launch Configuration. Once you completed below steps, you base architecture will be as follows:\nBuilding each components in this section manually will take a bit of time, and because our objective in this lab is to show you how to automate patching through AMI build and deployment. To save time, we have created a cloudformation template that you can deploy to expedite the process.\nPlease follow the steps below to do so :\n2.1. Get the Cloudformation Template. To deploy the second CloudFormation template, you can either deploy directly from the command line or via the console.\nYou can get the template here .\nClick here for CloudFormation command line deployment steps Command Line: 2.1.1. To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\naws cloudformation create-stack --stack-name pattern3-app \\ --template-body file://pattern3-application.yml \\ --parameters ParameterKey=AmazonMachineImage,ParameterValue=ami-0f96495a064477ffb\t\\ ParameterKey=BaselineVpcStack,ParameterValue=pattern3-base \\ --capabilities CAPABILITY_IAM \\ --region ap-southeast-2 Important Note: For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. We have also pre configured the Golden Amazon Machine Image Id to be the AMI id of Amazon Linux 2 AMI (HVM) in Sydney region ami-0f96495a064477ffb. If you choose to to use a different region, please change the AMI Id accordingly for your region. Click here for CloudFormation console deployment steps Console: 2.1.1. If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. To deploy the pattern1-app stack from the console, ensure that you follow below requirements:\nUse pattern3-app as the Stack Name. Provide the name of the VPC CloudFormation stack you create in section 1 ( we used pattern3-base as default ) as the BaselineVpcStack parameter value. Use the AMI Id of Amazon Linux 2 AMI (HVM) as the AmazonMachineImage parameter value. ( In Sydney region ami-0f96495a064477ffb if you choose to to use a different region, please change the AMI Id accordingly for your region. ) 2.2. Confirm Successful Application Installation Once the stack creation is complete, let\u0026rsquo;s check that the application deployment has been successful. To do this follow below steps:\nGo to the Outputs section of the cloudformation stack you just deployed.\nNote the value of OutputPattern3ALBDNSName and you can find the DNS name as per screen shot below: Copy the value and paste it into a web browser.\nIf you have configured everything correctly, you should be able to view a webpage with \u0026lsquo;Welcome to Re:Invent 2020 The Well Architected Way\u0026rsquo; as the page title.\nAdding /details.php to the end of your DNS address will list the packages currently available, together with the AMI which has been used to create the instance as follows: Take note of the installed packages and AMI Id (Copy and paste this elsewhere we will use this to confirm the changes later).\nWhen you have confirmed that the application deployment was successful, move to section 3 which will deploy your AMI Builder Pipeline.\nEND OF SECTION 2\n"},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/2_deploy_webapp/","title":"Deploy Web Application and Infrastructure using CloudFormation","tags":[],"description":"","content":"Wait until the VPC CloudFormation stack status is CREATE_COMPLETE, then continue. This will take about four minutes.\nDownload the CloudFormation template: staticwebapp.yaml You can right-click then choose Save link as; or you can right click and copy the link to use with wget Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next\nFor Stack name use CloudFormationLab\nParameters\nLook over the Parameters and their default values.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/2_prepare_csv/","title":"Download and prepare the RI CSV files","tags":[],"description":"","content":" Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days, and clicking on Download CSV: The next steps MUST be followed carefully, ensure you name everything exactly as specified or the formulas will not work.\nIf you do not have sufficient usage, you can download the two sample files: Open both files in a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec, in the same spreadsheet.\n7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv Create a new column called RI ID to the left of the Recommended Instance Quantity Purchase column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type, Location,OS and Tenancy. On row 2 of the sample files, paste the formula below into the first row of data and fill the remaining rows below.\n=CONCATENATE(C2,L2,M2,N2)\nAdd a column in the 30Day worksheet to the right of the Recommended Instance Quantity Purchase column. Label it 7Day recommendation. Add a VLOOKUP formula to get the values from the 7Day worksheet, paste the formula below into the first row of data and fill the remaining rows below. If using your own data, modify the U$48 component to the number of rows in your data.\n=VLOOKUP(T2,'7Day Rec'!T$2:U$48,2,FALSE) We will now create a Fully Paid Day column. This shows us how long it will take to pay off the full term of the RI, and will help to measure risk. The closer to 12months the fully paid day is, the higher the risk. The break even is the wrong measure, as it only shows how quickly you pay off the upfront component, and not the full amount. Paste the following formula into the last column z:\n=(R2+S2*12)/(R2/12+S2+W2) The formula for the fully paid day is: (yearly RI cost) / (monthly on-demand cost)\nDelete the following columns as they are not necessary: Recommendation Date Size Flexible Recommendation Max hourly normalized unit usage in Historical Period Min hourly normalized unit usage in Historical Period Average hourly normalized unit usage in Historical Period Projected RI Utilization Payment Option Break Even Months. You have compiled a complete set of recommendations with the required data to be able to analyse and make low risk high return recommendations.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/2_understand_deploy/","title":"Explore your Deployed Infrastructure","tags":[],"description":"","content":"Here you will explore the CloudFormation stack you deployed in the previous step.\nHow many resources were created? From the CloudFormation console, click the Resources tab for the CloudFormationLab stack. The listing shows all the resources that were created. Now, look at the simple_stack.yaml template (in your text editor) and compare. How many resources are defined there? Investigate: Why did the deployment not create all of the resources? You may click below for the answer. Try to figure this out before continuing. click here for answer: The deployed stack only has one resource, the VPC. But the CloudFormation Template contains many more in its Resources section. Why are these different?\nThe Condition PublicEnabled is set using the Parameter PublicEnabledParam Similarly the Condition EC2SecurityEnabled is set using the Parameter EC2SecurityEnabledParam The Default value for both of these Parameters is false And therefore both conditions PublicEnabled and EC2SecurityEnabled evaluate to false Look in the template at how the PublicEnabled and EC2SecurityEnabled Conditions are used IGWAttach: Condition: PublicEnabled #https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-vpc-gateway-attachment.html Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref SimpleVPC InternetGatewayId: !Ref IGW The Condition: \u0026lt;Condition_Name\u0026gt; statement on a resource means If this condition is true then create this resource else do not create this resource All resources except the VPC have a Condition statement. Since the conditions were false only the VPC was created Complare the CloudFormation template to the VPC resource that was created Return to the AWS CloudFormation console Click the Resources tab for the CloudFormationLab stack. The listing shows all the resources that were created. In this case just the VPC Note the Logical ID for the VPC is SimpleVPC. Look at the CloudFormation template file and determine where this name came from Under the Resources tab click on the Physical ID link for SimpleVPC This takes you to the VPC console where you can see the VPC you created Select the checkbox next to your VPC (if not already selected) Look at the VPC attributes under the Description tab. How do these compare to the CloudFormation template? For more information see the syntax and properties for a VPC in Cloudformation here . X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/2_handle_dependency/","title":"Handle failure of service dependencies","tags":[],"description":"","content":"2.1 System dependency initially healthy You already observed that all three EC2 instances are successfully serving requests\nIn a new tab navigate to ELB Target Groups console\nBy clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services \u0026gt; EC2 \u0026gt; Load Balancing \u0026gt; Target Groups Leave this tab open as you will be referring back to it multiple times Click on the Targets tab (bottom half of screen)\nUnder Registered Targets observe the three EC2 instances serving your web service\nNote that they are all healthy (see Status and Description)\nIn this state the ELB will route traffic to any of the three servers From the Target Groups console, now click on the the Health checks tab\nNote here that the Path is configured to /healthcheck Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL\nThe new URL should look like:\nhttp://healt-alb1l-\u0026lt;...\u0026gt;.elb.amazonaws.com/healthcheck Refresh several times and observe the health check on the three servers\nNote the check is successful\nThe EC2 servers receive user requests (for a TV show recommendation) on the path / and they receive health check requests from the Elastic Load Balancer on the path /healthcheck\nThe health check always returns an http 200 code for any request to it. The server code running on each EC2 instance can be viewed here , or you can view the health check code excerpt below: # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == \u0026#39;/healthcheck\u0026#39;: # Return a healthy code self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() 2.2 Simulate dependency not available 2.2.1 Disable RecommendationService You will now simulate a complete failure of the RecommendationService. Every request in turn makes a (simulated) call to the getRecommendation API on this service. These will all fail for every request on every server.\nIn a new tab, navigate to the Parameter Store on the AWS Systems Manager console By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services \u0026gt; Systems Manager \u0026gt; Parameter Store Leave this tab open as you will be referring back to it one additional time Click on RecommendationServiceEnabled Click Edit In the Value box, type false Click Save Changes A status message should say Edit parameter request succeeded The RecommendationServiceEnabled parameter is used only for this lab. The server code reads its value, and simulates a failure in RecommendationService (all reads to the DynamoDB table simulating the service will fail) when it is false.\n2.2.2 Observe behavior when dependency not available Refresh the test web service multiple times\nNote that it fails with 502 Bad Gateway For each request one of the servers receiving the request attempts to call the RecommendationService but catastrophically fails and fails to return a reply (empty reply) to the load balancer, which in turn presents this as a http 502 failure. You can observe this by opening a new tab and navigating to ELB Load Balancers console:\nBy clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services \u0026gt; EC2 \u0026gt; Load Balancing \u0026gt; Load Balancers Click on the Monitoring tab (bottom half of screen)\nObserve the ELB 5XXs (Count) and HTTP 502s (Count) errors for the load balancer It will take a minute for the metrics to show up. Make sure you refresh the web service page multiple times in your browser These are the error codes the load balancer returns on every request during this simulated outage Compare these metrics to those for the target group (the EC2 servers themselves)\nReturn to the Target Groups console and click the Monitoring tab there Observe HTTP 5XXs ( Count ) errors shows no data The servers themselves are not returning actual http error codes, they are failing to return any data at all We need to update the server code to handle when the dependency is not available 2.3 Update server code to handle dependency not available The getRecommendation API is actually a get_item call on a DynamoDB table. Examine the server code to see how errors are currently handled\nThe server code running on each EC2 instance can be viewed here Search for the call to the RecommendationService. It looks like this:\nresponse = call_getRecommendation(self.region, user_id) What happens if this call fails? Choose one of the options below (Option 1 - Expert or Option 2 - Assisted) to improve the code and handle the failure\n2.3.1 Option 1 - Expert option: make and deploy your changes to the code You may choose this option, or skip to Option 2 - Assisted option\nThis option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver.\nDownload the existing server code from here: server_basic.py Modify the code to handle the call to the RecommendationService When the call to RecommendationService fails then instead of using the response data you requested and did not get, return a static response: Instead of user first name return Valued Customer Instead of a personalized recommended TV show, return I Love Lucy Try to also return some diagnostic information on the cause of the error Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it: Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue If you completed the Option 1 - Expert option, then skip the Option 2 - Assisted option section and continue with 2.3.3 Error handling code\n2.3.2 Option 2 - Assisted option: deploy workshop provided code The new server code including error handling can be viewed here Search for Error handling in the comments (occurs twice). What will this code do now if the dependency call fails? Deploy the new error handling code Navigate to the AWS CloudFormation console\nClick on the HealthCheckLab stack\nClick Update\nLeave Use current template selected and click Next\nFind the ServerCodeUrl parameter and enter the following:\nhttps://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_errorhandling.py Click Next until the last page\nAt the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names\nClick Update stack\nClick on Events, and click the refresh icon to observe the stack progress\nNew EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue 2.3.3 Error handling code This is the error handling code from server_errorhandling.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option, you can consult this code as a guide.\nCode: Click here to see the code: # Error handling: # surround the call to RecommendationService in a try catch try: # Call the getRecommendation API on the RecommendationService response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value # {\u0026#39;Item\u0026#39;: { # \u0026#39;ServiceAPI\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;getRecommendation\u0026#39;}, # \u0026#39;UserID\u0026#39;: {\u0026#39;N\u0026#39;: \u0026#39;1\u0026#39;}, # \u0026#39;Result\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;M*A*S*H\u0026#39;}, ... tv_show = response[\u0026#39;Item\u0026#39;][\u0026#39;Result\u0026#39;][\u0026#39;S\u0026#39;] user_name = response[\u0026#39;Item\u0026#39;][\u0026#39;UserName\u0026#39;][\u0026#39;S\u0026#39;] message += recommendation_message (user_name, tv_show, True) # Error handling: # If the service dependency fails, and we cannot make a personalized recommendation # then give a pre-selected (static) recommendation # and report diagnostic information except Exception as e: message += recommendation_message (\u0026#39;Valued Customer\u0026#39;, \u0026#39;I Love Lucy\u0026#39;, False) message += \u0026#39;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;h2\u0026gt;Diagnostic Info:\u0026lt;/h2\u0026gt;\u0026#39; message += \u0026#39;\u0026lt;br\u0026gt;We are unable to provide personalized recommendations\u0026#39; message += \u0026#39;\u0026lt;br\u0026gt;If this persists, please report the following info to us:\u0026#39; message += str(traceback.format_exception_only(e.__class__, e)) 2.3.4 Observe behavior of web service with added error handling After the new error-handling code has successfully deployed, refresh the test web service page multiple times. Observe:\nIt works. It no longer returns an error All three EC2 instances and Availability Zones are being used A default recommendation for Valued Customer is displayed instead of a user-personalized one There is now Diagnostic Info. What does it mean? Refer back to the newly deployed code to understand why the website behaves this way now\nThe Website is working again, but in a degraded capacity since it is no longer serving personalized recommendations. While this is less than ideal, it is much better than when it was failing with http 502 errors. The RecommendationService is not available, so the app instead returns a static response (the default recommendation) instead of the data it would have obtained from RecommendationService.\nWell-Architected for Reliability: Best practice Implement graceful degradation to transform applicable hard dependencies into soft dependencies: When a component\u0026rsquo;s dependencies are unhealthy, the component itself can still function, although in a degraded manner. For example, when a dependency call fails, instead use a predetermined static response. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/","title":"Identity &amp; Access Management","tags":[],"description":"","content":"2.1 Investigate AWS CloudTrail As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab.\n2.1.1 AWS Console The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed.\nOpen the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query. IAM access denied attempts:\nTo list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details:\nfilter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent\nIAM access key:\nIf you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE:\nfilter userIdentity.accessKeyId =\u0026quot;AKIAIOSFODNN7EXAMPLE\u0026quot; | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent\nIAM source ip address:\nIf you suspect a particular IP address as an adversary you can search such as 192.0.2.1:\nfilter sourceIPAddress = \u0026quot;192.0.2.1\u0026quot; | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent\nIAM access key created\nAn access key id will be part of the responseElements when its created so you can query that:\nfilter responseElements.credentials.accessKeyId =\u0026quot;AKIAIOSFODNN7EXAMPLE\u0026quot; | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent\nIAM users and roles created\nListing users and roles created can help identify unauthorized activity:\nfilter eventName=\u0026quot;CreateUser\u0026quot; or eventName = \u0026quot;CreateRole\u0026quot; | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode\nS3 List Buckets\nListing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details:\nfilter eventName =\u0026quot;ListBuckets\u0026quot; | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent\n2.1.2 AWS CLI Remember you might need to update the \u0026ndash;log-group-name, \u0026ndash;region and/or \u0026ndash;start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com .\nIAM access denied attempts:\nTo list all IAM access denied attempts you can use CloudWatch Logs with \u0026ndash;filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\nIAM access key:\nIf you need to search for what actions an access key has performed you can modify the \u0026ndash;filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\nIAM source ip address:\nIf you suspect a particular IP address as an adversary you can modify the \u0026ndash;filter-pattern parameter to be the IP address to search such as 192.0.2.1:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\nS3 List Buckets\nListing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details:\naws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'\n2.2 Block access in AWS IAM Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions.\n2.3 List AWS IAM roles/users/groups If you need to confirm the name of a role, user or group you can list:\n2.3.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field. 2.3.2 AWS CLI aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query:\naws iam list-roles --output table --query 'Roles[*].RoleName' List all users:\naws iam list-users --output table --query 'Users[*].UserName' List all groups:\naws iam list-groups --output table --query 'Groups[*].GroupName'\n2.4 Attach inline deny policy Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations. Please note that the role will still be able to call the sts API to obtain information on itself, e.g. using get-caller-identity will return the account ID, user ID and ARN.\n2.4.1 AWS Console Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups, Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy. Click the JSON tab then replace the example with the following: { \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Click Review policy. Enter Name of DenyAll then click Create policy. Note that the console may incorrectly display the access level. 2.4.2 AWS CLI Block a role, modify ROLENAME to match your role name:\naws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' Block a user, modify USERNAME to match your user name:\naws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' Block a group, modify GROUPNAME to match your user name:\naws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }'\n2.5 Delete inline deny policy To delete the policy you just attached and restore the original permissions the entity had:\n2.5.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role. Confirm the role to delete then click Yes, delete 2.5.2 AWS CLI Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/2_install_cw_agent/","title":"Install the CloudWatch Agent","tags":[],"description":"","content":"The CloudWatch agent monitors activity on your EC2 instance to collect logs and metrics. This improves your security posture by providing detailed records you can use to investigate security incidents. The CloudWatch agent needs to be installed on the EC2 instance using AWS Systems Manager Run Command. Run Command enables you to perform actions on EC2 instances remotely. This tool is especially helpful at scale, where you can manage the configuration of many instances with a single command. It is possible to completely automate this process using user data scripts, but that is beyond the scope of this lab.\nOpen the Systems Manager console . Choose Run Command from the left side menu under Instances \u0026amp; Nodes. Click Run Command on the page that opens up. In the Command document box, click in the search bar. Select “Document name prefix”, then “Equals”, and enter AWS-ConfigureAWSPackage. Select the command that appears below. This command allows you to install packages on EC2 instances without directly accessing the instance; the AmazonCloudWatchAgent package we will use in this lab is one of these packages. Under Command parameters: Set Action to Install Set Installation Type to Uninstall and Reinstall In the Name field, enter AmazonCloudWatchAgent In the Version field, enter latest Do not modify the Additional Arguments field Under Targets: Select Choose instances manually. For the purpose of this lab, there is only one EC2 Instance you need to run a command on. If you have a large fleet of EC2 instances, you can assign a tag to those instances and choose Specify instance tags to run a command on many tagged instances easily. You should see a list of running instances. Select the instance that was launched by the CloudFormation template you deployed for this lab, which will be named Security-CW-Lab-Instance. In order to use Systems Manager with an instance, the instance needs certain IAM permissions. The initial CloudFormation stack you deployed created and assigned an IAM role to this instance. The policy document AmazonSSMManagedInstanceCore is attached to this role, allowing Systems Manager to perform operations on the instance. Under Output Options, deselect Enable writing to an S3 bucket. Choose Run. Optionally, in the Targets and outputs areas, select the button next to an instance name and choose View output. Systems Manager should show that the agent was successfully installed. Recap: In this portion of the lab, you installed the AWS CloudWatch agent on an EC2 Instance using AWS Systems Manager Run Command. Run Command facilitated installing the package on the instance without directly accessing it using SSH - exemplifying the Well-Architected Best Practice of “enabling people to perform actions at a distance” and “reducing attack surface”.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/2_knowledge_check/","title":"Knowledge Check","tags":[],"description":"","content":"The security best practices followed in this lab are:\nConfigure service and application logging AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Evaluate and implement new security services and features regularly: New features such as Amazon GuardDuty have been adopted. Automate testing and validation of security controls in pipelines: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/2_multiple_curs/","title":"Multiple CURs","tags":[],"description":"","content":"This step is used when there are multiple CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports.\nThe easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process.\nLog into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: \u0026lt;bucket name\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;report_name\u0026gt;/ Configuration: \u0026lt;bucket name\u0026gt;/DailyCUR/daily/ \u0026lt;bucket name\u0026gt;/HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor\nModify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase:\nName: \u0026#39;athenacurcfn_daily\u0026#39; Name: \u0026#39;athenacurcfn\u0026#39; Under AWSCURCrawlerComponentFunction:\nResource: arn:aws:s3:::\u0026lt;bucket name\u0026gt;/DailyCUR/daily/daily* Resource: arn:aws:s3:::\u0026lt;bucket name\u0026gt;* Under AWSCURCrawler:\nName: AWSCURCrawler-daily Name: AWSCURCrawler and\nPath: \u0026#39;s3://\u0026lt;bucket name\u0026gt;/DailyCUR/daily/daily\u0026#39; Path: \u0026#39;s3://\u0026lt;bucket name\u0026gt;\u0026#39; and under Exclusions after .zip add:\n\u0026#39;aws-programmatic-access-test-object\u0026#39; Under AWSPutS3CURNotification:\nReportKey: \u0026#39;DailyCUR/daily/daily\u0026#39; ReportKey: \u0026#39;\u0026#39; Under AWSCURReportStatusTable:\nDatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and\nLocation: \u0026#39;s3://\u0026lt;bucket name\u0026gt;/DailyCUR/daily/cost_and_usage_data_status/\u0026#39; Location: \u0026#39;s3://\u0026lt;bucket name\u0026gt;/cost_and_usage_data_status/\u0026#39; A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line\nSave the template file.\nGo to the CloudFormation dashboard and execute the template you just created Go to the Glue dashboard and verify that there is a single database, containing multiple tables: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/2_backfill_data/","title":"Perform one off Fill of Member/Linked Data","tags":[],"description":"","content":"Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data.\n1 - In the management/payer account go into the Athena service dashboard\n2 - Create your query using the template below:\nThe following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month, and output it to the S3 output folder.\nCREATE TABLE (database).temp_table WITH ( format = \u0026#39;Parquet\u0026#39;, parquet_compression = \u0026#39;GZIP\u0026#39;, external_location = \u0026#39;s3://(bucket)/(folder)\u0026#39;, partitioned_by=ARRAY[\u0026#39;year_1\u0026#39;,\u0026#39;month_1\u0026#39;]) AS SELECT *, year as year_1, month as month_1 FROM \u0026#34;(database)\u0026#34;.\u0026#34;(table)\u0026#34; where line_item_usage_account_id like \u0026#39;(account ID)\u0026#39; Some key points for your queries:\nPartitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb:\nUsing Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard\n5 - Go to the output bucket and folder\n6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees: 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data)\nDROP TABLE (database).temp_table X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_playbook_with_jupyter-aws_iam/2_playbook_run/","title":"Playbook Run","tags":[],"description":"","content":"2.1 Download Playbook and Helper Download the example version of the notebook Incident_Response_Playbook_AWS_IAM.ipynb and helper incident_response_helpers.py , place them in the same directory.\n2.2 Run the Playbook In your command line or terminal change directory to where you downloaded or cloned the notebook and helper. Enter jupyter notebook to start the local webserver, and connect to the url provided in the console e.g. The Jupyter Notebook is running at:, a web browser may automatically open to the correct url. Click on the Incident_Response_Playbook_AWS_IAM.ipynb file to execute the playbook. Follow the instructions in the playbook. "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/2_setup_env/","title":"Setup","tags":[],"description":"","content":"Requirements You will need the following to be able to perform this lab:\nYour own device for console access An AWS account that you are able to use for testing, that is not used for production or other purposes An available region within your account with capacity to add 2 additional VPCs User and Group Management When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account.\nWe strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .\nIAM Users \u0026amp; Groups As a best practice, do not use the AWS account root user for any task where it\u0026rsquo;s not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \u0026ldquo;Administrators\u0026rdquo; group to which the AdministratorAccess managed policy is attached.\nUse administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it .\n2.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group:\nUse your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user. In Set user details for User name, type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type, select the check box next to AWS Management Console access, select Custom password, and then type your new password in the text box. If you\u0026rsquo;re creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions. In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group. In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group. Back at Add user to group, in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Tags to optionally add key-value pair tags to an IAM entitiy. You can use tags to control an entity\u0026rsquo;s access to resources or to control what tags can be attached to an entitiy. In Add tags (optional) enter a Key of \u0026ldquo;ROLE\u0026rdquo; and a Value of \u0026ldquo;Administrator\u0026rdquo;. Learn more about Tagging IAM users and roles . Next choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user. At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it\u0026rsquo;s created, see Adding and Removing Users in an IAM Group .\n2.2 Log in to the AWS Management Console using your administrator account You can now use this administrator user instead of your root user for this AWS account. Choose the link https://\u0026lt;yourAccountNumber\u0026gt;.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/ ) and in the Resources section reviewing the number of VPCs. 2.3 Create an EC2 Key Pair Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance.\nUse your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the EC2 navigation pane under Network \u0026amp; Security, choose Key Pairs and then choose Create Key Pair. In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create. Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/2_simulate_application_issue/","title":"Simulate an Application Issue","tags":[],"description":"","content":"Understanding the health of your workload is an essential component of Operational Excellence. Defining metrics and thresholds, together with appropriate alerts will ensure that issues can be acknowledged and remediated within an appropriate timeframe.\nIn this section of the lab, you will simulate a performance issue within the API. Using Amazon CloudWatch synthetic, your API will utilize a canary monitor, which continuously checks API response time to detect an issue.\nIn this example, should the API take longer than 6 seconds to respond, an alert will be created, triggering a notification email.\nActions items in this section: You will run a script that will send a large amount of traffic to the API. You will observe and confirm the issue through AWS monitoring tools. The following resources had been deployed to perform these actions.\n2.0 Sending traffic to the application In this section, you will send multiple concurrent requests to the application, simulating a large surge of incoming traffic. This will overwhelm the API, which will gradually increase the response time of the application. This results in the canary monitoring exceeding the set threshold, triggering the CloudWatch Alarm to send notification.\nFollow below steps to continue:\nFrom the Cloud9 terminal, run the command shown below to change directory to the working script folder:\ncd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/scripts/ Confirm that you have the test.json in the folder and it contains the following text:\n{\u0026#34;Name\u0026#34;:\u0026#34;Test User\u0026#34;,\u0026#34;Text\u0026#34;:\u0026#34;This Message is a Test!\u0026#34;} Go to CloudFormation console and take note of the OutputApplicationEndpoint value under Output tab of walab-ops-sample-application stack. This is the DNS endpoint of the Application Load Balancer.\nExecute the command below, replacing the \u0026lsquo;OutputApplicationEndpoint\u0026rsquo; with the DNS endpoint value you recorded previously:\nbash simulate_request.sh OutputApplicationEndpoint This script uses the Apache Benchmark to send 60,000,000 requests, 3000 concurrent request at a time.\nWhen you run the command you will see the output gradually change from a consistently successful 200 response to include 504 time-out responses.\nThe requests generated by the script are overwhelming the application API and result in occasional timeouts by your load balancer.\nKeep the command running in the background as you proceed through the lab.\n2.1 Observing the alarm being triggered. After approximately 6 minutes, you will see an alarm which is triggered as a response to the generated activity. This will trigger an email indicating that the CloudWatch alarm has been triggered.\nCheck and confirm the alarm by going to the CloudWatch console.\nClick on the Alarms section on the left menu.\nClick on the Alarms called mysecretword-canary-duration-alarm, which should be in an alarm state.\nClick on the alarm to display the CloudWatch metrics that the alarm data is based from.\nThe alarm is based on the Duration metric data emitted by the mysecretword-canary CloudWatch synthetic canary monitor. The Duration metric measures how long it takes for the canary requests to receive a response from the application.\nThe alarm is triggered whenever the value of the Duration metric is above 6 seconds within a 1 minute duration.\nOn the left menu click on Synthetics and locate the canary monitor named mysecretword-canary.\nClick on the canary and the select the Configuration tab.\nFrom here you will see the canary configuration and a snippet of the canary script.\nIn the canary script section, scroll down to the section that contains let requestOptionStep1 as shown in the screenshot below. This is the configuration that controls the destination of the request (hostname, path and payload body).\nClick on the Monitoring tab.\nFrom here you will see the visualization of the metrics that the canary monitor generates.\nLocate the \u0026lsquo;Duration\u0026rsquo; metric that is being used to trigger the CloudWatch alarm.\nYou will see the average duration value of the canary request representing the time to complete. A value above 6000ms signifies that the request has taken more than 6 seconds to receive a response from the application, indicating a performance issue in the API.\nYou have now completed the second section of the lab.\nYou should still have the simulate_request.sh running in the background, simulating a large influx of traffic to your API. This causes the application to respond slowly and time-out periodically. The CloudWatch Alarm will be triggering and performance issue notifications sent to your System Operator to prompt them into action.\nThis concludes Section 2 of this lab. Click \u0026lsquo;Next step\u0026rsquo; to continue to the next section of the lab where we will build an automated playbook to assist investigation of the issue.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_aws_account_and_root_user/2_cleanup/","title":"Tear down","tags":[],"description":"","content":"Please note that the changes you made to secure your account and root user should remain in place, and have no charges associated with them.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_certificate_manager_request_public_certificate/2_tear_down/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the certificate you have created.\nSign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select the region where you created the certificate. Click the check box for the domain name of the certificate to delete. Click Actions then Delete. Verify this is the certificate to delete and click Delete. Note: You cannot delete an ACM Certificate that is being used by another AWS service. To delete a certificate that is in use, you must first remove the certificate association. References \u0026amp; useful resources AWS Certificate Manager Create an HTTPS Listener for Your Application Load Balancer "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_for_web_application/2_tear_down/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nDelete the CloudFront distribution:\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. References \u0026amp; useful resources Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_with_cloudwatch_dashboards/2_cleanup/","title":"Teardown","tags":[],"description":"","content":"Remove the dashboard you created The AWS free tier allows for 3 Dashboards for up to 50 metrics per month for free, but to ensure you are not charged for the dashboard, you should remove it if you created one in the previous step.\nReferences \u0026amp; useful resources See Key Metrics From All AWS Services Focus on Metrics and Alarms in a Single AWS Service Focus on Metrics and Alarms in a Resource Group X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/2_usage_trend/","title":"Understand your usage trend","tags":[],"description":"","content":"In large organizations usage can be distributed across many teams, and could take significant effort to collect. We can assist this effort by using tooling to understand your overall trends in usage to make an informed choice on Savings Plan commitments.\nYou can use the Compute Savings Plan for this exercise if you plan on purchasing a compute plan. For this lab we will use an EC2 Instance Savings Plans to provide more granularity and insights into usage.\nOn the AWS Cost Management page, select Recommendations under Savings Plans and then select EC2 Instance Savings Plans Savings Plans type, 1-year Savings Plans term, All upfront, and 60 days time period: If Payer is selected and you see few or no recommendations, select Linked account to display recommendations to the linked accounts within your AWS Organization.\nScroll down to Recommended Savings Plans, take note of the Commitment: Scroll up and change it to 30 days: Scroll down to Recommended Savings Plans, take note of the Commitment: Scroll up and change it to 7 days: Scroll down to Recommended Savings Plans, take note of the Commitment: Compare the trends in usage to see if your usage is increasing or decreasing, the higher the recommended commitment, the higher the usage. If usage is decreasing make a smaller initial hourly commitment, then re-analyze in 2-4 weeks. If usage is steady or increasing make a commitment closer to the recommended commitment:\nYou now have an understanding of your overall usage trend, and can use this information to make a commitment that is matched to your business requirements.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/2_upload_file/","title":"Upload example index.html file","tags":[],"description":"","content":" Create a simple index.html file, you can create by coping the following text into your favourite text editor. \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Example Heading\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Example paragraph.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list. "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/2_setup_athena/","title":"Use AWS Glue to enable access to CUR files via Amazon Athena","tags":[],"description":"","content":"We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included.\nWe will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution.\nGo to the Glue console: Click on Get started if you have not used Glue before\nEnsure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler: Enter a Crawler name starting with Cost, and click Next: Select Data stores, and click Next: Ensure you select Specified path in another account, and enter the S3 path of your bucket s3://(CUR bucket), expand Exclude patterns, enter the following patterns one line at a time and click next:\n**.json, **.yml, **.sql, **.csv, **.gz, **.zip, **/cost_and_usage_data_status/*, aws-programmatic-access-test-object If you replicated the objects to the Cost Management Account or if using in the same account select Specified path my account instead of Specified path in another account.\nAdd another data store, click Next: Select Create an IAM role, enter a role name of Cost_Crawler, and click Next: Click the Down arrow, and select a Daily Frequency: The CUR refreshes multiple times a day. You can adjust your frequency if you would like to run it more than once a day. Enter in a Start Hour and Start Minute, then click Next: Click Add database: Enter a Database name of cost, and click Create: NOTE: Your Database name cannot contain - Configure the Crawler\u0026rsquo;s output by updating the following fields then select **Next:\nSelect Create a single schema for each S3 path Select Add new columns only Select Ignore the Change and don\u0026rsquo;t update the table in the data catalog Review the crawler and click Finish: Select the checkbox next to the crawler, click Run crawler: You will see the Crawler was successful and created a table: Click Databases Select the cost database that Glue created: Click Tables in cost: Click the table name: Verify the recordCount is not zero, if it is - go back and verify the steps above: Go to the Athena Console: Select the drop down arrow, and click on the new database: A new table will have been created (named after the CUR), we will now load the partitions. Click on the 3 dot menu and select Load partitions: You will see it execute the command MSCK REPAIR TABLE, and in the results it may add partitions to the metastore for each month that has a billing file: NOTE: It may or may not add partitions and show the messages above.\nIf you are using the supplied files for this lab, check:\nThe folder names year and month are in S3 and the case matches There are parquet files in each of the month folders We will now preview the data. Click on the 3 dot menu and select Preview table: It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/2_resource_opt/","title":"Using AWS Cost Management Rightsizing Recommendations","tags":[],"description":"","content":" In order to complete this step you need to have findings within Rightsizing Recommendations. You can do that by going to AWS Cost Explorer\u0026raquo;Rightsizing Recommendations (left bar) section. Allow up to 24 hours after enabling this feature (no additional cost) to start getting recommendations.\nAWS Cost Explorer Rightsizing Recommendations offers EC2 resource optimization recommendations without any additional cost. These recommendations identify idle and underutilized instances across your accounts, regions, and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (using Amazon CloudWatch metrics) and your existing reservation footprint to identify opportunities for cost savings (e.g., by terminating idle instances or downsizing active instances to lower-cost options within the same family/generation).\nViewing your Rightsizing Recommendations Navigate to the AWS Cost Explorer page Select Rightsizing recommendations in the left menu bar In case you haven’t enabled the Amazon Rightsizing Recommendations please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or a management account can enable Rightsizing Recommendations. After you enable the feature, both member and management account can access Rightsizing Recommendations unless the management account specifically prohibits member account access on the settings page.\nTo improve the recommendation quality, AWS might use your published utilization metrics, such as disk or memory utilization, to improve our recommendation models and algorithms. All metrics are anonymized and aggregated before AWS uses them for model training. If you want to opt out of this experience and request that your metrics not be stored and used for model improvement, contact AWS Support. For more information, see AWS Service Terms .\nGetting to know the Amazon Rightsizing recommendation parameters AWS Cost Explorer Rightsizing recommendations will analyze the usage for the last 14 days for each account. If the instance was stopped or terminated, AWS removes it from recommendation. For all remaining instances, AWS uses Amazon CloudWatch to get maximum CPU utilization data, memory utilization (if enabled), network in/out, local disk input/ output (I/O), and performance of attached EBS volumes for the last 14 days. This is to produce conservative recommendations, not to recommend instance modifications that could be detrimental to application performance or that could unexpectedly impact your performance.\nRecommendation parameters\nDisplay recommendations: Option to generate recommendations within the instance family (more conservative), or across multiple instance families. The last will present more instance options to choose from which can lead to higher savings. More options also means that you should separate more time to test instance types that you may not be familiar with. Finding types: Filter your recommendations by selecting any or all of the following check boxes: idle (terminate) and underutilized instances. Advanced options: Select to consider (or not) existing Savings Plans or Reserved Instance coverage in recommendation savings calculations. Amazon EC2 Spot usage is not considered for Rightsizing recommendations. Recommendations\nOptimization opportunities: The number of recommendations available based on your resource consumption Estimated monthly savings: The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%): The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list Determining if an instance is idle, underutilized, or neither\nAmazon Resource Recommendations looks at the maximum CPU utilization of the instance for the last 14 days to make one of the following assessments: Idle: If the maximum CPU utilization is at or below 1%. A termination recommendation is generated, and savings are calculated. Underutilized: If the maximum CPU utilization is above 1% and cost savings are available in modifying the instance type, a modification recommendation is generated. If the instance isn\u0026rsquo;t idle or underutilized, no recommendations will be generated.\nUse filtering to sort recommendations by Account ID, Region, and Tag: Under Findings, click on the Instance ID to view the details of the recommendation Understanding the AWS Cost Explorer Rightsizing recommendations Rightsizing recommendations uses machine learning to identify the optimal Amazon EC2 instance types for a particular workload. The recommendations engine analyzes the configuration and resource usage of a workload to identify dozens of defining characteristics. For example, it can determine whether a workload is CPU-intensive or whether it exhibits a daily pattern. The recommendations engine analyzes these characteristics and identifies the hardware resources that the workload requires. Finally, it simulates how the workload would perform on various Amazon EC2 instances to later make recommendations for the optimal AWS compute resources.\nIdle instance recommendation\nIt\u0026rsquo;s not uncommon for companies, due to lack of governance and control, to have resources that were launched and forgotten. Amazon EC2 and EBS are the most common cases, and at scale they can contribute to a significant waste for your company. Rightsizing recommendations will help you track these resources and will recommend users to terminate instances where the CPU utilization is at or below 1% over the past 14 days.\nAWS recommends you to start your rightsizing exercises with idle instances because they represent a higher savings (for larger instances) and it\u0026rsquo;s easier to identify if a workload is using that instance or not. In addition to the instance utilization metrics shown below, Rightsizing recommendations will also report which account the instance belongs to, the instance id, region, type, and tags. These is useful information if you are trying to track the resource owner to discuss terminating it.\nPlease check with your technical team and advisors before moving forward and terminating idle instances. There are unique situations (eg disaster recover systems) where an idle utilization is expected.\nBelow you can see an example of an idle recommendation. Highlighted areas show the estimated annual savings after turning off this instance as well as the the account name, region and instance ID to facilitate tracking it accross your environment. On the bottom of the page you can also find tagging information.\nKeep scrolling down to find additional information about that instance, like CPU, Network and Disk utilization. You can also add AWS CloudWatch agents to collect memory utilization from your instances and get a more accurate recommendation. Check the 200 level Rightsizing recommendations lab for more information. Finally, under the running hours section you can see which pricing model this instance is running, on the example above all the 336 hours (14 days) are On Demand.\nUnderutilized instance recommendation\nFor instances where the CPU utilization is above 1% over the past 14 days Rightsizing recommendations will look for instances that are cheaper and can sustain the utilization reported on CloudWatch. You can check the estimated savings on the top row as well as the target instance that workload should be using to achieve these savings. AWS also uses the rightsizing engine to project the CPU utilization for the recommended instance type.\nDepending on the case, Rightsizing recommendations will generate multiple recommendations per underutilized instance. If you want to check additional suggestions just expand the \u0026ldquo;Other recommended options\u0026rdquo; section. For each suggested instance you will also get an estimated savings and the projected CPU utilization.\nRemember to select \u0026ldquo;Across instance families\u0026rdquo; radio button within the Recommendation parameters to see recommendations outside of the current instance family.\nSavings Calculation\nIn order to estimate the savings for your organization AWS first examine the instance running in the last 14 days to identify whether it was partially or fully covered by a Reserved Instance (RI), Savings Plans (SP), or running On-Demand. Another factor is whether the RI is size-flexible . The cost to run the instance is calculated based on the On-Demand hours and the rate of the instance type.\nFor each recommendation, we calculate the cost to operate a new instance. We assume that a size-flexible RI covers the new instance in the same way as the previous instance if the new instance is within the same family. Estimated savings are calculated based on the number of On-Demand running hours and the difference in On-Demand rates. If the RI isn\u0026rsquo;t size-flexible or if the new instance is in a different instance family, the estimated savings calculation is based on whether the new instance had been running during the last 14 days as On-Demand. You can choose to view the estimated savings with or without consideration for RI or Savings Plans discounts. Considering RI or Savings Plans discounts might result in some recommendations showing a savings value of $0.\nAWS Cost Explorer Rightsizing recommendations only provides recommendations with an estimated savings greater than or equal to $0. These recommendations are a subset of AWS Compute Optimizer results. For more performance-based recommendations that might result in a cost increase, see AWS Compute Optimizer .\nRightsizing recommendations do not capture second-order effects of rightsizing, such as the resulting RI hour’s availability and how they will apply to other instances. Potential savings based on reallocation of the RI hours aren\u0026rsquo;t included in the calculation.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/2_cost_usage_account/","title":"View your cost and usage by account","tags":[],"description":"","content":"We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multiple accounts for this exercise to be effective.\nSelect Saved reports from the left menu: Click on Monthly costs by linked account: It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account, select the checkbox next to the account we want to focus on, then click Include only and Apply filters: You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services and change it to a line graph: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type and change it to a line graph: You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type and change it to a line graph: Here is the usage type breakdown: You have now viewed the costs by account and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/2_cost_usage_detail/","title":"View your cost and usage in detail","tags":[],"description":"","content":"You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service.\nGo to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service, where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name, then drill down into the specific service cost and usage: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/","title":"Level 100: Cost and Usage Governance","tags":[],"description":"","content":"Last Updated July 2021\nAuthors Nathan Besh, Cost Lead Well-Architected Contributor Cy Hopkins, Enterprise Solutions Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.\nGoals Implement AWS Budgets to notify on usage and spend Create an AWS Budget report to notify users every week on budget status Prerequisites A small amount of usage in your account, less than $5 to trigger the budget AWS Account Setup has been completed Permissions required Access to the Cost Optimization team created in AWS Account Setup Costs https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed Time to complete The lab should take approximately 15 minutes to complete Steps: Create and implement an AWS Budget for monthly forecasted cost Create and implement an AWS Budget for EC2 actual cost Create and implement an AWS Budget for EC2 Savings Plan coverage Create and implement an AWS Budget Report Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/","title":"Level 200: Cost and Usage Governance","tags":[],"description":"","content":"Last Updated May 2020\nAuthors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.\nGoals Implement IAM Policies to control usage Prerequisites AWS Account Setup has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Add the following IAM Policy for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs A small number of instances will be started \u0026amp; then immediately terminated Costs will be less than $5 if all steps including the teardown are performed Time to complete The lab should take approximately 15 minutes to complete Steps: Create a group of users for testing Create an IAM Policy to restrict service usage by region Create an IAM Policy to restrict EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/","title":"Level 200: Deploy and Update CloudFormation","tags":["implement_change"],"description":"Improve reliability of a service by using automation to make changes in your cloud infrastructure","content":"Authors Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected Mahanth Jayadeva, Solutions Architect, Well-Architected Introduction This hands-on lab will guide you through the steps to improve reliability of a service by using automation to make changes in your cloud infrastructure. When this lab is completed, you will have deployed and edited a CloudFormation template. Using this template you will deploy and modify a VPC, an S3 bucket and an EC2 instance running a simple web server.\nAWS Well-Architected offers two different CloudFormation labs illustrating Reliability best practices. Choose which lab you prefer (or do both):\nThis is the 200 level lab where you create an infrastructure using CloudFormation and then make several modifications to it. Because this 200 level lab includes modification and update as part of the exercise, it uses a simplified, single-tier architecture, which does not follow best practices for reliability If you prefer a simpler lab that does deployment only, or want to see how to use CloudFormation to deploy a a multi-tier reliable architecture using Amazon EC2, see this 100 level lab: Deploy a Reliable Multi-tier Infrastructure using CloudFormation The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals By the end of this lab, you will be able to:\nAutomate change for your workload Document and track changes in code Implement infrastructure as a service Prerequisites If you are running this at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites:\nAn AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create IAM Roles, EC2 instances, S3 buckets, VPCs, Subnets, and Internet Gateways NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy Infrastructure using a CloudFormation Stack Explore your Deployed Infrastructure Configure Deployed Resources using Parameters Add an Amazon S3 Bucket to the Stack Add an Amazon EC2 Instance to the Stack Multi-region Deployment with CloudFormation StackSets Tear down this lab Costs NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nThis lab will cost approximately $1.00 per day when deployed It may be less (or zero) if you have remaining AWS Free Tier usage on your account The majority of this cost is the charge for EC2 BoxUsage (per hour usage charge) for the single EC2 instance you deploy Please follow the directions for Tear Down to avoid unwanted costs after you have concluded this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/","title":"Level 300: Automated CUR Updates and Ingestion","tags":[],"description":"","content":"Authors Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework.\nGoals Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multiple Cost and Usage Reports in the same bucket Prerequisites An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features Steps: Create the CloudFormation Stack Multiple CURs Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/","title":"Level 300: IAM Tag Based Access Control for EC2","tags":[],"description":"","content":"Introduction This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals IAM least privilege IAM policy conditions Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Steps: Create IAM policies Create Role Test Role Knowledge Check Tear down References \u0026amp; useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/","title":"Quest: Quick Steps to Security Success","tags":[],"description":"In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture.","content":"Authors Byron Pogson, Solutions Architect About this Guide This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. Further discussion can be found in best practices for your AWS environment For more context on this quest see Essential Security Patterns from Public Sector Summit Canberra 2019 and the associated slide deck on SlideShare Implementing multiple AWS accounts for your workload improves your security by isolating parts of your workload to limit the blast radius. Understanding cross account access ensures that common resources can continue to be shared among separate workloads. This quest will guide you to setting up a foundational multi-account environment which allows you to implement appropriate controls on top of it while still maintaining a centralized view and flexibility to adapt to your business processes.\nThis quest leverages AWS Control Tower to implement your best practice landing zone. AWS Control Tower is a managed service to setup up and govern secure multi-account AWS environment. Control Tower is not currently available in all regions so instructions are also provided for an alternate approach too. It is strongly recommend that you set up your account landing zone with Control Tower as it is a managed service supported directly by AWS and includes many best practices and guardrails.\nSteps: Control Tower Centralize Identities Enable Additional Guardrails Monitoring and Alerting Operating "},{"uri":"https://wellarchitectedlabs.com/cost/expenditureawareness/","title":"Expenditure Awareness","tags":[],"description":"","content":"About expenditure awareness The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.\nStep 1 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount.\n100 Level Lab: This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 2 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption.\n100 Level Lab: This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 3 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights.\n100 Level Lab: This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 4 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur.\n200 Level Lab: This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 5 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available.\n200 Level Lab: This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 6 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards.\n200 Level Lab: This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source. Step 7 - Monitor Workload Efficiency This hands-on lab will guide you through the steps to measure the efficiency of a workload. It shows you how to get the overall efficiency, then look deeper for patterns in usage to be able to allocate different weights to different outputs of a system.\n200 Level Lab: This lab combines your application logs with cost data, to provide an efficiency metric and insights for your workloads. Step 8 (Optional) - Automated CUR Updates and Ingestion This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena, every time a new CUR file is delivered.\n300 Level Lab: This lab uses s3 events and Lambda to trigger a Glue crawler and update Athena when a new CUR is delivered. Step 9 (Optional) - Splitting and sharing the CUR This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. Ideal for partners to deliver a sub-account only CUR to each of its customers, or large enterprises.\n300 Level Lab: This Lab uses S3 events, Lambda and Athena to extract part of a CUR file and deliver it to an S3 bucket for another account. Step 10 (Optional) - Automated CUR Reports and email delivery 300 Level Lab: This Lab uses CloudWatch events, Lambda, Athena and SES to run queries against the CUR file, and send financial reports to recipients. Step 11 (Optional) - Enterprise Dashboards 200 Level Lab: Build an Enterprise QuickSight dashboard for cost and usage analysis. "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_cost_estimation/2_add_and_configure_services/","title":"Add &amp; Configure Services","tags":[],"description":"","content":"Workload Description Lets take the case of a customer facing Web Application. This general purpose workload takes input data from users (over the internet), processes it and returns the results. It is a spiky workload which receives 100 new connections per second, each lasting approximately 3 min. Per connection, the workload processes 1000 bytes of data across 4 requests per sec. The workload requires 2 instances at peak, with 2 GB RAM, 2 vCPU each and 30 GB of storage per instance. The workload needs a 100 GB database which can support transactional traffic.\nA simple 3 tier LAMP (Linux Apache MySQL PHP) stack based Web Application on AWS uses Amazon Application Load balancer, Amazon EC2, Amazon RDS MySQL (Relational Database Service). We will now add and configure these 3 services in the Pricing Calculator.\nAdd \u0026amp; Configure Services Add Load Balancer On the Add Service page, choose Elastic Load Balancing by clicking Configure on that tile. You can also use the search bar by typing Load Balancer to narrow down the results. For the Description , enter \u0026ldquo;Load Balancer\u0026rdquo;\nChoose US West (Oregon) for the Region\nIn the Elastic Load Balancing section, choose Application Load Balancer. Also choose Load Balancer in AWS Region\nIn the Service settings section, enter \u0026ldquo;1\u0026rdquo; for Number of Application Load Balancers In the Load Balancer Capacity Units (LCUs) section: Skip Processed bytes (Lambda functions as targets) since we are using EC2 instances For Processed bytes (EC2 Instances and IP addresses as targets) enter \u0026ldquo;0.36\u0026rdquo; GB per hour . We get this number since 1,000 bytes of data is processed connection. For Average number of new connections per ALB enter \u0026ldquo;100\u0026rdquo; connections per second For Average connection duration enter \u0026ldquo;3\u0026rdquo; minute For Average number of requests per second per ALB enter \u0026ldquo;400\u0026rdquo; Rules determine how the load balancer routes requests. For example, the default rule only routes HTTP traffic on port 80 to the EC2 instances (targets). Enter \u0026ldquo;20\u0026rdquo; for Average number of rule evaluations per request Click on Add to my estimate Add EC2 On the My Estimate page, click on Add Service. Choose EC2 by clicking Configure on that tile. You can also use the search bar by typing EC2 to narrow down the results. For the Description , enter “EC2”. Leave the default value for Region to be US West (Oregon)\nClick on Advanced estimate :\nIn the EC2 instance specifications section, choose Linux for the Operating system\nIn the Workload section, you can select the pattern that best describes the workload. Select Daily spike traffic Expand the section Daily spike pattern by clicking on the arrow Leave the workloads days to the default option - Monday to Friday Enter 1 for the Baseline and 2 for the Peak, indicating that this workload requires 1 instance at normal times and 2 instances during peak. Leave the default value - 8 hrs and 30 min for Duration of peak In the EC2 Instances section, you can choose the instance needed for this workload. Given that this is a spiky workload, the t instance family is a good fit. Choose t4g.small , which has the requires 2 vCPU, 2 GB RAM In the Pricing Strategy section, you can choose the option that best fits your need. For this example, we will choose On-Demand. Once deployed, you can use the Lab on Pricing Models to analyze and determine the best Pricing Strategy for this workload. In the Amazon Elastic Block Storage (EBS) section, configure the storage required for this workload Choose gp3 for the storage type Leave the default values for IOPS and Throughput - which is sufficient for this workload. Enter 30 for Storage amount Lets chose a daily backup schedule. Choose Daily for Snapshot Frequency and enter \u0026ldquo;1\u0026rdquo; GB for Amount changed per snapshot In the Data Transfer section, you can specify the networking requirements for this workload Enter 50 and GB per month for Inbound Data Transfer Enter 200 and GB per month for Outbound Data Transfer Click on Add to my estimate Add RDS On the My Estimate page, click on Add Service. Choose RDS for MySQL by clicking Configure on that tile. You can also use the search bar by typing RDS to narrow down the results. For the Description , enter \u0026ldquo;Database\u0026rdquo;. Leave the default value for Region to be US West (Oregon)\nChoose US West (Oregon) for the Region\nIn the section MySQL instance specifications:\nEnter 1 for Quantity For this workload, a m6g.large database instance would be sufficient. Choose db.m6g.large Choose Multi-AZ for Deployment Model Leave the default options for Pricing model, Term and Purchase option In the section Storage: Choose General Purpose SSD (gp2) for Storage for each RDS instance Enter 100 GB for Storage amount For this workload, the default RDS backups are sufficient. You can skip the Backup section. Click on Add to my estimate X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/2_govern_usage/","title":"Govern Usage","tags":[],"description":"","content":"Govern Usage Controls Notifications Goal: Ensure relevant people are notified when costs are predicted to exceed the set budget Target: All accounts have a forecasted budget set, with notifications sent to management, finance and technical leads Best Practice: Controls - Notifications Measures: % of accounts with budgets set Good/Bad: Good Why? When does it work well or not?: Ensure relevant staff are made aware of potential deviations from expected costs and usage Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/2c_kpi_dashboard/","title":"KPI Dashboard","tags":[],"description":"","content":"Authors Alee Whitman, Sr. Commercial Architect (AWS OPTICS) Contributors Aaron Edell, Global Head of Business and GTM - Customer Cloud Intelligence Alex Head, OPTICS Manager Georgios Rozakis, AWS Technical Account Manager Oleksandr Moskalenko, Sr. AWS Technical Account Manager Timur Tulyaganov, AWS Principal Technical Account Manager Yash Bindlish, AWS Technical Account Manager Yuriy Prykhodko, AWS Sr. Technical Account Manager KPI Dashboard The KPI and Modernization Dashboard helps your organization combine DevOps and IT infrastructure with Finance and the C-Suite to grow more efficiently and effectively on AWS. This dashboard lets you set and track modernization and optimization goals such as percent OnDemand, Spot adoption, and Graviton usage. By enabling every line of business to create and track usage goals, and your cloud center of excellence to make recommendations organization-wide, you can grow more efficiently and innovate more quickly on AWS.\nExplore a sample KPI Dashboard Prerequisites KPI dashboard can be only be installed if following products are in billing history (CUR)\nRDS Elasticache If you do not have these products you can install and then delete instances of each product after 1 hour. Deployment Options There are 3 options to deploy the KPI Dashboard. The CloudFormation template is coming soon. Bookmark the KPI Dashboard Changelog for the latest version and updates.\nOption 1: Manual Deployment This option is the manual deployment and will walk you through all steps required to create this dashboard without any automation. We recommend this option to users who are new to Athena and QuickSight. Click here to continue with the manual deployment Create Athena Views NOTE: This dashboard uses the account_map and summary_view as shown in the CID/CUDOS dashboards. If you have not created these dashboards, you will need to create one or both of the dashboards prior to creating the KPI Dashboard The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR). The default dashboard assumes you have both Savings Plans and Reserved Instances. If you do not have both, follow the instructions within each view below to adjust the query accordingly.\nLogin via SSO in your Cost Optimization account, go into the Athena console:\nCreate the KPI Instance Mapping view by modifying the following code, and executing it in Athena:\nKPI Instance Mapping Create the KPI Instance All view by modifying the following code, and executing it in Athena:\nKPI Instance All Create the KPI S3 Storage All view by modifying the following code, and executing it in Athena:\nKPI S3 Storage All Create the KPI EBS Storage All view by modifying the following code, and executing it in Athena:\nKPI EBS Storage All Create the KPI EBS Snap view by modifying the following code, and executing it in Athena:\nKPI EBS Snap Create the KPI Tracker view by modifying the following code, and executing it in Athena:\nKPI Tracker NOTE: The Athena Views are updated to reflect any additions in the cost and usage report. If you created your dashboard prior to January 18, 2022 you will want to update to the latest views above. Create QuickSight Data Sets Create Datasets Go to the QuickSight service homepage inside your account. Be sure to select the correct region from the top right user menu or you will not see your expected tables\nFrom the left hand menu, choose Datasets\nClick New dataset displayed in the top right corner\nSelect your existing data source you created for your CID and/or CUDOS dashboard\nNOTE: Your existing data sources are at the bottom of the page Select the database which holds the views you created (reference Athena if you’re unsure which one to select), and select the kpi_tracker view then click Edit/Preview data\nSelect SPICE to change your Query mode\nSelect Save \u0026amp; Publish\nSelect Cancel\nSelect the kpi_tracker dataset\nClick Schedule refresh\nClick Create\nEnter a daily schedule, in the appropriate time zone and click Create\nClick Cancel to exit\nClick x to exit\nRepeat steps 3-14, creating data sets with the remaining Athena views. You will reuse your existing Cost_Dashboard data source, and select the following views as the table:\nkpi_instance_all\nkpi_s3_storage_all\nkpi_ebs_storage_all\nkpi_ebs_snap_view\nNOTE: Make sure to reuse the existing Athena data source by scrolling to the bottom of the Data source create/select page when creating a new Dataset instead of creating a new data source When this step is complete, your Datasets tab should have 5 new SPICE Datasets as well as your existing summary_view dataset and any existing datasets\nNOTE: This completes the QuickSight Data Preparation section. Next up is the Import process to generate the QuickSight Dashboard. Import Dashboard Template We will now use the AWS CLI to create the dashboard from the KPI Dashboard template.\nEdit and Run list-users and make a note of your User ARN: aws quicksight list-users --aws-account-id \u0026lt;Account_ID\u0026gt; --namespace default --region \u0026lt;Region\u0026gt; Edit and Run list-data-sets and make a note of the Name and Arn for the 5 Datasets ARNs: aws quicksight list-data-sets --aws-account-id \u0026lt;Account_ID\u0026gt; --region \u0026lt;Region\u0026gt; Create an kpi_import.json file using the below sample { \u0026#34;AwsAccountId\u0026#34;: \u0026#34;\u0026lt;Account_ID\u0026gt;\u0026#34;, \u0026#34;DashboardId\u0026#34;: \u0026#34;kpi_dashboard\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;KPI Dashboard\u0026#34;, \u0026#34;Permissions\u0026#34;: [ { \u0026#34;Principal\u0026#34;: \u0026#34;\u0026lt;User ARN\u0026gt;\u0026#34;, \u0026#34;Actions\u0026#34;: [ \u0026#34;quicksight:DescribeDashboard\u0026#34;, \u0026#34;quicksight:ListDashboardVersions\u0026#34;, \u0026#34;quicksight:UpdateDashboardPermissions\u0026#34;, \u0026#34;quicksight:QueryDashboard\u0026#34;, \u0026#34;quicksight:UpdateDashboard\u0026#34;, \u0026#34;quicksight:DeleteDashboard\u0026#34;, \u0026#34;quicksight:DescribeDashboardPermissions\u0026#34;, \u0026#34;quicksight:UpdateDashboardPublishedVersion\u0026#34; ] } ], \u0026#34;DashboardPublishOptions\u0026#34;: { \u0026#34;AdHocFilteringOption\u0026#34;: { \u0026#34;AvailabilityStatus\u0026#34;: \u0026#34;DISABLED\u0026#34; } }, \u0026#34;SourceEntity\u0026#34;: { \u0026#34;SourceTemplate\u0026#34;: { \u0026#34;DataSetReferences\u0026#34;: [ { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_tracker\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;summary_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_instance_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_s3_storage_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; },\t{ \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_ebs_storage_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_ebs_snap\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;Region\u0026gt;:\u0026lt;Account ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }\t], \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:quicksight:us-east-1:223485597511:template/kpi_dashboard\u0026#34; } }, \u0026#34;VersionDescription\u0026#34;: \u0026#34;1\u0026#34; } Update the kpi_import.json to match your details by replacing the following placeholders:\nPlaceholder Replace with \u0026lt;Account_ID\u0026gt; AWS Account ID where the dashboard will be deployed \u0026lt;Region\u0026gt; Region Code where the dashboard will be deployed (Example eu-west-1) \u0026lt;User ARN\u0026gt; ARN of your user \u0026lt;DataSetId\u0026gt; Replace with Dataset ID\u0026rsquo;s from the data sets you created in the Preparing Quicksight section NOTE: There are 6 unique Dataset IDs Run the import\naws quicksight create-dashboard --cli-input-json file://kpi_import.json --region \u0026lt;Region\u0026gt; --dashboard-id kpi_dashboard Check the status of your deployment aws quicksight describe-dashboard --dashboard-id kpi_dashboard --region \u0026lt;Region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; If you encounter no errors, open QuickSight from the AWS Console, and navigate to Dashboards. You should now see KPI Dashboard available. This dashboard can be shared with other users, but is otherwise ready for viewing and customizing.\nIf something goes wrong in the dashboard creation step, correct the issue then delete the failed deployment before re-deploying\naws quicksight delete-dashboard --dashboard-id kpi_dashboard --region \u0026lt;Region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; Option 2: Command Line Interface Deployment The CID command line tool is an optional way to create the Cloud Intelligence Dashboards. The command line tool will allow you to complete the deployments in less than half the time as the standard manual setup.\nClick here to continue with the Automation Scripts Deployment Navigate to the Cloud Intelligence Dashboards automation repo and follow the instructions to run the command line tool. You will have the option of deploying the KPI dashboard from the list of supported dashboards. Once complete, visit the account mapping page and follow the steps there to get your account names into the dashboard.\nOption 3: CloudFormation Deployment This section is optional and automates the creation of the KPI Dashboard using CloudFormation templates. The CloudFormation templates allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates and create an IAM role. If you do not have the required permissions use the Manual Deployment.\nClick here to continue with the CloudFormation Deployment NOTE: An IAM role will be created when you create the CloudFormation stack. Please review the CloudFormation template with your security team and switch to the manual setup if required Create the KPI Dashboard using a CloudFormation Template Login via SSO in your Cost Optimization account\nClick the Launch CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch CloudFormation Template Enter a Stack name for your template such as KPI-Dashboard-QuickSight Review 1stReadMe parameter to confirm prerequisites before specifying the other parameters Update your AthenaQueryResultsBucket with the Athena results location where your CUR table is To validate your Athena primary workgroup has an output location by\nOpen a new tab or window and navigate to the Athena console Select Workgroup: primary Click the bubble next to primary and then select view detail Confirm your Query result location is configured with an S3 bucket path. If configured, add the location to the AthenaQueryResultsBucket in your CloudFormation Template. If not configured, continue to setting up by clicking Edit workgroup Add the S3 bucket path you have selected for your Query result location and click save Add the location to the AthenaQueryResultsBucket in your CloudFormation Template. Update your BucketFolderPath with the S3 path where your year partitions of CUR data are stored To validate the correct path for your year partitions of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the S3 console Select the S3 Bucket your CUR is located in Navigate your folders until you find the folder with the year partitions of the CUR Tip: Your yearly partitions folder is located in the folder with your .yml file, monthly folders, and status report Add the identified BucketFolderPath to the CloudFormation parameter making sure to not add trailing / (eg - BucketName/FolderName/\u0026hellip;/FolderName) Tip: copy and paste the S3 URI then remove the leading \u0026lsquo;s3://\u0026rsquo; and the ending \u0026lsquo;/\u0026rsquo; Update your CURDatabaseName and CURTableName with the name of the CUR Athena Database and Table To validate the Athena Database and Table of the CUR data follow the tasks below:\nOpen a new tab or window and navigate to the Glue console Select the Athena Table your CUR is located in Find your Database CURDatabaseName and Table CURTableName Add the identified CURDatabaseName and CURTableName to the CloudFormation parameter Update your QuickSightUser with your QuickSight username To validate your QuickSight complete the tasks below:\nOpen a new tab or window and navigate to the QuickSight console Find your username in the top right navigation bar Add the identified username to the CloudFormation parameter Update your QuicksightIdentityRegion with your QuickSight region Optional add a Suffix if you want to create multiple instances of the same account. Select Next at the bottom of Specify stack details and then select Next again on the Configure stack options page\nReview the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack. You will see the stack will start in CREATE_IN_PROGRESS NOTE: This step can take 5-15mins Once complete, the stack will show CREATE_COMPLETE Navigate to Dashboards page in your QuickSight console, click on your KPI Dashboard name NOTE: You have successfully created the KPI Dashboard. Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you can share your dashboard with users or customize your own version of this dashboard\nClick to navigate QuickSight steps Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you can share your dashboard with users or customize your own version of this dashboard.\nClick to navigate QuickSight steps Update Dashboard Template - Optional Click here to update your dashboard with the latest version If you are tracking our Changelog , you already know that we are always improving the Cloud Intelligence Dashboards.\nOption 1: Command Line Tool Visit the GitHub repository to download and install the CID Command Line Tool and follow the instructions for running the update command.\nOption 2: Manual Update To pull the latest version of the dashboard from the public template please use the following steps.\nCreate a kpi_update.json file by removing permissions section from the kpi_import.json file. Sample for KPI Dashboard kpi_update.json file below: { \u0026#34;AwsAccountId\u0026#34;: \u0026#34;\u0026lt;Account_ID\u0026gt;\u0026#34;, \u0026#34;DashboardId\u0026#34;: \u0026#34;kpi_dashboard\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;KPI Dashboard\u0026#34;, \u0026#34;DashboardPublishOptions\u0026#34;: { \u0026#34;AdHocFilteringOption\u0026#34;: { \u0026#34;AvailabilityStatus\u0026#34;: \u0026#34;DISABLED\u0026#34; } }, \u0026#34;SourceEntity\u0026#34;: { \u0026#34;SourceTemplate\u0026#34;: { \u0026#34;DataSetReferences\u0026#34;: [ { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_tracker\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;summary_view\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_instance_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_s3_storage_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; }, { \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_ebs_storage_all\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; },\t{ \u0026#34;DataSetPlaceholder\u0026#34;: \u0026#34;kpi_ebs_snap\u0026#34;, \u0026#34;DataSetArn\u0026#34;: \u0026#34;arn:aws:quicksight:\u0026lt;region\u0026gt;:\u0026lt;Account_ID\u0026gt;:dataset/\u0026lt;DatasetID\u0026gt;\u0026#34; } ], \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:quicksight:us-east-1:223485597511:template/kpi_dashboard\u0026#34; } } } If needed update the kpi_update.json to match your details by replacing the following placeholders:\nPlaceholder Replace with \u0026lt;Account_ID\u0026gt; AWS Account ID where the dashboard will be deployed \u0026lt;Region\u0026gt; Region Code where the dashboard will be deployed (Example eu-west-1) \u0026lt;DatasetID\u0026gt; Replace with Dataset ID\u0026rsquo;s from the datasets you created in the Preparing QuickSight section NOTE: There are 6 unique Dataset IDs Pull the latest published version of the dashboard template. Example for KPI Dashboard below:\naws quicksight update-dashboard --cli-input-json file://kpi_update.json --region \u0026lt;region\u0026gt; Query the version number of the published dashboard. Example for KPI Dashboard below: aws quicksight list-dashboard-versions --region \u0026lt;region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; --dashboard-id kpi_dashboard Apply the latest pulled changes to the deployed dashboard with this CLI command. Example for KPI Dashboard below: aws quicksight update-dashboard-published-version --region \u0026lt;region\u0026gt; --aws-account-id \u0026lt;Account_ID\u0026gt; --dashboard-id kpi_dashboard --version-number \u0026lt;version\u0026gt; NOTE: The update commands were successfully tested in AWS CloudShell (recommended) "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/3_failure_injection_prep/","title":"Preparation for Failure Injection","tags":[],"description":"","content":"Failure injection is the means by which we will simulate disruptive real-world events that affect production environments. Such events are used in chaos engineering to validate and understand the resiliency of your workload. Chaos engineering is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your workload reacts.\nBefore testing, please prepare the following:\n3.1 Region must be Ohio We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the Ohio region 3.2 Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs\n1 - Tick the checkbox next to ResiliencyVPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever \u0026lt;vpc-id\u0026gt; is indicated in a command\n3.3 View the website used for the test application for this lab Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation .\nclick on the WebServersforResiliencyTesting stack click on the \u0026ldquo;Outputs\u0026rdquo; tab For the Key WebSiteURL copy the value. This is the URL of your test web service. Click the URL and it will bring up the website:\n(image will vary depending on what you supplied for websiteimage)\nGet familiar with the service website\nNote the availability_zone and instance_id Refresh this website several times watching these values Note the values change. You have deployed one web server per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. Refer to the diagram at the start of Step 1 to review your deployed system architecture. Availability Zones (AZs) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more: After the lab learn more about Regions and Availability Zones here X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_manage_workload_risks_with_opscenter/3_update_workload/","title":"Configure workload updates","tags":[],"description":"","content":"Updating workloads The AWS WA Tool should be the source of truth for information related to workload risks. After new best practices are implemented for a workload, it is important to reflect this by updating the workload on the AWS WA Tool. In this section, you will expand the solution to include automated updates to the workload when best practices are implemented.\n2.1 Create and configure Lambda function You will create a Lambda function that will be invoked using SNS whenever a Well-Architected OpsItem is resolved. The function will update the workload on the AWS WA Tool to reflect the implementation of the best practice and also update the workload state in the DynamoDB table. Click here to view the Lambda function code for automating workload updates.\nDownload the update_workload.zip Lambda function package Navigate to the AWS Lambda console and select Create function. Choose the option to Author from scratch and enter wa-update-workload for the function name. Select Python 3.9 as the runtime. Under Permissions, expand Change default execution role. Choose Use an existing role and select wa-risk-tracking-lambda-role from the dropdown. This is the IAM role that was created as part of the CloudFormation stack in the previous section. Click Create function. Lambda provisions a new function which uses the IAM role that was specified. After the function has been created, scroll down to the Code source section and select Upload from and then .zip file. Upload the function package that you selected at the beginning of this section: update_workload.zip Scroll down to Runtime settings, click Edit and replace the value for Handler with update_workload.lambda_handler. Click Save. On the function overview page, click Add Trigger to configure a trigger for the Lambda function. Select SNS under Trigger configuration Select wa-risk-tracking under SNS Topic and click Add. 2.2 Test workload updates To test workload updates, navigate to the Systems Manager console and click on OpsCenter under Operations Management. Select an OpsItems with Well-Architected as the Source.\nScroll down to the Operational data section and expand it. Note down values for the WorkloadName, Pillar, Question, and Best practice missing.\nOpen a new tab on your browser and navigate to the AWS WA Tool console . Click on the workload listed in the OpsItem in the previous step to view its details. Scroll down to the Lenses section and click on AWS Well-Architected Framework to see pillar level risk data for the workload.\nScroll down to the Pillars section, click on the pillar listed in the OpsItem from the previous step.\nScroll down to Questions and expand the Answer details for the question that is listed in the OpsItem from the previous step. Note that the best practice listed in the OpsItem does not appear under Selected choice(s) for this question.\nSwitch back to the browser tab that has the OpsItem open. Assume that you have used the Improvement Plan and implemented the best practice listed in the OpsItem for your workload. After this implementation is complete, you can set the status of the OpsItem to Resolved to reflect completion.\nSwitch back to the browser tab with the AWS WA Tool console. Refresh the page and then scroll down to Questions and expland Answer details for the question that was listed in the OpsItem you resolved. You should see that the best practice listed in the OpsItem now appears under Selected choice(s).\nWhen the OpsItem was resolved, a notification was sent to the wa-risk-tracking SNS topic which then invoked the wa-update-workload Lambda function. The function updated the workload on the AWS WA Tool to reflect the best practice specified in the OpsItem as being implemented. With this approach, workloads on the AWS WA Tool will always be a single source of truth for you to be aware of workload risks.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_manage_workload_risks_with_opscenter/","title":"Level 200: Manage Workload Risks with OpsCenter","tags":["workload_risk_management"],"description":"Create a risk management workflow for improved Operational Excellence","content":"Authors Mahanth Jayadeva, Solutions Architect, AWS Well-Architected Introduction In this lab, you will become familiar with how you can better manage workload risks identified in the AWS Well-Architected Tool (AWS WA Tool). You will learn how to efficiently track risks across your entire technology portfolio while maintaining a single source of truth for risk information in an automated manner.\nMost workloads contain risks or opportunities for improvement which can lead to better business outcomes when addressed. Risk mitigation should be prioritized based on the impact it can have on your business. As the number of workloads increases, it can be a challenge to manage and prioritize which risks to address first.\nBy tracking all risks in a single location, you can better understand which risks are related, prioritize them accordingly, and implement best practices to mitigate them. Being able to track risks across workloads will allow you to prevent duplication of efforts and enables teams to be aligned on priorities for risk remediation.\nIn this lab, you will use AWS WA Tool APIs and create OpsItems in AWS Systems Manager OpsCenter to track best practices missing from your workloads. You can then view, investigate, and resolve those OpsItems in a single location, and automatically update the risk status of the workload on the AWS WA Tool. The entire process will be automated using AWS Lambda functions.\nThe skills you learn will help you create risk management workflows which will help you determine your priorities in alignment with Operational Excellence best practices of the AWS Well-Architected Framework Goals: Create actionable work items from workload risks Maintain a single source of truth for workload risk information Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. Define and document the workload state for one or more workloads in the AWS WA Tool. NOTE: You will be billed for any applicable AWS resources used as part of this lab, that are not covered in the AWS Free Tier.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy infrastructure Configure risk tracking Configure workload updates Teardown "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_vpc_flow_logs_analysis_dashboard/3_create_vpc_flow_logs_analysis/","title":"Create VPC Flow Logs QuickSight Analysis Dashboard","tags":[],"description":"","content":" To manage VPC Flow Logs and QuickSight dashboard in central account please make sure you create resources for the central account in the region supported by QuickSight. Refer to this link to see supported regions.\nCreate QuickSight Dataset and Dashboard We will now create the data sets in QuickSight from the Athena view and an analysis dashboard. All the steps from this section are required to execute one time in central account.\nLogin to your central AWS account.\nRun CloudFormation stack to create QuickSight Athena dataset and a Dashboard.\nDownload CloudFormation Template:\nCSV file format - vpc_flowlogs_quicksight_template.yaml OR\nParquet file format - vpc_flowlogs_quicksight_multi_view_template.yaml From AWS Console navigate to CloudFormation. Then click on Create stack Create stack page:\nIn Specify template section, select Upload a template file.\nThen Choose File and upload the appropriate template below (you have downloaded previously)\nCSV file format: vpc_flowlogs_quicksight_template.yaml\nOR\nParquet file format: vpc_flowlogs_quicksight_multi_view_template.yaml\nThen Click Next\nIn Specify stack details page: Provide unique stack name e.g. VPCFlowLogsQuickSightStack-01\nQuickSightUserArn: You will need to provide ARN so that you will get permission to access the dashboard\nRun below command in AWS Cloudshell after replcing \u0026lt;your account id\u0026gt; with central AWS account id and \u0026lt;your region\u0026gt; with region where QuickSight user is created. Copy the arn from response as shown in screenshot below.\naws quicksight list-users --aws-account-id \u0026lt;your account id\u0026gt; --namespace default --region \u0026lt;your region\u0026gt; Example Response screenshot:\nVpcFlowLogsAthenaDatabaseName: This is required as QuickSight dataset will be created on this database\nClick Next Add tags Name=VPCFlowLogs-QuickSight-Stack and Purpose=WALabVPCFlowLogs. Keep rest of the selections to default vaules. Then Click Next Review the Stack parameters Then, click on Create Stack You will see the progress of the stack creation under Events tab as below. Please wait for the stack to complete the execution. Once complete it will show the status CREATE_COMPLETE in green against stack name, then proceed to the next step. From AWS console navigate to the QuickSight and click on Dashboards link on the left panel.\nYou will see the newly created dashboard in QuickSight under Dashboards, click on the Dashboard name VPC Flow Logs Analysis Dashboard integrated with AWS VPC Service: Click Share, click Share dashboard:, Click on Manage dashboard access: Add the required users, or share with all users, ensure you check Save as for each user, then click the x to close the window: Click Save as: Enter an Analysis name and click Create: Perform steps 11 - 15 above to create additional analyses for other teams, this will allow each team to have their own customizable analysis.\nYou will now have an analysis created from the template that you can edit and modify: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Teardown "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_incident_response_credential_misuse/","title":"Quest: AWS Incident Response - Credential Misuse","tags":[],"description":"This quest is the guide for incident response workshop on credential misuse at AWS organized events.","content":"About this Guide This is a guide for an AWS led event, to help you learn about responding to an incident related to the misuse of credentials. The credentials are specifically an IAM User with an access key. It has been designed to run in teams of 2 to 4 people where you have 1 or more partners to work with. You can also use this guide to run your own event or training. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nThis guide makes use of IAM users and access keys which should not be used for production purposes. Instead of IAM users follow the recommendations as detailed in the Identity and Access Management section of Well-Architected to centralize your identities and use the AWS Single Sign-On service.\nPrerequisites At least 2 AWS accounts that you are able to use for training, that are not used for production or any other purpose. Permission in the accounts to use the following services: IAM, CloudTrail, S3, Athena, GuardDuty, Config, EC2. AWS CloudTrail configured to log to an S3 bucket, you can enable this by following this lab NOTE: If you use your own AWS account you will be billed for any applicable AWS resources used that are not covered in the AWS Free Tier .\nPractical - Prerequisites: Implement Detection Services In this practical we are going to:\nLogin to the AWS console Use CloudFormation to automate configuration of AWS CloudTrail , Amazon GuardDuty , AWS Config , and AWS Security Hub services. Accept email subscription request for GuardDuty AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail event logs, Amazon VPC Flow Logs, and DNS logs.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines.\nAWS Security Hub gives you a comprehensive view of your security alerts and security posture across your AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, Amazon Macie, AWS Identity and Access Management (IAM) Access Analyzer, AWS Systems Manager, and AWS Firewall Manager, as well as from AWS Partner Network (APN) solutions. AWS Security Hub continuously monitors your environment using automated security checks based on the AWS best practices and industry standards that your organization follows.\n1. Login to console 1.1 Login to the AWS console of your AWS account dedicated to training, and select your closest region. If you are at an AWS event follow the instructions provided.\n2. Deploy detective controls using CloudFormation 2.1 Follow the instructions in Automated Deployment of Detective Controls and wait for the deployment to complete. It\u0026rsquo;s important that you name your S3 buckets to be globally unique and adhere to bucket naming rules . It\u0026rsquo;s a common practice to include your account ID as part of the name.\n3. Accept email subscription request for GuardDuty 3.1 You will receive an email notification from Amazon SNS, click the link to confirm your subscription to receive GuardDuty alerts via email.\nPractical 1: Create IAM user, keys, test In this practical we are going to:\nLogin to the AWS console Create IAM user with an access key Test new access key using CloudShell 1. Login to console 1.1 Login to the AWS console of your AWS account dedicated to training, with access to the services listed in the prerequisites, and select your closest region. If you are at an AWS event follow the instructions provided.\n2. Create IAM user with an access key The reason we are going to create an IAM user and access key and go against recommendations is IAM users are still commonly used. The steps for responding are similar for both IAM users and federated users (e.g. AWS Single Sign-On).\n2.1 Create an IAM user using the console without console access, with programmatic access (access \u0026amp; secret key), and attach existing policy directly: AdministratorAccess. Name it your name or nickname, adhering to the requirements. Securely save the access and secret key for future use.\nNOTE: Never use the AdministratorAccess policy other than for testing or training purposes, as it provides full access to all services in every region. You can learn about least privilege in a blog: Techniques for writing least privilege IAM policies .\n3. Test new access key using CloudShell To test an access key you must use the AWS API, which the AWS Command Line Interface (CLI) uses to interact with AWS services allowing you to test without writing any code. The AWS CLI Command Reference documents all commands available with examples. You can install the AWS CLI in Linux, macOS, Docker and Windows, however the easiest way is to use the AWS CloudShell service. AWS CloudShell is a browser-based, pre-authenticated shell that you can launch directly from the AWS Management Console. You can run AWS CLI commands against AWS services using your preferred shell (Bash, PowerShell, or Z shell). And you can do this without needing to download or install command line tools.\n3.1 Using the CloudShell console , once your shell is prepared test your shell by issuing the following command to list all your IAM users:\naws iam list-users This uses the aws CLI to select the iam service and issue list-users command. You will see a list of IAM users in your account in JSON format.\n3.2 Now that works using your existing console credentials, test your new user and its access key that you just created. We can do this by setting an environment variable that the CLI will use. Issue the following commands replacing YOUR_ACCESS_KEY and YOUR_SECRET_KEY with the ones you saved from previous step.\nexport AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY 3.3 Issue the aws iam list-users command again to test your new user. If you receive an error check your export commands do not have any extra spaces and the keys are identical to what you saved previously. If you receive a permissions error check the policy you attached to the IAM user. Note: If CloudShell times out at any stage you can simply press enter and it will resume / prepare the session.\nPractical 2: Discover CloudTrail, test partners keys In this practical we are going to:\nFind the API action in CloudTrail where you created your user Share your access \u0026amp; secret key with your partner List account information in your partners account Investigate what the actions they performed 1. Find the API action in CloudTrail where you created your user 1.1 Access the CloudTrail console and select Event history from the left menu (you may need to skip a welcome page). The recent account history is displayed and you can filter the events. Each API action is a separate event, and you can expand on the details by selecting the event name. You are looking in the Resource Name column for the user you created, note the User name column is the user that initiated the request, e.g. the one you are using.\n1.2 To filter the event history for the creation of the IAM user select Event name from the lookup attributes, then CreateUser in the search field. To the right is the time period to search. Note the event may take 5-30 minutes to appear.\n2. Share your access \u0026amp; secret key with your partner 2.1 Securely share your access and secret key with your partner, you may choose to encrypt it in a zip archive and exchange via email or memory device, or simply read it out loud (if no one is listening!). The proper way of storing credentials in AWS is secrets manager, however this is a training scenario in a controlled environment with no sensitive data or systems.\n3. List account information in your partners account 3.1 Following step 3 in practical 1 again, replace your CloudShell environment variables for the access and secret key from your partner. You should see their user name they created in the message returned instead of yours. Now you can test a few CLI commands in their account:\nList managed policies attach to user test: aws iam list-attached-user-policies --user-name test\nList S3 buckets: aws s3 ls\nList roles: aws iam list-roles\n4. Investigate the actions they performed 4.1 Start a new CloudShell session by simply opening the service in a new tab and verify that you are back to using your credentials again.\n4.2 Now we want to see what actions your partner has performed back in the CloudTrail console. Clear the filter by selecting Event history again, and as your test account doesn’t have much activity you should see their actions appear.\nIf you are at an event and you and your partner are ahead, experiment with other read-only type commands e.g. describe, list, get.\nPractical 3: Querying CloudTrail using Amazon Athena In this practical we are going to:\nSetup Amazon Athena Query CloudTrail logs 1. Setup Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. You can use Athena to query CloudTrail logs directly stored in S3.\n1.1\tCheck that CloudTrail is configured and the S3 bucket that contains your logs by accessing the CloudTrail console and choose trails from the left side menu.\n1.2\tTake note of the bucket name next to your trail. If you followed the previous prerequisite step your trail will be named default, there may be another trail if you are using an AWS supplied training account.\n1.3\tAccess the S3 console and create a new S3 bucket that will contain logs for the Athena service, in the region where you will be running Athena queries. It’s recommended that this is the same region where your CloudTrail bucket is located. Don\u0026rsquo;t forget bucket names must be globally unique and adhere to bucket naming rules .\n1.4\tAccess the Athena console .\n1.5\tClick Get Started to display the main Athena console.\n1.6 A number of banners will be displayed, look for one Before you run your first query, you need to set up a query result location in Amazon S3 and click the link to set up a query result location in Amazon S3.\n1.7\tIn Query result location Click select and choose the bucket you created before by selecting the small right arrow next to the name. Accept the defaults then Save.\n1.8\tNow create the table for querying the CloudTrail logs. Athena supports both partitioned and unpartitioned tables, this example uses unpartitioned tables as the queries are easier, however if you have lots of logs then partitioning based on date or region can reduce query times. Find out more from the Athena documentation for CloudTrail.\nIn the query editor insert the following query to create the table then click Run query, replace CLOUDTRAIL-BUCKET-NAME with your CloudTrail bucket name:\nCREATE EXTERNAL TABLE cloudtrail_log ( eventversion STRING, useridentity STRUCT\u0026lt; type:STRING, principalid:STRING, arn:STRING, accountid:STRING, invokedby:STRING, accesskeyid:STRING, userName:STRING, sessioncontext:STRUCT\u0026lt; attributes:STRUCT\u0026lt; mfaauthenticated:STRING, creationdate:STRING\u0026gt;, sessionissuer:STRUCT\u0026lt; type:STRING, principalId:STRING, arn:STRING, accountId:STRING, userName:STRING\u0026gt;\u0026gt;\u0026gt;, eventtime STRING, eventsource STRING, eventname STRING, awsregion STRING, sourceipaddress STRING, useragent STRING, errorcode STRING, errormessage STRING, requestparameters STRING, responseelements STRING, additionaleventdata STRING, requestid STRING, eventid STRING, resources ARRAY\u0026lt;STRUCT\u0026lt; ARN:STRING, accountId:STRING, type:STRING\u0026gt;\u0026gt;, eventtype STRING, apiversion STRING, readonly STRING, recipientaccountid STRING, serviceeventdetails STRING, sharedeventid STRING, vpcendpointid STRING ) ROW FORMAT SERDE \u0026#39;com.amazon.emr.hive.serde.CloudTrailSerde\u0026#39; STORED AS INPUTFORMAT \u0026#39;com.amazon.emr.cloudtrail.CloudTrailInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026#39; LOCATION \u0026#39;s3://CLOUDTRAIL-BUCKET-NAME/\u0026#39;; 2. Query CloudTrail logs 2.1 To run a test query, click the triple dots next to the table name under table, then click Preview table. It will automatically create a query for you like SELECT * FROM \u0026quot;default\u0026quot;.\u0026quot;cloudtrail_log\u0026quot; limit 10; and you should see some results. This means Athena is querying your CloudTrail logs directly from S3!\n2.2 Here are some sample queries to get you started, you can find more in an open source repository too: https://github.com/easttimor/aws-incident-response Identities What ARNs are creating the most events? An ARN is an Amazon Reource Name, in this case it\u0026rsquo;s an identity that is creating events by accessing:\nSELECT useridentity.arn, COUNT(useridentity.arn) as count FROM cloudtrail_log group by useridentity.arn order by count DESC Alternate, by PrincipalID:\nSELECT useridentity.principalid, COUNT(useridentity.principalid) as count FROM cloudtrail_log group by useridentity.principalid order by count DESC Check for all uses of specific access key:\nselect eventTime, userIdentity.userName, useridentity.accesskeyid eventName, requestParameters from default.cloudtrail_log WHERE useridentity.accesskeyid = \u0026#39;AKIAEXAMPLE\u0026#39; AND from\\_iso8601\\_timestamp(eventtime) \u0026gt; date\\_add(\u0026#39;day\u0026#39;, -90, now()); order by eventTime Actions \u0026amp; Regions If you want a quick look at what is going on in your account(s) you can do a count per event, look for any services that you don\u0026rsquo;t recognise:\nselect eventsource, eventname, COUNT (eventname) as eventcount FROM cloudtrail_log group by eventsource, eventname order by eventcount DESC High level number of events per region to detect regions in use that you would not normally use:\nselect awsregion, COUNT(awsregion) as region FROM cloudtrail_log group by awsregion order by region DESC Similar query, which events per region ordered by individual event count. Note that it\u0026rsquo;s normal to get a small number of requests for console use:\nselect awsregion, eventname, COUNT (eventname) as eventcount FROM cloudtrail_log group by awsregion, eventname order by eventcount DESC To drill down on a specific region to see what is going on there. Look for EC2 instance launches, access to data, anything that could indicate misuse. In this example region sa-east-1 is queried:\nselect awsregion, eventsource, eventname, COUNT (eventname) as eventcount FROM cloudtrail_log WHERE awsregion = \u0026#39;sa-east-1\u0026#39; group by awsregion, eventsource, eventname order by eventcount DESC Query all actions for specific service:\nselect * from cloudtrail_log where eventsource = \u0026#39;wafv2.amazonaws.com\u0026#39; Check for access denied attempts:\nSELECT * FROM cloudtrail_log where errorcode = \u0026#39;Unauthorized\u0026#39; OR errorcode = \u0026#39;Denied\u0026#39; OR errorcode = \u0026#39;Forbidden\u0026#39; Practical 4: Establish persistence In this practical we are going to:\nCreate a method for persistence in your partners account Monitor your partners actions Share with the class 1. Create a method for persistence in your partners account You need to create at least one method of maintaining persistence in your partners account with the objective of not being discovered quickly. This is a common method that adversaries use to continue their activities after you discover the initial entry point. There are many methods you can use with a few listed below as a start. As you will see they all require privileged access to be granted in the first place, which is why you should only use least-privileged permissions outside of this training. Also consider a combination of methods, the more complex it is the more difficult it will be to contain and eradicate. For this training do not take any action that could lockout your partner from managing their account – play nice! Now is also a good time to create two timelines; one for the actions you are taking on them, and another for the actions they are taking on you.\n1.1 Create an access key for an existing IAM user\nEach IAM user can have 2 access keys, each of which can be enabled or disabled. It’s simple to create a new access key for an existing user if they only have 1 assigned or 1 disabled. If your partner is looking for new events from the IAM user they will start to see the new access key used. This is very simple and can easily be discovered.\n1.2 Create new IAM user\nSimple creating a new IAM user, especially if the account has many of them, is simple however you can gain console access by using a new password instead of changing an existing one. This is very simple and can easily be discovered.\n1.3 Create new IAM role\nCreate an IAM role that can be assumed by an IAM user (using the trust policy) or another AWS service. You could name the new IAM role to look similar to existing ones however you are limited in the names so experiment. You can use this new IAM role in the CLI, the console, and AWS services like EC2.\n1.4 Launch EC2 instance with IAM role\nLaunching an EC2 instance with an existing or new IAM role can allow you to run commands from the instance using the role attached to the instance. Follow the instructions to use the launch wizard and when you get to step 3 create a new IAM role and attach. You could make the instance “look” like others that are running, or even hide it in a region that you know is not being used. You could also simply get the credentials that are vended to the EC2 instance and use them elsewhere – beware that GuardDuty will detect this very quickly!\n1.5 Obfuscate your IP address\nYou can use a VPC endpoint for your EC2 instance that will obfuscate the IP address that will appear in the CloudTrail logs. This could make it more difficult for your partner to find as they won’t be able to search by your IP. This is covered in detail in this article .\n1.6 Get-session-token\nThe get-session-token command in the CLI and associated API action has the ability to create a temporary access and secret key along with a token. You can then use these credentials instead for a limited period of time.\n2. Monitor your partners actions 2.1 Using CloudTrail either through the console or querying via Athena, monitor the actions your partner is taking.\nFirst thing you should look for is attempts to establish persistence, the adverary will want a way back in if their primary mechanism is discovered and stopped. The most simple things they could do would be to get temporary credentials, create IAM access keys, or create IAM users or roles.\nselect eventtime, eventsource, eventname, recipientaccountid, useridentity.arn, useridentity.principalId, awsregion, sourceipaddress, useragent, errorcode, requestparameters, responseelements FROM cloudtrail_log where eventname = \u0026#39;CreateRole\u0026#39; OR eventname = \u0026#39;CreateUser\u0026#39; OR eventname = \u0026#39;CreateAccessKey\u0026#39; OR eventname = \u0026#39;GetFederationToken\u0026#39; order by eventtime 2.2 Using the AWS GuardDuty service explore any findings in your account.\n3. Share with the class Share your methods of establishing persistence with the class, and refer to your timeline.\nPractical 5: Contain \u0026amp; eradicate threat In this practical we are going to:\nContain your partner Monitor your partners actions Eradicate your partner Share with the class 1. Contain your partner The aim of containment is to stop the spread of the threat, in this case its the spreading and re-use of credentials. Using your knowledge of what your partner has created from their actions taken, stop the methods they are using to authenticate into your account.\nHave they created a new IAM user or access key you can disable (deleting means you will not receive any CloudTrail access denied attempts)? Have they launched an EC2 instance? Did that have a new IAM role? Should you isolate it, stop it, or terminate it? Have they created new credentials using get-session-token? 2. Monitor your partners actions When you believe you have contained your partner, keep monitoring for any suspicous actions that will help you eradicate them.\n3. Eradicate your partner The aim of eradication is to completely remove all traces of the threat. What you need to eradicate depends on the actions the adversary performed. Refer to AWS Documentation for removal of different resources you find.\n4. Share with the class Share your methods of establishing persistence with the class, and refer to your timeline.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/","title":"Automated Deployment of Detective Controls","tags":[],"description":"","content":"Last Updated: June 2021\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, Amazon GuardDuty, and AWS Security Hub. You will use the AWS Management Console and AWS CloudFormation to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers.\nAWS Security Hub is a service that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, Amazon Macie, AWS Identity and Access Management (IAM) Access Analyzer, AWS Systems Manager, and AWS Firewall Manager, as well as from AWS Partner Network (APN) solutions. AWS Security Hub continuously monitors your environment using automated security checks based on the AWS best practices and industry standards that your organization follows.\nPrerequisites An AWS account that you are able to use for testing. Permissions to create resources in CloudFormation, CloudTrail, GuardDuty, Config, S3, CloudWatch. Costs Typically less than $2 per month if the account is only used for personal testing or training, and the tear down is not performed AWS CloudTrail pricing Amazon GuardDuty pricing AWS Config pricing Amazon S3 pricing AWS Security Hub pricing AWS Pricing Steps: Create Stack Knowledge Check Tear down References \u0026amp; Useful Resources Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Implement new security services and features: New features such as Amazon GuardDuty have been adopted. Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_reviewing_security_essential_best_practice/","title":"Quest: Reviewing Security Essential Best Practice - Well-Architected Webinar","tags":[],"description":"This quest is a collection of lab patterns which are covered in the June 2021 Webinar &#39;Reviewing Security Essential Best Practice&#39;","content":"About this Guide This quest is a collection of featured lab patterns with are covered in the June 2021 Webinar Reviewing Security Essential Best Practice.\nUsing this collection of labs, the user will be able to walk through the featured patterns from the session which cover best practice relating to multilayered API security, autonomous patching with EC2 Image Builder and Systems Manager and building incident response playbooks with Jupiter notebooks.\nUsing either an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nLab 1 - Multilayered API Security With Cognito and WAF. In this lab we will walk you through an example scenario of building out a multilayered approach to protecting an API using the following services:\nAmazon API Gateway - Used for securing REST API. AWS Secrets Manager - Used to securely store secrets. Amazon CloudFront - Used to prevent direct access to API as well as to enforce encrypted end-to-end connections to origin. AWS WAF - Used to protect our API by filtering, monitoring, and blocking malicious traffic. Amazon Cognito - Used to enable access control for our API layer. Start now! Lab 2 - Autonomous Patching With EC2 Image Builder and Systems Manager. In this lab we will walk you through a blue/green deployment methodology to build an entirely new Amazon Machine Image (AMI) that contains the latest operating system patch, which can be deployed into an application cluster. We will use the following services to complete the workload deployment:\nEC2 Image Builder to automate creation of the AMI Systems Manager Automated Document to orchestrate the execution. CloudFormation with AutoScalingReplacingUpdate update policy, to gracefully deploy the newly created AMI into the workload with minimal interruption to the application availability. Start now! Lab 3 - Incident Response Playbook with Jupyter - AWS IAM. In this lab we will walk you through a hands-on lab which will guide you through running a basic incident response playbook using Jupyter. It is a best practice to be prepared for an incident, and practice your investigation and response tools and processes. We will achieve this :\nJupyter Notebook AWS IAM Start now! Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Authors Tim Robinson - Well-Architected Geo Solution Architect Ben Potter - Principal Security Lead Well-Architected Stephen Salim - Well-Architected Geo Solution Architect Jang Whan Han - Well-Architected Geo Solution Architect "},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/100_labs/100_automating_serverless_best_practices_with_dashbird/3_modern_load_test/","title":"Modern Load Test","tags":[],"description":"","content":"The on-demand nature of the AWS Cloud allows for a production-scale test environment to be deployed in a matter of minutes.\nWhile AWS serverless services allow for efficient scaling individually, it is important to consider the effect of Service Quotas at both region and account level. Ignoring these maximum limits can become immediately impactful to a production workload.\nIn this section we will perform load testing on our deployed Blue Car application to ensure that the architecture scales in alignment to the traffic which is being processed.\n3.1. Get the API Invoke URL and Auth Token. Within our example application, the map is used to request an ambulance. At the point that the selection is made, an HTTPS POST method will be sent to the API with latitude and longitude details. In order to perform an appropriate load test, we will need to ensure that access to the API is authorized. To authorize the API, you will need to generate an authorization token within Amazon Cognito, which will allow for automated load testing.\nComplete the following steps:\nOpen the CloudFormation console at https://console.aws.amazon.com/cloudformation and click oncall-health-amplify-api stack on the left panel. Click Output and copy the ApiEndpoint. From the Blue Car Application, click auth token to see the valid token. Copy YOUR AUTH TOKEN as shown. 3.2. Update the API Invoke URL and Auth Token to perform a load test. Artillery is one of the most popular open-source tools for load testing serverless API. You will configure the number of requests per second and test duration.\nIn Cloud9, open test.yaml in altilery_load_test directory. Please ensure you are still in aws-well-architected-labs/static/wapartners/100_Automating_Serverless_Best_Practices_with_Dashbird/Code/oncall-health-sample-app/ cd artillery_load_test/ vi test.yaml We will need to replace API Invoke URL and Auth Token with the values you have generated. The current configuration is set to 200 requests per a second over a 10-second duration. You should be seeing test.yaml as shown: Run load test by running artillery cli with test configuration as shown below. Run this cli 2-3 times and check to see that no 5xx errors are generated.\nartillery run ./test.yaml Note: If you see 4XX error, your auth token probably has expired.(Access token expiration: 60 minutes) You can refresh the blue car application and get a new auth token by clicking auth token. Please update auth token correctly in test.yaml.\nAs shown below, Dashbird should now identify a new insight. Click the insight labelled oncall-health-amplify-api-AppLambda as shown: You will be redirected to the Events page where you can see the event details associated with the insight. Select the event labelled oncall-health-amplify-api-AppLambda as shown: You will see the error labelled as Lambda function has high error percentage. Click Task timed out after 3.00 seconds to see further error details. You will be able to note that multiple occurrences of the error have occurred. Select one of the occurrences to show the Lambda related statistics such as duration, memory usage, errors and logs. As you can see in the example, the memory utilization is less than 30% of 128MB, so we can confirm that the timeout issue was not related to memory shortage within the Lambda. Now click the aws logo located in the Trace field to debug and analyze our application using AWS X-Ray. From the trace window, configure the view to display the Last 30 minutes and click RESPONSE TIME Tab. This will display Trace IDs that took more than 3 seconds to be completed. Click one of Trace IDs to view further details as shown. You can now view the total time length required for the lambda to be initialized and run. The total duration of the Lambda function shown below appears to be more than 3 seconds. As there is no Overhead shown, we do not have an issue with the Lambda Initialization. However, we can see that most of the run time has been spent on Invocation and we do not see any operations related to DynamoDB. This could indicate that DynamoDB may not be ready to respond to a new Lambda request. Click Traces to have a look at other Trace IDs that were successfully completed in 3 seconds. The Initialization subsegment represents the init phase of the Lambda execution environment lifecycle. During this phase, Lambda creates or unfreezes an execution environment with the resources you have configured, downloads the function code and all associated layers, initializes extensions, initializes the runtime and runs the function\u0026rsquo;s initialization code. The Invocation subsegment represents the invoke phase where Lambda invokes the function handler. This begins with runtime and extension registration and ends when the runtime is ready to send the response. The Overhead subsegment represents the phase that occurs between the time when the runtime sends the response and the signal for the next invoke. During this time, the runtime finishes all tasks related to an invoke and prepares to freeze the sandbox.\nNow add responsetime \u0026lt; 3 in search bar to sort by response time and click RESPONSE TIME Tab. Select one of Trace IDs that has 2.9 seconds as shown: responsetime \u0026lt; 3 The Lambda function has succesfully added items to the DynamoDB table. However, within the Invocation subsegment, we found that the Lambda function was waiting for the majority of the 3 seconds of the actual response time to put item into DynamoDB which only takes a few milliseconds. This indicates that DynamoDB may not have enough write capacity to put new items requested by Lambda after its initialization subsegment which could indicate our underlying issue. Let\u0026rsquo;s go to DynamoDB and see if we can get more information there. From the DynamoDB console, select the tables view and click Rides. In the Overview tab, DynamoDB provide Table capacity metrics. As you can see from the below screenshot, there are a high number of Write throttled requests triggered by DynamoDB. The throttling behaviour from DynamoDB explains why the Lambda function spent most of its time in a waiting state. Take a note of Average item size to calculate the appropriate write capacity unit. However, DynamoDB updates the following information approximately every six hours. Average item size for this lab will be 137 byte. Let\u0026rsquo;s use this size.\nTo address the throttling, go to the Additional settings and select Edit to enable DynamoDB auto-scaling. In Capacity calculation, we can calculate the Maximum Write capacity units based on Average item size and the number of item write per second. Use the previously recorded Average item size and 200 write items per second. If so, you will get 200 Write capacity units. Again, this is Maximum Write capacity and DynamoDB will dynamically adjust the provisioned capacity. Our test ultimately reached a peak of 200 write requests per second but we do not consistently write this amount of write. Hence, we will configure Auto scaling that dynamically adjusts provisioned throughput capacity on our behalf, in response to actual traffic patterns under monitoring. Auto Scaling enables us to effectively eliminate the guesswork involved in provisioning adequate capacity. Please see Managing Throughput Capacity Automatically with DynamoDB Auto Scaling and check the estimated cost before you enable auto scaling. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nDynamoDB auto scaling will modify provisioned throughput settings only when the actual workload stays elevated (or depressed) for a sustained period of several minutes. After running the workload test multiple times for a minute in cloud 9, DynamoDB auto scaling increased write capacity units from 1 to 81 based on the actual workload. You should be seeing Auto scaling activities in Additional settings. If you perform the same load test a few times, 5xx errs will be gone. Now I am able to put all 2000 items with 201 response. If you see 4XX error, please update auth token correctly in test.yaml as your auth token has expired. artillery run ./test.yaml Once you stop testing, DynamoDB auto scaling will automatically scale down the write capacity units to 1. END OF SECTION 3\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/3_prevent_requests_from_accessing_api_directly/","title":"Prevent requests from accessing API directly","tags":[],"description":"","content":"In this section you will be building your own distribution of Amazon CloudFront which offers protection at the edge of your architecture. When integrating CloudFront with regional API endpoints, not only does the service distribute traffic across multiple edge locations to improve performance, but also it supports geoblocking, which you can use to help block requests from particular geographic locations from being served. With Amazon CloudFront, you can also enforce encrypted end-to-end connections to an origin API by using HTTPS. Additionally, CloudFront can automatically close connections from slow reading or slow writing attackers. API Gateway can then be configured to only accept requests only from CloudFront. This helps prevent anyone from accessing your API Gateway deployment directly. In this section of the lab, you will use a CloudFront custom header value generated by AWS Secrets Manager in combination with AWS WAF .\nThis will build out the architecture as follows:\nTo complete this section, follow the steps listed below:\n3.1. Get the Cloudformation Template. To deploy the second CloudFormation template, you can deploy directly via the console. You can get the template here. Click here for CloudFormation console deployment steps Console: If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation .\nChoose a stack template. Go to the Outputs section of your previous cloudformation stack that you used to deploy the base lab infrastructure. Copy and paste APIGatewayURL and provide your S3 bucket name. The other values can be left as default. When you are finished with the configuration click \u0026lsquo;Next\u0026rsquo; as shown below. It may take 3~4 minutes to complete this deployment. Review the template and scroll to the bottom of the screen. Make sure you acknowledge the IAM resource creation and Auto_expand capability by selecting the three tick boxes as shown. When you are ready, click on create stack.\n3.2 Add custom HTTP headers to the requests. When your stack has finished deployment, you can now add a custom header to the request in CloudFront. As previously described, this will prevent users from bypassing CloudFront to access your API directly. More information on this topic can be found here. To complete this configuration, complete the following steps:\nWhen the current CloudFormation stack has finished deployment, go to the Outputs section of the stack. Verify that your OriginVerifyHeaderName is set to X-Origin-Verify. Now click the value for OriginVerifyHeader to take you back to AWS Secrets Manager as shown: From AWS Secrets Manager, click the OriginVerifyHeader secret to get the X-Origin-Verify header value: From the Secret Value dialog box, click Retrieve secret value. Record the Secret Value of HEADERVALUE. We will use this value as the Origin Header value in CloudFront. Secrets Manager will now be responsible for automatically rotating this header value to prevent possible future compromise in the same way that we rotated the password information earlier in the lab. Now we can configure CloudFront. You do this by either lauching CloudFront from the console in the standard way, or by clicking the URL of CloudFrontConsole in the Outputs section of the current CloudFormation stack. You should be able to see our newly created CloudFront distribution. Click on the ID field as shown: Go to the Origins and Origin Groups tab and select your CloudFront Origin to edit its configuration as shown: In the Origin Custom Headers section, enter \u0026lsquo;X-Origin-Verify\u0026rsquo; as the Header Name and use the previously recorded value from the AWS Secrets Manager HEADERVALUE as the Value as shown: CloudFront will now add the Origin Custom Header into all incoming request from the client applications or user traffic.\nNote that your configuration won\u0026rsquo;t take effect until the distribution has propagated to the AWS edge locations. It may take 3~5 minutes so please be patient. You can check the status of the propagation as shown: When your CloudFront propagation completes, we will move to configuring WAF. The second CloudFormation template which we deployed, automatically creates Web ACLs and added associated rules to inspect the incoming request header X-Origin-Verify. A Web ACL (Web Access Control List) is the core resource in an AWS WAF deployment. It contains rules that are evaluated for each request that it receives. To review the WAF rules, go to the current CloudFormation stack Outputs tab and click WAFWebACLR to go to Web ACLs as shown:\nGo to Rules, there will be only one rule created as {Your Stack Name}-XRule. By default, any requests that do not match the listed rules in the Web ACLs will be blocked. In this case, the rule will allow the request ONLY if the string value of X-Origin-Verify is valid. If the header isn’t valid, AWS WAF blocks the request:\nLet\u0026rsquo;s test our configuration to check functionality. To do this go to the stack Outputs tab of the current CloudFormation template. From there, click CloudFrontEndpoint to verify if now API clients access your API only through a CloudFront distribution. It will automatically have query string. Clicking on the link should show a successful query of your favourite player.\nTake a note of CloudFrontEndpoint as we will often use this URL for testing.\n3.3 Verify if API only accepts the request from CloudFront. From the console access the Cloud9 service and go to your IDE. We will now test with both the CloudFrontEndpoint and the APIGatewayURL to see the difference in behaviour. Firstly, change to the walab-script directory and execute the script called sendRequest.py with the argument of your CloudFrontEndpoint as shown: python sendRequest.py \u0026#39;CloudFrontEndpoint\u0026#39; Note the output from the previous command and now execute the same script with the argument of your APIGatewayURL as shown: python sendRequest.py \u0026#39;APIGatewayURL\u0026#39; You should notice a different output based on the argument that you pass to the script. When you run the script with the CloudFrontEndPoint argument, you should receive a 200 response code, which indicates a successful query. However, when you run the same script against the APIGatewayURL directly, you will notice that a 403 response code is generated. This indicates that the request is detected as forbidden as this request does not have the X-Origin-Verify field together with the required value. Run both script outputs a few times to generate some data.\nCheck that your output is similar to the screenshot below:\nNow go back to the WAF Console and select the Overview tab to see the metrics of the ACL. You can confirm your request was blocked by WAF from this metrics. Click {Your Stack Name}-ACLMetricR BlockedRequests BlockedRequests to see only blocked request by SQL database. Your graph shape will be different depending on the number of times you executed the script with the CloudFront Endpoint and the APIgateway address.\nEND OF SECTION 3\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_8_tag_policies/3_compliance_report/","title":"Check for non-compliant resources","tags":[],"description":"","content":"Launch an EC2 with compliant tags and one with noncompliant tags\nNavigate to the EC2 service in the navigation bar and click Launch instance Select the Amazon Linux Free tier eligible AMI Select the free tier eligible t2.micro instance type and click Review and Launch Click “Edit tags” on the right side of the page across from the Tags dropdown and Click Add Tag Add the Tag Key (environment) and Tag Value (uat) and click Review and Launch On the Review Instance Launch page click Launch Select Key Pair and click Launch Instances. Repeat steps 1 - 6 to launch an instance with the Tag Key (environment) and Tag Value (unknown) View non-compliant resources\nNavigate to the Resource Groups and Tag Editor service in the navigation bar and click Tag Policies in the left-hand panel Click “This AWS account” You should now see the tag policies you created, and the non-compliant resources (with tag value \u0026ldquo;research\u0026rdquo;). You can also filter compliance results by region. Please note, that it might take a couple of minutes for newly created resources to be included in results. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/trusted-advisor-dashboards/dashboards/3_deployment/","title":"TAO Dashboard Deployment","tags":[],"description":"","content":"Deployment Options There are 3 options to deploy the TAO Dashboard. If you are unsure what option to select, we recommend using the Manual deployment\nOption 1: Manual Deployment This option is the manual deployment and will walk you through all steps required to create this dashboard without any automation. We recommend this option users new to Athena and QuickSight. Click here to continue with the manual deployment Stage 1 - Prepare config files Collect information to create config files\n{account} - AWS Account in which Dashboard is deployed {region} - AWS region for dashboard deployment {user_arn} - QuickSight user arn with admin permissions. Can be retrieved with following command:\naws quicksight list-users --aws-account-id {account} --namespace default --region {region} --query \u0026#39;UserList[*].Arn\u0026#39; {s3FolderPath} - path to S3 folder created in Stage 1 in following format s3://{bucket_name}/reports/ {databaseName} - AWS glue data catalog database name. You can use any existing database or create new Create AWS Glue database Download following files and replace placeholders with respective values\nathena-table.json - placeholders {databaseName} in line 2, {account} in line 3 and {s3FolderPath} in line 7 dashboard-input.json - placeholders {account} in lines 2 and 30, {region} in line 30 and {user_arn} in line 7 data-set-input.json - placeholders {user_arn} in line 6, {account} in line 24, {region} in line 24, {databaseName} in line 26 data-source-input.json - placeholder {user_arn} in line 15 update-dashboard-input.json - placeholder {account} in line 2 and 15 and {region} in line 15 OR\ndownload all templates in one click Stage 2 - Create required resources and deploy dashboard We recommend to use AWS CloudShell for workshop Verify AWS CLI is v2.1.16 and above. Check the version by issuing the aws --version command at the shell prompt. To upgrade AWS CLI, find the instructions here Upload all 5 config files to CloudShell\nUsing CloudShell change a directory to where you\u0026rsquo;ve uploaded files in the previous step\nCreate resources\nCreate Glue table is the metadata definition: aws glue create-table --region {region} --cli-input-json file://athena-table.json Create QuickSight datasource: aws quicksight create-data-source --aws-account-id {account} --region {region} --cli-input-json file://data-source-input.json Create QuickSight dataset: aws quicksight create-data-set --aws-account-id {account} --region {region} --cli-input-json file://data-set-input.json Create QuickSight dashboard: aws quicksight create-dashboard --aws-account-id {account} --region {region} --cli-input-json file://dashboard-input.json Get status of dashboard deployment: aws quicksight describe-dashboard --aws-account-id {account} --region {region} --dashboard-id ta-organizational-view NOTE: Congratulations dashboard is deployed! Please log in to QuickSight and open https://{region}.quicksight.aws.amazon.com/sn/dashboards/ta-organizational-view/ Option 2: Automation Scripts Deployment The Cloud Intelligence Dashboards automation repo is an optional way to create the Cloud Intelligence Dashboards using a collection of setup automation scripts. The supplied scripts allow you to complete the workshops in less than half the time as the standard manual setup.\nClick here to continue with the Automation Scripts Deployment Follow the How to use steps for installation and dashboard deployment. We recommend to use AWS CloudShell for automated deployment Option 3: CloudFormation Deployment This section is optional way to deploy TAO Dashboard using a CloudFormation template. The CloudFormation template allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates and create an IAM role. If you do not have the required permissions use the Manual or Automation Scripts Deployment.\nClick here to continue with the CloudFormation Deployment NOTE: An IAM role will be created when you create the CloudFormation stack. Please review the CloudFormation template with your security team and switch to the manual setup if required Create TAO Dashboard using a CloudFormation Template Login via SSO in your Cost Optimization account\nClick the Launch CloudFormation button below to open the pre-populated stack template in your CloudFormation console and select Next\nLaunch CloudFormation Template Enter a Stack name for your template such as TAO-Dashboard-QuickSight Review Information parameter to confirm prerequisites before specifying the other parameters Update your S3 Bucket Path to the TA Reports with the S3 path where your Trusted Advisor reports are stored. Path should look like s3://{bucketname}/reports or s3://costoptimizationdata{account_id}/optics-data-collector/ta-data/ depending on data collection method used in Create and Upload Trusted Advisor Report step before. Update your Athena Database Name with the name of the CUR Athena Database where you want to deploy table for TA reports. Leave default if you are not sure which database name provide: Update QuickSight Username parameter with your QuickSight username To validate your QuickSight username complete the tasks below:\nOpen a new tab or window and navigate to the QuickSight console Find your username in the top right navigation bar Add the identified username to the CloudFormation parameter Update Quicksight Identity Region parameter with your QuickSight region Optional add a Suffix if you want to create multiple instances of the same account. Optional specify a Preferred Refresh Schedule for QuickSight dataset refresh. Leave empty if no automated refresh is needed. Select Next at the bottom of Specify stack details and then select Next again on the Configure stack options page\nReview the configuration, click I acknowledge that AWS CloudFormation might create IAM resources, and click Create stack. You will see the stack will start in CREATE_IN_PROGRESS NOTE: This step can take 5-15mins Once complete, the stack will show CREATE_COMPLETE Navigate to Dashboards page in your QuickSight console, click on Trusted Advisor Organizational View dashboard to open it Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you will need want to share your dashboard with users or customize your own version of this dashboard - Click to navigate QuickSight steps X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/exportimport_utility/3_executing/","title":"Script usage examples","tags":[],"description":"","content":"Exporting a workload to a JSON file Example Command: ./exportImportWAFR.py -f workload_output.json --exportWorkload --profile acct2 -w c896b2b1142f6ea8dc22874674400002\n$ ./exportImportWAFR.py -f workload_output.json --exportWorkload --profile acct2 -w c896b2b1142f6ea8dc22874674400002 2021-05-19 15:39:46.921 INFO exportImportWAFR - main: Script version 0.1 2021-05-19 15:39:46.921 INFO exportImportWAFR - main: Starting Boto 1.17.27 Session 2021-05-19 15:39:47.066 INFO exportImportWAFR - main: Exporting workload \u0026#39;c896b2b1142f6ea8dc22874674400002\u0026#39; to file workload_output.json 2021-05-19 15:39:47.473 INFO exportImportWAFR - main: Gathering overall review for lens wellarchitected 2021-05-19 15:39:47.780 INFO exportImportWAFR - main: Retrieving all answers for lens wellarchitected 2021-05-19 15:40:00.024 INFO exportImportWAFR - main: Gathering overall review for lens serverless 2021-05-19 15:40:00.217 INFO exportImportWAFR - main: Retrieving all answers for lens serverless 2021-05-19 15:40:02.823 INFO exportImportWAFR - main: Gathering overall review for lens softwareasaservice 2021-05-19 15:40:03.052 INFO exportImportWAFR - main: Retrieving all answers for lens softwareasaservice 2021-05-19 15:40:14.197 INFO exportImportWAFR - main: Gathering overall review for lens foundationaltechnicalreview 2021-05-19 15:40:14.437 INFO exportImportWAFR - main: Retrieving all answers for lens foundationaltechnicalreview 2021-05-19 15:40:28.523 ERROR exportImportWAFR - findAllQuestionId: ERROR - Unexpected error: An error occurred (InternalServerErrorException) when calling the ListAnswers operation (reached max retries: 4): [InternalServerError] An internal error occurred. 2021-05-19 15:40:38.184 ERROR exportImportWAFR - findAllQuestionId: ERROR - Unexpected error: An error occurred (InternalServerErrorException) when calling the ListAnswers operation (reached max retries: 4): [InternalServerError] An internal error occurred. 2021-05-19 15:40:40.925 INFO exportImportWAFR - main: Export completed to file workload_output.json Importing a workload from a JSON file Example Command: ./exportImportWAFR.py -f workload_output.json --importWorkload --profile acct2 --region us-west-1\n$ ./exportImportWAFR.py -f workload_output.json --importWorkload --profile acct2 --region us-west-1 2021-05-19 15:41:31.711 INFO exportImportWAFR - main: Script version 0.1 2021-05-19 15:41:31.711 INFO exportImportWAFR - main: Starting Boto 1.17.27 Session 2021-05-19 15:41:31.836 INFO exportImportWAFR - main: Creating a new workload from file workload_output.json 2021-05-19 15:41:32.567 INFO exportImportWAFR - main: New workload id: c896b2b1142f6ea8dc22874674400003 (arn:aws:wellarchitected:us-west-1:222222222222:workload/c896b2b1142f6ea8dc22874674400003) 2021-05-19 15:41:32.567 INFO exportImportWAFR - main: Verifying lens version before restoring answers 2021-05-19 15:41:32.812 INFO exportImportWAFR - main: Versions match (2020-07-02) 2021-05-19 15:41:32.812 INFO exportImportWAFR - main: Retrieving all answers for lens wellarchitected 2021-05-19 15:41:33.048 INFO exportImportWAFR - main: Copying answers into new workload for lens wellarchitected 2021-05-19 15:41:49.368 INFO exportImportWAFR - main: Verifying lens version before restoring answers 2021-05-19 15:41:49.601 INFO exportImportWAFR - main: Versions match (2020-02-04) 2021-05-19 15:41:49.601 INFO exportImportWAFR - main: Retrieving all answers for lens serverless 2021-05-19 15:41:49.848 INFO exportImportWAFR - main: Copying answers into new workload for lens serverless 2021-05-19 15:41:52.862 INFO exportImportWAFR - main: Verifying lens version before restoring answers 2021-05-19 15:41:53.197 INFO exportImportWAFR - main: Versions match (2020-12-03) 2021-05-19 15:41:53.197 INFO exportImportWAFR - main: Retrieving all answers for lens softwareasaservice 2021-05-19 15:41:53.499 INFO exportImportWAFR - main: Copying answers into new workload for lens softwareasaservice 2021-05-19 15:41:58.677 INFO exportImportWAFR - main: Verifying lens version before restoring answers 2021-05-19 15:41:58.899 INFO exportImportWAFR - main: Versions match (2020-12-03) 2021-05-19 15:41:58.900 INFO exportImportWAFR - main: Retrieving all answers for lens foundationaltechnicalreview 2021-05-19 15:41:59.228 INFO exportImportWAFR - main: Copying answers into new workload for lens foundationaltechnicalreview 2021-05-19 15:42:04.291 INFO exportImportWAFR - main: Copy complete - exiting X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/copy-to-secondary/","title":"Copy to Secondary Region","tags":[],"description":"","content":"Now we are going to copy our backup resources to the secondary N. California (us-west-1) region.\nWe will perform the following:\nCopy the RDS backup Copy the EC2 AMI (Amazon Machine Image) X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/backup-resources/s3/","title":"S3","tags":[],"description":"","content":"Create the S3 UI bucket in the secondary N. California (us-west-1) region. 1.1 Click S3 to navigate to the dashboard.\n1.2 Copy the name of the UI bucket that is in N.Virginia (us-east-1) region. It will be similar to backupandrestore-uibucket-xxxx.\n1.3 Click the Create bucket button.\n1.4 Enter the bucket name using the UI bucket that you copied in Step 1.2 and append “-dr”.\n1.5 Select N. California (us-west-1) as the AWS Region.\n1.6 In the Object Ownership section. Select ACLs enabled.\n1.7 In the Block Public Access settings for this bucket section. Disable the Block all public access checkbox including all children. Enable the I acknowledge that the current settings\u0026hellip;. checkbox.\n1.8 Click the Create Bucket button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/disaster/modify-application/","title":"Modify Application","tags":[],"description":"","content":"EC2 1.1 Click EC2 to navigate to the dashboard in the N. California (us-west-1) region.\n1.2 Click the Instances (running) link.\n1.3 Select the BackupAndRestoreSecondary instance, then click the Connect button.\n1.4 Click the Session Manager link, then click the Connect button.\n1.5 After a brief moment, a terminal prompt will display.\n1.6 Let\u0026rsquo;s connect to the database in the secondary N. California (us-west-1) region.\nsudo su ec2-user cd /home/ec2-user Replace the below backupandrestore-secondary-region.xxxx.us-west-1.rds.amazonaws.com with the endpoint you copied from RDS Section .\nsudo mysql -u UniShopAppV1User -P 3306 -pUniShopAppV1Password -h backupandrestore-secondary-region.xxxx.us-west-1.rds.amazonaws.com Type show databases.\nMySQL [(none)]\u0026gt; show databases; You should see the following:\n+--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | unishop | +--------------------+ 5 rows in set (0.02 sec) Type exit to return to the command prompt.\nMySQL [(none)]\u0026gt; exit 1.7 Open the unishopcfg.sh file for editing with either nano or vi.\nsudo nano unishopcfg.sh Tip: You can use the vi (Debian ManPage ) or nano command (Debian ManPage ) to edit the document.\n1.8 Replace the backupandrestore-secondary-region.xxxx.us-west-1.rds.amazonaws.com with the endpoint you copied from RDS Section . Change the *AWS_DEFAULT_REGION to us-west-1. Add the -dr to the end of the UI_RANDOM_NAME.\n#!/bin/bash export Database=backupandrestore-secondary-region.xxxx.us-west-1.rds.amazonaws.com export DB_ENDPOINT=backupandrestore-secondary-region.xxxx.us-west-1.rds.amazonaws.com export AWS_DEFAULT_REGION=us-west-1 export UI_RANDOM_NAME=backupandrestore-uibucket-xxxx-dr Exit the editor, saving your changes.\n1.9 Lets use the source command which reads and executes commands from the unishopcfg.sh file\nsource unishopcfg.sh 1.10 Let\u0026rsquo;s copy the application files to the S3 buckets.\nIf our S3 buckets contained application data then it would be necessary to schedule recurring backups to meet the target RPO. This could be done with Cross Region Replication. Since our buckets contains no data, only code, we will restore the contents from the EC2 instance.\nsudo aws s3 cp /home/ec2-user/UniShopUI s3://$UI_RANDOM_NAME/ --recursive --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers 1.11 Reboot the EC2 instance so our changes take effect.\nsudo reboot Create application configuration file 2.1 Using your favorite editor, create a new file named config.json file. Set the host property equal to the EC2 public IPv4 DNS name copied from EC2 Section . Add (\u0026lsquo;http://\u0026rsquo;) and remove any trailing slash (/) if one is present. Finally, set the region property to us-west-1.\n{ \u0026#34;host\u0026#34;: \u0026#34;http://{{Replace with your EC2 public IPv4 DNS name copied from EC2 section}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-1\u0026#34; } Your final config.json should look similar to this example.\n{ \u0026#34;host\u0026#34;: \u0026#34;http://ec2-XXX-XXX-XXX-XXX.us-west-1.compute.amazonaws.com\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-1\u0026#34; } S3 3.1 Click S3 to navigate to the dashboard.\n3.2 Click the link for backupandrestore-uibucket-xxxx-dr.\n3.3 Click the Upload button.\n3.4 Click the Add Files button and specify the config.json file..\n3.5 In the Permissions Section section. Select the Specify Individual ACL permissions radio button. Enable the Read checkbox next to Everyone (public access) grantee.\n3.6 Enable the I understand the effects of these changes on the specified objects. checkbox. Click the Upload button.\nClick the Close button.\n3.7 Click the Properties link. In the Static website hosting section, click the Edit button.\n3.8 In the Static website hosting section select the Enable radio button. Enter index.html as the Index document and enter error.html as the Error document.\n3.9 Click the Save changes button.\n3.10 In the Static website hosting section. Click on the Bucket website endpoint link.\nCongratulations ! You should see your application The Unicorn Shop in the us-west-1 region. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/prerequisites/account-setup/secondary-region/","title":"Secondary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Create the application in the secondary region N. California (us-west-1) by launching CloudFormation Template .\n1.2 Specify stack details.\nChange the IsPrimary parameter to value no.\nLeave IsPromote and LatestAmiId as the default values\n1.3 Click Next to continue.\n1.4 Leave the Configure stack options page defaults and click Next to continue.\n1.5 Scroll to the bottom of the page and click the checkbox to acknowledge IAM role creation, then click Create stack.\nWait for the stack creation to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/prerequisites/account-setup/secondary-region/","title":"Secondary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Create the application in the secondary region N. California (us-west-1) by launching this CloudFormation Template .\n1.2 Specify stack details.\nChange the IsPrimary parameter to value no.\nLeave IsPromote and LatestAmiId as the default values\n1.3 Click Next to continue.\n1.4 Leave the Configure stack options page defaults and click Next to continue.\n1.5 Scroll to the bottom of the page and click the checkbox to acknowledge IAM role creation, then click Create stack.\nWait for the stack creation to complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/prerequisites/account-setup/secondary-region/","title":"Secondary Region","tags":[],"description":"","content":"Deploying the Amazon CloudFormation Template 1.1 Create the application in the secondary region N. California (us-west-1) by launching this CloudFormation Template .\n1.2 Specify stack parameters.\nChange the IsPrimary parameter to value no.\nLeave LatestAmiId as the default values\n1.3 Click Next to continue.\n1.4 Leave the Configure stack options page defaults and click Next to continue.\n1.5 Scroll to the bottom of the page and click the checkbox to acknowledge IAM role creation, then click Create stack.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/verify-aurora-writefwd/","title":"Verify Aurora Write Forwarding","tags":[],"description":"","content":"Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.\nThe Read-Replica Write Forwarding feature\u0026rsquo;s typical latency is under one second from secondary to primary databases. This capability enables low latency global reads across your global presence. In disaster recovery situations, you can promote a secondary region to take full read-write responsibilities in under a minute.\nNow, let us verify Amazon Aurora MySQL Read-Replica Write Forwarding on our Amazon Aurora MySQL Replica instance!\nVerifying Amazon Aurora Write Forwarding 1.1 Click RDS to navigate to the dashboard in the N. California (us-west-1) region.\n1.2 Click DB Instances link.\n1.3 Click the hot-standby-passive-secondary link.\n1.4 Click the Configuration link and verify Read replica write forwarding is Enabled.\nCongratulations! You have verified the Amazon Aurora Global Database supports Read-Replica Write Forwarding! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/verify-primary-website/","title":"Verify Website","tags":[],"description":"","content":"Primary Region 1.1 Navigate to CloudFormation Stacks in N. Virginia (us-east-1) region.\n1.2 Choose the Pilot-Primary stack.\n1.3 Navigate to the Outputs tab.\n1.4 Click on the WebsiteURL output link.\nPlay with the Primary application 2.1 Register yourself into the application. You need to provide an e-mail address, which does not need to be valid. However, be sure to remember this value to verify the data replication later.\n2.2 You will see a confirmation message saying Successfully Signed Up. You may now Login.\n2.3 Log in to the application using your e-mail address from the previous step.\n2.4 Add/remove items to your shopping cart by clicking on a Unicorn, followed by clicking the Add to cart button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/verify-websites/","title":"Verify Websites","tags":[],"description":"","content":"Primary Region 1.1 Navigate to CloudFormation Stacks in N. Virginia (us-east-1) region.\n1.2 Choose the Warm-Primary stack.\n1.3 Then navigate to the Outputs tab.\n1.4 Click on the WebsiteURL output link.\nPlay with the Primary application 2.1 Register yourself into the application. You need to provide an e-mail address, which does not need to be valid. However, be sure to remember this value to verify the data replication later.\n2.2 You will see a confirmation message saying Successfully Signed Up. You may now Login.\n2.3 Log in to the application using your e-mail address from the previous step.\n2.4 Add/remove items to your shopping cart by clicking on a Unicorn, followed by clicking the Add to cart button.\nSecondary Region 3.1 Navigate to CloudFormation Stacks in N. California (us-west-1) region.\n3.2 Choose the Warm-Secondary stack.\n3.3 Then navigate to the Outputs tab.\n3.4 Click on the WebsiteURL output link.\nPlay with the Secondary application 4.1 Log in to the application using your e-mail address from the previous step.\n4.2 Try adding a new item to the cart. The web app should increase the total cart items count shown at the top of the page.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/3_replace_hardcoded_credentials/","title":"Step 3 - Replace hardcoded credentials","tags":[],"description":"","content":"In this exercise we will use AWS Secrets Manager to easily manage and retrieve credentials i.e., username/passwords, API Keys and other secrets through their Lifecyle.\nAs a Pre-requisite this lab requires Amazon Relational Database Service (RDS) MySQL server, Amazon Elastic Container Service (ECS) cluster (with a container-based application), Amazon Elastic Container Registry (ECR).\nFrom the AWS console, click Services and select Secrets Manager.\nOn the Secrets Manager console click on Store a new secret.\nOn \u0026lsquo;Store a new secret\u0026rsquo; screen click on Select the Credentials for RDS database radio button.\nEnter the values for User name and Password fields respectively.\nSelect DefaultEncryptionKey in the dropdown menu.\nScroll down to the bottom of the page and you will see a list of your RDS instances. Select the RDS instance for which you want to store the secret.\nClick Next.\nEnter a name for the secret and provide optional description.\nClick Next.\nOn \u0026lsquo;Configure automatic rotation\u0026rsquo; screen leave the default values as is i.e., Disable automatic rotation, click Next.\nOn the Review screen, click Store. You will see a message saying that your secret has been successfully stored.\nNow click the Secret name that you have just created.\nCopy the Amazon Resource Name or ARN for later use.\nFrom the AWS console, click Services and select Elastic Container Service.\nSelect the Clusters menu item to view the stack that you want to configure.\nClick the Task Definitions menu item.\nClick the check box next to the appropriate task definition name and then click Create new revision.\nLeave all of the current values in place. Scroll down and click Configure via JSON. Look for the list named secrets. It should have null value for now i.e., \u0026ldquo;\u0026ldquo;secrets\u0026rdquo;:null,\u0026rdquo;\nEdit text and insert the copied ARN of the secret that was created earlier i.e., smsdemo, as shown below.\n\u0026#34;secrets\u0026#34;: [ { \u0026#34;valueFrom\u0026#34; :\u0026#34;\u0026lt;paste the ARN you copied earlier\u0026#34;\u0026gt; \u0026#34;name\u0026#34;: :TASKDEF_SECRET\u0026#34; } ] Click Save to save the revised JSON definition.\nClick Create to create the new revision of the Task Definition that includes the JSON revisions.\nYou will see a message saying that the new revision has been created. Notice that the revision has a version number attached to it as shown in the figure below.\nFor more information please read the AWS User Guide: https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/export_to_xlsx/","title":"Export Well-Architected content to XLSX Spreadsheet","tags":[],"description":"","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction The purpose of this lab is to teach you how to use the AWS SDK for Python (Boto3) to retrieve all of the questions, best practices, and improvement plan links for each lens and export the results to a XLSX spreadsheet. You can use this spreadsheet to prepare for upcoming Well-Architected reviews or to utilize WA reviews in non-supported regions.\nPrerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Python 3.9+ AWS SDK for Python (Boto3) installed Costs: There are no costs for copying or creating new WellArchitected Reviews Steps: Configure Environment Python Code Script usage examples X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/export_to_xlsx/3_executing/","title":"Script usage examples","tags":[],"description":"","content":"Generating a XLSX spreadsheet with all questions, best practices, and improvement plan links Example Command: ./exportAnswersToXLSX.py --fileName ./demo.xlsx --profile acct2 --region us-east-1\n$ ./exportAnswersToXLSX.py --fileName ./demo.xlsx --profile acct2 --region us-east-1 2021-05-05 10:27:24.917 INFO exportAnswersToXLSX - main: Script version 0.1 2021-05-05 10:27:24.918 INFO exportAnswersToXLSX - main: Starting Boto 1.17.27 Session 2021-05-05 10:27:25.018 INFO exportAnswersToXLSX - main: No workload ID specified, we will create a TEMP workload 2021-05-05 10:27:25.314 INFO exportAnswersToXLSX - main: Lenses available: [\u0026#34;serverless\u0026#34;, \u0026#34;wellarchitected\u0026#34;, \u0026#34;softwareasaservice\u0026#34;, \u0026#34;foundationaltechnicalreview\u0026#34;] 2021-05-05 10:27:25.314 INFO exportAnswersToXLSX - main: Creating a new workload to gather questions and answers 2021-05-05 10:27:25.574 INFO exportAnswersToXLSX - main: Creating xlsx file \u0026#39;./demo.xlsx\u0026#39; 2021-05-05 10:29:32.879 ERROR exportAnswersToXLSX - findAllQuestionId: ERROR - Unexpected error: An error occurred (InternalServerErrorException) when calling the ListAnswers operation (reached max retries: 4): [InternalServerError] An internal error occurred. 2021-05-05 10:29:36.341 ERROR exportAnswersToXLSX - findAllQuestionId: ERROR - Unexpected error: An error occurred (InternalServerErrorException) when calling the ListAnswers operation (reached max retries: 4): [InternalServerError] An internal error occurred. 2021-05-05 10:29:58.579 INFO exportAnswersToXLSX - main: Closing Workbook File 2021-05-05 10:29:58.743 INFO exportAnswersToXLSX - main: Removing TEMP Workload 2021-05-05 10:29:59.652 INFO exportAnswersToXLSX - main: Done Example spreadsheet Generating a XLSX spreadsheet from an existing WellArchitected Workload Example Command: ./exportAnswersToXLSX.py --fileName ./demo_existing.xlsx --profile acct2 --region us-east-1 --workloadid c896b2b1142f6ea8dc22874674400002\n$ ./exportAnswersToXLSX.py --fileName ./demo_existing.xlsx --profile acct2 --region us-east-1 --workloadid c896b2b1142f6ea8dc22874674400002 2021-05-05 11:00:01.031 INFO exportAnswersToXLSX - main: Script version 0.1 2021-05-05 11:00:01.032 INFO exportAnswersToXLSX - main: Starting Boto 1.17.27 Session 2021-05-05 11:00:01.215 INFO exportAnswersToXLSX - main: User specified workload id of c896b2b1142f6ea8dc22874674400002 2021-05-05 11:00:01.743 INFO exportAnswersToXLSX - main: Lenses for c896b2b1142f6ea8dc22874674400002: [\u0026#34;wellarchitected\u0026#34;] 2021-05-05 11:00:01.743 INFO exportAnswersToXLSX - main: Creating xlsx file \u0026#39;./demo_existing.xlsx\u0026#39; 2021-05-05 11:00:15.073 INFO exportAnswersToXLSX - main: Closing Workbook File 2021-05-05 11:00:15.186 INFO exportAnswersToXLSX - main: Done Example spreadsheet X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/copy_wa_review_between_accounts/3_executing/","title":"Script usage examples","tags":[],"description":"","content":"Below are some examples on how you can utilize the script:\nCopying a WellArchitected Tool Review from one region to another Example Command: ./duplicateWAFR.py --fromaccount acct2 --toaccount acct2 --workloadid c896b2b1142f6ea8dc228746744c0000 --fromregion us-east-1 --toregion us-east-2\n$ ./duplicateWAFR.py --fromaccount acct2 --toaccount acct2 --workloadid c896b2b1142f6ea8dc228746744c0000 --fromregion us-east-1 --toregion us-east-2 2021-04-19 09:43:37.832 INFO duplicateWAFR - main: Starting Boto 1.17.27 Session 2021-04-19 09:43:38.130 INFO duplicateWAFR - main: Copy WorkloadID \u0026#39;c896b2b1142f6ea8dc228746744c0000\u0026#39; from \u0026#39;us-east-1:acct2\u0026#39; to \u0026#39;us-east-2:acct2\u0026#39; 2021-04-19 09:43:38.920 INFO duplicateWAFR - main: New workload id: 76a20b805f62a136ad54bf6c1483a98b (arn:aws:wellarchitected:us-east-2:222222222222:workload/76a20b805f62a136ad54bf6c1483a98b) 2021-04-19 09:43:38.920 INFO duplicateWAFR - main: Retrieving all answers for lens wellarchitected 2021-04-19 09:43:40.206 INFO duplicateWAFR - main: Copying answers into new workload for lens wellarchitected 2021-04-19 09:44:00.213 INFO duplicateWAFR - main: Retrieving all answers for lens serverless 2021-04-19 09:44:00.532 INFO duplicateWAFR - main: Copying answers into new workload for lens serverless 2021-04-19 09:44:03.690 INFO duplicateWAFR - main: Copy complete - exiting Copying a WellArchitected Tool Review from one account to another in the same region Example Command: ./duplicateWAFR.py --fromaccount acct2 --toaccount acct3 --workloadid c896b2b1142f6ea8dc228746744c0000 --fromregion us-east-1 --toregion us-east-1\n$ ./duplicateWAFR.py --fromaccount acct2 --toaccount acct3 --workloadid c896b2b1142f6ea8dc228746744c0000 --fromregion us-east-1 --toregion us-east-1 2021-04-19 09:46:05.400 INFO duplicateWAFR - main: Starting Boto 1.17.27 Session 2021-04-19 09:46:05.543 INFO duplicateWAFR - main: Copy WorkloadID \u0026#39;c896b2b1142f6ea8dc228746744c0000\u0026#39; from \u0026#39;us-east-1:acct2\u0026#39; to \u0026#39;us-east-1:acct3\u0026#39; 2021-04-19 09:46:06.533 INFO duplicateWAFR - main: New workload id: 3a701d540b4b09579e99c303ac4a0499 (arn:aws:wellarchitected:us-east-1:333333333333:workload/3a701d540b4b09579e99c303ac4a0499) 2021-04-19 09:46:06.534 INFO duplicateWAFR - main: Retrieving all answers for lens wellarchitected 2021-04-19 09:46:07.734 INFO duplicateWAFR - main: Copying answers into new workload for lens wellarchitected 2021-04-19 09:46:31.283 INFO duplicateWAFR - main: Retrieving all answers for lens serverless 2021-04-19 09:46:31.598 INFO duplicateWAFR - main: Copying answers into new workload for lens serverless 2021-04-19 09:46:35.143 INFO duplicateWAFR - main: Copy complete - exiting Copying a WellArchitected Tool Review from one account to another in a different region Example Command: ./duplicateWAFR.py --fromaccount acct2 --toaccount acct3 --workloadid c896b2b1142f6ea8dc228746744c0000 --fromregion us-east-1 --toregion us-east-2\n$ ./duplicateWAFR.py --fromaccount acct2 --toaccount acct3 --workloadid c896b2b1142f6ea8dc228746744c0000 --fromregion us-east-1 --toregion us-east-2 2021-04-19 09:47:14.227 INFO duplicateWAFR - main: Starting Boto 1.17.27 Session 2021-04-19 09:47:14.451 INFO duplicateWAFR - main: Copy WorkloadID \u0026#39;c896b2b1142f6ea8dc228746744c0000\u0026#39; from \u0026#39;us-east-1:acct2\u0026#39; to \u0026#39;us-east-2:acct3\u0026#39; 2021-04-19 09:47:15.374 INFO duplicateWAFR - main: New workload id: 8f1df75b01f132a4b31649e834d4c606 (arn:aws:wellarchitected:us-east-2:333333333333:workload/8f1df75b01f132a4b31649e834d4c606) 2021-04-19 09:47:15.374 INFO duplicateWAFR - main: Retrieving all answers for lens wellarchitected 2021-04-19 09:47:16.456 INFO duplicateWAFR - main: Copying answers into new workload for lens wellarchitected 2021-04-19 09:47:34.067 INFO duplicateWAFR - main: Retrieving all answers for lens serverless 2021-04-19 09:47:34.433 INFO duplicateWAFR - main: Copying answers into new workload for lens serverless 2021-04-19 09:47:37.326 INFO duplicateWAFR - main: Copy complete - exiting X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/generate_custom_html_wafr_report/3_executing/","title":"Script usage examples","tags":[],"description":"","content":"Generating a WellArchitected Framework HTML Report Example Command: ./generateWAFReport.py --profile acct2 --workloadid c896b2b1142f6ea8dc22874674400002 --region us-east-1\n$ ./generateWAFReport.py --profile acct2 --workloadid c896b2b1142f6ea8dc22874674400002 --region us-east-1 2021-04-19 12:47:06.672 INFO generateWAFReport - main: Starting Boto 1.17.27 Session 2021-04-19 12:47:15.097 INFO generateWAFReport - main: Creating HTML file /var/folders/sw/1xkhcl751fj_fxr63jb5qr2m0000gs/T/tmpft2fam0k.html 2021-04-19 12:47:15.097 INFO generateWAFReport - main: Opening HTML URL (file:///var/folders/sw/1xkhcl751fj_fxr63jb5qr2m0000gs/T/tmpft2fam0k.html) in default WebBrowser Example HTML Output How do you detect and investigate security events? - AWS Well-Architected Framework Python Well-Architected Report v0.1 Workload Properties Workload Name: WA Lab Test Workload ARN: arn:aws:wellarchitected:us-east-1:222222222222:workload/c896b2b1142f6ea8dc22874674400002 Description: Test Workload for WA Lab Review Owner: WA Python Script Table of Contents Operational Excellence Security Reliability Performance Efficiency Cost Optimization Operational Excellence Improvement Plans OPS 5 - How do you reduce defects, ease remediation, and improve flow into production? Current Risk: HIGH Test and validate changes Test and validate changes: Changes should be tested and the results validated at all lifecycle stages (for example, development, test, and production). Use testing results to confirm new features and mitigate the risk and impact of failed deployments. Automate testing and validation to ensure consistency of review, to reduce errors caused by manual processes, and reduce the level of effort. What is AWS CodeBuild? Local build support for AWS CodeBuild Share design standards Share design standards: Share existing best practices , design standards, checklists, operating procedures, and guidance and governance requirements across teams to reduce complexity and maximize the benefits from development efforts. Ensure that procedures exist to request changes, additions, and exceptions to design standards to support continual improvement and innovation. Ensure that teams are aware of published content so that they can take advantage of content, and limit rework and wasted effort. Delegating access to your AWS environment Share an AWS CodeCommit repository Easy authorization of AWS Lambda functions Sharing an AMI with specific AWS accounts Speed template sharing with an AWS CloudFormation designer URL Using AWS Lambda with Amazon SNS Implement practices to improve code quality Implement practices to improve code quality: Implement practices to improve code quality to minimize defects and the risk of their being deployed. For example, test-driven development, pair programming, code reviews, and standards adoption. Make frequent, small, reversible changes Make frequent, small, reversible changes: Frequent, small, and reversible changes reduce the scope and impact of a change. This eases troubleshooting, enables faster remediation, and provides the option to roll back a change. It also increases the rate at which you can deliver value to the business. Use version control Use version control: Maintain assets in version controlled repositories. Doing so supports tracking changes, deploying new versions, detecting changes to existing versions, and reverting to prior versions (for example, rolling back to a known good state in the event of a failure). Integrate the version control capabilities of your configuration management systems into your procedures. Introduction to AWS CodeCommit What is AWS CodeCommit? Fully automate integration and deployment Use build and deployment management systems: Use build and deployment management systems to track and implement change, to reduce errors caused by manual processes, and reduce the level of effort. Fully automate the integration and deployment pipeline from code check-in through build, testing, deployment, and validation. This reduces lead time, enables increased frequency of change, and reduces the level of effort. What is AWS CodeBuild? Continuous integration best practices for software development Slalom: CI/CD for serverless applications on AWS Introduction to AWS CodeDeploy - automated software deployment with Amazon Web Services What is AWS CodeDeploy? Security Improvement Plans SEC 2 - How do you manage identities for people and machines? Current Risk: HIGH Use temporary credentials Implement least privilege policies: Assign access policies with least privilege to IAM groups and roles to reflect the user's role or function that you have defined. Grant least privilege Remove unnecessary permissions: Implement least privilege by removing permissions that are unnecessary. Reducing policy scope by viewing user activity View role access Consider permissions boundaries: A permissions boundary is an advanced feature for using a managed policy that sets the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. Lab: IAM permissions boundaries delegating role creation Consider resource tags for permissions: You can use tags to control access to your AWS resources that support tagging . You can also tag IAM users and roles to control what they can access. Lab: IAM tag based access control for EC2 Attribute-based access control (ABAC) Store and use secrets securely Use AWS Secrets Manager: AWS Secrets Manager is an AWS service that makes it easier for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. AWS Secrets Manager Rely on a centralized identity provider Centralize administrative access: Create an IAM identity provider entity to establish a trust relationship between your AWS account and your identity provider (IdP). IAM supports IdPs that are compatible with OpenID Connect (OIDC) or SAML 2.0 ( Security Assertion Markup Language 2.0). Identity Providers and Federation Centralize application access: Consider Amazon Cognito for centralizing application access. It lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. Amazon Cognito Remove old IAM users and groups: After you start using an identity provider (IdP), remove IAM users and groups that are no longer required. Finding unused credentials Deleting an IAM group Audit and rotate credentials periodically Regularly audit credentials: Use credential reports, and IAM Access Analyzer to audit IAM credentials and permissions. IAM Access Analyzer Getting credential report Lab: Automated IAM user cleanup Use Access Levels to Review IAM Permissions: To improve the security of your AWS account, regularly review and monitor each of your IAM policies. Make sure that your policies grant the least privilege that is needed to perform only the necessary actions. Use access levels to review IAM permissions Consider automating IAM resource creation and updates: AWS CloudFormation can be used to automate the deployment of IAM resources including roles and policies, to reduce human error, as the templates can be verified and version controlled. Lab: Automated deployment of IAM groups and roles Leverage user groups and attributes If you are using AWS Single Sign-On (SSO), configure groups: AWS SSO provides you with the ability to configure groups of users, and assign groups the desired level of permission. AWS Single Sign-On - Manage Identities Learn about attribute-based access control (ABAC): Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. What Is ABAC for AWS? Lab: IAM Tag Based Access Control for EC2 Reliability Improvement Plans REL 2 - How do you plan your network topology? Current Risk: HIGH Provision redundant connectivity between private networks in the cloud and on-premises environments Ensure that you have highly available connectivity between AWS and on-premises environment: Use multiple AWS Direct Connect (DX) connections or VPN tunnels between separately deployed private networks. Use multiple DX locations for high availability . If using multiple AWS Regions , ensure redundancy in at least two of them. You might want to evaluate AWS Marketplace appliances that terminate VPNs. If you use AWS Marketplace appliances, deploy redundant instances for high availability in different Availability Zones . Ensure that you have a redundant connection to your on-premises environment: You may need redundant connections to multiple AWS Regions to achieve your availability needs. AWS Direct Connect Resiliency Recommendations Using Redundant Site-to-Site VPN Connections to Provide Failover Use service API operations to identify correct use of Direct Connect circuits DescribeConnections DescribeConnectionsOnInterconnect DescribeDirectConnectGatewayAssociations DescribeDirectConnectGatewayAttachments DescribeDirectConnectGateways DescribeHostedConnections DescribeInterconnects If only one Direct Connect connection exists or you have none, set up redundant VPN tunnels to your virtual private gateways What is AWS Site-to-Site VPN? Capture your current connectivity (for example, Direct Connect, virtual private gateways, AWS Marketplace appliances) Use service API operations to query configuration of Direct Connect connections DescribeConnections DescribeConnectionsOnInterconnect DescribeDirectConnectGatewayAssociations DescribeDirectConnectGatewayAttachments DescribeDirectConnectGateways DescribeHostedConnections DescribeInterconnects Use service API operations to collect virtual private gateways where route tables use them DescribeVpnGateways DescribeRouteTables Use service API operations to collect AWS Marketplace applications where route tables use them DescribeRouteTables Ensure IP subnet allocation accounts for expansion and availability Plan your network to accommodate for growth, regulatory compliance, and integration with others: Growth can be underestimated, regulatory compliance can change, and acquisitions or private network connections can be difficult to implement without proper planning. Select relevant AWS accounts and Regions based on your service requirements, latency , regulatory, and disaster recovery (DR) requirements Identify your needs for regional VPC deployments Determine if you are going to deploy multi-VPC connectivity What Is a Transit Gateway? Single Region Multi-VPC Connectivity Determine if you need segregated networking for regulatory requirements Identify the size of the VPCs Make VPCs as large as possible. The initial VPC CIDR block allocated to your VPC cannot be changed or deleted, but you can add additional non-overlapping CIDR blocks to the VPC. This however may fragment your address ranges Allow for use of Elastic Load Balancers, Auto Scaling groups, concurrent AWS Lambda invocations, and service endpoints Prefer hub-and-spoke topologies over many-to-many mesh Prefer hub-and-spoke topologies over many-to-many mesh: If more than two network address spaces (VPCs, on-premises networks) are connected via VPC peering, AWS Direct Connect , or VPN, then use a hub-and-spoke model like that provided by AWS Transit Gateway. For only two such networks, you can simply connect them to each other, but as the number of networks grows, the complexity of such meshed connections becomes untenable. AWS Transit Gateway provides an easy to maintain hub-and-spoke model, allowing routing of traffic across your multiple networks. What Is a Transit Gateway? Enforce non-overlapping private IP address ranges in all private address spaces where they are connected Monitor and manage your CIDR use: Evaluate your potential usage on AWS, add CIDR ranges to existing VPCs, and create VPCs to allow planned growth in usage. Capture current CIDR consumption (for example, VPCs, subnets, etc.) Use service API operations to collect current CIDR consumption Capture your current subnet usage Use service API operations to collect subnets per VPC in each Region DescribeSubnets Record the current usage Determine if you created any overlapping IP ranges Calculate the spare capacity Note overlapping IP ranges: You can either migrate to a new range of addresses or use Network and Port Translation (NAT) appliances from AWS Marketplace if you need to connect the overlapping ranges. Enforce non-overlapping private IP address ranges in all private address spaces where they are connected Monitor and manage your CIDR use: Evaluate your potential usage on AWS, add CIDR ranges to existing VPCs, and create VPCs to allow planned growth in usage. Capture current CIDR consumption (for example, VPCs, subnets, etc.) Use service API operations to collect current CIDR consumption Capture your current subnet usage Use service API operations to collect subnets per VPC in each Region DescribeSubnets Record the current usage Determine if you created any overlapping IP ranges Calculate the spare capacity Note overlapping IP ranges: You can either migrate to a new range of addresses or use Network and Port Translation (NAT) appliances from AWS Marketplace if you need to connect the overlapping ranges. Performance Efficiency Improvement Plans PERF 2 - How do you select your compute solution? Current Risk: HIGH Understand the available compute configuration options Learn about available configuration options: For your selected compute option, use the available configuration options to optimize for your performance requirements.Utilize AWS Nitro Systems to enable full consumption of the compute and memory resources of the host hardware. Dedicated Nitro Cards enable high speed networking, high speed EBS, and I/O acceleration. AWS Nitro System Collect compute-related metrics Collect compute-related metrics: Amazon CloudWatch can collect metrics across the compute resources in your environment. Use a combination of CloudWatch and other metrics-recording tools to track the system-level metrics within your workload . Record data such as CPU usage levels, memory , disk I/O, and network to gain insight into utilization levels or bottlenecks. This data is crucial to understand how the workload is performing and how effectively it is using resources. Use these metrics as part of a data-driven approach to actively tune and optimize your workload 's resources. Amazon CloudWatch Determine the required configuration by right-sizing Modify your workload configuration by right sizing: To optimize both performance and overall efficiency, determine which resources your workload needs. Choose memory -optimized instances for systems that require more memory than CPU, or compute-optimized instances for components that do data processing that is not memory -intensive. Right sizing enables your workload to perform as well as possible while only using the required resources Use the available elasticity of resources Take advantage of elasticity : Elasticity matches the supply of resources you have against the demand for those resources. Instances, containers, and functions provide mechanisms for elasticity either in combination with automatic scaling or as a feature of the service. Use elasticity in your architecture to ensure that you have sufficient capacity to meet performance requirements at all scales of use. Ensure that the metrics for scaling up or down elastic resources are validated against the type of workload being deployed. If you are deploying a video transcoding application, 100% CPU is expected and should not be your primary metric. Alternatively, you can measure against the queue depth of transcoding jobs waiting to scale your instance types. Ensure that workload deployments can handle both scale up and scale down events . Scaling down workload components safely is as critical as scaling up resources when demand dictates. Create test scenarios for scale-down events to ensure that the workload behaves as expected. Re-evaluate compute needs based on metrics Use a data-driven approach to optimize resources: To achieve maximum performance and efficiency, use the data gathered over time from your workload to tune and optimize your resources. Look at the trends in your workload 's usage of current resources and determine where you can make changes to better match your workload 's needs. When resources are over-committed, system performance degrades, whereas underutilization results in a less efficient use of resources and higher cost . Cost Optimization Improvement Plans COST 4 - How do you decommission resources? Current Risk: HIGH Implement a decommissioning process Create and implement a decommissioning process. : Working with the workload developers and owners, build a decommissioning process for the workload and its resources. The process should cover the method to verify if the workload is in use, and also if each of the workload resources are in use. The process also covers the steps necessary to decommission the resource, removing them from service while ensuring compliance with any regulatory requirements. Any associated resources are also covered, such as licenses or attached storage. Finally the process provides notification to the workload owners that the decommissioning process has been executed. Decommission resources Decommission resources : Using the decommissioning process, decommission each of the resources that have been identified as orphaned. Decommission resources automatically Implement AWS Auto Scaling : For resources that are supported, configure them with AWS Auto Scaling . Getting Started with Amazon EC2 Auto Scaling Configure CloudWatch to Terminate Instances : Instances can be configured to terminate using CloudWatch alarms. Using the metrics from the decommissioning process, implement an alarm with an EC2 action. Ensure you verify the operation in a non-production environment before rolling out. Create Alarms to Stop, Terminate, Reboot, or Recover an Instance Implement code within the workload : You can use the AWS SDK or AWS CLI to decommission workload resources. Implement code within the application that integrates with AWS and terminates or removes resources that are no longer used. X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/","title":"CloudFront with S3 Bucket Origin","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing. Permissions to Amazon S3 and Amazon CloudFront. Costs Typically less than $1 per month (depending on the number of requests) if the account is only used for personal testing or training, and the tear down is not performed. Amazon S3 pricing Amazon CloudFront pricing AWS Pricing Steps: Create S3 bucket Upload example index.html file Configure Amazon CloudFront Tear down References \u0026amp; useful resources Amazon S3 Developer Guide Amazon CloudFront Developer Guide "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/3_how_to_use_in_cfn/","title":"How to utilize AWS Lambda-backed custom resources in CloudFormation","tags":[],"description":"","content":"Overview In order to utilize the two Lambda-backed custom resources in CloudFormation, you will need the pass the Lambda function ARN along with expected parameters using the CloudFormation custom resource type. Below are examples for creating a new workload as well as updating two questions in the Operational Excellence pillar. The general workflow for this lambda-backed CloudFormation function: Example Workload Creation When you use this custom function, it will either create the WA Workload (if it doesn\u0026rsquo;t exist) or update the given parameters if it already does exist.\nExpected passed parameters The AWS Lambda function expects the following parameters to be passed from CloudFormation:\nWorkloadName - Name of the Workload WorkloadDesc - Short description of the Workload WorkloadOwner - Team or person who owns this Workload WorkloadEnv - Environment for the workload. Must either be PRODUCTION or PREPRODUCTION WorkloadRegion - Region where the workload will live. Must be a valid AWS region short name. WorkloadLenses - A list of Lenses you want to apply to the Workload. Must be a valid WA Review Lens name. Tags - This can be any tag pair you wish to pass along Example AWS CloudFormation YAML 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CreateWAWorkload: Type: Custom::createWAWorkloadHelperFunction Properties: ServiceToken: \u0026#34;arn:aws:lambda:us-east-2:123456789012:function:CreateNewWAFRFunction\u0026#34; WorkloadName: \u0026#34;My Workload Name\u0026#34; WorkloadDesc: \u0026#34;This is my new workload\u0026#34; WorkloadOwner: \u0026#34;Eric Pullen\u0026#34; WorkloadEnv: \u0026#34;PRODUCTION\u0026#34; WorkloadRegion: \u0026#34;us-east-2\u0026#34; WorkloadLenses: - \u0026#34;wellarchitected\u0026#34; - \u0026#34;serverless\u0026#34; Tags: WorkloadType: \u0026#34;ECSWebApp\u0026#34; WorkloadName: \u0026#34;ACMECustomerPortal\u0026#34; Update specific questions in a given pillar Once you have a given Workload created, you can specify a set of questions and best practices you wish to select. This function requires the WorkloadId to be passed along, but it can be returned from the CreateWAWorkload function above.\nExpected passed parameters The AWS Lambda function expects the following parameters to be passed from CloudFormation:\nWorkloadId - WorkloadId number that you wish to answer the question Pillar - The Well-Architected pillar you want to answer the question within. Must be a valid WA Review Lens name. Lens - The Well-Architected Lens you want to answer the question within. Must be a valid WA Review Lens name. QuestionAnswers - A List of questions and best practices you wish to answer. These must match the text strings within the Well-Architected Tool You can answer multiple questions within one pillar and lens with one call Example AWS CloudFormation YAML 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SECWAWorkloadQuestions: Type: Custom::AnswerSECWAWorkloadQuestionsHelperFunction Properties: ServiceToken: \u0026#34;arn:aws:lambda:us-east-2:123456789012:function:UpdateWAQFunction\u0026#34; WorkloadId: !GetAtt CreateWAWorkload.WorkloadId Pillar: \u0026#34;operationalExcellence\u0026#34; Lens: \u0026#34;wellarchitected\u0026#34; QuestionAnswers: - \u0026#34;How do you determine what your priorities are\u0026#34;: # OPS1 - \u0026#34;Evaluate governance requirements\u0026#34; - \u0026#34;Evaluate compliance requirements\u0026#34; - \u0026#34;How do you reduce defects, ease remediation, and improve flow into production\u0026#34;: #OPS5 - \u0026#34;Use version control\u0026#34; - \u0026#34;Perform patch management\u0026#34; - \u0026#34;Use multiple environments\u0026#34; Now that you understand the various parameters, let\u0026rsquo;s deploy a sample application to show how this works. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/","title":"300 Labs","tags":[],"description":"","content":"List of labs available Level 300: Using custom resource in AWS CloudFormation to create and update Well-Architected Reviews Level 300: Build custom reports of AWS Well-Architected Reviews "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_building_custom_aws_well-architected_reports_with_amazon_athena_and_amazon_quicksight/3_query_data/","title":"Query the workload data","tags":[],"description":"","content":"Although structured data remains the backbone of many data platforms, unstructured or semi structured data is used to enrich existing information or to create new insights. Amazon Athena enables you to analyze a variety of data, including:\nTabular data in comma-separated value (CSV) or Apache Parquet files Data extracted from log files using regular expressions JSON-formatted data Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. The AWS WA Tool data is represented as a nested JSON object. For example, running a simple SELECT query on the JSON data in Amazon Athena returns the following, where the underlying report_answers field is selected, but still represented in JSON format.\nSELECT workload_id, workload_owner, report_answers FROM \u0026#34;YOUR_DATABASE_NAME\u0026#34;.\u0026#34;workloadreports\u0026#34; LIMIT 4; Create a view for the workload answers data Follow these steps to create a view in Amazon Athena to present the JSON data as a tabular view for reporting and visualizing. A view in Amazon Athena is a logical, not physical table. The query that defines a view runs each time the view is referenced in a query. From within the Amazon Athena console, open a new query tab and execute the following query:\nCREATE OR REPLACE VIEW well_architected_reports_view AS SELECT workload_id, workload_name, workload_owner, CAST(from_iso8601_timestamp(workload_lastupdated) AS timestamp) AS \u0026#34;timestamp\u0026#34;, answers.questionid answers.QuestionTitle, answers.LensAlias, answers.pillarid, answers.risk FROM \u0026#34;workloadreports\u0026#34; CROSS JOIN unnest(report_answers) AS t(answers) The SQL statement, UNNEST, takes the report_answers column from the original table as a parameter. It creates a new dataset with the new column answers, which is later cross-joined. The enclosing SELECT statement can then reference the new answers column directly. You can quickly query the view to see the result to understand how the report_answers are now represented.\nSELECT workload_id, workload_owner, questionid, pillarid FROM \u0026#34;YOUR_DATABASE_NAME\u0026#34;.\u0026#34;well_architected_reports_view\u0026#34; LIMIT 4; Create a view for the workload risk counts data Now create a view for a summary of risks associated with each lens for each workload. Open a new query tab and execute the following query:\nCREATE OR REPLACE VIEW well_architected_workload_lens_risk_view AS SELECT workload_id, workload_name, lens.LensAlias, lens_pillar_summary.PillarId, lens_pillar_summary.RiskCounts.UNANSWERED, lens_pillar_summary.RiskCounts.HIGH, lens_pillar_summary.RiskCounts.MEDIUM, lens_pillar_summary.RiskCounts.NONE, lens_pillar_summary.RiskCounts.NOT_APPLICABLE FROM \u0026#34;workloadreports\u0026#34; CROSS JOIN unnest(lens_summary) AS t(lens) CROSS JOIN unnest(lens.PillarReviewSummaries) AS tt(lens_pillar_summary) Previewing the newly created well_architected_workload_risk_view:\nSELECT * FROM \u0026#34;YOUR_DATABASE_NAME\u0026#34;.\u0026#34;well_architected_workload_risk_view\u0026#34; LIMIT 4; Create a view for the workload\u0026rsquo;s milestone data A milestone records the state of a workload at a particular point in time. As a workload changes, you can add more milestones to measure progress. To enable visualization of this improvement across all workloads, create a view for a workload milestone. Open a new query tab and execute the following query:\nCREATE OR REPLACE VIEW well_architected_workload_milestone_view AS SELECT CAST(from_iso8601_timestamp(milestone.RecordedAt) AS timestamp) AS \u0026#34;timestamp\u0026#34;, workload_id, workload_name, workload_owner, milestone.MilestoneName, milestone.MilestoneNumber, milestone.WorkloadSummary.ImprovementStatus, milestone.WorkloadSummary.RiskCounts.HIGH, milestone.WorkloadSummary.RiskCounts.MEDIUM, milestone.WorkloadSummary.RiskCounts.UNANSWERED, milestone.WorkloadSummary.RiskCounts.NONE, milestone.WorkloadSummary.RiskCounts.NOT_APPLICABLE FROM \u0026#34;workloadreports\u0026#34; CROSS JOIN unnest(milestones) AS t(milestone) Previewing the newly created well_architected_workload_risk_view:\nSELECT * FROM \u0026#34;YOUR_DATABASE_NAME\u0026#34;.\u0026#34;well_architected_workload_milestone_view\u0026#34; LIMIT 4; X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/3_create_data_transfer_cost_analysis/","title":"Create Data Transfer Cost Analysis Dashboard","tags":[],"description":"","content":" This Lab has moved under the Level 200 Cloud Intelligence Dashboards.Click this link to navigate to the updated Lab Authors Chaitanya Shah, Sr. Technical Account Manager (AWS) FAQ The FAQ for this dashboard is here. Introduction The Data Transfer Dashboard is an interactive, customizable and accessible QuickSight dashboard to help customers gain insights into their data transfer. It will analyze any data transfer that incurs a cost such as outbound internet and regional data transfer from all services.\nThis dashboard contains data transfer breakdowns with the following visuals:\nAmount and cost by service and region Between regions Internet data transfer Regional Data transfer Request Template Access Ensure you have requested access to the Cost Intelligence template here. Create Athena Views The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR).\nLogin via SSO in your Cost Optimization account, go into the Athena console:\nCreate the Data Transfer view by modifying the following code, and executing it in Athena or using aws cli:\nData Transfer View Create QuickSight Data Sets We will now create the data sets in QuickSight from the Athena views.\nGo to the QuickSight service homepage\nClick on the Datasets and then click on New dataset Click Athena Enter a data source name of DataTransfer_Cost_Dashboard and click Create data source: Select the costmaster database, and the data_transfer_view table, click Edit/Preview data: Select SPICE to change your Query mode: Click on linked_account_id to get the drop down arrow and click on it, then hover over Change data type then select # Int/Integer: Repeat step 7 for:\npayer_account_id Click on region to get the drop down arrow and click on it, then hover over Change data type then select # String\nClick on blended_cost to get the drop down arrow and click on it, then hover over Change data type then select # Decimal: Ensure the following fields are all # Decimal, repeat step 10 if necessary for:\nusage_quantity unblended_cost public_cost belnded_rate unblended_rate public_ondemand_rate Select Save: Select the data_transfer_view Data Set: Click Schedule refresh: Click Create: Enter a schedule, it needs to be refreshed daily, and click Create: Click Cancel to exit: Click the x in the top corner: Create the Dashboard We will now use the CLI to create the dashboard from the Data Transfer Cost and Usage Analysis Dashboard template, then create an Analysis you can customize and modify in the next step.\nIf you have not requested access, go to this we page to request access to the template: Template Access Edit the following command, replacing AccountID with your account ID, and region with the region you are working in, then using the CLI list the QuickSight datasets and copy the Name and Arn for the dataset: data_transfer_view:\naws quicksight list-data-sets --aws-account-id (AccountID) --region (region) { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:\u0026lt;your account id\u0026gt;:dataset/fc0cf1eb-173b-4aca-93b6-f58784637732\u0026quot;, \u0026quot;DataSetId\u0026quot;: \u0026quot;fc0cf1eb-173b-4aca-93b6-f58784637732\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;data_transfer_view\u0026quot;, \u0026quot;CreatedTime\u0026quot;: \u0026quot;2020-08-09T23:06:41.666000-04:00\u0026quot;, \u0026quot;LastUpdatedTime\u0026quot;: \u0026quot;2020-08-11T23:15:35.438000-04:00\u0026quot;, \u0026quot;ImportMode\u0026quot;: \u0026quot;SPICE\u0026quot; } Get your users Arn by editing the following command, replacing AccountID with your account ID, and region with the region you are working in, then using the CLI run the command:\naws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region) { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:\u0026lt;your account id\u0026gt;:user/default/\u0026lt;your user\u0026gt;\u0026quot;, \u0026quot;UserName\u0026quot;: \u0026quot;\u0026lt;your user\u0026gt;\u0026quot;, \u0026quot;Email\u0026quot;: \u0026quot;\u0026lt;your user email\u0026gt;\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;ADMIN\u0026quot;, \u0026quot;Active\u0026quot;: true, \u0026quot;PrincipalId\u0026quot;: \u0026quot;\u0026lt;principal id\u0026gt;\u0026quot; } Create a local file create-data-transfer-dashboard.json with the text below, replace the values (Account ID) with your account ID on line 2 and line 25, (User ARN) with your user ARN on line 7, and (DataTransfer view Dataset ID) with your dataset ARN on line 25:\n{ \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;data_transfer_cost_analysis_dashboard_enhanced\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;DataTransfer Cost Analysis Dashboard Enhanced\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;data_transfer_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:(Account ID):dataset/(DataTransfer view Dataset ID)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/data-transfer-cost-analysis-template-enhanced\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; } To create the dashboard from the template, edit then run the following command, replacing (region) with the region you are working in, and you should receive a 202 response:\naws quicksight create-dashboard --cli-input-json file://create-data-transfer-dashboard.json --region (region) Response: After a few minutes the dashboard will become available in QuickSight under All dashboard, click on the Dashboard name: Click here - if you do not see your dashboard \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Click Share, click Share dashboard:, Click Manage dashboard access: Add the required users, or share with all users, ensure you check Save as for each user, then click the x to close the window: Click Save as: Enter an Analysis name and click Create: You will now have an analysis created from the template that you can edit and modify: To view data transfer charts for desired period change From and To date filters. Dashboard will automatically refresh upon date changes. "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/3_tear_down/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\nS3 Bucket: (custom name) Lambda Function: Cost_W-A_Journey IAM Role: extract-wa-reports_role IAM Policy: WAReportAccess X Congratulations! Now that you have your Cost Optimization Journey, you can continue to implement the best practices within your organization and workloads.\nClick here to access the Well-Architected Tool Previous Step Complete this lab Suggested definitions These have been used to configure the templated journey file, and should help guide you to making modifications.\nEffort Effort Level People Involved Time Taken non-tech example tech example 1 2 \u0026lt;3min execute a step click on something 2 1 \u0026lt;15min execute a simple process, follow a few steps single click, multiple times 3 1-5 \u0026lt;1hr execute an easy process execute a lab, no discussions 4 1-5 \u0026lt;4hrs execute an easy, well defined, long process bulid something simple, a script, complete a 100level lab solo 5 3-10 \u0026lt;4-8hrs create a simple process, execute a process with non-diverse roles, multile inputs/consideration with no contention 100 lab with a few people, 200-300 lab solo 6 \u0026lt;3days refactor a component 400+ level solo, highly technical 7 \u0026lt;3days thorough \u0026amp; deep analysis - one session, create a complex/large process with non-diverse roles, execute a process with diverse roles, multiple inputs/contention 8 3-5days multiple sessions of thought \u0026amp; deep analysis 9 1month refactor multiple components 10 6months refactor a workload, create a complex and large process with multiple inputs/contention Frequency Frequency Example 1 once 2 yearly 3 bi-annual 4 quarterly 5 monthly 6 multiple times a month 7 weekly 8 multiple times a week 9 daily 10 multiple times a day 11 on event Duration Duration Example - time to build capability 1 can be done at the same time as the previous best practice 2 an hour 3 a few hours 4 1 day 5 2-3 days 6 1 week 7 2-3 weeks 8 1 month 9 3 months 10 6 months "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/3_enable_notifications/","title":"Enable Notifications","tags":[],"description":"","content":"In the cloud, setting up notifications to be aware of events within your workload is easily achieved. AWS Backup leverages AWS SNS to send notifications related to backup activities that are occurring. This will allow visibility into backup job statuses, restore job statuses, or any failures that may have occurred, allowing your Operations teams to respond appropriately.\nOpen a terminal where you have access to the AWS CLI. Ensure that the CLI is up to date and that you have AWS Administrator Permissions to run AWS CLI commands. https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html Edit the following AWS CLI command and include the ARN of the SNS TOPIC that you created. Replace with the ARN of the SNS TOPIC obtained from the outputs section of the CloudFormation Stack. Note that the backup vault name is case sensitive.\naws backup put-backup-vault-notifications --region us-east-1 --backup-vault-name BACKUP-LAB-VAULT --backup-vault-events BACKUP_JOB_COMPLETED RESTORE_JOB_COMPLETED --sns-topic-arn \u0026lt;YOUR SNS TOPIC ARN\u0026gt;\nOnce edited, run the above command, it will enable notifications with messages published to the SNS TOPIC every time a backup or restore job is completed. This will ensure the Operations team is aware of any failures with backing up or restoring data.\nYou can verify that notifications have been enabled by running the following command. The output will include a section called SNSTopicArn followed by the ARN of the SNS Topic that was created as part of the lab.\naws backup get-backup-vault-notifications --backup-vault-name BACKUP-LAB-VAULT --region us-east-1\nYou have now successfully enabled notifications for the backup vault BACKUP-LAB-VAULT, ensuring that the Operations team is aware of completion of backup and restore activities involving this vault, and any failures associated with those activities.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/","title":"Level 200: Testing Backup and Restore of Data","tags":["data_backup"],"description":"Create a strategy to backup data sources periodically using AWS Backup, and automate the testing of the restore process","content":"Authors Mahanth Jayadeva, Solutions Architect, Well-Architected Introduction It is not sufficient to just create backups of data sources, you must also test these backups to ensure they can be used to recover data. A backup is useless if you are unable to restore your data from it. Testing the restore process after each backup will ensure you are aware of any issues that might arise during a restore down the line.\nIn this lab, you will create an EC2 Instance as a data source. You will then create a strategy to backup these data sources periodically using AWS Backup, and finally, automate the testing of the restore process as well as cleanup of resources using AWS Lambda.\nThe skills you learn will help you define a backup and restore plan in alignment with the AWS Well-Architected Framework Goals: Create a Backup Strategy to ensure mission-critical data is being backed up regularly Test restoring from backups to ensure there are no data recovery issues Learn how to automate this process Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. Launch the CloudFormation Stack to provision resources that will act as data sources. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the Infrastructure Create Backup Plan Enable Notifications Test Restore Teardown "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/compute-optimizer-dashboards/dashboards/3_manage-business-units/","title":"Optional Steps","tags":[],"description":"","content":"Manage Business Units Map For managing Business Units please modify business_units_map view. You can update view definition providing your values, or you can create an csv file upload to s3, create a table and set business_units_map view to select from this table.\nCREATE OR REPLACE VIEW business_units_map AS SELECT * FROM ( VALUES ROW (\u0026#39;111111111\u0026#39;, \u0026#39;account1\u0026#39;, \u0026#39;Business Unit 1\u0026#39;) , ROW (\u0026#39;222222222\u0026#39;, \u0026#39;account2\u0026#39;, \u0026#39;Business Unit 2\u0026#39;) ) ignored_table_name (account_id, account_name, bu) Alos you can use business_units_map view as a proxy to other data sources.\nIn case if you do not need Business Units functionality and you have CUDOS dashboard installed with account_map, you can use this view to SELECT from account_map.\nCREATE OR REPLACE VIEW business_units_map AS SELECT account_id as account_id, account_name as account_name, \u0026#39;Undefined\u0026#39; as bu FROM account_map X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/3_create_workload/","title":"Create a Well-Architected Workload with Tags","tags":[],"description":"","content":"Overview Well-Architected Reviews are conducted per workload. A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Workloads vary in levels of architectural complexity, from static websites to architectures with multiple data stores and many components.\nWe will start with creating a Well-Architected workload to use throughout this lab. Click Define workload. If this is your first time using AWS WA Tool, you see Defining a workload steps that introduces you to the features of the service. Now you can create a new Well-Architected workload:\nThe following are the required workload properties:\nworkload-name - This is a uniquie identifier for the workload. Must be between 3 and 100 characters. description - A brief description of the workload to document its scope and intended purpose. Must be between 3 and 250 characters. review-owner - The name, email address, or identifier for the primary individual or group that owns the review process. Must be between 3 and 255 characters. environment - The environment in which your workload runs. This must either be PRODUCTION or PREPRODUCTION aws-regions - The aws-regions in which your workload runs (us-east-1, etc). lenses - The list of lenses associated with the workload. All workloads must include the \u0026ldquo;wellarchitected\u0026rdquo; lens as a base, but can include additional lenses. We will need a mechanism to allow a user to complete a true workload focused Well-Architected Framework Review rather than all resources in AWS account. When AWS lambda function retrieve cost optimizato data, it will also validate if Amazon EC2 Instances are being used for the particular workload that you defined in Well-Architected tool using Amazon Tags before ingesting data points into notes in Well-Architected Tool. Therefore, a reviewer can focus on EC2 Instances used for only the particular workload. Attach the same tags we have attached to Amazon EC2 Instances to this workload. Use workload as Key and wademo as Value. AWS Lambda functions will verify if cost optimization data from AWS Compute Optimizer and AWS Trusted Advisor has the same tags as the following tags to ensure Amazon EC2 Instances are being used for this workload. Key = workload Value = wademo Choose the lenses that apply to this workload. AWS Well-Architected Framework has been selected by default. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/3_perform_review/","title":"Performing a review","tags":[],"description":"","content":"Overview Now that we have created a workload, we will answer the question OPS 5. How do you reduce defects, ease remediation, and improve flow into production. For this question, we will select a subset of the best practices, affirming them as true (turning them from unchecked to checked):\nUse version control Use configuration management systems Use build and deployment management systems Perform patch management Use multiple environments Step 1 - Find the QuestionId and ChoiceID for a particular pillar question and best practice Make sure you have the WorkloadId from the previous step and replace WorkloadId with it\nTo find the OPS 5 question listed above, we will search through the pillar for any question that begins with the string of the question.\nThe simplest way to find the QuestionId for a pillar is to search the output from the list-answers API call.\nUsing the list-answers API , we can search the output for the question we want to answer.\naws wellarchitected list-answers --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --pillar-id \u0026#34;operationalExcellence\u0026#34; --query \u0026#39;AnswerSummaries[?starts_with(QuestionTitle, `How do you reduce defects, ease remediation, and improve flow into production`) == `true`].QuestionId\u0026#39; This will return the value of the QuestionId, in this case dev-integ Next, using the get-answer API we can find the ChoiceId values of each answer. For this first command, we will get the ChoiceId for \u0026ldquo;Use version control\u0026rdquo;\naws wellarchitected get-answer --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --question-id \u0026#34;dev-integ\u0026#34; --query \u0026#39;Answer.Choices[?starts_with(Title, `Use version control`) == `true`].ChoiceId\u0026#39; This will return the value of the ChoiceId, in this case ops_dev_integ_version_control Now we need to get the rest of the ChoiceID\u0026rsquo;s for each of the best practices we want to select for the question\naws wellarchitected get-answer --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --question-id \u0026#34;dev-integ\u0026#34; --query \u0026#39;Answer.Choices[?starts_with(Title, `Use configuration management systems`) == `true`].ChoiceId\u0026#39; aws wellarchitected get-answer --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --question-id \u0026#34;dev-integ\u0026#34; --query \u0026#39;Answer.Choices[?starts_with(Title, `Use build and deployment management systems`) == `true`].ChoiceId\u0026#39; aws wellarchitected get-answer --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --question-id \u0026#34;dev-integ\u0026#34; --query \u0026#39;Answer.Choices[?starts_with(Title, `Perform patch management`) == `true`].ChoiceId\u0026#39; aws wellarchitected get-answer --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --question-id \u0026#34;dev-integ\u0026#34; --query \u0026#39;Answer.Choices[?starts_with(Title, `Use multiple environments`) == `true`].ChoiceId\u0026#39; This will return the rest of the values we need for ChoiceID:\nops_dev_integ_conf_mgmt_sys ops_dev_integ_build_mgmt_sys ops_dev_integ_patch_mgmt ops_dev_integ_multi_env Step 2 - Use the QuestionID and ChoiceID to update the answer in well-architected review After finding the ChoiceID\u0026rsquo;s in the previous step, we can \u0026ldquo;check\u0026rdquo; each of those items off as achieved using the CLI as well. The next step will set the choiceID\u0026rsquo;s to true, but leave the rest of the best practices as false (unchecked). Using the update-answer API we can update the well-architected workload using the the QuestionId and the ChoiceId\u0026rsquo;s we gathered above. aws wellarchitected update-answer --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --question-id \u0026#34;dev-integ\u0026#34; --selected-choices ops_dev_integ_version_control ops_dev_integ_conf_mgmt_sys ops_dev_integ_build_mgmt_sys ops_dev_integ_patch_mgmt ops_dev_integ_multi_env This will return the JSON object for the question, and at the bottom you will see SelectedChoices is now populated with the answers we have provided. Because we still have have not checked all critical best practices, this question has still been identified as a high risk item (HRI). OPTIONAL: Repeat steps 1 and 2 but for the other pillar questions and best practices listed below SEC 1. How do you securely operate your workload? Separate workloads using accounts Secure AWS account Keep up to date with security threats REL 2. How do you plan your network topology? Use highly available network connectivity for your workload public endpoints Provision redundant connectivity between private networks in the cloud and on-premises environments Ensure IP subnet allocation accounts for expansion and availability Prefer hub-and-spoke topologies over many-to-many mesh Enforce non-overlapping private IP address ranges in all private address spaces where they are connected PERF 5. How do you configure your networking solution? Understand how networking impacts performance Evaluate available networking features Choose appropriately sized dedicated connectivity or VPN for hybrid workloads Leverage load-balancing and encryption offloading Choose network protocols to improve performance Choose your workload’s location based on network requirements COST 3. How do you monitor usage and cost? Configure detailed information sources Identify cost attribution categories Establish organization metrics Configure billing and cost management tools Add organization information to cost and usage X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/3_implement_sharding/","title":"Implement sharding","tags":[],"description":"","content":"In this section you will update the architectural design of the workload and implement sharding. Similar to sharding a database where a large database or table is broken up into smaller chunks distributed across multiple servers, you will shard the overall capacity of the workload and segment it so that each shard is responsible for handling a subset of customers. By minimizing the number of \u0026ldquo;components\u0026rdquo; a single customer is able to interact with within the workload, we will be able to reduce the impact of a potential posion pill. This will result in a much smaller scope of impact depending on the number of shards within the workload.\nThe following diagram shows the updated architecture you will deploy. This architecture implements sharding (but not shuffle sharding): Update the workload architecture Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and select the stack that was created as part of this lab - Shuffle-sharding-lab\nClick on Update\nUnder Prerequisite - Prepare template, select Replace current template\nFor Template source select Amazon S3 URL In the text box under Amazon S3 URL specify https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/300_Fault_Isolation_with_Shuffle_Sharding/sharding.yaml Click Next\nNo changes are required for Parameters. Click Next\nFor Configure stack options click Next\nOn the Review page:\nScroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here . Note: The template creates an IAM role and Instance Profile for EC2. These are the minimum permissions necessary for the instances to be managed by AWS Systems Manager. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - InstanceRole.\nClick Update stack This will take you to the CloudFormation stack status page, showing the stack update in progress. The stack takes about 1 minute to go through all the updates. Periodically refresh the CloudFormation stack events until you see that the Stack Status is in UPDATE_COMPLETE.\nWith this stack update, the architecture of the workload has been updated by introducing 4 Application Load Balancer listener rules and Target Groups. These listener rules have been configured to inspect the incoming request for the query-string name. Depending on the value provided, the request is routed to one of four target groups where each target group consists of 2 EC2 instances.\nTest the sharded application Now that the application has been deployed, it is time to test it to understand how it works. The sample application used in this lab is the same as before, a simple web application that returns a message with the Worker that responded to the request. Customers pass in a query string as part of the request to identify themselves. The query string used here is name.\nVisit the Outputs section of the CloudFormation stack created in the previous step. You will see a list of URLs next to customer names.\nOpen the link for customer Alpha in a new browser tab. Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end.\nThe list of EC2 instances in your workload can be viewed in the AWS Console here Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end.\nNotice that after implementing sharding, you are seeing responses being returned from only 2 instances for customer Alpha\u0026rsquo;s requests. No matter how many times you refresh the page or try a different browser, customer Alpha will only receive responses from 2 EC2 instances. This is because you have created Application Load Balancer listener rules that divert traffic to a specific subset of the overall capacity of the workload, also known as a shard. In this case, customers Alpha and Bravo are mapped to Shard 1 containing Worker 1 and Worker 2. Customers Charlie and Delta are mapped to Shard 2 containing Worker 3 and Worker 4. Customers Echo and Foxtrot are mapped to Shard 3 containing Worker 5 and Worker 6. Customers Golf and Hotel are mapped to Shard 4 containing Worker 7 and Worker 8. Open the links for a few other customers and verify that they are able to get responses from only 2 EC2 instances. The different customers are - Alpha, Bravo, Charlie, Delta, Echo, Foxtrot, Golf, and Hotel and their corresponding URLs can be obtained from the CloudFormation stack Outputs.\nRefresh the web browser multiple times to verify that customers are only receiving responses from EC2 instances in the shard they are mapped to.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/","title":"Level 300: Fault Isolation with Shuffle Sharding","tags":["mitigate_failure"],"description":"Implement shuffle sharding to minimize scope of impact of failures","content":"Author Mahanth Jayadeva, Solutions Architect, AWS Well-Architected Introduction In this lab, you will become familiar with the concept of shuffle sharding and how it can help reduce blast radius during failures. You will learn how to distribute user requests to resources in a combinatorial way so that any failures affect only a small subset of users. \u0026ldquo;Users\u0026rdquo; in this case refers to any source of requests to your workload. This can be other services that call your workload, in addition to actual human users.\nWithout any sharding, any worker (such as servers, queues, or databases) in your workload can handle any request. With this architecture a poisonous request or a flood of requests from one user will spread unabated through the entire fleet. Sharding is the practice of providing logical isolation of capacity (one ore more workers) for each set of users, limiting the propagation of such failures. Shuffle sharding further limits the impact of any one \u0026ldquo;bad actor\u0026rdquo; among your users.\nIn this lab, you will create a workload with a sample application running on EC2 instances. You will then create a strategy to distribute traffic using shuffle sharding and reduce the blast radius when a failure occurs.\nThe skills you learn will help you use fault isolation to protect your workload in alignment with Reliability best practices of the AWS Well-Architected Framework Goals: Deploy a workload with a sharded architecture to limit propagation of failures Implement shuffle sharding to further limit failure propagation Test failure within the workload and ensure the scope of impact is minimized Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. NOTE: You will be billed for any applicable AWS resources used as part of this lab, that are not covered in the AWS Free Tier.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the workload Scope of Impact of failures Implement sharding Impact of failures with sharding Implement shuffle sharding Impact of failures with shuffle sharding Teardown References \u0026amp; useful resources "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/3_creating_cloudwatch_dashboard/","title":"Create CloudWatch Dashboard","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have deployed a single Amazon Linux 2 EC2 instance and we will now create a CloudWatch Dashboard to monitor the memory and CPU resources consumed by the instance.\nFrom the AWS Console, click the search box and type in CloudWatch (or you can open this link directly https://console.aws.amazon.com/cloudwatch/home ) Click on Dashboards link on the left side Click on Create Dashboard button If you have not done so already, make sure to click \u0026ldquo;Try out the new interface\u0026rdquo; to see the updated CloudWatch interface.\nUnder Dashboard Name, type in \u0026ldquo;LinuxEC2Server” and then click “Create Dashboard” You will be presented with dialog box to “Add to the dashboard”. Select Line and click Next Select Metrics and click Configure Under “All metrics”, scroll down to “Custom Namespaces” and click on “PerfLab” You will find multiple metrics, but we will start with each of the CPU’s in the machine. Click on “ImageId, InstanceId, InstanceType, cpu” Make sure the Instance Name starts with LinuxMachineDeploy, then click on one of the InstanceId and select “Search for this only”. This will ensure we are only looking at instances created for this lab. Under Metric Name, look for “cpu_usage_user” and then click “Add to search”. This will limit your choices again to only the CPU metrics for this machine. In this case, you should see 2 processors listed, but if the machine you have deployed has more, you will see them each listed. On the left side of the screen, select the box right above the list, this will add both metrics to your graph. Click on the “Graphed Metrics (2)” tab, and on the right side next to Period, select “5 seconds” The metric graph will now update with more data points. Click “Create Widget” You should now see the dashboard we have created displayed with the first metric widget. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/3_creating_cloudwatch_dashboard/","title":"Create CloudWatch Dashboard","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have deployed a single Windows 2 EC2 instance and we will now create a CloudWatch Dashboard to monitor the memory and CPU resources consumed by the instance.\nFrom the AWS Console, click the search box and type in CloudWatch (or you can open this link directly https://console.aws.amazon.com/cloudwatch/home ) Click on Dashboards link on the left side Click on Create Dashboard button If you have not done so already, make sure to click \u0026ldquo;Try out the new interface\u0026rdquo; to see the updated CloudWatch interface.\nUnder Dashboard Name, type in \u0026ldquo;WindowsEC2Server\u0026rdquo; and then click \u0026ldquo;Create Dashboard\u0026rdquo; You will be presented with dialog box to \u0026ldquo;Add to the dashboard\u0026rdquo;. Select Line and click Next Select Metrics and click Configure Under \u0026ldquo;All metrics\u0026rdquo;, scroll down to \u0026ldquo;Custom Namespaces\u0026rdquo; and click on \u0026ldquo;CWAgent\u0026rdquo; You will find multiple metrics, but we will start with each of the CPU\u0026rsquo;s in the machine.\nClick on \u0026ldquo;ImageId, InstanceId, InstanceType, instance, o\u0026hellip;\u0026rdquo; Make sure the Instance Name starts with WindowsMachineDeploy, then click on one of the InstanceId and select \u0026ldquo;Search for this only\u0026rdquo;. This will ensure we are only looking at instances created for this lab. Under Metric Name, look for \u0026ldquo;Processor % User Time\u0026rdquo; and then click \u0026ldquo;Add to search\u0026rdquo;. This will limit your choices again to only the CPU metrics for this machine. In this case you should see 2 processors listed, but if the machine you have deployed has more, you will see them each listed. On the left side of the screen, select the box right above the list, this will add both metrics to your graph. Click on the \u0026ldquo;Graphed Metrics (2)\u0026rdquo; tab, and on the right side next to Period, select \u0026ldquo;5 seconds\u0026rdquo; The metric graph will now update with more data points. Click \u0026ldquo;Create Widget\u0026rdquo; You should now see the dashboard we have created displayed with the first metric widget. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/","title":"Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Windows EC2 instance.","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction This hands-on lab will guide you through creating an Amazon EC2 instance for Windows and then configuring a Amazon CloudWatch Dashboard to get aggregated views of the health and performance information for that instance. This lab should enables you to quickly get started with CloudWatch monitoring and explore account and resource-based views of metrics. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Monitor a Windows EC2 machine to identify CPU and memory bottlenecks Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes An IAM role in your AWS account Costs https://aws.amazon.com/cloudwatch/pricing/ You can create 3 dashboards for up to 50 metrics per month on the free tier and then it is $3.00 per dashboard per month This lab creates one dashboard, so the maximum cost would be $3.00 per month if you have already consumed the free tier. The default lab uses a t3.large EC2 instance which will consume approximately $3.00 for every day the lab is running The VPC that is created for this lab will build a Nat Gateway, and will consume $5.50 per day when deployed. Using defaults, the total cost of the lab would be at least $8.50 per day NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploying the infrastructure Deploying an instance Create CloudWatch Dashboard Add metrics to Dashboard Generate CPU and Memory load Teardown "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/1-2_discover_cur_data/","title":"Discover CUR data with Amazon Athena","tags":[],"description":"","content":"Lab 1.2 In the previous step, you provided AWS Cost \u0026amp; Usage Report data in an Amazon S3 bucket. In this step you will make this usage data available in the AWS Glue data catalog for Amazon Athena. Amazon Athena allows you to run SQL (Structured Query Language) queries on the data without loading it into a database and calculate proxy metrics for sustainability.\nGo to the Amazon Athena console in a region in which your CUR data is stored. This is the same region your S3 bucket resides as defined in lab 1.1 . If this is your first time to visit the Athena console in your current AWS Region, choose Explore the query editor, then choose set up a query result location in Amazon S3 and follow the steps from the Amazon Athena docs to specify a query result location . Depending on your setup, you may need to create a new workgroup within Amazon Athena. Create a new AWS Glue database via the Amazon Athena\u0026rsquo;s query editor. Enter the query: CREATE DATABASE proxy_metrics_lab Click Run query Choose the Database proxy_metrics_lab from the drop down menu. Now you need to tell AWS Glue where the data to query is stored. You create a table in AWS Glue via a DDL (Data Definition Language) SQL statement in Amazon Athena. Fill the New Query field with the following query. You need to replace S3 URL HERE on line 160 of the query with the URI of the root of your cost and usage reports in the form of s3://\u0026lt;bucket\u0026gt;/\u0026lt;Report path prefix\u0026gt;/\u0026lt;Report name\u0026gt;/. If you followed option B in lab 1.1 , your S3 URI may look like s3://\u0026lt;bucket\u0026gt;/cur-data/hourly/proxy-metrics-lab/proxy-metrics-lab/. If you followed option C in lab 1.1 , your S3 URI may look like s3://\u0026lt;bucket\u0026gt;/cur-data/hourly/proxy-metrics-lab/. CREATE EXTERNAL TABLE `cur_hourly`( `identity_line_item_id` string, `identity_time_interval` string, `bill_invoice_id` string, `bill_billing_entity` string, `bill_bill_type` string, `bill_payer_account_id` string, `bill_billing_period_start_date` timestamp, `bill_billing_period_end_date` timestamp, `line_item_usage_account_id` string, `line_item_line_item_type` string, `line_item_usage_start_date` timestamp, `line_item_usage_end_date` timestamp, `line_item_product_code` string, `line_item_usage_type` string, `line_item_operation` string, `line_item_availability_zone` string, `line_item_resource_id` string, `line_item_usage_amount` double, `line_item_normalization_factor` double, `line_item_normalized_usage_amount` double, `line_item_currency_code` string, `line_item_unblended_rate` string, `line_item_unblended_cost` double, `line_item_blended_rate` string, `line_item_blended_cost` double, `line_item_line_item_description` string, `line_item_tax_type` string, `line_item_legal_entity` string, `product_product_name` string, `product_activity_type` string, `product_alarm_type` string, `product_availability` string, `product_capacitystatus` string, `product_category` string, `product_clock_speed` string, `product_current_generation` string, `product_data_type` string, `product_datatransferout` string, `product_dedicated_ebs_throughput` string, `product_description` string, `product_durability` string, `product_ecu` string, `product_edition` string, `product_endpoint_type` string, `product_enhanced_networking_supported` string, `product_event_type` string, `product_fee_code` string, `product_fee_description` string, `product_finding_group` string, `product_finding_source` string, `product_finding_storage` string, `product_from_location` string, `product_from_location_type` string, `product_group` string, `product_group_description` string, `product_instance_family` string, `product_instance_type` string, `product_instance_type_family` string, `product_intel_avx2_available` string, `product_intel_avx_available` string, `product_intel_turbo_available` string, `product_license_model` string, `product_location` string, `product_location_type` string, `product_logs_destination` string, `product_logs_source` string, `product_logs_type` string, `product_max_iops_burst_performance` string, `product_max_iopsvolume` string, `product_max_throughputvolume` string, `product_max_volume_size` string, `product_maximum_extended_storage` string, `product_memory` string, `product_memory_gib` string, `product_message_delivery_frequency` string, `product_message_delivery_order` string, `product_network_performance` string, `product_normalization_size_factor` string, `product_operating_system` string, `product_operation` string, `product_physical_processor` string, `product_pre_installed_sw` string, `product_processor_architecture` string, `product_processor_features` string, `product_product_family` string, `product_protocol` string, `product_queue_type` string, `product_region` string, `product_request_description` string, `product_request_type` string, `product_resource_price_group` string, `product_routing_target` string, `product_routing_type` string, `product_servicecode` string, `product_servicename` string, `product_sku` string, `product_standard_group` string, `product_standard_storage` string, `product_standard_storage_retention_included` string, `product_storage` string, `product_storage_class` string, `product_storage_media` string, `product_storage_type` string, `product_subscription_type` string, `product_tenancy` string, `product_to_location` string, `product_to_location_type` string, `product_transfer_type` string, `product_usagetype` string, `product_vcpu` string, `product_version` string, `product_volume_api_name` string, `product_volume_type` string, `pricing_rate_id` string, `pricing_currency` string, `pricing_public_on_demand_cost` double, `pricing_public_on_demand_rate` string, `pricing_term` string, `pricing_unit` string, `reservation_amortized_upfront_cost_for_usage` double, `reservation_amortized_upfront_fee_for_billing_period` double, `reservation_effective_cost` double, `reservation_end_time` string, `reservation_modification_status` string, `reservation_normalized_units_per_reservation` string, `reservation_number_of_reservations` string, `reservation_recurring_fee_for_usage` double, `reservation_start_time` string, `reservation_subscription_id` string, `reservation_total_reserved_normalized_units` string, `reservation_total_reserved_units` string, `reservation_units_per_reservation` string, `reservation_unused_amortized_upfront_fee_for_billing_period` double, `reservation_unused_normalized_unit_quantity` double, `reservation_unused_quantity` double, `reservation_unused_recurring_fee` double, `reservation_upfront_value` double, `savings_plan_total_commitment_to_date` double, `savings_plan_savings_plan_a_r_n` string, `savings_plan_savings_plan_rate` double, `savings_plan_used_commitment` double, `savings_plan_savings_plan_effective_cost` double, `savings_plan_amortized_upfront_commitment_for_billing_period` double, `savings_plan_recurring_commitment_for_billing_period` double, `resource_tags_aws_cloudformation_logical_id` string, `resource_tags_aws_cloudformation_stack_id` string, `resource_tags_aws_cloudformation_stack_name` string, `resource_tags_user_name` string) PARTITIONED BY ( `year` int, `month` int) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION \u0026#39;S3 URL HERE\u0026#39; \u0026lt;\u0026lt;\u0026lt; REPLACE ME TBLPROPERTIES ( \u0026#39;classification\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;has_encrypted_data\u0026#39;=\u0026#39;false\u0026#39;, \u0026#39;projection.enabled\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;projection.month.range\u0026#39;=\u0026#39;1,12\u0026#39;, \u0026#39;projection.month.type\u0026#39;=\u0026#39;integer\u0026#39;, \u0026#39;projection.year.range\u0026#39;=\u0026#39;2018,2022\u0026#39;, \u0026#39;projection.year.type\u0026#39;=\u0026#39;integer\u0026#39;) Choose Run query. The Results section of the Query editor should display \u0026ldquo;Query successful [\u0026hellip;]\u0026rdquo;. As the query does use partition projection (see the TBLPROPERTIES clause), you do not load the partitions to query the data. Amazon Athena is clever enough to calculate them as long as they are in the range of Jan 2018 to December 2022. Now you can preview random 10 entries. Choose the three dots next to the cur_hourly table and choose Preview table. Congratulations! You now can explore your CUR data with Amazon Athena with SQL queries. As soon as new CUR data gets written to Amazon S3, it will be returned in your next query immediately. Take the time to expirement with Amazon Athena\u0026rsquo;s query editor and explore the columns of your reports.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/2_prepare_redshift_producer_cluster/","title":"Prepare Amazon Redshift Producer Cluster","tags":[],"description":"","content":"Lab 2 We will first create the producer Amazon Redshift cluster (we will refer this as the producer cluster throughout the lab) in us-east-1 region, and will also load sample dataset which we will use for our sustainability use case.\nStep-1: Create Redshift Producer Cluster Login into the AWS Console (make sure us-east-1 region is selected in top right corner), and click Create Cluster.\nProvide Cluster name as redshift-cluster-east, and select ra3.xlplus node type -\nNOTE: If you get access error launching cluster with ra3.xlplus node type, then select ra3.4xlarge node type. Please note, Amazon Redshift Data Sharing feature is not supported for previous generation dc2 node types, and Amazon Redshift only supports data sharing on the ra3.16xlarge, ra3.4xlarge, and ra3.xlplus instance types for producer and consumer clusters. Amazon Redshift ra3 nodes incurs cost as these nodes are not part of the Amazon Redshift free trial, or AWS Free Tier.\nSelect “Load Sample data”.\nSupply a password for Admin user.\nOther configuration settings can be left as default.\nClick the Create Cluster button – it will take few minutes to create cluster, and load sample data into database. Step-2: Connect to database using query editor Once the cluster is created (Status = Available), using one of the Amazon Redshift query editors is the easiest way to query the Amazon Redshift database. After creating your cluster, use the query editor v2 to connect to newly created database.\nStep-3: Validate database In the query editor, click on the newly created cluster, and it will establish connection to the database. You will then see two databases created automatically – dev, sample_data_dev. The dev database has one schema called public, which holds the 7 sample tables loaded during the cluster creation. Expand the public schema under dev database, and you will see list of tables. We will refer to this as producer database throughout the lab. These tables were bootstrapped during cluster creation, and can’t be shared using Amazon Redshift Data Sharing feature. For this lab, we will use these bootstrapped tables to create our own tables to test the Amazon Redshift Data Sharing feature. Go to the query editor and execute these SQL commands: CREATE TABLE lab_users AS SELECT * FROM users; CREATE TABLE lab_venue AS SELECT * FROM venue; CREATE TABLE lab_category AS SELECT * FROM category; CREATE TABLE lab_date AS SELECT * FROM date; CREATE TABLE lab_event AS SELECT * FROM event; CREATE TABLE lab_sales AS SELECT * FROM sales; CREATE TABLE lab_listing AS SELECT * FROM listing; Once above CREATE TABLE commands are successfully completed, then drop the bootstrapped tables using below SQL commands. This will help with estimating data storage consumed, and comparison between producer and consumer databases. DROP TABLE users; DROP TABLE venue; DROP TABLE category; DROP TABLE date; DROP TABLE event; DROP TABLE sales; DROP TABLE listing; So far, we have installed \u0026amp; configured the producer cluster, and loaded a sample dataset into the producer database in us-east-1 region. Next, we will install and configure the Amazon Redshift consumer cluster in us-west-1 region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/2_prepare_redshift_producer_cluster/","title":"Prepare Amazon Redshift Producer Cluster","tags":[],"description":"","content":"Lab 2 We will first create the producer Amazon Redshift cluster (we will refer this as the producer cluster throughout the lab) in us-east-1 region, and will also load sample dataset which we will use for our sustainability use case.\nStep-1: Create Redshift Producer Cluster Login into the AWS Console (make sure us-east-1 region is selected in top right corner), and click Create Cluster.\nProvide Cluster name as redshift-cluster-east, and select ra3.xlplus node type -\nNOTE: If you get access error launching cluster with ra3.xlplus node type, then select ra3.4xlarge node type. Please note, Amazon Redshift Data Sharing feature is not supported for previous generation dc2 node types, and Amazon Redshift only supports data sharing on the ra3.16xlarge, ra3.4xlarge, and ra3.xlplus instance types for producer and consumer clusters. Amazon Redshift ra3 nodes incurs cost as these nodes are not part of the Amazon Redshift free trial, or AWS Free Tier.\nSelect “Load Sample data”.\nSupply a password for Admin user.\nOther configuration settings can be left as default.\nClick the Create Cluster button – it will take few minutes to create cluster, and load sample data into database. Step-2: Connect to database using query editor Once the cluster is created (Status = Available), using one of the Amazon Redshift query editors is the easiest way to query the Amazon Redshift database. After creating your cluster, use the query editor v2 to connect to newly created database.\nStep-3: Validate database In the query editor, click on the newly created cluster, and it will establish connection to the database. You will then see two databases created automatically – dev, sample_data_dev. The dev database has one schema called public, which holds the 7 sample tables loaded during the cluster creation. Expand the public schema under dev database, and you will see list of tables. We will refer to this as producer database throughout the lab. These tables were bootstrapped during cluster creation, and can’t be shared using Amazon Redshift Data Sharing feature. For this lab, we will use these bootstrapped tables to create our own tables to test the Amazon Redshift Data Sharing feature. Go to the query editor and execute these SQL commands: CREATE TABLE lab_users AS SELECT * FROM users; CREATE TABLE lab_venue AS SELECT * FROM venue; CREATE TABLE lab_category AS SELECT * FROM category; CREATE TABLE lab_date AS SELECT * FROM date; CREATE TABLE lab_event AS SELECT * FROM event; CREATE TABLE lab_sales AS SELECT * FROM sales; CREATE TABLE lab_listing AS SELECT * FROM listing; Once above CREATE TABLE commands are successfully completed, then drop the bootstrapped tables using below SQL commands. This will help with estimating data storage consumed, and comparison between producer and consumer databases. DROP TABLE users; DROP TABLE venue; DROP TABLE category; DROP TABLE date; DROP TABLE event; DROP TABLE sales; DROP TABLE listing; So far, we have installed \u0026amp; configured the producer cluster, and loaded a sample dataset into the producer database in us-east-1 region. Next, we will install and configure the Amazon Redshift consumer cluster in us-west-1 region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/","title":"300 Level Advanced Labs","tags":[],"description":"","content":"List of labs available Level 300: Optimize Data Pattern using Amazon Redshift Data Sharing Level 300: Turning Cost \u0026amp; Usage Reports into Efficiency Reports "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/3_cur/","title":"Configure Cost and Usage reports","tags":[],"description":"","content":"Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information.\nConfigure a Cost and Usage Report If you configure multiple Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multiple CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports.\nLog in to your management account as an IAM user with the required permissions, and go to the Billing console: Select Cost \u0026amp; Usage Reports from the left menu: Click on Create report: Enter a Report name (it can be any name, but we recommend including the management account id in the name), ensure you have selected Include resource IDs and Data refresh settings, then click on Next: Click on Configure: Enter a unique bucket name, and ensure the region is correct, click Next: Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct, then click Save: Verify the settings:\nEnsure your bucket is a Valid Bucket (if not, verify the bucket policy) Enter a Report path prefix (it can be any word, but we recommend cur-\u0026lt;Your Management Account ID) without any \u0026lsquo;/\u0026rsquo; characters Ensure the Time Granularity is Hourly Report Versioning is set to Overwrite existing report Under Enable report data integration for select Amazon Athena, and click Next: Review the configuration, scroll to the bottom and click on Review and Complete: You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered.\nThere will be S3 Costs incurred to store the CUR, however the CUR is compressed to minimize costs.\nConfigure the CUR Bucket for your Cost Optimization Account We will update the CUR bucket so that the Cost Optimization linked account can access the CURs. There are two options. Option 1 allows the Cost Optimization linked account to access the CURs, but does not copy the CUR files to the account. Option 2 uses S3 Replication to create a copy of the CUR in an S3 Bucket in your Cost Optimization Account. If you are unsure what option to use we recommend option 1.\nOption 1: Configure Cost Optimization Access to the CUR Bucket Option 1 allows the Cost Optimization linked account to access the CURs, but does not copy the CUR files to the account. If you are unsure what option to use we recommend option 1.\nClick here to continue with the option 1 Go to the S3 console, select the CUR Bucket, select Permissions: Scroll down to the Bucket Policy section and select Edit Add S3 read access to the Cost Optimization account by adding the following statements under the current bucket policy. Edit (Cost Optimization Member account ID) and (CUR bucket) and update the bucket policy:\n{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR bucket)\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR bucket)/*\u0026quot; } Click here for a completed example policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2008-10-17\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;123\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1335892150622\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::386209384616:root\u0026quot; }, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetBucketAcl\u0026quot;, \u0026quot;s3:GetBucketPolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1335892526596\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::386209384616:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:PutObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;s3:GetObject\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(CUR Bucket)/*\u0026quot; } ] } Scroll down to the Object Ownership section and select Edit Select Bucket owner preferred, click Save When CUR files are delivered they will now automatically have permissions allowing the bucket owner full control. Re-write of the object ACLs is no longer necessary.\nUpdate existing CURs If there are existing CURs from other reports that need permissions to be updated, you can use the following CLI - which will copy the objects over themselves and update the permissions as it copies. You can use this link to find you canonical ID\u0026rsquo;s.\naws s3 cp --recursive s3://(CUR bucket) s3://(CUR bucket) --grants read=id=(sub account canonical ID) full=id=(management account canonical ID) --storage-class STANDARD NOTE: Congratulations - you will now have CURs delivered and accessible by your Cost Optimization account. Option 2: Replicate the CUR Bucket to your Cost Optimization account (Consolidate Multi-Payer CURs) Option 2 uses S3 Replication to create a copy of the CUR in an S3 Bucket in your Cost Optimization Account. If you have multiple Management Accounts (multi-Payer) or wish you create a single CUR source for groupings of your member account CUR(s) we recommend this option.\nClick here to continue with the option 2 Create your Cost Optimization account CUR Bucket We will now create a bucket in your Cost Optimization account that will hold the replicated CUR(s)\nLog into you Cost Optimization Account and navigate to Amazon S3\nSelect Create bucket Add an S3 Bucket name select your preferred region and Enable Bucket versioning Select your new S3 Bucket, select Permissions: Scroll down to the Bucket Policy section and select Edit Edit, apply and save the following S3 bucket policy replacing respective placeholders (ManagementAccountA), (ManagementAccountB) and (Cost Optimization Account CUR BucketName). You can add more management accounts to the policy if needed. If using only one Management account you will remove ,\u0026quot;(ManagementAccountB)\u0026quot;\n{ \u0026quot;Version\u0026quot;: \u0026quot;2008-10-17\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;PolicyForCombinedBucket\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Set permissions for objects\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: [\u0026quot;(ManagementAccountA)\u0026quot;,\u0026quot;(ManagementAccountB)\u0026quot;] }, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ReplicateObject\u0026quot;, \u0026quot;s3:ReplicateDelete\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(Cost Optimization Account CUR BucketName)/*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Set permissions on bucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: [\u0026quot;(ManagementAccountA)\u0026quot;,\u0026quot;(ManagementAccountB)\u0026quot;] }, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:List*\u0026quot;, \u0026quot;s3:GetBucketVersioning\u0026quot;, \u0026quot;s3:PutBucketVersioning\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(Cost Optimization Account CUR BucketName)\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;Set permissions to pass object ownership\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: [\u0026quot;(ManagementAccountA)\u0026quot;,\u0026quot;(ManagementAccountB)\u0026quot;] }, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ReplicateObject\u0026quot;, \u0026quot;s3:ReplicateDelete\u0026quot;, \u0026quot;s3:ObjectOwnerOverrideToBucketOwner\u0026quot;, \u0026quot;s3:ReplicateTags\u0026quot;, \u0026quot;s3:GetObjectVersionTagging\u0026quot;, \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(Cost Optimization Account CUR BucketName)/*\u0026quot; } ] } NOTE: This policy supports objects encrypted with either SSE-S3 or not encrypted objects. For SSE-KMS encrypted objects additional policy statements and replication configuration will be needed: see https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html Configure Bucket Replication for your Cost Optimization account CUR Bucket Log into your Management Account and navigate to Amazon S3 Select the CUR Bucket, then select Properties: Scroll down to the Bucket Versioning section click Edit Set Bucket versioning to Enabled Select the Management tab, then click on Create replication rule under Replication rules/ Create your replication rule by updating the following fields then click Save Add a Replication rule name of CUR-Bucket-Replication Select Specify a bucket in another account under Destination Add your Cost Optimization Account ID Add your Cost Optimization S3 CUR Bucket name Select Change object ownership to destination bucket owner Select Create new role under the IAM role section Leave rest of the settings as default NOTE: If you have a multi-Management (multi-Payer) structure or are using multiple member CURs, repeat the replication process in each Management or member CUR account Update existing CURs - Optional If you would like to sync historical objects in your Management account CUR S3 bucket to your Cost Optimization account S3 bucket, you can use the following CLI:\naws s3 sync s3://\u0026lt;Management_Account_CUR_Bucket_Name\u0026gt; s3://\u0026lt;Cost_Optimization_Account_CUR_Bucket_Name\u0026gt; --acl bucket-owner-full-control NOTE: Congratulations - you will now have CURs delivered and accessible by your Cost Optimization account. Visit the Well-Architected Level 200: Cost and Usage Analysis lab to learn how to analyze your CUR in Athena and create a single Athena CUR table for multi-Management (multi-Payer) or multiple member CURs X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/3_data_collection_modules/","title":"Data Collection Modules and Testing Deployment","tags":[],"description":"","content":"Data Collection Modules These modules templates are managed in AWS owned buckets. If you do not wish to have updates on them from AWS then please save a copy to a bucket in your account and use instead.\nBelow are the modules we have available in this lab. You can read more about them by expanding the sections. You have selected your chosen modules in the Deploy Main Resources section so no action is needed.\nCost Explorer Rightsizing Recommendations Cost Explorer Rightsizing Recommendations This module will collect rightsizing recommendations from AWS Cost Explorer in your management account. You can use the saved Athena query as a view to query these results and track your recommendations. Find out more about the recommendations here. This Data will be partitioned by year, month, day.\nIAM Policy deployed with OptimizationManagementDataRoleStack:\n- PolicyName: \u0026quot;RightsizeReadOnlyPolicy\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;ce:GetRightsizingRecommendation\u0026quot; Resource: \u0026quot;*\u0026quot; CloudFormation Stack deployed with OptimizationDataCollectionStack\nRightsizeStack: Type: AWS::CloudFormation::Stack Properties: Parameters: DestinationBucket: !Ref S3Bucket DestinationBucketARN: !GetAtt S3Bucket.Arn RoleName: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026quot; TemplateURL: \u0026quot;https://aws-well-architected-labs.s3-us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/organization_rightsizing_lambda.yaml\u0026quot; TimeoutInMinutes: 5 Test your Lambda Inventory Collector Inventory Collector This module is designed to loop through your AWS Organizations account and collect data that could be used to find optimization data. It has two components, firstly the AWS accounts collector which used the management role built before. This then passes the account id into an SQS queue which then is used as an event in the next component. This section assumes a role into the account the reads the data and places into an Amazon S3 bucket in the Cost Account. See the Utilize Data Section for more information on how to use this data. This Data will be partitioned by year, month.\nIAM Policy deployed with OptimizationDataRoleStack CloudFormation StackSet depending on what you want to ingest:\n- PolicyName: \u0026quot;InventoryCollectorPolicy\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;ec2:DescribeImages\u0026quot; - \u0026quot;ec2:DescribeVolumeStatus\u0026quot; - \u0026quot;ec2:DescribeVolumes\u0026quot; - \u0026quot;ec2:DescribeSnapshots\u0026quot; - \u0026quot;ec2:DescribeSnapshotAttribute\u0026quot; Resource: \u0026quot;*\u0026quot; CloudFormation Stack deployed with OptimizationDataCollectionStack:\nThe services you can collect data are below. Use these in the Prefix and CFDataName parameters:\nami ebs snapshot ta DataStackMulti: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026ldquo;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/lambda_data.yaml\" TimeoutInMinutes: 2 Parameters: DestinationBucket: !Ref S3Bucket DestinationBucketARN: !GetAtt S3Bucket.Arn Prefix: \u0026ldquo;ami\u0026rdquo; # example CFDataName: \u0026ldquo;AMI\u0026rdquo; # example GlueRoleARN: !GetAtt GlueRole.Arn MultiAccountRoleName: !Ref MultiAccountRoleName CodeBucket: !Ref CodeBucket AccountCollector: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026ldquo;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/get_accounts.yaml\" TimeoutInMinutes: 2 Parameters: RoleARN: !Sub \u0026ldquo;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026rdquo; TaskQueuesUrl: !Sub \u0026ldquo;${DataStackMulti.Outputs.SQSUrl}\u0026rdquo; Optional Parameters with current defaults: DataStackMulti\nDatabaseName: optimization_data CodeKey: Cost/Labs/300_Optimization_Data_Collection/fof.zip AccountCollector - Suffix: \u0026rsquo;' - Schedule: rate(14 days)\nTest your Lambda Trusted Advisor Trusted Advisor This module will retrieve all AWS Trusted Advisor recommendations from all your linked account. See the Utilize Data Section for more information on how to use this data. This Data will be partitioned by year, month, day.\nOnce this module is deployed and TA data is collected you can visualize it with TAO Dashboard . To deploy TAO Dashboard please follow either automated or manual deployment steps and specify organizational data collection bucket created in this lab as a source.\nIAM Policy added OptimizationDataRoleStack CloudFormation StackSet:\n- PolicyName: \u0026quot;TAPolicy\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;trustedadvisor:*\u0026quot; - \u0026quot;support:DescribeTrustedAdvisorChecks\u0026quot; - \u0026quot;support:DescribeTrustedAdvisorCheckResult\u0026quot; Resource: \u0026quot;*\u0026quot; CloudFormation Stack added to OptimizationDataCollectionStack:\nTrustedAdvisor: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/trusted_advisor.yaml\u0026quot; TimeoutInMinutes: 2 Parameters: DestinationBucket: !Ref S3Bucket DestinationBucketARN: !GetAtt S3Bucket.Arn Prefix: \u0026quot;ta\u0026quot; CFDataName: \u0026quot;TA\u0026quot; GlueRoleARN: !GetAtt GlueRole.Arn MultiAccountRoleName: !Ref MultiAccountRoleName CodeBucket: !Ref CodeBucket AccountCollector: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/get_accounts.yaml\u0026quot; TimeoutInMinutes: 2 Parameters: RoleARN: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026quot; TaskQueuesUrl: !Sub \u0026quot;${TrustedAdvisor.Outputs.SQSUrl}\u0026quot; Optional Parameters with current defaults: DataStackMulti\nDatabaseName: optimization_data CodeKey: Cost/Labs/300_Optimization_Data_Collection/ta.zip CFDataName: Trusted_Advisor Prefix: ta AccountCollector - Suffix: \u0026rsquo;' - Schedule: rate(14 days)\nTest your Lambda Compute Optimizer Collector Compute Optimizer The Compute Optimizer Service by default only shows current point in time recommendations looking at the past 14 days of usage. In this module, the data will be collected together so you will access to all accounts and regions recommendations in one place. This can be accessed through the Management Account. You can use the saved Athena queries as a view to query these results and track your recommendations. Also we recommend to install Compute Optimizer Dashboard for visualizing.\nCompute Optimizer Data will be separated by type service and partitioned by year, month. Please make sure you enable Compute Optimizer following this guide. Compute Optimizer is regional service and the Compute Optimizer Collector will deploy one bucket for each region. The user must specify DeployRegions - a comma separated list of regions with EC2, EBS, ASG and Lambda workloads. If blank, the current region will be used.\nIAM Policy added to OptimizationManagementDataRoleStack:\n- PolicyName: \u0026quot;ComputeOptimizer-ExportLambdaFunctionRecommendations\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;compute-optimizer:ExportLambdaFunctionRecommendations\u0026quot; - \u0026quot;compute-optimizer:GetLambdaFunctionRecommendations\u0026quot; - \u0026quot;lambda:ListFunctions\u0026quot; - \u0026quot;lambda:ListProvisionedConcurrencyConfigs\u0026quot; Resource: \u0026quot;*\u0026quot; - PolicyName: \u0026quot;ComputeOptimizer-ExportAutoScalingGroupRecommendations\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;compute-optimizer:ExportAutoScalingGroupRecommendations\u0026quot; - \u0026quot;compute-optimizer:GetAutoScalingGroupRecommendations\u0026quot; - \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot; Resource: \u0026quot;*\u0026quot; - PolicyName: \u0026quot;ComputeOptimizer-ExportEBSVolumeRecommendations\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;compute-optimizer:ExportEBSVolumeRecommendations\u0026quot; - \u0026quot;compute-optimizer:GetEBSVolumeRecommendations\u0026quot; - \u0026quot;EC2:DescribeVolumes\u0026quot; Resource: \u0026quot;*\u0026quot; - PolicyName: \u0026quot;ComputeOptimizer-ExportEC2InstanceRecommendations\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;compute-optimizer:ExportEC2InstanceRecommendations\u0026quot; - \u0026quot;compute-optimizer:GetEC2InstanceRecommendations\u0026quot; - \u0026quot;EC2:DescribeInstances\u0026quot; Resource: \u0026quot;*\u0026quot; CloudFormation Stack added to OptimizationDataCollectionStack :\nComputeOptimizerModule: Type: AWS::CloudFormation::Stack Condition: DeployComputeOptimizerModule Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/compute_optimizer.yaml\u0026quot; Parameters: DestinationBucketARN: !GetAtt S3Bucket.Arn DestinationBucket: !Ref S3Bucket GlueRoleARN: !GetAtt GlueRole.Arn RoleNameARN: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${RolePrefix}${ManagementAccountRole}\u0026quot; S3CrawlerQue: !GetAtt S3CrawlerQue.Arn RolePrefix: !Ref RolePrefix BucketPrefix: !Ref DestinationBucket DeployRegions: Fn::If: - ComputeOptimizerRegionsIsEmpty - !Sub \u0026quot;${AWS::Region}\u0026quot; - !Join [ \u0026quot;,\u0026quot;, !Ref ComputeOptimizerRegions ] Optional Parameters with current defaults: DataStackMulti\nDatabaseName: optimization_data CFDataName: ComputeOptimizer AccountCollector - Suffix: \u0026rsquo;' - Schedule: rate(14 days)\nTest your Lambda ECS Chargeback Data ECS Chargeback This module will enable you too automated report to show costs associated with ECS Tasks leveraging EC2 instances within a Cluster. Instructions on how to use this data can be found here. This Data will be partitioned by year, month, day.\nPre-Requisites Completion of Well-Architected Lab: 100_1_aws_account_setup or similar setup of the Cost and Usage Report (CUR) with resource Id enabled A CUR file has been established for the existing Management/Payer account within the Billing Console The ECS Cluster leveraging EC2 instances for compute resides in a Linked Account connected to the Management Account through the \u0026ldquo;Consolidated Billing\u0026rdquo; option within the Billing Console AWS generated tag is active in Cost Allocation Tags aws:ecs:serviceName this will appear in the CUR as resource_tags_aws_ecs_service_Name User-defined Cost Allocation Tags Name is active You will need an S3 bucket in your Analytics account to upload source files into Your Tasks MUST have the Name of the Service as a tag Name. This is best done with Tag propagation on service creation, see below: - Note: If you cannot re-create your task using this the see the source/tag.py IAM Policy added to OptimizationDataRoleStack CloudFormation StackSet:\n- PolicyName: \u0026quot;ECSReadAccess\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;ecs:ListAttributes\u0026quot; - \u0026quot;ecs:DescribeTaskSets\u0026quot; - \u0026quot;ecs:DescribeTaskDefinition\u0026quot; - \u0026quot;ecs:DescribeClusters\u0026quot; - \u0026quot;ecs:ListServices\u0026quot; - \u0026quot;ecs:ListAccountSettings\u0026quot; - \u0026quot;ecs:DescribeCapacityProviders\u0026quot; - \u0026quot;ecs:ListTagsForResource\u0026quot; - \u0026quot;ecs:ListTasks\u0026quot; - \u0026quot;ecs:ListTaskDefinitionFamilies\u0026quot; - \u0026quot;ecs:DescribeServices\u0026quot; - \u0026quot;ecs:ListContainerInstances\u0026quot; - \u0026quot;ecs:DescribeContainerInstances\u0026quot; - \u0026quot;ecs:DescribeTasks\u0026quot; - \u0026quot;ecs:ListTaskDefinitions\u0026quot; - \u0026quot;ecs:ListClusters\u0026quot; Resource: \u0026quot;*\u0026quot; CloudFormation Stack added to OptimizationDataCollectionStack:\nECSStack: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/ecs_data.yaml\u0026quot; TimeoutInMinutes: 2 Parameters: DestinationBucket: !Ref S3Bucket GlueRoleArn: !GetAtt GlueRole.Arn MultiAccountRoleName: !Ref MultiAccountRoleName CodeBucket: !Ref CodeBucket AccountCollector: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/get_accounts.yaml\u0026quot; TimeoutInMinutes: 2 Parameters: RoleARN: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026quot; TaskQueuesUrl: !Sub \u0026quot;${ECSStack.Outputs.SQSUrl}\u0026quot; Optional Parameters with current defaults: DataStackMulti\nDatabaseName: optimization_data CodeKey: Cost/Labs/300_Optimization_Data_Collection/ecs.zip CFDataName: ecs-services-clusters CURTable: managementcur AccountCollector - Suffix: \u0026rsquo;' - Schedule: rate(14 days)\nTest your Lambda RDS Utilization Data RDS Utilization The module will collect RDS CloudWatch metrics from your accounts. Using this data you can identify possible underutilized instances. You can use the saved Athena query as a view to query these results and track your recommendations. This is partitioned by TBC.\nIAM Policy added to OptimizationDataRoleStack CloudFormation StackSet:\n- PolicyName: \u0026quot;RDSUtilReadOnlyPolicy\u0026quot; PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: \u0026quot;Allow\u0026quot; Action: - \u0026quot;rds:DescribeDBProxyTargetGroups\u0026quot; - \u0026quot;rds:DescribeDBInstanceAutomatedBackups\u0026quot; - \u0026quot;rds:DescribeDBEngineVersions\u0026quot; - \u0026quot;rds:DescribeDBSubnetGroups\u0026quot; - \u0026quot;rds:DescribeGlobalClusters\u0026quot; - \u0026quot;rds:DescribeExportTasks\u0026quot; - \u0026quot;rds:DescribePendingMaintenanceActions\u0026quot; - \u0026quot;rds:DescribeEngineDefaultParameters\u0026quot; - \u0026quot;rds:DescribeDBParameterGroups\u0026quot; - \u0026quot;rds:DescribeDBClusterBacktracks\u0026quot; - \u0026quot;rds:DescribeCustomAvailabilityZones\u0026quot; - \u0026quot;rds:DescribeReservedDBInstancesOfferings\u0026quot; - \u0026quot;rds:DescribeDBProxyTargets\u0026quot; - \u0026quot;rds:DownloadDBLogFilePortion\u0026quot; - \u0026quot;rds:DescribeDBInstances\u0026quot; - \u0026quot;rds:DescribeSourceRegions\u0026quot; - \u0026quot;rds:DescribeEngineDefaultClusterParameters\u0026quot; - \u0026quot;rds:DescribeInstallationMedia\u0026quot; - \u0026quot;rds:DescribeDBProxies\u0026quot; - \u0026quot;rds:DescribeDBParameters\u0026quot; - \u0026quot;rds:DescribeEventCategories\u0026quot; - \u0026quot;rds:DescribeDBProxyEndpoints\u0026quot; - \u0026quot;rds:DescribeEvents\u0026quot; - \u0026quot;rds:DescribeDBClusterSnapshotAttributes\u0026quot; - \u0026quot;rds:DescribeDBClusterParameters\u0026quot; - \u0026quot;rds:DescribeEventSubscriptions\u0026quot; - \u0026quot;rds:DescribeDBSnapshots\u0026quot; - \u0026quot;rds:DescribeDBLogFiles\u0026quot; - \u0026quot;rds:DescribeDBSecurityGroups\u0026quot; - \u0026quot;rds:DescribeDBSnapshotAttributes\u0026quot; - \u0026quot;rds:DescribeReservedDBInstances\u0026quot; - \u0026quot;rds:ListTagsForResource\u0026quot; - \u0026quot;rds:DescribeValidDBInstanceModifications\u0026quot; - \u0026quot;rds:DescribeDBClusterSnapshots\u0026quot; - \u0026quot;rds:DescribeOrderableDBInstanceOptions\u0026quot; - \u0026quot;rds:DescribeOptionGroupOptions\u0026quot; - \u0026quot;rds:DescribeDBClusterEndpoints\u0026quot; - \u0026quot;rds:DescribeCertificates\u0026quot; - \u0026quot;rds:DescribeDBClusters\u0026quot; - \u0026quot;rds:DescribeAccountAttributes\u0026quot; - \u0026quot;rds:DescribeOptionGroups\u0026quot; - \u0026quot;rds:DescribeDBClusterParameterGroups\u0026quot; - \u0026quot;ec2:DescribeRegions\u0026quot; Resource: \u0026quot;*\u0026quot; CloudFormation Stack added to OptimizationDataCollectionStack:\nRDSMetricsStack: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/rds_util_template.yaml\u0026quot; TimeoutInMinutes: 2 Parameters: DestinationBucket: !Ref S3Bucket DestinationBucketARN: !GetAtt S3Bucket.Arn GlueRoleArn: !GetAtt GlueRole.Arn MultiAccountRoleName: !Ref MultiAccountRoleName Optional Parameters with current defaults: DataStackMulti\nDatabaseName: optimization_data CFDataName: RDSMETRICS DAYS: 1 Test your Lambda AWS Organization Data Export AWS Organization Data This module will extract the data from AWS Organizations, such as account ID, account name, organization parent and specified tags. This data can be connected to your AWS Cost \u0026amp; Usage Report to enrich it or other modules in this lab. In Tags list all the tags from your Organization you would like to include separated by a comma.It is not partitioned.\nCurrently this data looks for tags \u0026lsquo;Env\u0026rsquo; This can be updated in the lambda function Environment Variables separated by a comer.\nCloudFormation added to OptimizationDataCollectionStack:\nOrganizationData: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026quot;https://aws-well-architected-labs.s3.us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/organization_data.yaml\u0026quot; TimeoutInMinutes: 2 Parameters: DestinationBucket: !Ref S3Bucket GlueRoleARN: !GetAtt GlueRole.Arn ManagementAccountRole: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026quot; Tags: \u0026quot;Env\u0026quot; Optional Parameters:\nSuffix: '\u0026rsquo; Schedule: rate(14 days) Test your Lambda AWS Budgets Export AWS Budgets AWS Budgets allows you to set custom budgets to track your cost and usage from the simplest to the most complex use cases. This module will export the data from all budgets so you can group together reports and combine with dashboards. This Data will be separated by type service and partitioned by year, month. This also has a saved query to create a view.\nBudgetsModule: Type: AWS::CloudFormation::Stack Condition: DeployBudgetsModule Properties: Parameters: DestinationBucket: !Ref S3Bucket DestinationBucketARN: !GetAtt S3Bucket.Arn ManagementAccountID: !Ref ManagementAccountID RoleName: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026quot; TemplateURL: \u0026quot;https://aws-well-architected-labs.s3-us-west-2.amazonaws.com/Cost/Labs/300_Optimization_Data_Collection/Budgets.yaml\u0026quot; Test your Lambda Testing your deployment Once you have deployed your modules you will be able to test your Lambda function to get your first set of data in Amazon S3.\nDepending on the module which you would like to test the following Lambda functions should be triggered: Inventory Collector module -\u0026gt; AWS-Organization-Account-Collector Lambda function Trusted Advisor module -\u0026gt; AWS-Organization-Account-Collector Lambda function ECS Chargeback Data module -\u0026gt; AWS-Organization-Account-Collector Lambda function RDS Utilization Data module module -\u0026gt; AWS-Organization-Account-Collector Lambda function AWS Budgets Export module module -\u0026gt; AWS-Organization-Account-Collector Lambda function Cost Explorer Rightsizing Recommendations module -\u0026gt; aws-cost-explorer-rightsizing-recommendations-function Lambda function Compute Optimizer Collector module -\u0026gt; ComputeOptimizer-Trigger-Export Lambda function AWS Organization Data Export module -\u0026gt; Lambda_Organization_Data_Collector Lambda function To test your Lambda function open respective Lambda in AWS Console and click Test Enter an Event name of Test, click Create:\nClick Test\nThe function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and view the output.\nFor Compute Optimizer Collector module processing can take up to 30 mins (15 mins for Compute Optimizer to produce exports requested by lambda, and then another 15 mins for the replication from region buckets to the main bucket)\nGo to the Athena service page You will be able to see your data in the optimization_data Database If your module has a saved query you will be able to see it in the Saved queries section. Otherwise you can query each table directly by clicking on Preview Table button If you have just deployed all resources into your Management Account please see below. Deployed just into Management Account In some cases we have seen customers who have deployed all CloudFormation into just their Management Account have role access issues. If you have this issue then please do the below, if not please ignore.\nTo fix this, all you have to do is remove/comment out the assume role parts of the Lambda code. This will be on different lines in each lambda function.\nOnce this is done you can redeploy.\nIf you would like to make your own modules then go to the next section to learn more on how they are made!\nNow you have your data in AWS Athena you can use this to identify optimization opportunities using Athena Queries or visualizing data in Amazon QuickSight.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_ec2_web_application/","title":"Automated Deployment of EC2 Web Application","tags":[],"description":"","content":"Last Updated: September 2020\nAuthors: Ben Potter, Security Lead, Well-Architected \u0026amp; Rodney Lester, Manager, Well-Architected\nIntroduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach incorporating a number of AWS security best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.\nThis lab will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. The components created include:\nApplication load balancer Auto scaling group of web instances A role attached to the auto-scaled instances allows temporary security credentials to be used Instances use Systems Manager instead of SSH for administration Amazon Aurora serverless database cluster Secrets manager secret for database cluster AWS Key Management Service is used for key management of Aurora database Security groups for load balancer and web instances to restrict network traffic Custom CloudWatch metrics and logs for web instances IAM role for web instances that grants permission to Systems Manager and CloudWatch Instances are configured from the latest Amazon Linux 2 Amazon Machine Image at boot time using user data to install agents and configure services Overview of wordpress stack architecture: An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method.\nThe Application Load Balancer will listen on unencrypted HTTP (port 80), it is a best practice to encrypt data in transit, you can configure a HTTPS listener after completion of this lab.\nAn example amazon-cloudwatch-agent.json file is provided and automatically downloaded by the instances to configure CloudWatch metrics and logs, this requires that you follow the example naming prefix of WebApp1.\nPrerequisites An AWS account that you are able to use for testing. Permissions to create resources in CloudFormation, EC2, VPC, IAM, Elastic Load Balancing, CloudWatch, Aurora RDS, KMS, Secrets Manager, Systems Manager. Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC . Costs Typically less than $20 per month if the account is only used for personal testing or training, and the tear down is not performed:\nEC2 instance default t3.micro X2 (default value) is $0.0208 per hour in us-east-1 region Aurora serverless database default of 2 capacity units is $0.12 per hour in us-east-1 region AWS KMS key for Aurora database is $1.00 per month plus $0.03 per 10,000 requests in us-east-1 region Elastic Load Balancing, Application Load Balancer for Application Load Balancer is $0.0225 per hour in us-east-1 region AWS Secrets Manager secret for database password is $0.40 per month Amazon CloudWatch custom metrics X8 is $2.40 per month per instance in us-east-1 region Amazon CloudWatch logs is $0.50 per GB in us-east-1 region AWS Pricing Steps: Create Web Stack Tear down this lab "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/","title":"Automated Deployment of IAM Groups and Roles","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Fine-grained authorization Automate security best practices Prerequisites An AWS account that you are able to use for testing. Permissions to create resources in IAM. Costs There are no costs for this lab AWS Pricing Steps: AWS CloudFormation to Create Groups, Policies and Roles with MFA Enforced Assume Roles from an IAM user Tear down "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/","title":"300 Level Advanced Labs","tags":[],"description":"","content":"List of labs available Level 300: Multilayered API Security with Cognito and WAF Level 300: Autonomous Monitoring Of Cryptographic Activity With KMS Level 300: Autonomous Patching With EC2 Image Builder And Systems Manager Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response Playbook with Jupyter - AWS IAM Level 300: Incident Response with AWS Console and CLI Level 300: Lambda Cross Account Using Bucket Policy Level 300: Lambda Cross Account IAM Role Assumption Level 300: VPC Flow Logs Analysis Dashboard "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/3_create_alarm/","title":"Create an Alarm","tags":[],"description":"","content":"Now that the right metric has been identified to monitor the dependency, it is time to create an alarm to monitor the metric and send notifications based on thresholds defined. CloudWatch Alarms can be used to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy.\nAn alarm needs to be created that checks the Lambda function invocation every minute to ensure that it has been invoked at least one time, and treats missing data as an indication that the function has not not been invoked, and as such the dependency that is being monitoring has failed. If the alarm is triggered, a notification should be sent to an SNS topic so that someone can be notified and respond, or an automatic remediation activity can be triggered as a result.\nGo to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch , click on Alarms, and then Create alarm\nClick on Select metric\nIn the search bar under All metrics, enter the name of the data read function - WA-Lab-DataReadFunction and press enter\nIn the metric breakdown, select Lambda \u0026gt; By Resource and you will see a list of Lambda metrics available\nCheck the box for the metric Invocations and click Select metric\nOn the Specify metric and conditions page, make the following changes for the Metric:\nChange Statistic from Average to Sum Change Period from 5 minutes to 1 minute Scroll down to the Conditions section and configure it as follows:\nThreshold type - set to Static Whenever invocations is\u0026hellip; - set to Lower than\u0026hellip; - enter 1 since this is the minimum number of invocations that is expected every minute Click on the arrow next to Additional Configuration to expand that section and make the following configuration changes:\nDatapoints to alarm should be 1 out of 1 Missing data treatment should be set to Treat missing data as bad (breaching threshold) since Lambda will not report any metrics if a function was not invoked Click Next to go to the Configure actions page\nUnder Notification, make the following changes:\nAlarm state trigger - select In alarm Select an SNS topic - choose Select an existing SNS topic since a topic was created as part of the CloudFormation stack Send a notification to\u0026hellip; - select WA-Lab-Dependency-Notification and verify that the Email (endpoints) listed below is the same that was specified during stack creation Click Next to go to the Add name and description page\nUnder Name and description, specify the following:\nAlarm name - enter a recognizable name for the alarm such as WA-Lab-Dependency-Alarm Alarm description - this is an optional field that can be left blank for this exercise Click Next to go to the Preview and create page\nClick Create alarm\nOnce the alarm has been created, you will be returned to the Alarms page on the CloudWatch console. In the search bar, enter the name of the alarm that was just created WA-Lab-Dependency-Alarm. The alarm will be listed with a state of Insufficient data. This is because CloudWatch is currently evaluating the underlying metric to determine the current state. In a few minutes, you will see the alarm transition from Insufficient data to OK. Click on the alarm name to go to the alarm details page. Review contents of the page to understand the configuration of the alarm such as metric used, threshold set, evaluation interval, etc. The red line on the graph indicates the threshold that has been set for the alarm. Based on the alarm conditions, the alarm will go into an In alarm state if the metric graph falls below this threshold.\nUnder the History section, you will be able to view all state changes with respect to the alarm. Once the alarm is in an OK state, you will see a State update event under History.\nAn alarm has now been created on a suitable metric to identify if the external service that the workload is dependent on is experiencing outage. Additionally, notifications have been configured to alert relevant stakeholders when the workload outcome is at risk due to failure/unavailability of the external service.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/compute-optimizer-dashboards/","title":"Compute Optimizer Dashboard (COD)","tags":[],"description":"","content":"Last Updated May 2022\nAuthors Iakov Gan, Senior Technical Account Manager, EMEA Yuriy Prykhodko, Senior Technical Account Manager, EMEA Timur Tulyaganov, Principal Technical Account Manager, EMEA Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: cloud-intelligence-dashboards@amazon.com Get Help Ask your questions on re:Post and get answers from our team, other AWS experts, and other customers using the dashboards.\nSubscribe to our YouTube channel to see guides, tutorials, and walkthroughs on all things Cloud Intelligence Dashboards.\nIntroduction AWS Compute Optimizer recommends optimal AWS resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Compute Optimizer Dashboard lets you view cost optimization and risk reduction opportunities for all accounts in your AWS Organizations across all AWS Regions. Out-of-the-box benefits of the COD include (but are not limited to):\nFind over and underutilized resources (EC2, AutoScaling Groups, EBS, Lambda). Get right-sizing recommendations. Identify potential savings across all payer accounts and regions. Track optimization progress over time by AWS Account team or business unit. See also:\nA basics of Compute Optimizer Right Sizing in a lab 200 Rightsizing with Compute Optimizer AWS Compute Optimizer FAQ Demo Dashboard Get more familiar with Dashboard using the live, interactive demo dashboard following this link These dashboards and their content: (a) are for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS content, products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers.\nGoals Create Compute Optimization Dashboard Share the dashboard in your organization Permissions This workshop can be deployed by any user who has permission to access the AWS Compute Optimizer, S3, Athena, Glue and QuickSight.\nCosts A QuickSight Enterprise license starts at $18 per month. Incremental costs associated with AWS Glue, Amazon Athena, and Amazon S3. Estimated total cost for all Dashboards together in a large AWS deployment is $54 per month. Time to Complete Pre-requisites: Should take approximately 15 minutes to complete Installation: Should take approximately 60 minutes to complete Steps: Prerequisites COD Deployment Optional Steps X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/trusted-advisor-dashboards/","title":"Trusted Advisor Organizational (TAO) Dashboard","tags":[],"description":"","content":"Last Updated April 2022\nAuthors Yuriy Prykhodko, Senior Technical Account Manager, EMEA Timur Tulyaganov, Senior Technical Account Manager, EMEA Contributors Oleksandr Moskalenko, Technical Account Manager, EMEA Georgios Rozakis, Technical Account Manager, EMEA Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: cloud-intelligence-dashboards@amazon.com Get Help Ask your questions on re:Post and get answers from our team, other AWS experts, and other customers using the dashboards.\nSubscribe to our YouTube channel to see guides, tutorials, and walkthroughs on all things Cloud Intelligence Dashboards.\nIntroduction Amazon Trusted Advisor helps you optimize your AWS infrastructure, improve security and performance, reduce over all costs, and monitors service limits. Organizational view lets you view Trusted Advisor check for all accounts in your AWS Organizations. The only way to visualize the organizational view is to use the TAO dashboard. The TAO dashboard is a set of visualizations that provide comprehensive details and trends across your entire AWS Organization. Out-of-the-box benefits of the TAO dashboard include (but are not limited to):\nQuickly locate accounts and users that haven\u0026rsquo;t rotated their AWS IAM keys Identify idle and underutilized resources by cost or account See a list of accounts that have reached over 80% of individual service limits Demo Dashboard Get more familiar with TAO Dashboard using the live, interactive demo dashboard below or following this link These dashboards and their content: (a) are for informational purposes only, (b) represents current AWS product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS content, products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers.\nGoals Create TAO dashboard Share the dashboard in your organization Permissions This workshop can be deployed by any user who has permission to access the Trusted Advisor Organizational view, S3, Athena, Glue and QuickSight.\nThe management account in your organization must have a Business or Enterprise support plan.\nCosts A QuickSight Enterprise license starts at $18 per month. Incremental costs associated with AWS Glue, Amazon Athena, and Amazon S3. Estimated total cost for all Dashboards together in a large AWS deployment is $54 per month. Time to Complete Pre-requisites: Should take approximately 15 minutes to complete Installation: Should take approximately 60 minutes to complete Steps: Prerequisites Create \u0026amp; Upload Trusted Advisor Report TAO Dashboard Deployment Optional Steps X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/3_tear_down/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\nRemove Athena before and after tables:\ndrop table costmaster.before drop table costmaster.after Delete the before and after folders from S3.\nDelete s3 folder starting with the name cost-, ensure you select the correct folder.\nAdditional user permissions in SSO (if configured):\nec2:DescribeImages ec2:DescribeVpcs ec2:DescribeSubnets X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 5 - \u0026ldquo;How do you evaluate cost when you select services?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/contributing/03_creatingfork/","title":"Creating a Fork","tags":[],"description":"","content":"You should work on updates and new features on your own fork of the Well-Architected Labs repository , make the edits and then submit a Pull Request to be merged back into production. A local fork will be your local copy of the Well-Architected Labs repository.\nCreating a Fork Go to the production Well-Architected Labs repository On the right side of the page, click on the Fork icon Select your GitHub account to Fork to You will then have a remote repository of /aws-well-architected-labs This is effectively your own version of the labs, stored on GitHub Create a local repository from the Fork You will create a local copy of the Fork to work on, and make the required edits, his allows you to work locally and view the changes.\nCreate a directory on your PC to hold the local repository Change to that directory or navigate to that directory and right click and select Git Bash Here Replace the repository name (username)/aws-well-architected-labs below with your repository name, and clone it: git clone git@github.com:(username)/aws-well-architected-labs.git It will download the repository You now have your local copy setup \u0026amp; have your own GitHub repository to push to Serve Hugo After creating your local Fork verify locally\nNavigate back to the aws-well-architected-labs parent folder\nServe the content locally:\nhugo serve -D Open a browser and navigate to http://localhost:1313/ Refresh your local Fork from the remote See the GitHub docs "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/aws_cost_management/","title":"AWS Cost Management","tags":[],"description":"","content":"These are queries for AWS Services under the AWS Cost Management product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents AWS Marketplace Refund and Credit Detail Reservation Savings AWS Marketplace Query Description This query provides AWS Marketplace subscription costs including subscription product name, associated linked account, and monthly total unblended cost. This query includes tax, however this can be filtered out in the WHERE clause. Please refer to the CUR Query Library Helpers section for assistance.\nPricing Please refer to the AWS Marketplace FAQ .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, CASE line_item_usage_start_date WHEN NULL THEN DATE_FORMAT(DATE_PARSE(CONCAT(SPLIT_PART(\u0026#39;${table_name}\u0026#39;,\u0026#39;_\u0026#39;,5),\u0026#39;01\u0026#39;),\u0026#39;%Y%m%d\u0026#39;),\u0026#39;%Y-%m-01\u0026#39;) ELSE DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-01\u0026#39;) END AS case_line_item_usage_start_time, bill_billing_entity, product_product_name, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND bill_billing_entity = \u0026#39;AWS Marketplace\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, --refers to case_line_item_usage_start_time, bill_billing_entity, product_product_name ORDER BY case_line_item_usage_start_time ASC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Refund and Credit Detail Query Description This query provides a list of refunds and credits issued. Output is grouped by payer, linked account, month, line item type, service, and line item description. This allows for analysis of refunds and credits along any of these grouped dimensions. For example, \u0026ldquo;what refunds or credits were issued to account 111122223333 for service ABC in January 2022?\u0026rdquo; or \u0026ldquo;what was the total refund issued across all accounts for payer 444455556666 with line item description XYZ?\u0026rdquo;\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_TRUNC(\u0026#39;month\u0026#39;,line_item_usage_start_date) as month_line_item_usage_start_date, line_item_line_item_type, CASE WHEN (bill_billing_entity = \u0026#39;AWS Marketplace\u0026#39; AND line_item_line_item_type NOT LIKE \u0026#39;%Discount%\u0026#39;) THEN product_product_name WHEN (product_servicecode = \u0026#39;\u0026#39;) THEN line_item_product_code ELSE product_servicecode END case_service, line_item_line_item_description, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND line_item_unblended_cost \u0026lt; 0 AND line_item_line_item_type \u0026lt;\u0026gt; \u0026#39;SavingsPlanNegation\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_TRUNC(\u0026#39;month\u0026#39;,line_item_usage_start_date), line_item_line_item_type, 5, --refers to case_service line_item_line_item_description ORDER BY month_line_item_usage_start_date ASC, sum_line_item_unblended_cost ASC; Help \u0026amp; Feedback Back to Table of Contents Reservation Savings Query Description This query provides an aggregated report of savings from purchased reservations across multiple services - EC2, Elasticache, OpenSearch (formerly Amazon ElasticSearch), and RDS. This is similar to what can be found in Cost Explorer Reservation Utilization reports, except aggregated across all services offering reservations, allowing for easier organizational reporting on total savings. Output can be used to identify savings per specific reservation ARN, as well as savings per service, savings per linked account, savings per region, and savings per specific instance/node type.\nPricing Please refer to the relevant service reservation pricing page.\nEC2 ElastiCache OpenSearch (formerly Amazon ElasticSearch) Redshift RDS Sample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, line_item_product_code, reservation_reservation_a_r_n, SPLIT_PART(line_item_usage_type,\u0026#39;:\u0026#39;, 2) AS split_line_item_usage_type, SPLIT_PART(reservation_reservation_a_r_n,\u0026#39;:\u0026#39;, 4) AS split_product_region, -- split ARN for region due to product_region inconsistencies SUM(CAST(pricing_public_on_demand_cost AS DECIMAL (16,8))) AS sum_pricing_public_on_demand_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 END) AS sum_case_reservation_effective_cost, SUM(TRY_CAST(pricing_public_on_demand_cost AS DECIMAL(16, 8))) - SUM(CASE WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 END) AS sum_case_reservation_net_savings FROM ${table_name} WHERE ${date_filter} AND line_item_product_code IN (\u0026#39;AmazonEC2\u0026#39;,\u0026#39;AmazonRedshift\u0026#39;,\u0026#39;AmazonRDS\u0026#39;,\u0026#39;AmazonES\u0026#39;,\u0026#39;AmazonElastiCache\u0026#39;) AND line_item_line_item_type IN (\u0026#39;Fee\u0026#39;,\u0026#39;RIFee\u0026#39;,\u0026#39;DiscountedUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_product_code, reservation_reservation_a_r_n, SPLIT_PART(line_item_usage_type,\u0026#39;:\u0026#39;, 2), SPLIT_PART(reservation_reservation_a_r_n,\u0026#39;:\u0026#39;, 4) ORDER BY month_line_item_usage_start_date, line_item_product_code, split_line_item_usage_type; CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/","title":"Level 300: AWS CUR Query Library","tags":[],"description":"","content":"Last Updated June 2022\nFeedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: curquery@amazon.com Introduction Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. The CUR Query Library is a collection of curated SQL queries to get you started with analyzing your Cost and Usage Report (CUR) data. Cost analysis is unique to each business and these queries are intended to be modified to suit your specific needs. The library will be updated periodically as we build new queries and receive feedback from customers.\nCUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com Query Help Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. The CUR Query Library Help section is intended to provide tips and information about navigating the CUR dataset. We will cover beginner topics like getting started with querying the CUR, filtering query results, common query format, links to public documentation, and retrieving product information. We will also cover advanced topics like understanding your AWS Cost Datasets while working with the CUR data.\nCUR Library Query Help Queries Analytics Application Integration AWS Cost Management Compute Container Cost Optimization Customer Engagement Database End User Computing Global Machine Learning Management \u0026amp; Governance Networking \u0026amp; Content Delivery Security Identity \u0026amp; Compliance Storage Prerequisites An AWS Account Completed the AWS Account Setup lab Completed the Cost and Usage Analysis lab Usage in your AWS account Contributing Community contributions are encouraged and welcome. Please follow the Contribution Guide . The goal is to pull together useful CUR queries in to a single library that is open, standardized, and maintained.\nContributors Please refer to the CUR Query Library Contributors section .\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/3_additional_dashboards/","title":"Additional Dashboards","tags":[],"description":"","content":"Introduction In addition to the two foundational Cloud Intelligence Dashboards CUR Dashboards, Cost Intelligence Dashboard and CUDOS, we have service and specific use case dashboards that help you dive deeper and gain additional insights.\nData Transfer Dashboard The Data Transfer Dashboard is an interactive, customizable and accessible QuickSight dashboard to help customers gain insights into their data transfer. It will analyze any data transfer that incurs a cost such as outbound internet and regional data transfer from all services.\nThis dashboard contains data transfer breakdowns with the following visuals:\nAmount and cost by service and region Between regions Internet data transfer, AWS Global Accelerator usage details for estimation Regional Data transfer CloudFront cost and usage analysis Authors Chaitanya Shah, Sr. Technical Account Manager Click here to continue with the Data Transfer Dashboard deployment FAQ The FAQ for this dashboard is here. Introduction The Data Transfer Dashboard is an interactive, customizable and accessible QuickSight dashboard to help customers gain insights into their data transfer. It will analyze any data transfer that incurs a cost such as outbound internet and regional data transfer from all services.\nThis dashboard contains data transfer breakdowns with the following visuals:\nAmount and cost by service and region Between regions Internet data transfer Regional Data transfer Request Template Access Ensure you have requested access to the Cost Intelligence template here. Create Athena Views The data source for the dashboard will be an Athena view of your existing Cost and Usage Report (CUR).\nLogin via SSO in your Cost Optimization account, go into the Athena console:\nCreate the Data Transfer view by modifying the following code, and executing it in Athena or using aws cli:\nData Transfer View NOTE The Athena Views are updated to reflect any additions in the cost and usage report. If you created your dashboard prior to July 26, 2020 you will want to update to the latest views. Create QuickSight Data Sets We will now create the data sets in QuickSight from the Athena views.\nGo to the QuickSight service homepage\nClick on the Datasets and then click on New dataset Click Athena Enter a data source name of DataTransfer_Cost_Dashboard and click Create data source: Select the costmaster database, and the data_transfer_view table, click Edit/Preview data: **NOTE:**If you have large data for data transfer in CUR, we do NOT recommend using SPICE when setting up your data set in QuickSight, you can quickly use up the 10GB/user allocation and start to incur additional charges. Please use your judgment before enabling it. Select SPICE to change your Query mode: Click on region to get the drop down arrow and click on it, then hover over Change data type then select # String\nSelect Save \u0026amp; Publish and then Click on Cancel: Select the data_transfer_view Data Set: Click Schedule refresh: Click Create: Enter a schedule, it needs to be refreshed daily, and click Create: Click Cancel to exit: Click the x in the top corner: Create the Dashboard We will now use the CLI to create the dashboard from the Data Transfer Cost and Usage Analysis Dashboard template, then create an Analysis you can customize and modify in the next step.\nIf you have not requested access, go to this we page to request access to the template: Template Access Edit the following command, replacing AccountID with your account ID, and region with the region you are working in, then using the CLI list the QuickSight datasets and copy the Name and Arn for the dataset: data_transfer_view:\naws quicksight list-data-sets --aws-account-id (AccountID) --region (region) { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:\u0026lt;your account id\u0026gt;:dataset/fc0cf1eb-173b-4aca-93b6-f58784637732\u0026quot;, \u0026quot;DataSetId\u0026quot;: \u0026quot;fc0cf1eb-173b-4aca-93b6-f58784637732\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;data_transfer_view\u0026quot;, \u0026quot;CreatedTime\u0026quot;: \u0026quot;2020-08-09T23:06:41.666000-04:00\u0026quot;, \u0026quot;LastUpdatedTime\u0026quot;: \u0026quot;2020-08-11T23:15:35.438000-04:00\u0026quot;, \u0026quot;ImportMode\u0026quot;: \u0026quot;SPICE\u0026quot; } Get your users Arn by editing the following command, replacing AccountID with your account ID, and region with the region you are working in, then using the CLI run the command:\naws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region) { \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:\u0026lt;your account id\u0026gt;:user/default/\u0026lt;your user\u0026gt;\u0026quot;, \u0026quot;UserName\u0026quot;: \u0026quot;\u0026lt;your user\u0026gt;\u0026quot;, \u0026quot;Email\u0026quot;: \u0026quot;\u0026lt;your user email\u0026gt;\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;ADMIN\u0026quot;, \u0026quot;Active\u0026quot;: true, \u0026quot;PrincipalId\u0026quot;: \u0026quot;\u0026lt;principal id\u0026gt;\u0026quot; } Create a local file create-data-transfer-dashboard.json with the text below, replace the values (Account ID) with your account ID on line 2 and line 25, (User ARN) with your user ARN on line 7, and (DataTransfer view Dataset ID) with your dataset ARN on line 25:\n{ \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;data_transfer_cost_analysis_dashboard_enhanced\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;DataTransfer Cost Analysis Dashboard Enhanced\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;data_transfer_view\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:(Account ID):dataset/(DataTransfer view Dataset ID)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/data-transfer-aga-est-cost-analysis-template-enhanced-v1\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; } To create the dashboard from the template, edit then run the following command, replacing (region) with the region you are working in, and you should receive a 202 response:\naws quicksight create-dashboard --cli-input-json file://create-data-transfer-dashboard.json --region (region) Response: After a few minutes the dashboard will become available in QuickSight under All dashboard, click on the Dashboard name: Follow step 7 if you do not see your dashboard\nEdit and run the following command:\naws quicksight describe-dashboard --aws-account-id (Account ID) --dashboard-id data_transfer_cost_analysis_dashboard --region (region) Correct the listed errors and run the delete-dashboard command followed by the original create-dashboard command:\naws quicksight delete-dashboard --aws-account-id (Account ID) --dashboard-id data_transfer_cost_analysis_dashboard --region (region) NOTE: You have successfully created the analysis from a template. For a detailed description of the dashboard read the FAQ Saving and Sharing your Dashboard in QuickSight Now that you have your dashboard created you can share your dashboard with users or customize your own version of this dashboard\nClick to navigate QuickSight steps Trends Dashboard The Trends Dashboard provides Financial and Technology organizational leaders access to proactive trends, signals, insights and anomalies to understand and analyze their AWS cloud usage.\nClick here to continue with the Trends Dashboard deployment Click to navigate Trends Dashboard workshop NOTE: The Trends Dashboard is provided as an AWS Workshop and not an official Well-Architected lab due to the differences in the data sets and attribute names. All dashboards should be validated before use. X Lab complete! Click here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/3_utilize_organization_data_source/","title":"Utilize Organization Data Source","tags":[],"description":"","content":"Create Glue Crawler We will prepare the organization data source which we will use to join with the CUR.\nGo to the Glue Service page: Click Crawlers from the left menu: Click Add crawler: Enter a crawler name of Org_Glue_Crawler and click Next: Ensure Data stores is the source type, click Next: Click the folder icon to list the S3 folders in your account and find your S3 bucket and find the organisation-data folder and click Next: Create an IAM role with a name of AWS-Organization-Data-Glue-Crawler, click Next: Change the frequency as Custom and put in 0 8 ? * MON *, and click Next: Click on Add database. Enter a database name of your CUR database managementcur, and click Next: Click Finish: Select the crawler OrgGlueCrawler and click Run crawler: Once its run, you should see tables created.\nGo to the Athena service page\nRun the below query, to view your data in Amazon S3. As you can see, we have the account number, the name, when it was created and the current status of that account.\nSELECT * FROM \u0026quot;managementcur\u0026quot;.\u0026quot;organisation_data\u0026quot; limit 10;\rYou have now created your Athena table that will query the organization data in the S3 Bucket.\nJoin with Cost and Usage Report We will be running an example query on how you can connect your CUR to this Organizations data as a one off. In this query you will see the service costs split by account names.\nIn the Athena service page run the below query to join the Organizations data with the CUR table. Make the below changes as needed: Change managementcur if your named your database differently\nmonth = Chosen Month\nyear = Chosen Year\nSELECT line_item_usage_account_id,\rline_item_product_code,\rname,\rsum(line_item_unblended_cost) AS line_item_unblended_cost_cost\rFROM \u0026quot;managementcur\u0026quot;.\u0026quot;cur\u0026quot; cur\rJOIN \u0026quot;managementcur\u0026quot;.\u0026quot;organisation_data\u0026quot;\rON \u0026quot;cur\u0026quot;.line_item_usage_account_id = organisation_data.id\rWHERE month = '10'\rAND year = '2020'\rGROUP BY line_item_usage_account_id, name, line_item_product_code\rlimit 10;\rThe important part of this query is the join. The line_item_usage_account_id from your Cost \u0026amp; Usage Report should match a account_number from the Organizations data. You can now see the account name in your data. Create a View with Cost and Usage Report If you would like to always have your Organizations data connected to your CUR then we can create a view.\nIn the Athena service page run the below query to join the Organizations data with the CUR table as a view.\nCREATE OR REPLACE VIEW org_cur AS\rSELECT *\rFROM (\u0026quot;managementcur\u0026quot;.\u0026quot;cur\u0026quot; cur\rINNER JOIN \u0026quot;managementcur\u0026quot;.\u0026quot;organisation_data\u0026quot;\rON (\u0026quot;cur\u0026quot;.\u0026quot;line_item_usage_account_id\u0026quot; = \u0026quot;organisation_data\u0026quot;.\u0026quot;id\u0026quot;)) Going forward you will now be able to run your queries from this view and have the data connected to your Organizations data. To see a preview where your org data is, which is at the end of the returned data, run the below query.\nSELECT * FROM \u0026quot;managementcur\u0026quot;.\u0026quot;org_cur\u0026quot; limit 10;\rHaving run these queries, you can now see how the Organization data connects to your Cost and Usage Report.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/3_change_clock/","title":"Changing clock type on the Xen based EC2 instance","tags":[],"description":"","content":"We can change the clock type on Xen based EC2 instances to get better performance than the standard Xen clock source. You will notice that the speed for Xen and Nitro/KVM instances will be close to identical after this change.\nCode to change the clock source echo \u0026#34;tsc\u0026#34; \u0026gt; /sys/devices/system/clocksource/clocksource0/current_clocksource This changes the default time clock from xen to tsc (Time Stamp Counter), which is considered the best practice for Xen based EC2 instances.\nUsing SSM to run the clock change document Open the AWS Console (https://console.awa.amazon.com ) In the “Find Services” search bar, type “systems manager” and press enter Under “Instances \u0026amp; Nodes” click on “Run Command” and on the left side of the screen again click on “Run a command” In the search bar under Command Document, select “Owner” from the pulldown and then select “Owned by me”. You should see 2 SSM documents that have been created for you by the CloudFormation Template. Click on the one with “setTSCdocument” in the name. Scroll down and under “Targets”, select the checkbox next to the instance that is labeled “XenTimeInstanceTest” Scroll down and uncheck the box that says “Enable writing to an S3 bucket” and then click the “Run” button at the bottom of the screen. You should see the command running on the node selected. Click Refresh until the command is in the “Success” state You can now go back to Step 2 and re-run the tests. You should see the time for the Xen based EC2 instance has dramatically improved from 110 to 24 seconds. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/3_vpc/","title":"Amazon VPC","tags":[],"description":"","content":"A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled.\n3.1 Investigate Amazon VPC Flow Logs 3.1.1 AWS Management Console The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed.\nOpen the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query. Rejected requests by IP address:\nRejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\u0026quot;REJECT\u0026quot; | stats count(*) as numRejections by srcAddr | sort numRejections desc\nReject requests originating from inside your VPC\nRejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10.: filter action=\u0026quot;REJECT\u0026quot; and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc\nRequests from an IP address\nIf you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \u0026quot;192.0.2.1\u0026quot; | fields @timestamp, interfaceId, dstAddr, dstPort, action\nRequest count from a private IP address by destination address\nIf you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \u0026quot;10.1.1.1\u0026quot; | stats count(*) as numConnections by dstAddr | sort numConnections desc\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/3_analyze_recommendations/","title":"Analyze your Savings Plan recommendations","tags":[],"description":"","content":"We have an understanding of the potential savings available to us, and how we can adjust that based on our business requirements. We also know our usage trends across the business, which will help us with our initial commitment. We will now go deeper to help you understand exactly what Savings Plan commitment is right for you.\nYou can think of a single Savings Plan as a highly flexible group of Reserved Instances (RI\u0026rsquo;s), without the same management overhead. Depending on the discount level and your usage, Savings Plans can pay themselves off very quickly and offer large savings, or pay themselves off over a longer period with less savings. We will look into a Savings Plan to ensure our commitment pays off in the right amount of time and offers the amount of savings we need.\nWe will analyze our Savings Plan to further refine our initial commitment level to purchase, this commitment will be very low risk and high return. Once that is purchased you then re-analyze every couple of weeks or every month and \u0026ldquo;top up\u0026rdquo; your commitment levels. This ensures you maintain high levels of discounts, and you can continually adjust as your business evolves.\nClick on Recommendations and select EC2 Instance Savings Plans, 1-year, No upfront, 30 days: If Payer is selected and you see few or no recommendations, select Linked account to display recommendations to the linked accounts within your AWS Organization.\nScroll down and click Download CSV There is a sample file here if you do not have data:\nSample Savings Plan Add the new column headings O, P and Q. and put in the formulas for each:\nMonthly OnDemand Cost, Cell O2: =G2*730 Monthly Cost after SP, Cell P2: =O2-K2 Fully Paid Day, Cell Q2: =P2*12/O2 Updated Sample Savings Plan The Fully Paid Day is the number of months it takes to pay off the entire 12 month savings plan. It is a combination of the discount level and your utilization level. At your current usage, after this day even if you turn off all your usage you will not lose money and be better off than paying on demand. The sooner the period the lower risk the purchase. We want the lowest risk purchases for our initial commitment, so sort by the Fully Paid Day, in order of smallest to largest, and insert some blank lines before a fully paid day of 9 months:\nUpdated Sample Savings Plan Sort the top group and bottom group by Estimated monthly savings amount, in order of largest to smallest, insert some lines above anything less than $50 in savings:\nUpdated Sample Savings Plan You now have four groups of Savings Plan usage:\nVery low risk, high return Very low risk, low return low to medium risk, high return low to medium risk, low return Combining this information with the previous exercises, your initial purchase will be typically focused on the low risk and high return, and some of the medium risk high return. Add up the hourly commitment for the recommendations that match your business requirements. In this example we have taken all of the very low risk high return, and 40% of the medium risk high return:\nUpdated Sample AnalyzeSP Take this commitment level, apply the findings from the previous exercises (type of Savings Plan and Usage Trend) to make your initial purchase.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/3_attach_iamrole/","title":"Attach CloudWatch IAM role to selected EC2 Instances","tags":[],"description":"","content":" We are now going to attach the IAM Role created on the previous step in one of our EC2 Instances, to do that let\u0026rsquo;s go to the Amazon EC2 Dashboard. From the EC2 Dashboard, scroll down a bit an click on Launch Instance Select Linux 2 AMI (HVM) Select t2.micro (free tier eligible) and click review and launch It is possible to assign the IAM role during EC2 creation by clicking \u0026ldquo;Next: Configure Instance Details\u0026rdquo;. This lab shows the steps to assign it manually.\nClick Launch Select Proceed without a key pair\nAfter the instance is launched and running, select the instance you want to start collecting Memory data and go to Actions on the top bar, select Security \u0026raquo; Modify IAM role. Look for the created IAM role CloudWatchAgentServerRole under the IAM role box, select it and apply. Click on the Instance ID and validate the IAM role CloudWatchAgentServerRole is attached to the instance X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/3_build_run_investigative_playbook/","title":"Build &amp; Run an Investigative Playbook","tags":[],"description":"","content":"The efficiency of issue resolution within an Operations team is directly linked to their tenure and experience. Where an Operator has prior knowledge of a particular issue, they will have a headstart in being able to reach resolution in terms of understanding logs and metrics which were used in previous situations. Whilst this constitutes value to an Operations group, it also represents a single point of failure and a scalability challenge.\nThis is where playbooks become important. Playbooks are a documented set of predefined steps, which are run to identify an issue. The result of each step can be used to either call more steps to run, or alternatively to trigger manual intervention.\nAutomating playbook activities wherever possible, is critical to reducing the time to respond to an incident.\nThe AWS Cloud offers multiple services you can use to build an automated playbook, one which is AWS Systems Manager.\nAWS Systems Manager offers an automation document capability (known within Systems Manager as runbooks ), which allows for the creation of a series of executable steps to orchestrate your investigation and remediation. AWS Systems Manager Automation Documents allow a user to run custom scripts, call AWS service APIs, or even run remote commands on cloud or on-premise compute instances.\nIn this section, you will focus on creating an automated playbook in assisting your investigation, as a Systems Operator.\nActions items in this section: You will build a playbook to gather information about the workload and query the relevant metrics and logs. You will run the automation document to investigate your issue. 3.0 Prepare Automation Document IAM Role The Systems Manager Automation Document you are building will require assumed permissions to run the investigation and remediation steps. You will need to create the IAM role that will assume the permissions to perform the playbook activities. To simplify the deployment process, a CloudFormation template has been provided that you can deploy via the console or AWS CLI. Please choose one of the two following deployment steps:\nClick here for CloudFormation Console deployment step Download the template here. Follow this guide for information on how to deploy the CloudFormation template. Use waopslab-automation-role as the Stack Name, as this is referenced by other stacks later in the lab. Click here for CloudFormation CLI deployment step Note: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\nFrom the Cloud9 terminal change to the appropriate folder as shown: cd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/templates Then run the command listed below: aws cloudformation create-stack --stack-name waopslab-automation-role \\ --capabilities CAPABILITY_NAMED_IAM \\ --template-body file://automation_role.yml Confirm that the stack has installed correctly. You can do this by running the describe-stacks command: aws cloudformation describe-stacks --stack-name waopslab-automation-role Locate the StackStatus and confirm it is set to CREATE_COMPLETE\nOnce you have deployed the CloudFormation stack above, go to the IAM Console.\nOn the side menu, click on Roles and locate the IAM role named AutomationRole.\nTake note of the ARN of the role, as we will need it later in the lab.\n3.1 Building the \u0026ldquo;Gather-Resources\u0026rdquo; Playbook. In preparation for the investigation, you need to know all services and resources associated to the issue. When the email notification is sent, information in the email does not contain any resources information. To gather this necessary information, we will build a playbook to acquire all related resources using our CloudWatch alarm ARN as a reference.\nCodifying your playbook with AWS Systems Manager allows for maximum code reusability. This will reduce overhead in re-writing codes that has identical objectives.\nNote: Follow these step to build and run playbook. Select a guide to deploy using either the AWS console, the AWS CLI or via a CloudFormation template deployment.\nClick here for CloudFormation Console deployment step Download the template here. If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\nFollow this guide for information on how to deploy the CloudFormation template. Use waopslab-playbook-gather-resources as the Stack Name, as this is referenced by other stacks later in the lab. Click here for CloudFormation CLI deployment step Note: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials.\nFrom the Cloud9 terminal, run the command to get into the working script folder cd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/templates Then run the below commands, replacing the \u0026lsquo;AutomationRoleArn\u0026rsquo; with the Arn of AutomationRole you took note in previous step 3.0. aws cloudformation create-stack --stack-name waopslab-playbook-gather-resources \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=AutomationRoleArn \\ --template-body file://playbook_gather_resources.yml Example:\naws cloudformation create-stack --stack-name waopslab-playbook-gather-resources \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=arn:aws:iam::000000000000:role/AutomationRole \\ --template-body file://playbook_gather_resources.yml Note: Please adjust your command-line if you are using profiles within your aws command line as required.\nConfirm that the stack has installed correctly. You can do this by running the describe-stacks command below, locate the StackStatus and confirm it is set to CREATE_COMPLETE. aws cloudformation describe-stacks --stack-name waopslab-playbook-gather-resources Click here for Console step-by-step Go to the AWS Systems Manager console. Click Documents under Shared Resources on the left menu. Then click Create Automation as show in the screen shot below: Enter Playbook-Gather-Resources in the Name field and copy the notes shown below into the Document description field. # What does this **playbook** do? Query the CloudWatch Synthetics Canary and look for all resources related to the application based on it\u0026#39;s Application Tag. This **playbook** takes an input of the CloudWatch Alarm ARN triggered by the canary Note : Application resources must be deployed using CloudFormation and properly tagged accordingly. ## Actions taken in this playbook. 1. Describe CloudWatch Alarm ARN and identify the Canary resource. 2. Describe the Canary resource to gather the value of \u0026#39;Application\u0026#39; tag 3. Gather CloudFormation Stack with the same value of \u0026#39;Application\u0026#39; tag. 4. List all resources in CloudFormation Stack. 5. Parse list of resources into String Output. In the Assume role field, enter the IAM role ARN we created in the previous section 3.0 Prepare Automation Document IAM Role. Expand the Input Parameters section and enter AlarmARN as the Parameter name. Set the type as String and Required as Yes. This will define a Parameter within our playbook, so that the value of the CloudWatch Alarm ARN can be passed into the playbook to run the action. Under Step 1 section specify Gather_Resources_For_Alarm Step name, select aws::executeScript as the Action type.\nUnder Inputs set Python3.6 as the Runtime and specify script_handler as the Handler.\nPaste in below python codes into the Script section.\nimport json import re from datetime import datetime import boto3 import os def arn_deconstruct(arn): arnlist = arn.split(\u0026#34;:\u0026#34;) service=arnlist[2] region=arnlist[3] accountid=arnlist[4] servicetype=arnlist[5] name=arnlist[6] return { \u0026#34;Service\u0026#34;: service, \u0026#34;Region\u0026#34;: region, \u0026#34;AccountId\u0026#34;: accountid, \u0026#34;Type\u0026#34;: servicetype, \u0026#34;Name\u0026#34;: name } def locate_alarm_source(alarm): cwclient = boto3.client(\u0026#39;cloudwatch\u0026#39;, region_name = alarm[\u0026#39;Region\u0026#39;] ) alarm_source = {} alarm_detail = cwclient.describe_alarms(AlarmNames=[alarm[\u0026#39;Name\u0026#39;]]) if len(alarm_detail[\u0026#39;MetricAlarms\u0026#39;]) \u0026gt; 0: metric_alarm = alarm_detail[\u0026#39;MetricAlarms\u0026#39;][0] namespace = metric_alarm[\u0026#39;Namespace\u0026#39;] # Condition if NameSpace is CloudWatch Syntetics if namespace == \u0026#39;CloudWatchSynthetics\u0026#39;: if \u0026#39;Dimensions\u0026#39; in metric_alarm: dimensions = metric_alarm[\u0026#39;Dimensions\u0026#39;] for i in dimensions: if i[\u0026#39;Name\u0026#39;] == \u0026#39;CanaryName\u0026#39;: source_name = i[\u0026#39;Value\u0026#39;] alarm_source[\u0026#39;Type\u0026#39;] = namespace alarm_source[\u0026#39;Name\u0026#39;] = source_name alarm_source[\u0026#39;Region\u0026#39;] = alarm[\u0026#39;Region\u0026#39;] alarm_source[\u0026#39;AccountId\u0026#39;] = alarm[\u0026#39;AccountId\u0026#39;] result = alarm_source return result def locate_canary_endpoint(canaryname,region): result = None synclient = boto3.client(\u0026#39;synthetics\u0026#39;, region_name = region ) res = synclient.get_canary(Name=canaryname) canary = res[\u0026#39;Canary\u0026#39;] if \u0026#39;Tags\u0026#39; in canary: if \u0026#39;TargetEndpoint\u0026#39; in canary[\u0026#39;Tags\u0026#39;]: target_endpoint = canary[\u0026#39;Tags\u0026#39;][\u0026#39;TargetEndpoint\u0026#39;] result = target_endpoint return result def locate_app_tag_value(resource): result = None if resource[\u0026#39;Type\u0026#39;] == \u0026#39;CloudWatchSynthetics\u0026#39;: synclient = boto3.client(\u0026#39;synthetics\u0026#39;, region_name = resource[\u0026#39;Region\u0026#39;] ) res = synclient.get_canary(Name=resource[\u0026#39;Name\u0026#39;]) canary = res[\u0026#39;Canary\u0026#39;] if \u0026#39;Tags\u0026#39; in canary: if \u0026#39;Application\u0026#39; in canary[\u0026#39;Tags\u0026#39;]: apptag_val = canary[\u0026#39;Tags\u0026#39;][\u0026#39;Application\u0026#39;] result = apptag_val return result def locate_app_resources_by_tag(tag,region): result = None # Search CloufFormation Stacks for tag cfnclient = boto3.client(\u0026#39;cloudformation\u0026#39;, region_name = region ) list = cfnclient.list_stacks(StackStatusFilter=[\u0026#39;CREATE_COMPLETE\u0026#39;,\u0026#39;ROLLBACK_COMPLETE\u0026#39;,\u0026#39;UPDATE_COMPLETE\u0026#39;,\u0026#39;UPDATE_ROLLBACK_COMPLETE\u0026#39;,\u0026#39;IMPORT_COMPLETE\u0026#39;,\u0026#39;IMPORT_ROLLBACK_COMPLETE\u0026#39;] ) for stack in list[\u0026#39;StackSummaries\u0026#39;]: app_resources_list = [] stack_name = stack[\u0026#39;StackName\u0026#39;] stack_details = cfnclient.describe_stacks(StackName=stack_name) stack_info = stack_details[\u0026#39;Stacks\u0026#39;][0] if \u0026#39;Tags\u0026#39; in stack_info: for t in stack_info[\u0026#39;Tags\u0026#39;]: if t[\u0026#39;Key\u0026#39;] == \u0026#39;Application\u0026#39; and t[\u0026#39;Value\u0026#39;] == tag: app_stack_name = stack_info[\u0026#39;StackName\u0026#39;] app_resources = cfnclient.describe_stack_resources(StackName=app_stack_name) for resource in app_resources[\u0026#39;StackResources\u0026#39;]: app_resources_list.append( { \u0026#39;PhysicalResourceId\u0026#39; : resource[\u0026#39;PhysicalResourceId\u0026#39;], \u0026#39;Type\u0026#39;: resource[\u0026#39;ResourceType\u0026#39;] } ) result = app_resources_list return result def script_handler(event, context): result = {} arn = event[\u0026#39;CloudWatchAlarmARN\u0026#39;] alarm = arn_deconstruct(arn) # Locate tag from CloudWatch Alarm alarm_source = locate_alarm_source(alarm) # Identify Alarm Source tag_value = locate_app_tag_value(alarm_source) #Identify tag from source if alarm_source[\u0026#39;Type\u0026#39;] == \u0026#39;CloudWatchSynthetics\u0026#39;: endpoint = locate_canary_endpoint(alarm_source[\u0026#39;Name\u0026#39;],alarm_source[\u0026#39;Region\u0026#39;]) result[\u0026#39;CanaryEndpoint\u0026#39;] = endpoint # Locate cloudformation with tag resources = locate_app_resources_by_tag(tag_value,alarm[\u0026#39;Region\u0026#39;]) result[\u0026#39;ApplicationStackResources\u0026#39;] = json.dumps(resources) return result Under Additional inputs specify the input value to the step, passing in the parameter we created previously. To do this, specify below values:\nInputPayload as the Input name CloudWatchAlarmARN: '{{AlarmARN}}' as the Input Value. Under Outputs specify below values:\nResources as Name $.Payload.ApplicationStackResources as Selector String as Type Once your settings match the screenshot below, click on Create Automation\nOnce the automation document is created, you can now give it a test.\nYou can then find the newly created document under the Owned by me tab of the Document section in Systems Manager Console. Click on the playbook called Playbook-Gather-Resources and click on Execute Automation to run your playbook. Paste in the CloudWatch Alarm ARN ( You can find this ARN in the email notification in section 2.1 Observing the alarm being triggered ) and click on Execute to test the playbook. Once the playbook run is completed successfully, click on the Step Id to see the final message and output of the step. You should be able to see this output listing all the resources of the application Copy the Resources list output from the section as highlighted in the screenshot below. This list consist of the all the resources defined in the CloudFormation stack related to our application. These information includes the Elastic Load Balancer, ECS and RDS resource id that we can now use to further our investigation of the underlying issue. You can Paste the output into a temporary location like notepad for now. You will need this value for our next step. 3.2 Building the \u0026ldquo;Investigate-Application-Resources\u0026rdquo; Playbook. In the previous step, you have created a playbook that finds all related AWS resources in the application. In this step you will create a playbook that will interrogate resources, capture recent metrics and logs, to look for insights and better understand the root cause of the issue.\nIn practice, there can be various possibilities of actions that the playbook can take to investigate, depending on the scenario presented by the issue. The purpose of this Lab is to showcase how you can use playbook to aid investigation, rather than advise on a specific action path.\nTherefore, in this lab we will assume an example scenario. The playbook will look at metrics and logs of the ELB, ECS and RDS services in the resource list. The playbook will then highlight the metrics and logs that is considered outside of normal operational threshold.\nPlease follow the below instructions to build this playbook:\nNote: We will deploy this playbook via CloudFormation template to simplify deployment. Please follow the steps below to deploy the CloudFormation template via CLI / or Console.\nClick here for CloudFormation Console deployment step Download the template here. If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\nPlease follow this guide for information on how to deploy the CloudFormation template. Use waopslab-playbook-investigate-resources as the Stack Name, as this is referenced by other stacks later in the lab. Click here for CloudFormation CLI deployment step From the Cloud9 terminal, change to the required folder as shown: cd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/templates Run the command below, replacing the \u0026lsquo;AutomationRoleArn\u0026rsquo; with the Arn of AutomationRole you took note in previous step 3.0 Prepare Automation Document IAM Role. aws cloudformation create-stack --stack-name waopslab-playbook-investigate-resources \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=AutomationRoleArn \\ --template-body file://playbook_investigate_application_resources.yml Example:\naws cloudformation create-stack --stack-name waopslab-playbook-investigate-resources \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=arn:aws:iam::000000000000:role/xxxx-playbook-role \\ --template-body file://playbook_investigate_application_resources.yml Confirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows: aws cloudformation describe-stacks --stack-name waopslab-playbook-investigate-resources Locate the StackStatus and confirm it is set to CREATE_COMPLETE When the document is created, you can go ahead and run a quick test.\nYou can find the newly created document under the Owned by me tab of the Document resource in the Systems Manager console.\nClick on the playbook called Playbook-Investigate-Application-Resources and click on Execute Automation to run our playbook.\nPaste in the resources list you took note from the output of the previous playbook ( refer to section 3.1 Building the \u0026ldquo;Gather-Resources\u0026rdquo; Playbook ) under Resources and click on Execute\nUnder Executed Steps you should be able to see each of the step the playbook. If you view the content of the document you will be able to see the code and find out what each step does.\nFor simplicity, we have created a list of output and description for each step. Expand the list below to view.\nOutput list Step Name Description Output list Gather_ELB_Statistics Go through the resource list and locate the ELB. Query data from the ELB CloudWatch metrics, looking at metrics from the last 60 minutes. TargetResponseTime (Average) HTTPCode_Target_2XX_Count (Sum) HTTPCode_Target_3XX_Count (Sum) HTTPCode_Target_4XX_Count (Sum) HTTPCode_Target_5XX_Count (Sum) TargetConnectionErrorCount (Sum) UnHealthyHostCount (Average) ActiveConnectionCount (Sum) HTTPCode_ELB_3XX_Count (Sum) HTTPCode_ELB_4XX_Count (Sum) HTTPCode_ELB_5XX_Count (Sum) HTTPCode_ELB_500_Count (Sum) HTTPCode_ELB_502_Count (Sum) HTTPCode_ELB_503_Count (Sum) HTTPCode_ELB_504_Count (Sum) Gather_RDS_Statistics Go through resource list and locate the RDS resource. Query data from the RDS CloudWatch metrics, looking at metrics from the last 60 minutes. BinLogDiskUsage (Sum) BinLogDiskUsage (Sum) BurstBalance (Average) CPUUtilization (Average) CPUCreditUsage (Sum) CPUCreditBalance (Maximum) DatabaseConnections (Sum) DiskQueueDepth (Maximum) FailedSQLServerAgentJobsCount (Average) FreeableMemory (Maximum) MaximumUsedTransactionIDs (Maximum) NetworkReceiveThroughput (Average) OldestReplicationSlotLag (Average) ReadIOPS (Average) ReadLatency (Average) ReadThroughput (Maximum) ReplicaLag (Average) ReplicationSlotDiskUsage (Maximum) SwapUsage (Maximum) TransactionLogsDiskUsage (Maximum) TransactionLogsGeneration (Average) ReplicationSlotDiskUsage (Maximum) WriteIOPS (Average) WriteLatency (Average) WriteThroughput (Average) Gather_ECS_Statistics Go through the resource list and locate the ECS resource. Query data from the ECS CloudWatch metrics, looking at metrics from the last 6 minutes. CPUUtilization (Maximum) MemoryUtilization (Maximum) Gather_ECS_Error_Logs Go through the resource list and locate the ECS Service. Search in CloudWatch logs for any Error occurrence. Gather_ECS_Config Go through the resource list and locate the ECS resource. Describe the ECS service configuration. Gather_RDS_Config Go through the resource list and locate the RDS resource. Describe RDS Instance Config \u0026amp; Parameters. Inspect_Playbook_Results Go through the output of above steps, inspect results and check if it is above the threshold. TargetResponseTime = 5 (ELB) TargetConnectionErrorCount= 0 (ELB) UnHealthyHostCount = 0 (ELB) ELB5XXCount = 0 (ELB) ELB500Count = 0 (ELB) ELB502Count = 0 (ELB) ELB503Count = 0 (ELB) ELB504Count = 0 (ELB) Target4XXCount = 0 (ELB) Target5XXCount = 0 (ELB) CPUUtilization = 80 (ECS) Wait until all steps are completed successfully.\n3.3 Building the \u0026ldquo;Investigate-Application-From-Alarm\u0026rdquo; Playbook. So far we have 2 separate playbooks. The first playbook gathers the list of resources associated with the application. The second playbook queries the relevant resources and investigates the appropriate logs and metrics.\nIn this step we will automate our playbooks further by creating a parent playbook that orchestrates the 2 Investigative playbooks. We will add another step to send notification to our Developers and System Owners.\nFollow the instructions below to build the parent Playbook.\nNote: Select a step-by-step guide below to build the parent playbook using either the AWS console a CloudFormation template.\nClick here for CloudFormation Console deployment step Download the template here. If you decide to deploy the stack from the console, follow these steps:\nPlease follow this guide for information on how to deploy the CloudFormation template. Use waopslab-playbook-investigate-application as the Stack Name, as this is referenced by other stacks later in the lab. In the parameter input screen, under PlaybookIAMRole enter ARN of playbook IAM role (defined in previous step), under NotificationEmail enter your designated email for playbook notification Click here for CloudFormation CLI deployment step In the Cloud9 terminal go to the templates folder using the following command.\ncd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/templates Then run below command :\naws cloudformation create-stack --stack-name waopslab-playbook-investigate-application \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=\u0026lt;ARN of **playbook** IAM role (defined in previous step)\u0026gt; \\ --template-body file://playbook_investigate_application.yml Example:\naws cloudformation create-stack --stack-name waopslab-playbook-investigate-application \\ --parameters ParameterKey=PlaybookIAMRole,ParameterValue=arn:aws:iam::000000000000:role/xxxx-playbook-role \\ --template-body file://playbook_investigate_application.yml Note: Please adjust your command-line if you are using profiles within your aws command line as required.\nConfirm that the stack has installed correctly. You can do this by running the describe-stacks command as follows:\naws cloudformation describe-stacks --stack-name waopslab-playbook-investigate-application Locate the StackStatus and confirm it is set to CREATE_COMPLETE\nClick here for Console step-by-step guide From the AWS Systems Manager console, click on documents as shown below. Once you are there, click on Create Automation Next, enter in Playbook-Investigate-Application-From-Alarm in the Name and paste in the notes shown below into the Description box. This provides a description of the playbook. Systems Manager supports putting in notes as markdown, so feel free to format as required. # What is does this **playbook** do? This **playbook** will run **Playbook-Gather-Resources** to gather Application resources monitored by Canary. Then subsequently run **Playbook-Investigate-Application-Resources** to Investigate the resources for issues. Outputs of the investigation will be sent to SNS Topic Subscriber Under Assume role field, enter in the ARN of the IAM role we created in the previous step.\nUnder Input Parameters field, enter AlarmARN as the Parameter name. Set the type as String and Required as Yes. This will define a Parameter into our playbook, which allows the value of the CloudWatch Alarm to be passed to the main step that will run the action.\nAdd another parameter by clicking on the Add a parameter link. Enter SNSTopicARN as the Parameter name. Set the type as String and Required as Yes. This will define another Parameter into our playbook, so that we can send notification to the Owner and Developer.\nClick Add Step and create the first step of aws:executeAutomation Action type with StepName PlaybookGatherAppResourcesCanaryCloudWatchAlarm\nSpecify Playbook-Gather-Resources as the Document name under Inputs and under Additional inputs specify RuntimeParameters with {\u0026quot;AlarmARN\u0026quot;:'{{AlarmARN}}'} as it\u0026rsquo;s value (refer to screenshot below). This step we will be run the Gather-Resources playbook which we created previously.\nOnce this step is defined, add another step by clicking on Add Step at the bottom of the section.\nFor this second step, specify the Step name as PlaybookInvestigateAppResourcesELBECSRDS and an action type of aws:executeAutomation.\nSpecify Playbook-Investigate-Application-Resources as the Document name and RuntimeParameters as Resources: '{{PlaybookGatherAppResourcesCanaryCloudWatchAlarm.Output}}' This will take the output of the first step and pass to the second playbook to run the investigation of associated resources.\nFor the last step, take the output investigation from the second step and send that to the SNS topic where our owner, developers and admin are subscribed.\nSpecify the Step name as AWSPublishSNSNotification and the action type as aws:executeAutomation.\nSpecify AWS-PublishSNSNotification as the Document name and RuntimeParameters as shown below. This will take the output of the second step which contains summary data of the investigation and AWS-PublishSNSNotification which will send an email to the SNS we specified in the parameters.\nTopicArn: \u0026#39;{{SNSTopicARN}}\u0026#39; Message: \u0026#39;{{ PlaybookInvestigateAppResourcesELBECSRDS.Output }}\u0026#39; Our playbook will run investigative tasks and send the result to an SNS topic where our Systems administrator / engineer will subscribe to. To do this we will need to create an SNS topic that our playbook will send notification to. Please follow the instructions specified in this link and create a Standard SNS topic and name it PlaybookNotificationSNSTopic\nOnce you\u0026rsquo;ve created the topic, go ahead and subscribe your an email using this instruction here 3.4 Executing investigation Playbook. You can now run the playbook to discover the result of the investigation.\nGo to the Output section of the deployed CloudFormation stack walab-ops-sample-application and take note of below output values.\nGo to the Systems Manager Automation document we just created in the previous step, Playbook-Investigate-Application-From-Alarm.\nAnd then run the playbook passing the ARN as the AlarmARN input value, along with the SNSTopicArn.\nYou can get the AlarmARN from the email that you received from CloudWatch Alarm as described in step 3.1 Building the \u0026ldquo;Gather-Resources\u0026rdquo; Playbook. in this lab. To get the value for SNSTopicArn, go to the CloudFormation console output of walab-ops-sample-application stack and copy, paste the value of OutputSystemEventTopicArn When the playbook completed, an email will be send to you, which contains a summary of the investigation completed by the playbook as shown. Copy and paste the message section and use a json linter tool such as jsonlint.com to give better structure for visibility. The result from the playbook investigation might vary slightly, but the overall findings should be similar to the below screenshot. From the report being generated you should see a large number of ELB504Count error and a high TargetResponseTime from the Load balancer. This explains the delay we are seeing from our canary alarm.\nIf you then look at the ECS summary, you will notice that there is only 1 ECS TaskRunningCount, with a relatively high CPUUtilization average. The script calculates the average of maximum value on the ECS service in the last 6 minutes window. If you do not see CPUUtilization value in the json, you can confirm this by going to the ECS service console and click on the Metrics tab.\nTherefore, it is likely that the immediate cause of the latency is resource constrained at the application API level running in ECS. Ideally, if we can increase the number of tasks in the ECS service, the application should be able to release some of the CPU Utilization constraints.\nWith all of these information provided by our playbook findings, we should be able to determine what is the next course of action to attempt remediation to the issue.\nThis concludes Section 3 of this lab, click on the link below to move on to the next section to build the remediation runbook.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/3_cloudfront_waf/","title":"CloudFront with WAF Protection","tags":[],"description":"","content":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying.\nWalkthrough CloudFront with WAF Protection "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/3_config_cloudfront/","title":"Configure Amazon CloudFront","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created.\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, click Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In the Origin Domain Name field Select the S3 bucket you created previously. In Restrict Bucket Access click the Yes radio then click Create a New Identity. Click the Yes, Update Bucket Policy Button. Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html Click Create Distribution. To return to the main CloudFront page click Distributions from the left navigation menu. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation.\nYou now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/3_config_cloudfront/","title":"Configure Amazon CloudFront","tags":[],"description":"","content":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created.\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF.\nFor more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/3_configure_and_check_cloudtrail/","title":"Configure CloudTrail ","tags":[],"description":"","content":"We will now focus on the creation and configuration of the CloudTrail service. This represents the source of record for all API calls generated within our architecture which we will apply filters to later. Note in the architecture below how CloudTrail integrates with the other AWS services we will deploy:\nClick here for CloudFormation command-line deployment steps Command Line: 3.1. Command Line Deployment Firstly, download the logging template from here. To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials. When your environment is ready, run the following command, taking note of the points below.\naws cloudformation create-stack --stack-name pattern1-logging \\ --template-body file://pattern1-logging.yml \\ --parameters ParameterKey=AppECSTaskRoleArn,ParameterValue=\u0026#34;\u0026lt;ECS Task Role ARN\u0026gt;\u0026#34; ParameterKey=EmailAddress,ParameterValue=\u0026lt; Email Address \u0026gt; \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-2 Note : For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. For \u0026lt; ECS Task Role ARN \u0026gt;, use the ECS Task Role Arn value you took note of from section 2.3.3 for AppECSTaskRoleArn parameter. Use the email address you would like to use to be notified with under EmailAddress parameter. Click here for CloudFormation console deployment steps Console: 3.1. CloudFormation Console Deployment Firstly, download the logging template from here. To deploy the template from the console, please follow this guide for information on how to deploy the cloudformation template, noting the following points before starting your deployment:\nUse pattern1-logging as the Stack Name. Use the ECS Task Role Arn value you took note from section 2.3 for AppECSTaskRoleArn parameter. Use email address you would like to use to be notified with under EmailAddress parameter. Note: Dont forget to acknowledge the capabilities checkbox at the bottom of the screen.\nClick here for manual console deployment Manual Console Deployment 3.1. Create a Trail in CloudTrail Console To create a trail for use within this lab, complete the following steps:\n3.1.1. Navigate to CloudTrail within the console, then click on Create trail as shown here:\n3.1.2. Enter pattern1-logging-trail as the Trail name.\n3.1.3. Select Create new S3 bucket and enter a name for your logging s3 bucket.\nNote that the name needs to be globally unique, so you can use your accountid or uuid to keep it unique for you.\n3.1.4. Enter the remainder of the settings as per the following example:\n3.1.5. Complete the following configuration choices:\nTick on Enabled under CloudWatch Logs. Select New on the Log group radio button, and enter your log group name as pattern1-logging-loggroup Select New on IAM Role, and enter your role name as CloudTrailRoleForCloudWatchLogs_pattern1-logging Your configuration should match the screenshot below:\nWhen you are complete, click Next.\n3.1.6. On the next screen, complete the following configuration choices:\nSelect management events Select read write API Ensure that exclude AWS KMS event is NOT selected Check your selection against the following screenshot and then click Next.\n3.1.7. Review the settings and click Create Trail\n3.2. Confirm Your CloudWatch Log Group Is Operational. Now that your CloudWatch configuration is completed, we need to confirm that the log group is operational.\nFollow the steps below to confirm the state of the Log Group:\n3.2.1. Navigate to CloudWatch in your console and click on Log Groups on the side menu.\n3.2.2. Locate the pattern1-logging-loggroup you created before and click on the the log group as show:\n3.2.3. Click on the available log stream, and confirm that you are seeing logs being generated.\nIf you have completed the configuration correctly, you should see an ongoing record of all the API calls within your account as show here:\nIn the next section, we are going to filter out the Events which matter to us. In doing this we will be able to create an appropriate Alarm\nEND OF SECTION 3\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/3_cfn_params/","title":"Configure Deployed Resources using Parameters","tags":[],"description":"","content":"In this task, you will gain experience changing CloudFormation stack parameters and updating your CloudFormation stack\nYour objective is to deploy additional resources used by the VPC to enable connection to the internet 3.1 Update Parameters Go to the AWS CloudFormation console (if not already there) Click on Stacks Click on the CloudFormationLab stack Click Update Leave Use current template selected. You have not yet changed the template Click Next On the Specify stack details screen you now have the opportunity to change the Parameters Change PublicEnabledParam to true Click Next Click Next again, until you arrive at the Review CloudFormationLab screen Scroll down to Change set preview and note several resources are being added At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack It takes about a minute for the stack update to finish and the stack status is UPDATE_COMPLETE 3.2 Understanding the deployment You did not change any contents of the the CloudFormation Template Changing only one parameter, you re-deployed the stack which resulted in additional resources deployed Go to the AWS CloudFormation console (if not already there) Click the Resources tab for the CloudFormationLab stack. The listing now shows the VPC as before, plus additional resources required to enable us to deploy resources into the VPC that have access to the internet Click through on several of the Physical ID links and explore these resources The current deployment is now represented by this architecture diagram: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/3_cur_analysis/","title":"Cost and Usage analysis","tags":[],"description":"","content":"We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back.\nYou may need to change the database and table name depending on how you configured your CUR. Replace costmaster.workshop_c_u_r with your database and table name.\nClick here to copy and paste all the examples into a text editor to replace the database and table name quickly. SELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; SELECT distinct \u0026quot;line_item_line_item_description\u0026quot; from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; SELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_type\u0026quot; like '%Usage%' LIMIT 10; SELECT distinct bill_billing_period_start_date FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; SELECT \u0026quot;line_item_usage_account_id\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_usage_account_id\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%BoxUsage%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) \u0026gt;0 GROUP BY \u0026quot;resource_tags_user_cost_center\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20 SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) = 0 GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20 SELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as Usage, \u0026quot;line_item_unblended_rate\u0026quot;, sum(\u0026quot;line_item_unblended_cost\u0026quot;) as Cost, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, sum(\u0026quot;pricing_public_on_demand_cost\u0026quot;) as PublicCost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_Type\u0026quot; like '%DiscountedUsage%' GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot; LIMIT 20 SELECT \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_usage_type\u0026quot; like '%t2.%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20 SELECT \u0026quot;line_item_usage_type\u0026quot;, round(sum(\u0026quot;line_item_usage_amount\u0026quot;),2) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost, round(avg(\u0026quot;line_item_unblended_cost\u0026quot;/\u0026quot;line_item_usage_amount\u0026quot;),4) as hourly_rate from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%Usage%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20 SELECT bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(reservation_reservation_a_r_n) \u0026gt; 0 and reservation_unused_quantity \u0026gt; 0 ORDER BY bill_billing_period_start_date, reservation_unused_recurring_fee desc LIMIT 20 For each of the queries below, copy and paste each query into the query window, click Run query and view the results. What data is available in the CUR file? We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns.\nExecute each of the statements below, then spend a minute to thoroughly read through the results and observe the data returned.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nWhat are all the columns and data are in the CUR table?\nSELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; What are all the different values in a specific column? (the column we use is line_item_line_item_description)\nSELECT distinct \u0026quot;line_item_line_item_description\u0026quot; from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; 3 What are all the columns from the CUR, where a specific value is in the column (here the column line_item_line_item_type contains the word Usage, note the capital \u0026lsquo;U\u0026rsquo;):\nSELECT * from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_type\u0026quot; like '%Usage%' LIMIT 10; What billing periods are available?\nSELECT distinct bill_billing_period_start_date FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; LIMIT 10; Top Costs To efficiently optimize, it is useful to view the top costs in different categories, such as service, description or tags. Here are the most useful categories to get top costs by.\nTop10 Costs by AccountID:\nSELECT \u0026quot;line_item_usage_account_id\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_usage_account_id\u0026quot; ORDER BY cost desc LIMIT 10; Top10 Costs by Product:\nSELECT \u0026quot;line_item_product_code\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot; ORDER BY cost desc LIMIT 10; Top10 Costs by Line Item Description\nSELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; Top EC2 Costs\nSELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; Top EC2 OnDemand Costs\nSELECT \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%BoxUsage%' GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 10; Tagging and Cost Attribution Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency.\nThis will only work if you have tags enabled in your billing files, and they are the same as the examples here - resource_tags_user_cost_center. You may need to change the examples below to match your tags.\nTop 20 Costs by line item description and CostCenter Tag\nSELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) \u0026gt;0 GROUP BY \u0026quot;resource_tags_user_cost_center\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20 Top 20 costs by line item description, without a CostCenter Tag\nSELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, round(sum(line_item_unblended_cost),2) as cost FROM \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;resource_tags_user_cost_center\u0026quot;) = 0 GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot; ORDER BY cost desc LIMIT 20 Savings Plans, Reserved Instance, On Demand and Spot Usage To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Savings Plans and Reserved Instances, by finding top On Demand costs. It also identifies who is successful with pricing models by (Top users of spot).\nYou will need specific usage in your account that matches the instance types below, for this to work correctly.\nWho used Savings Plan Identify which usage was covered by a savings plan.\nSELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as Usage, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, sum(\u0026quot;pricing_public_on_demand_cost\u0026quot;) as PublicCost, savings_plan_savings_plan_rate, sum(savings_plan_savings_plan_effective_cost) as SavingsPlanCost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_Type\u0026quot; like 'SavingsPlanCoveredUsage' GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;savings_plan_savings_plan_rate\u0026quot; LIMIT 20 Unused Savings Plan For each savings plan, look at the total commitment and total usage each month.\nSELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;, sum(savings_plan_savings_plan_effective_cost) as SavingsPlanUsage, sum(\u0026quot;savings_plan_recurring_commitment_for_billing_period\u0026quot;) as SavingsPlanCommit from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot;) \u0026gt; 0 GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; order by bill_billing_period_start_date desc, SavingsPlanCommit desc, SavingsPlanUsage LIMIT 10 Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization.\nSELECT \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as Usage, \u0026quot;line_item_unblended_rate\u0026quot;, sum(\u0026quot;line_item_unblended_cost\u0026quot;) as Cost, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, sum(\u0026quot;pricing_public_on_demand_cost\u0026quot;) as PublicCost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_line_item_Type\u0026quot; like '%DiscountedUsage%' GROUP BY \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;reservation_reservation_a_r_n\u0026quot;, \u0026quot;line_item_product_code\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot; LIMIT 20 Specific Instance family usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances.\nSELECT \u0026quot;line_item_usage_type\u0026quot;, sum(\u0026quot;line_item_usage_amount\u0026quot;) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_usage_type\u0026quot; like '%t2.%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20 Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot).\nSELECT \u0026quot;line_item_usage_type\u0026quot;, round(sum(\u0026quot;line_item_usage_amount\u0026quot;),2) as usage, round(sum(\u0026quot;line_item_unblended_cost\u0026quot;),2) as cost, round(avg(\u0026quot;line_item_unblended_cost\u0026quot;/\u0026quot;line_item_usage_amount\u0026quot;),4) as hourly_rate from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE \u0026quot;line_item_product_code\u0026quot; like '%AmazonEC2%' and \u0026quot;line_item_usage_type\u0026quot; like '%Usage%' GROUP BY \u0026quot;line_item_usage_type\u0026quot; ORDER BY \u0026quot;line_item_usage_type\u0026quot; LIMIT 20 Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways:\nSee where you have spare RI\u0026rsquo;s and modify instances to match, so they will use the RIs\nConvert your existing RI\u0026rsquo;s if possible\nSELECT bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from \u0026quot;costmaster\u0026quot;.\u0026quot;workshop_c_u_r\u0026quot; WHERE length(reservation_reservation_a_r_n) \u0026gt; 0 and reservation_unused_quantity \u0026gt; 0 ORDER BY bill_billing_period_start_date, reservation_unused_recurring_fee desc LIMIT 20 You have now setup and completed basic analysis of a Cost and Usage Report (CUR).\nVisit the Well-Architected Cloud Intelligence Dashboards for ways to create a CUR QuickSight Dashboard using a ready to customize template Visit the Well-Architected CUR Query Library for queries to help you get started with analyzing your spend X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/3_iam_policy_and_role/","title":"Create an IAM policy and role for Lambda function","tags":[],"description":"","content":"This step is used to create an IAM policy and a role that allows Lambda function to perform Athena CUR query and deliver processed CUR report via SES.\nLog into IAM console, click on Policies and click on Create Policy: Click on the JSON tab, modify the following policy, replacing the your-cur-query-results-bucket string. Make sure you add \u0026ldquo;*\u0026rdquo; at the end of the bucket name so the whole bucket is writable:\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:PutObject\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::your-cur-query-results-bucket*\u0026quot; ] }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;athena:List*\u0026quot;, \u0026quot;athena:*QueryExecution\u0026quot;, \u0026quot;athena:Get*\u0026quot;, \u0026quot;athena:BatchGet*\u0026quot;, \u0026quot;glue:Get*\u0026quot;, \u0026quot;glue:BatchGet*\u0026quot;, \u0026quot;s3:Get*\u0026quot;, \u0026quot;s3:List*\u0026quot;, \u0026quot;SES:SendRawEmail\u0026quot;, \u0026quot;SES:SendEmail\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Copy the policy to JSON edit frame, ensure the bucket name has been changed, click Review policy: Configure the name Lambda_Auto_CUR_Delivery_Access, and click Create policy. Click on Roles, click Create Role: Choose Lambda as the service that will use this role, click Next Permissions: At Attach permissions policies page, search and choose Lambda_Auto_CUR_Delivery_Access policy created in the previous step. Click Next:Tags, click Next:Review. At Review page, configure a name Lambda_Auto_CUR_Delivery_Role, click Create role. This role will be used for lambda function execution. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/3_ec2_restrict_family/","title":"Create an IAM Policy to restrict EC2 usage by family","tags":[],"description":"","content":"AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase Savings Plan or Reserved Instance utilization, by ensuring these accounts will consume any available commitment discounts.\nWe will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.\nCreate the Instance family restrictive IAM Policy Log on to the console as your regular user with the required permissions, Go to the IAM service page: Select Policies from the left menu: Click Create Policy: Click on the JSON tab: Copy and paste the policy into the console: IAM Policy { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;ForAllValues:StringLike\u0026quot;: { \u0026quot;ec2:InstanceType\u0026quot;: [ \u0026quot;t3.*\u0026quot;, \u0026quot;a1.*\u0026quot;, \u0026quot;m5.*\u0026quot; ] } } } ] } Click Review policy: Enter the details:\nName: EC2_FamilyRestrict Description: Restrict to t3, a1 and m5 families Click on Create Policy: You have successfully created an IAM policy to restrict usage by Instance Family.\nApply the policy to your test group Click on Groups from the left menu: Click on the CostTest group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict: Click on Detach: Click on Attach Policy: Click on Policy Type, then click Customer Managed: Select the checkbox next to Ec2_FamilyRestrict, and click Attach Policy: You have successfully attached the policy to the CostTest group.\nLog out from the console\nVerify the policy is in effect Logon to the console as the TestUser1 user, go to the EC2 Service dashboard: Try to launch an instance by clicking Launch Instance, select Launch Instance: Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch: Make note of the security group created, click Launch: Select Proceed without a key pair, and click I acknowledge that I will not be able to\u0026hellip;, then click Launch Instances: You will receive an error, notice the failed step was Initiating launches. Click Back to Review Screen: Click Edit instance type: We will select an instance type we can launch (t3, a1 or m5) select t3.micro, and click Review and Launch: Select Yes, I want to continue with this instance type (t3.micro), click Next: Click Launch: Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances: You will receive a success message. Click on the Instance ID and terminate the instance as above: Log out of the console as TestUser1.\nYou have successfully implemented an IAM policy that restricts all EC2 actions to T3, A1 and M5 instance types.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/3_budget_spcoverage/","title":"Create and implement an AWS Budget for EC2 Savings Plan coverage","tags":[],"description":"","content":"We will create a monthly savings plan coverage budget, which will notify if the coverage of Savings Plan for EC2 is below the specified amount.\nYou should not set an arbitrary limit for the alarm, (i.e. alarm if coverage is less than 80%) instead select your current level of coverage - so if coverage reduces, you can act and increase coverage if required.\nFrom the Budgets dashboard in the console, click Create budget: Select Savings Plans budget, and click Next: Create a cost budget, enter the following details:\nPeriod: Monthly Monitor my spend against: Coverage of Savings Plans Coverage threshold: 90% Budget name: SP_Coverage Leave all other fields as defaults Click on the Next button to continue: NOTE: NEVER create a utilization budget, unless you are doing it for a single and specific discount rate by using filters. For example you want to track the utilization of m5.large Linux discount. A utilization budget across different discounts will most likely lead to confusion and unnecessary work. For the budget alert settings, input your email address in the Email recipients field and click on Next: Review the configuration, and click the Create budget button: You have created an Savings Plans Coverage budget: You will receive an email similar to below within a few minutes: You have created a Savings Plan budget. Use this type of budget to notify you if a change in a workload has reduced coverage, a Savings Plan has expired, or additional usage has been created and a new Savings Plan purchase may be required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/3_user_role/","title":"Create and Test User Role","tags":[],"description":"","content":"3.1 Create User Role While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1.\nVerify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! In the navigation pane, click Roles and then click Create role. Click Another AWS account, then enter your account ID that you have been using for this lab and tick Require MFA, then click Next: Permissions. In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions. In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags. For this lab we will not use IAM tags, click Next: Review. Enter the Role name of app1-user-region-restricted-services for the role and click Create role. The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps. 3.2 Test User Role Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions.\nIn the console, click your role\u0026rsquo;s Display Name on the right side of the navigation bar. Click Back to your previous username. You are now back to using your original IAM user. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. Select a different color to before, otherwise it will overwrite that profile in your browser. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. Congratulations! You have now learnt about IAM permission boundaries and have one working! "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/3_create_alb_with_waf/","title":"Create Application Load Balancer with WAF integration","tags":[],"description":"","content":"Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test.\n3.1 Create Application Load Balancer Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb. Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successful message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing. 3.2 Configure Application Load Balancer with WAF Open the AWS WAF console at https://console.aws.amazon.com/wafv2/home?region=global#/webacls . In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser. "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/3_athena_queries/","title":"Create Athena Saved Queries to Write new Data","tags":[],"description":"","content":"Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month.\nYou must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code):\nThe queries MUST start with: create_linked_ and delete_linked_ otherwise you\u0026rsquo;ll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),\u0026rsquo;-01%\u0026rsquo;) which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1, as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_folder-name, the following sample code is the accompanying query for the previous query above:\nCREATE TABLE (database).temp_table WITH ( format = \u0026#39;Parquet\u0026#39;, parquet_compression = \u0026#39;GZIP\u0026#39;, external_location = \u0026#39;s3://(bucket)/(folder)/subfolder\u0026#39;) AS SELECT * FROM \u0026#34;(database)\u0026#34;.\u0026#34;(table)\u0026#34; WHERE line_item_usage_account_id = \u0026#39;(some value)\u0026#39; AND (year=CAST(year(current_date- INTERVAL \u0026#39;__interval__\u0026#39; MONTH) AS VARCHAR)) AND month=CAST(month(current_date- INTERVAL \u0026#39;__interval__\u0026#39; MONTH) AS VARCHAR) 2 - Create the accompanying delete statement named delete_linked_folder-name to delete the temporary table:\ndrop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/3_create_bucket_policy/","title":"Create bucket policy for the S3 bucket in account 2","tags":[],"description":"","content":" In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Click on the name of the bucket you will use for this workshop\nGo to the Permissions tab\nClick Bucket Policy\nEnter the following JSON policy\nReplace account1 with the AWS Account number (no dashes) of account 1\nReplace bucketname with the S3 bucket name from account 2\nNote: This policy uses least privilege. Only resources using the IAM role from account 1 will have access\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;Stmt1565731301209\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::bucketname\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;:\u0026quot;arn:aws:iam::account1:role/Lambda-List-S3-Role\u0026quot; }, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;aws:UserAgent\u0026quot;: \u0026quot;*AWS_Lambda_python*\u0026quot; } } } ] } Click Save\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/3_create_lambda_acct_1/","title":"Create Lambda in account 1","tags":[],"description":"","content":" Open the Lambda console.\nClick Create a function.\nAccept the default Author from scratch.\nEnter function name as Lambda-Assume-Roles.\nSelect Python 3.6 runtime.\nExpand Permissions, click Use an existing role, then select the Lambda-Assume-Roles role.\nClick Create function.\nReplace the example function code with the following, replacing the RoleArn with the one from account 2 you created previously.\nimport json import boto3 import os import uuid def lambda_handler(event, context): try: client = boto3.client('sts') response = client.assume_role(RoleArn='arn:aws:iam::account2:role/LambdaS3ListBuckets',RoleSessionName=\u0026quot;{}-s3\u0026quot;.format(str(uuid.uuid4())[:5])) session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken']) s3 = session.client('s3') s3list = s3.list_buckets() print (s3list) return str(s3list['Buckets']) except Exception as e: print(e) raise e Click Save.\nClick Test, accept the default event template, enter event name of test, then click Create.\nClick Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API.\nHow could the example policies be improved?\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/3_visualizations/","title":"Create the Visualizations","tags":[],"description":"","content":"We will now create visualizations of our workload effiency. We will add the new dataset, and then build different visualizations to see what exactly impacts efficiency and where to look to improve it.\nBasic Efficiency visualization We will create a visualization from the application log files.\nGo into QuickSight\nClick Manage data: Click New data set: Click Athena: Enter the Data source name: Efficiency, and click Create data source\nSelect the costusage Database, and the efficiency Table and click Select\nSelect Spice and click Edit/Preview data\nMake sure there is data in the bottom pane:\nEnsure you have data and click Save \u0026amp; visualize:\nCreate a Clustered bar combo chart. Place datetime (aggregate hour) on the x-axis, cost (sum) in the bars column, add request (count) to the lines field\nLabel the chart Requests vs Cost: We can see that it roughly follows the same pattern, however there are times when the trends change. Below you can see the requests increase, but the cost decreases. Also the requests remain the same, and the cost decreases: Maybe it can be explained through something other than request count, lets add mbytes (sum) to the lines field well to see if there is correlation there: Again, similar trends and anomalies. MBytes remains constant and cost decreases: Lets now create our efficiency visualition. Add a calculated field named efficiency with the formula below. Our efficiency metric will be requests per dollar:\ncount(request) / sum(cost) Add a visualization, select a Line chart. Place datetime (hour) in the x-axis, efficiency as the value,\nWe now have a chart showing our efficiency over time. Notice how the efficiency changes significantly at the end of the day: You can now see increases and decreases in efficiency clearly, look when the output increases and cost remains the same, or the cost remains the same and the output decreases: You now have a baseline efficiency metric. Use this to look for areas of low efficiency of your workload - this will provide areas to cost optimize.\nRequest visualization Lets look deeper into the types of requests to see if we can get better insight into what is driving our costs and efficiency.\nLets look at a sample of a successful log request in our application log files:\n/index.php?name=Isabella,user=sponsored,work=26 We can see there are the fields:\nName User Work Lets create a calculated field RequestType with the formula below. This will separate out the types of requests, health checks, image requests and other/errors from the request field:\nifelse(locate(request,\u0026quot;index.html\u0026quot;) \u0026gt; 0,split(request,',',2),ifelse(locate(request,\u0026quot;health.html\u0026quot;) \u0026gt; 0,\u0026quot;HealthCheck\u0026quot;,ifelse(locate(request,\u0026quot;image_file\u0026quot;) \u0026gt; 0,\u0026quot;ImageFile\u0026quot;,\u0026quot;error\u0026quot;))) Create a Cluster bar combo chart visualization. Place datetime (HOUR) in the x-axis, add Request (count) in the bars, add RequestType in the Group/Color for bars, add efficiency to lines: We can see a correlation between the user type and the efficiency, when paid users increase the efficiency goes down, this indicates its more costly to service paid customers. Also look at the efficiency increase when there are large amounts of errors, you may which to filter and exclude low value outputs from the measures of efficiency. These insights can be used to categorize different types of requests in your workload and understand how to charge them back appropriately. You can use this to increase your efficiency by removing unwanted requests, for example you may use CloudFront to more efficiency handle errors - instead of processing them on the web servers.\nCongratulations! You have now calculated the efficiency of a workload, you can see how your efficiency changes over time, and look into the types of requests to understand what contributes cost to your workload.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/3_deploy_env_iaac/","title":"Deploy an Environment Using Infrastructure as Code","tags":[],"description":"","content":"Tagging We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources.\nAWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality).\nApply the following best practices when using tags:\nUse a standardized, case-sensitive format for tags, and implement it consistently across all resource types Consider tag dimensions that support the following: Managing resource access control with IAM Cost tracking Automation AWS console organization Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. Err on the side of using too many tags rather than too few tags. Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports.\nImportant: \u0026ldquo;Patch Group\u0026rdquo; is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words.\nManagement Tools: CloudFormation AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack).\nThere is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments.\n3.1 Deploy the Lab Infrastructure To deploy the lab infrastructure:\nDownload the CloudFormation script for this lab through this link . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack. On the Select Template page, select Upload a template file and select the OE_Inventory_and_Patch_Mgmt.json file you just downloaded. AWS CloudFormation Designer\nAWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources.\nOn the Select Template page, in the lower-right corner, click the View in Designer button. Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page. On the Select Template page, choose Next. A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented.\nIn the Specify Details section, define a Stack name, such as OELabStack1. In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to https://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test. Choose Next. On the Options page under Tags, define a Key of Owner, with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next. On the Review page, review your choices and then choose Create. On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud.\nNavigate to the EC2 console to view the deployed systems: Choose Instances. Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created. The impact of Infrastructure as Code With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment.\nThe ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/3_deploy_the_ami_builder_pipeline/","title":"Deploy The AMI Builder Pipeline","tags":[],"description":"","content":"In this section we will be building our Amazon Machine Image Pipeline leveraging EC2 Image Builder service. EC2 Image Builder is a service that simplifies the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images for use with Amazon EC2 and on-premises. Using this service, eliminates the automation heavy lifting you have to build in order to streamline the build and management of your Amazon Machine Image.\nUpon completion of this section we will have an Image builder pipeline that will be responsible for taking a golden AMI Image, and produce a newly patched Amazon Machine Image, ready to be deployed to our application cluster, replacing the outdated one.\nIn this section you have the option to build the pipeline manually using AWS console, or if you are keen to complete the lab quickly, you can simply deploy from the cloudformation template.\nClick here to build your pipeline using CloudFormation on the command line 3.1. Download The Cloudformation Template. Download the template here .\n3.2. Deploy Using The Command Line. Command Line: To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials:\naws cloudformation create-stack --stack-name pattern3-pipeline \\ --template-body file://pattern3-pipeline.yml \\ --parameters ParameterKey=MasterAMI,ParameterValue=ami-0f96495a064477ffb\t\\ ParameterKey=BaselineVpcStack,ParameterValue=pattern3-base \\ --capabilities CAPABILITY_IAM \\ --region ap-southeast-2 Note : For simplicity, we have used Sydney \u0026lsquo;ap-southeast-2\u0026rsquo; as the default region for this lab. We have also pre-configured the MasterAMI parameter to be the AMI id of Amazon Linux 2 AMI (HVM) in Sydney region ami-0f96495a064477ffb. If you choose to to use a different region, please change the AMI Id accordingly for your region. Click here to build your pipeline using CloudFormation through the console Console: 3.1. Download The Cloudformation Template. Download the template here .\n3.2. Deploy Using The Console. If you need detailed instructions on how to deploy CloudFormation stacks from within the console, please follow this guide. Use pattern3-pipeline as the Stack Name. Provide the name of the vpc cloudformation stack you create in section 1 ( we used pattern3-base as default ) as the BaselineVpcStack parameter value. Use the AMI Id of Amazon Linux 2 AMI (HVM) as the MasterAMI parameter value. ( In Sydney region ami-0f96495a064477ffb if you choose to to use a different region, please change the AMI Id accordingly for your region. ) 3.3. Take note of the ARN. When the CloudFormation template deployment is completed, note the output produced by the stack.\nYou can do this by clicking on the stack name you just created, and select the \u0026lsquo;Outputs Tab\u0026rsquo; as shown in diagram below.\nPlease take note of the Pipeline ARN specified under Pattern3ImagePipeline output\nClick here to build your pipeline interactively In this section we will go through the process manually to get a better understanding of the how the pipeline is constructed in EC2 Image Builder service.\nTo build this pipeline there are several subtasks we need to do:\nCreate an S3 bucket for logging purposes. Create an IAM role for use by the EC2 Image Builder. Create an Image Builder Component. Create an Image Builder Recipe. Create an Image Builder Pipeline. 3.1. Create an S3 Bucket. We are going to use an S3 bucket to store the the EC2 Image Build process, so lets create one.\n3.1.1. As S3 is a global namespace, for consistency please use the naming convention pattern3-logging with a unique UUID number at the end.\nYou can achieve this on a mac or UNIX terminal by setting a variable called $bucket as follows:\nbucket=pattern3-logging-`uuidgen | awk -F- \u0026#39;{print tolower($1$2$3)}\u0026#39;` echo $bucket 3.1.2. Hopefully you should have a bucket name returned to you which you can then use to create the bucket as follows:\naws s3 mb s3://$bucket --region ap-southeast-2 Alternatively you can use any randomized string at the end of the standard bucket name and create a bucket manually through the console. Please refer to this guide to create S3 bucket. 3.2 Create IAM role We will need to create an IAM role that will be used by the EC2 Image Builder service.This IAM role will be used as the instance profile role of the temporary EC2 instance the service will launch. The service will use this instance to run the necessary activity, in this case our patch update. Therefore the role will need to have the appropriate policies to do this activity.\nFollow below steps to create the IAM role:\n3.2.1. Navigate to IAM within the console and select \u0026lsquo;role\u0026rsquo; from the left hand panel and then select \u0026lsquo;create role\u0026rsquo; as shown:\n3.2.2. Select \u0026lsquo;AWS Service\u0026rsquo; from the types of trusted entities and then select \u0026lsquo;EC2\u0026rsquo;, and \u0026rsquo;next: Permissions\u0026rsquo; as shown:\n3.2.3. Using the filter, search \u0026amp; select the following policies: * EC2InstanceProfileForImageBuilder * AmazonSSMManagedInstanceCore\n3.2.4. Click \u0026lsquo;Next:Tags\u0026rsquo;.\n3.2.5. On the next screen click \u0026lsquo;Next:Review\u0026rsquo;.\n3.2.6. Enter pattern3-recipe-instance-role for the Role Name and add a description. The three policies listed above should be added as follows:\n3.2.7. In the IAM console, locate the role you just created.\n3.2.8. Click on the role and click + Add inline policy\n3.2.9. Select the JSON Tab and paste in below policy, replace the \u0026lt;s3 logging bucket\u0026gt; in the json snippet below with the bucket name you created in previous step.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;s3 logging bucket\u0026gt;/*\u0026#34; ] } ] } 3.2.10. Click Review Policy\n3.2.11. Enter a name for the policy, and click \u0026lsquo;Create Policy\u0026rsquo;\n3.2.12. Once you are done with this, you should now see another entry in the Policies with the name you just specified, expanding on that you should see the policy specified as screen shot below.\n3.3 Create a Security Group. Our EC2 Image Build pipeline is also going to need a security group that will be assigned to the temporary EC2 instance it uses, so lets create one now so that we can include it later in the lab.\n3.3.1. Follow this guide to create a Security Group.\n3.3.2. For this purpose, we do not need to assign anything in the Inbound rule of the security group.\n3.3.3. We do need to ensure that the outbound rules allow traffic out to the internet.\nYour Security group rules should look like below, so edit your security group accordingly:\n3.3.4. Ensure that the security group is created in the VPC id you\u0026rsquo;ve taken note of in section 1.2.\nIf you don\u0026rsquo;t remember the VPC-id, please refer to the instruction on section 1.2 in this lab for clarification.\n3.3.5. Name the Security Group pattern3-pipeline-instance-security-group\n3.4. Create a Component. In this section we will create a construct in EC2 Image Builder called a Component. This construct essentially contains instructions on what you would like to build into the AMI. For more information about EC2 Image builder Component, please refer to this guide .\nTo do this, Please follow below following steps:\n3.4.1. Navigate to the EC2 Image Builder service from the console main page.\n3.4.2. From the EC2 Image Builder service, select Components from the left hand menu and then select Create Component as shown here:\n3.4.3. Add the following values to to the options, leaving the rest of the settings as default:\nVersion: 1.0.0 Platform Linux Compatible OS versions: Amazon Linux 2 Component Name: pattern3-pipeline-ConfigureOSComponent Description: Component to update the OS with latest package versions. 3.4.4. Once that\u0026rsquo;s done, select \u0026lsquo;Define document content\u0026rsquo;\n3.4.5. Copy and paste in below definition document in the section under it.\nname: ConfigureOS schemaVersion: 1.0 phases: - name: build steps: - name: UpdateOS action: UpdateOS Please Note that this definition is specified in YAML, so please ensure indentation is correct.\nIn this scenario, we have a very simple definition in our component, which is to run an UpdateOS action which will the packages in our OS. There are many other action activity you can specify in the component. For more information about EC2 Image Builder component, please refer to this guide 3.4.6. When you have completed these inputs, select Create Component to complete the component setup.\n3.5. Create An Image Builder Recipe. Next, we will create an Image Builder Recipe, which specifies the components, and other configuration we are going to define for our pipeline.\nTo do this, please complete the following steps:\n3.5.1. Select Recipes from the left hand menu and then select Create Recipe.\n3.5.2. Enter the following as configuration details:\nName: pattern3-pipeline-ConfigureOSRecipe Version: 1.0.0 Description: Pattern3 Configure OS Recipe 3.5.3. Select Enter custom AMI ID and enter: the AMI ID for Amazon Linux 2 AMI (HVM) in your region:\n( In Sydney region ami-0f96495a064477ffb, please change the AMI Id accordingly if you use other region.)\n3.5.4. Under Build components select Browse build components and then filter by Created by me to include the component which you created earlier ( pattern3-pipeline-ConfigureOSComponent )/\n3.5.5. Once you have entered all of the configuration details, select \u0026lsquo;Create Recipe\u0026rsquo; at the bottom of the screen.\n3.6. Create An Image Builder Pipeline Using the Recipe We will now create the Image Builder Pipeline to run our recipe.\nTo do this, please complete the following steps:\n3.6.1. Remain in the Image Builder Recipe screen and use the tick box to select the recipe which you just created.\n3.6.2. From the Actions menu, select Create pipeline from this recipe as shown here:\n3.6.3. Enter the following information to configure the pipeline:\nName: pattern3-pipeline Description: Pattern 3 pipeline to update OS. Role: Specify the instance role which you created in step 3.2.2. Build Schedule: Manual Infrastructure Settings/Instance Type: Select an M4.large here if possible, although smaller instances can be used. Infrastructure Settings/VPC, subnet and security groups/Virtual Private Cloud: Select the VPC that have taken note in section 1.2 of the lab (the output components will list the VPC details). Infrastructure Settings/VPC, subnet and security groups/Subnet ID: Select the private Subnet ID from section 1.2 of the lab. Infrastructure Settings/VPC, subnet and security groups/Security Group Select the security group which you created before in step 3.2.3. Infrastructure Settings/Troubleshooting Settings/S3 location: Enter the S3 bucket that you specified in section 3.2.1. Note: For the instance types listed, an M4.large will take 20-30 minutes to build. If you want to save costs, please use a smaller instance but be prepared to wait for a bit longer for completion. 3.6.4. Once you have completed the above configuration, select Next at the bottom of the screen to go to the next configuation page.\n3.6.5. Leave the rest empty and click Review.\n3.6.6. Review the configuration is according to our specification above, and click Create Pipeline\n3.6.7. Take note of the pipeline ARN, as we will need this for the next section.\n3.7 Run Your Pipeline. Now that we have created all the construct, we can test the pipeline to ensure that it is working correctly. To do this select Run Pipeline from the Actions menu with the pipeline selected as shown here:\nOnce this is executed, you can observe the pipeline execution, and wait for the AMI to be built.\nNote: EC2 Image Builder pipeline will execute an SSM Automation Document in the background to orchestrate all the activities in building the AMI. If you go into your System Manager Automation document console, you should be able to see the execution running, and observe the activities in more detailed.\nPlease refer to this guide on how to view the Automation document execution details in your console.\nYou should be able to see an execution running under ImageBuilderBuildImageDocument document, which is the document used by EC2 Image builder to execute it\u0026rsquo;s activities.\nNow that you have completed the deployment of the Image Builder Pipeline, move to section 4 of the lab whre we will use Systems Manager to build the automation stage of the architecture.\nEND OF SECTION 3\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/3_cost_usage_download/","title":"Download your Monthly Report CSV","tags":[],"description":"","content":"It is possible to download a Monthly Report of your estimated AWS charges from the Bills page of the Billing and Cost Management console of your Management Account. This is typically used by customers looking to analyze their costs in a spreadsheet format with ease of use. This is part of a legacy feature called \u0026ldquo;Detailed Billing Reports\u0026rdquo;, but is used across many organizations for bill validations. If this is already enabled in your account you will be able to immediately download your monthly usage file and view it.\nGo to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV: It will download a CSV Monthly Report version of the bill you can use in a spreadsheet application. It is recommended to use this for monthly validations and NOT use this data source for cost analysis or chargeback, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_Analysis . If you get the following error, please pick the most recent month and try again. If you continue to receive this error please reference the Configure Monthly Reports Lab Guide for steps to enable your Monthly Report.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/3_additional_guardrails/","title":"Enable Additional Guardrails","tags":[],"description":"","content":"Control Tower guardrails Control Tower includes a number of guardrails to help improve your security posture. These guardrails are either preventative or detective. Preventative guardrails limit some actions and are implemented through AWS Organizations service control policies and are either enforced or not enabled. Detective guardrails detect resources in your landing zone which are in a noncompliant state. These are implemented via AWS Config ] and show resources that are either clear, in violation or not enabled.\nMake sure you review the mandatory guardrails and then review other guardrails you can enable . The strongly recommended guard rails follow the best practices for a Well-Architected environment. They are disabled by default but are strongly encouraged to be enabled. There are also additional elective guardrails to consider which may be suitable for your workload. If you want to add additional service control policies there is an AWS solution Customizations for AWS Control Tower to get started.\nService Control Policies AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization.\nWalk through for a non-control tower environment If you are not leveraging Control Tower it is strongly recommended that you implement the below service control policy to prevent AWS CloudTrail from being disabled.\nNavigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select *roots Press Attach to attach the policy to your organizations root Policy to prevent users disabling CloudTrail Note: AWS Control Tower already includes a mandatory guard rail preventing this\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cloudtrail:StopLogging\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/3_explore_webapp/","title":"Explore the Web Application","tags":[],"description":"","content":" Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation .\nWait until CloudFormationLab stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the CloudFormationLab stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service Hint: it will start with http://healt-alb and end in \u0026lt;aws region\u0026gt;.elb.amazonaws.com Click the URL and it will bring up the website:\nTroubleshooting: if you see an error such as 502 Bad Gateway, then wait 60 seconds and try again. It takes some time for the servers to initialize. The website simulates a recommendation engine making personalized suggestions for classic television shows. You should note the following features:\nArea A shows the personalized recommendation It shows first name of the user and the show that was recommended The workshop simulation is simple. On every request it chooses a user at random, and shows a recommendation statically mapped to that user. The user names, television show names, and this mapping are in a DynamoDB table, which is simulating the RecommendationService Area B shows metadata which is useful to you during the lab The instance_id and availability_zone enable you to see which EC2 server and Availability Zone were used for each request Use the following architectural diagram as you explore the site A - There is one EC2 instance deployed per Availability Zone B - Refresh the website several times, note that the EC2 instance and Availability Zone change from among the three available C - Elastic Load Balancing (ELB) is used here. An Application Load Balancer receives each request and distributes it among the available EC2 server instances across Availability Zones. The requests are stateless, and therefore can be routed to any of the available EC2 instances D - The EC2 instances are in an Amazon EC2 Auto Scaling Group . This Auto Scaling Group was configured to maintain three instances, therefore if one instance is detected as unhealthy it will be replaced to maintain three healthy instances. AWS Auto Scaling can also be configured to scale up/down dynamically in response to workload consitions such as CPU utilization or request count. Well-Architected for Reliability: Best practices Use highly available network connectivity for your workload public endpoints: These endpoints and the routing to them must be highly available. You used Elastic Load Balancing which provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases. Implement loosely coupled dependencies: Dependencies such as\u0026hellip; load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility. Deploy the workload to multiple locations: Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required. Automate healing on all layers: Upon detection of a failure, use automated capabilities to perform actions to remediate it You have deployed the cloud infrastructure architecture that can support a high reliability workload\nThis an example architecture of the cloud infrastructure necessary for reliable workloads\nAddition of dynamic auto scaling would further improve reliability\nReliability also depends on software architecture, network configuration, operational excellence, and testing (especially Chaos Engineering which tests resilience), which are outside the scope of this lab.\nWithout best practices for all of these, which can be found in the Reliability pillar of the AWS Well-Architected Framework , the workload will not achieve high reliability goals. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/3_deep_healthcheck/","title":"Implement deep health checks","tags":[],"description":"","content":"3.1 Re-enable the dependency service For the next part of the lab restore access to the getRecommendation API on the RecommendationService\nReturn to the AWS Systems Manager \u0026gt; Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled back to true and Save changes Confirm the web service is now returning \u0026ldquo;personalized\u0026rdquo; recommendations again 3.2 Inject fault on a single server Previously you simulated a failure of the service dependency. Now you will simulate a failure on a single server (of the three servers running). You will simulate a fault on this server that prevents only it from calling the otherwise healthy service dependency.\nNavigate to the EC2 Instances console There should be three EC2 instances with Instance State running, one in each Availability Zone (they will have Name WebApp1) Click the gear icon in the upper-right and select IAM Instance Profile Name (in addition to what is already selected) Select only the EC2 instance in Availability Zone us-east-2c\nClick Action \u0026gt; Instance Settings \u0026gt; Attach/Replace IAM Role\nFrom IAM role, click WebApp1-EC2-noDDB-Role-HealthCheckLab\nClick Apply\nClick Close\nThis will return you to the EC2 Instances console. Observe under IAM Instance Profile Name (it is one of the displayed columns) which IAM roles each EC2 instance has attached\nThe IAM role attached to an EC2 instance determines what permissions it has to access AWS resources. You changed the role of the us-east-2c instance to one that is almost the same as the other two, except it does not have access to DynamoDB. Since DynamoDB is used to mock our service dependency, the us-east-2c server no longer has access to the service dependency (RecommendationService). Stale credentials is an actual fault that servers might experience. Your actions above simulate stale (invalid) credentials on the us-east-2c server.\n3.4 Observe application behavior and determine how to fix it Observe the website behavior now\nRefresh the website multiple times noting which Availability Zone the serving the request The servers in us-east-2a and us-east-2b continue to function normally The server in us-east-2c still succeeds, but it uses the static response. Why is this? The service dependency RecommendationServiceEnabled is still healthy\nIt is the server in us-east-2c that is unhealthy - it has stale credentials\nReturn to the Target Groups and under the Targets tab observe the results of the ELB health checks They are all Status healthy, and are therefore all receiving traffic. Why does the server in us-east-2c show healthy for this check? The service would deliver a better experience if it:\nIdentified the us-east-2c server as unhealthy and did not route traffic to it Replaced this server with a healthy one Well-Architected for Reliability: Best practices Make services stateless where possible: Services should either not require state, or should offload state such that between different client requests, there is no dependence on locally stored data on disk or in memory. This enables servers to be replaced at will without causing an availability impact. Amazon ElastiCache or Amazon DynamoDB are good destinations for offloaded state. Automate healing on all layers: Upon detection of a failure, use automated capabilities to perform actions to remediate. Ability to restart is an important tool to remediate failures. As discussed previously for distributed systems, a best practice is to make services stateless where possible. This prevents loss of data or availability on restart. In the cloud, you can (and generally should) replace the entire resource (for example, EC2 instance, or Lambda function) as part of the restart. The restart itself is a simple and reliable way to recover from failure. Many different types of failures occur in workloads. Failures can occur in hardware, software, communications, and operations. Rather than constructing novel mechanisms to trap, identify, and correct each of the different types of failures, map many different categories of failures to the same recovery strategy. An instance might fail due to hardware failure, an operating system bug, memory leak, or other causes. Rather than building custom remediation for each situation, treat any of them as an instance failure. Terminate the instance, and allow AWS Auto Scaling to replace it. Later, carry out the analysis on the failed resource out of band. From the Target Groups console click on the the Health checks tab\nThe ELB health check is configured to return healthy when it receives an http 200 response on the /healthcheck path Since the healthcheck code simply always returns http 200, the bad server still returns http 200 and is seen as healthy. 3.4 Create a deep healthcheck to identify bad servers Update server code to add a deep health check response You will create and configure a new health check that will include a check on whether the server can access its dependency This is a deep health check \u0026ndash; it checks the actual function of the server including the ability to call service dependencies This will be implemented by updating the server code on the /healthcheck path Choose one of the options below (Option 1 - Expert or Option 2 - Assisted) to improve the code and add the deep health check.\n3.4.1 Option 1 - Expert option: make and deploy your changes to the code You may choose this option, or skip to Option 2 - Assisted option\nThis option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver.\nStart the existing server code that you added error handling to, or alternatively download the lab sample code from here: server_errorhandling.py Calls to /healthcheck should in turn make a test call to RecommendationService using User ID 0 If the RecommendationService returns the string test for both Result and UserName then it is healthy If it is healthy then return http code 200 (OK) If it is not healthy then return http code 503 (Service Unavailable) Also return the same EC2 meta-data that is returned on the call to the / path Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it: Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue If you completed the Option 1 - Expert option, then skip the Option 2 - Assisted option section and continue with 3.4.3 Health check code\n3.4.2 Option 2 - Assisted option: deploy workshop provided code The new server code including error handling can be viewed here Search for Healthcheck request in the comments. What will this code do now if called on this health check URL? Deploy the new health check code Navigate to the AWS CloudFormation console\nClick on the HealthCheckLab stack\nClick Update\nLeave Use current template selected and click Next\nFind the ServerCodeUrl parameter and enter the following:\nhttps://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_healthcheck.py Click Next until the last page\nAt the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names\nClick Update stack\nClick on Events, and click the refresh icon to observe the stack progress\nNew EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue 3.4.3 Health check code This is the health check code from server_healthcheck.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option, you can consult this code as a guide.\nCode: Click here to see the code: # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == \u0026#39;/healthcheck\u0026#39;: is_healthy = False error_msg = \u0026#39;\u0026#39; TEST = \u0026#39;test\u0026#39; # Make a request to RecommendationService using a predefined # test call as part of health assessment for this server try: # call RecommendationService using the test user user_id = str(0) response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value tv_show = response[\u0026#39;Item\u0026#39;][\u0026#39;Result\u0026#39;][\u0026#39;S\u0026#39;] user_name = response[\u0026#39;Item\u0026#39;][\u0026#39;UserName\u0026#39;][\u0026#39;S\u0026#39;] # Server is healthy of RecommendationService returned the expected response is_healthy = (tv_show == TEST) and (user_name == TEST) # If the service dependency fails, capture diagnostic info except Exception as e: error_msg += str(traceback.format_exception_only(e.__class__, e)) # Based on the health assessment # If it succeeded return a healthy code # If it failed return a server failure code message = \u0026#34;\u0026#34; if (is_healthy): self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() message += \u0026#34;\u0026lt;h1\u0026gt;Success\u0026lt;/h1\u0026gt;\u0026#34; # Add metadata message += get_metadata() else: self.send_response(503) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() message += \u0026#34;\u0026lt;h1\u0026gt;Fail\u0026lt;/h1\u0026gt;\u0026#34; message += \u0026#34;\u0026lt;h3\u0026gt;Error message:\u0026lt;/h3\u0026gt;\u0026#34; message += error_msg # Add metadata message += get_metadata() self.wfile.write( bytes( html.format(Title=\u0026#34;healthcheck\u0026#34;, Content=message), \u0026#34;utf-8\u0026#34; ) ) 3.4.4 Verify Elastic Load Balancer (ELB) is configured to use the new deep health check From the Target Groups console click on the the Health checks tab For Path verify the value is /healthcheck Click the Targets tab so you can monitor health check status 3.4.5 Observe behavior of web service with added deep health check Continue the lab after the HealthCheckLab CloudFormation stack is complete. The CloudFormation stack update reset the EC2 instance IAM roles, so the system is back to its original no-fault state. You will re-introduce the single-server fault and observe the new behavior.\nRefresh the web service multiple times and note all three servers are functioning without error\nCopy the URL of the web service to a new tab and append /healthcheck to the end of the URL\nThe new URL should look like:\nhttp://healt-alb1l-\u0026lt;...\u0026gt;.elb.amazonaws.com/healthcheck Refresh several times and observe the health check on the three servers\nNote the check is successful - the check now includes a call to the RecommendationService (the DynamoDB table)\nGo to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks.\nTo re-introduce the stale credentials fault, again change the IAM role for the EC2 instance in us-east-2c to WebApp1-EC2-noDDB-Role-HealthCheckLab\nSee 3.2 Inject fault on one of the servers if you need a reminder of how to do this. Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks (remember to refresh)\nNote that the server in us-east-2c is now failing the health check with a http code 503 Service Not Available\nWith an Interval of 15 seconds, and a Healthy threshold of 2, it can take up to 30 seconds to see the status update. The ELB has identified the us-east-2c server as unhealthy and will not route traffic to it\nThis is known as fail-closed behavior\nRefresh the web service multiple times and note it is however still functioning without error\nAnd unlike before it is no longer returning a static response - it only returns personalized recommendations Note that only the servers in us-east-2a and us-east-2b are serving requests Well-Architected for Reliability: Best practices Monitor all components of the workload to detect failures: Continuously monitor the health of your workload so that you and your automated systems are aware of degradation or complete failure as soon as they occur. Failover to healthy resources: Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. Well-Architected for Reliability: Health Checks The load balancer will only route traffic to healthy application instances. The health check needs to be at the data plane/application layer indicating the capability of the application on the instance. This check should not be against the control plane. A health check URL for the web application will be present and configured for use by the load balancer Repair the server Navigate to the EC2 Instances console and select only the instance in us-east-2c Click Action \u0026gt; Instance State \u0026gt; Terminate Click Yes, Terminate The EC2 instance will shut down Amazon EC2 Auto Scaling will recognize there are less then the three Desired Capacity and will start up a new EC2 instance The new instance replaces the one with the stale credentials fault, and loads fresh credentials From the Target Groups console Targets tab note the health check status of the new server in us-east-2c The new instance in us-east-2c will first show Description Target registration is in progress Then Description is This target is currently passing target group\u0026rsquo;s health checks, then you may continue the workshop (The Description may show Health checks failed with these codes: [502], before getting to a healthy state. This is expected as the server initializes) From the time you terminate the EC2 instance, it will take four to five minutes to get the new EC2 instance up and in a healthy state Refresh the web service multiple times and note that personalized recommendations are once again being served from all three servers X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/3_perform_review/","title":"Performing a review","tags":[],"description":"","content":" From the detail page for the workload, click the Start reviewing button, then select the AWS Well-Architected Framework to review: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability: Select the first question: REL 1. How do you manage service quotas and constraints? Answer the REL 1 to REL 13 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean and to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/3_prio_resource_opt/","title":"Prioritizing Rightsizing Recommendations","tags":[],"description":"","content":"Prioritizing Rightsizing Recommendations Now that you understand how AWS Cost Explorer Rightsizing recommendations work let\u0026rsquo;s run an example and find how to prioritize quick wins.\nDownload your Rightsizing Recommendations Select Download CSV to download your Rightsizing recommendations report If you don’t have any Rightsizing recommendations, download and use this Sample Rightsizing recommendations file (.csv) data for the lab.\nFilter your Rightsizing Recommendations Let’s filter out instances that are either too small or were only running for a few hours since the analysis was made. By doing so, we minimize the time required to perform rightsizing modifications that would otherwise result in minimal savings.\nDepending on how many cost allocation tags you have enabled on your account the columns may differ from the example. Try to match the formulas using the screenshots below and the default column names.\nInsert a new column to the right of the Recommended Action column.\nName the new column\u0026rsquo;s first row “TooSmall” This will be the label for this new column\nPaste the following formula in each row below the label\n=IF(X2\u0026lt;25,1,0) Where Column X = Recommended Instance Type 1 Estimated Savings\nThis formula will flag with a “1” any instance that will fail to deliver more than $25/month in savings (or $300/year). Feel free to adjust the threshold for your organization own savings expectation. If you prefer to do the analysis based on specific instance sizes instead of potential savings you can use the following formula to exclude smaller instances from the recommendations as well.\n=IF(N2=\u0026quot;Modify\u0026quot;,IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\u0026quot;nano\u0026quot;,\u0026quot;micro\u0026quot;,\u0026quot;small\u0026quot;,\u0026quot;medium\u0026quot;},F2)))))\u0026gt;0,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;),\u0026quot;0\u0026quot;) Where Column N = Recommended action and Column F = Instance Type\nFlag your Rightsizing Recommendations Now let’s flag EC2 instances that belong to old generations (C4, M3, etc).\nSince you are investing engineer time on rightsizing let\u0026rsquo;s make sure you are also leveraging the newest technology available. Newer EC2 generations have a superior performance increasing the changes of success for the rightsizing exercise, they also generally cost less than previous generations providing a higher cost vs benefit.\nInsert a new column to the right of the Instance Type column\nName the new column\u0026rsquo;s first row “OldGen” This will be the label for this new column\nPaste the following formula in each row below the label\n=IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\u0026quot;t1\u0026quot;,\u0026quot;m1\u0026quot;,\u0026quot;c1\u0026quot;,\u0026quot;m2\u0026quot;,\u0026quot;i2\u0026quot;,\u0026quot;g2\u0026quot;,\u0026quot;c3\u0026quot;,\u0026quot;m3\u0026quot;,\u0026quot;r3\u0026quot;,\u0026quot;c4\u0026quot;,\u0026quot;m4\u0026quot;,\u0026quot;r4\u0026quot;,\u0026quot;cr1\u0026quot;,\u0026quot;hs1\u0026quot;},F2)))))\u0026gt;0,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;) Column F = Instance Type\nThis formula will flag with a \u0026ldquo;1\u0026rdquo; instances from old generations.\nSort your recommendations by low complexity and higher savings Group 1: Idle EC2 resources\nFilter the data on Recommended Action = \u0026ldquo;Terminate\u0026rdquo; Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest Start filtering the idle resources or instances where CPU utilization \u0026lt;1%, it is likely these instances were launched and forgotten so the potential savings might represent the entire cost of running that instance. The resulting filtered list should be where you start rightsizing discussions with application owners. Perform an investigation to understand why these instance were launched in the first place and validate their usage with the resource owner. Don\u0026rsquo;t forget to review the column Total running hours to separate idle resources that were recently launched vs the ones who are running uninterruptedly over the past 14 days.\nIf you are using the rightsizing CSV file provided in this lab exercise, you will notice that we filtered down from an original 1,051 recommendations to 17 and identified $2,859 per month in potential savings. You will also find in the example some instances showing 0 estimated savings, that\u0026rsquo;s because they were running under Savings Plan (SP) or Reserved Instances (RI). On these cases AWS will not estimate savings, but if these instances can be terminated the committed SP/RI hours might float to On Demand usage within your account depending on the RI/SP scope\t.\nGroup 2: Previous generation instances\nFilter the data on Recommended Actions = “Modify” AND OldGen = “1” AND TooSmall = “0” Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest This list will focus on the underutilized resources (\u0026gt;1% CPU) that belongs to previous generations and can either be downsized within the same family (column P below) or modernized to the newest generation.\nMoving to a new generation will require additional investment in the form of more testing hours compared to instances identified on Group 1, but depending on the case it can maximize savings and performance. Refer to the EC2 previous generation page for information on upgrade paths to current generation instance families.\nAs AWS continues to innovate, new instance types become available often with a cheaper hourly cost and better performance versus current generation instances. Review the Amazon EC2 Instance Types to see current instance families. For example, the new Graviton instances offer even more savings and performance but require additional testing because they use a different processor architecture (ARM).\nIf you are using the rightsizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 1,051 recommendations to 15 with $7,089 per month in potential savings.\nGroup 3: Current generation instances\nFilter the data on Recommended Actions = “Modify” AND OldGen = “0” AND TooSmall = “0” Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest This will select underutilized resources from the current, most modern generation. We recommend sorting them by potential savings to make sure you are prioritizing the instances that will provide larger savings first.\nAlso, do not forget to check the other recommended instance types. Rightsizing recommendations will recommend up to 3 instances for each resource moving from a more conservative recommendation (the first recommendation) to a more aggressive and higher savings recommendation (second and third recommendations).\nIf you are using the rightsizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 1,051 recommendations to 37 with $18,728 per month in potential savings.\nConclusions During this lab exercise, we learned how to prioritize the rightsizing recommendations with the goal of identifying low complexity and high savings recommendations. We initially started with 1,051 recommendations with a potential saving of $29,603 but we managed to identify the top 69 cases with lowest complexity that together add up to $28,676 of the overall potential saving.\nGroup 1 (Idle) and Group 2 (Previous Generation) are the less complex cases where you may want to start the rightsizing exercises for your organization. As you gain more confidence and learn how to develop a regular process for rightsizing, your organization will be able to rapidly act on Group 3 (Current/modern generation) and other cases.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/3_quicksight_setup/","title":"Setup QuickSight Dashboard","tags":[],"description":"","content":"We will now setup the QuickSight dashboard to visualize your usage by cost, and setup the analysis to provide Savings Plan recommendations.\nGo to the QuickSight service: Click on Manage data: Click on New data set: Click Athena: Enter a Data source name of SP_Usage and click Create data source: Select the costmaster database, and the sp_usage table, click Select: Ensure SPICE is selected, click Visualize: Click on QuickSight to go to the home page: Click on Manage data: Select the sp_usage Dataset: Click Schedule refresh: Click Create: Enter a schedule, it needs to be refreshed daily, and click Create: Click Cancel to exit: Click the x in the top corner: You now have your data set setup ready to create a visualization.\nAdvanced Setup This section is optional and replaces the next two steps by creating the dashboard from a template. You will require access and knowledge of the AWS CLI, and Enterprise edition of QuickSight. If you do not have the access, go to the next step and manually create the dashboard as per the lab.\nGo to this web page to request access to the template. Enter you AWS AccountID and click Submit: Template Access Edit the following command, replacing AccountID and region, then using the CLI list the QuickSight datasets and copy the Arn for the sp_usage dataset:\naws quicksight list-data-sets --aws-account-id (AccountID) --region (region) Edit the following command, replacing AccountID and region, then using the CLI list your QuickSight users ARNs:\naws quicksight list-users --aws-account-id (AccountID) --namespace default --region (region) Create a local file create-dashboard.json with the text below, replace the values (Account ID), (User ARN), (Dataset ARN):\n{ \u0026quot;AwsAccountId\u0026quot;: \u0026quot;(Account ID)\u0026quot;, \u0026quot;DashboardId\u0026quot;: \u0026quot;SP_usage_analysis\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;sp_usage analysis\u0026quot;, \u0026quot;Permissions\u0026quot;: [ { \u0026quot;Principal\u0026quot;: \u0026quot;(User ARN)\u0026quot;, \u0026quot;Actions\u0026quot;: [ \u0026quot;quicksight:DescribeDashboard\u0026quot;, \u0026quot;quicksight:ListDashboardVersions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPermissions\u0026quot;, \u0026quot;quicksight:QueryDashboard\u0026quot;, \u0026quot;quicksight:UpdateDashboard\u0026quot;, \u0026quot;quicksight:DeleteDashboard\u0026quot;, \u0026quot;quicksight:DescribeDashboardPermissions\u0026quot;, \u0026quot;quicksight:UpdateDashboardPublishedVersion\u0026quot; ] } ], \u0026quot;SourceEntity\u0026quot;: { \u0026quot;SourceTemplate\u0026quot;: { \u0026quot;DataSetReferences\u0026quot;: [ { \u0026quot;DataSetPlaceholder\u0026quot;: \u0026quot;sp_usage\u0026quot;, \u0026quot;DataSetArn\u0026quot;: \u0026quot;(Dataset ARN)\u0026quot; } ], \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:quicksight:us-east-1:869004330191:template/SP-Analysis-template\u0026quot; } }, \u0026quot;VersionDescription\u0026quot;: \u0026quot;1\u0026quot; } Edit the following command, replacing (region), Run the command to create the dashboard and you should get a 200 response:\naws quicksight create-dashboard --cli-input-json file://create-dashboard.json --region (region) After a few minutes the dashboard will become available in QuickSight, click on the Dashboard name: Click Share, click Share dashboard, Add the required users, or share with all users, ensure you check Save as for each user: Click Save as: Enter an Analysis name and click Create: You will now have the analysis created automatically from the template: You have successfully created the analysis from a template, this lab is complete.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/3_share_analysis/","title":"Share your Analysis and Dashboard","tags":[],"description":"","content":"Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set.\nShare an analysis To share an analysis, click on Share on the top right, then select Share analysis: Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share: The users will then receive an email similar to the one below. When they click on Click to View they’ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.: Publish a dashboard To publish a dashboard click on Share in the upper right, and select Publish dashboard: Enter a name for the dashboard, and click Publish dashboard: Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share. For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/3_filter_csv/","title":"Sort and filter the RI CSV files","tags":[],"description":"","content":"RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of those high quality recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs.\nFilter out low risk, and high return RIs To get the lowest risk, we sort by Fully Paid Day smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI\u0026rsquo;s below are fully paid off in around 7months, so if they are used for 7 months - they have paid themselves off completely. We will separate the very low, low, and medium risk recommendations. Add in some empty lines before a Fully Paid Day of 8, and a fully paid day of 10, also copy the header line across: We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings, largest to smallest: Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for customers is in the range of $50-100. A recommendation with a saving lower than this may not save enough. Aim for the top 50-70% of recommendations. We have chosen $100, in each of the three groups grey out anything less than this: Filter out usage patterns It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern.\nIf the Max hourly usage is close to Min hourly usage, within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: If the Average hourly usage is close to the Max hourly usage, then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max: Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here:\nCombined_EC2_RI_Rec.xls Making recommendations We now go through the spreadsheet and apply business rules to make the best low risk \u0026amp; high return purchases that are right for the business. We look at each of the risk categories as follows:\nLow risk and very low risk - this is the first group of recommendations (fully paid below 8) For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that were not highlighted in the 7Day column but are highlighted in the Average hourly usage, select a percentage of either the 30Day or 7Day column (which ever is lower). Medium risk - this is the second group of recommendations (fully paid below 10) From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above, and are not greyed out:\nRe-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully processed all the recommendations. You now have the right low risk and high return recommendations, based on your usage patterns. Take the recommendations, and purchase the quantity required in the correct accounts.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/3_create_cw_config/","title":"Store the CloudWatch Config File in Parameter Store","tags":[],"description":"","content":"3. Create the CloudWatch Config File with Parameter Store You will use Parameter Store, a tool in Systems Manager, to store the CloudWatch agent configuration. Parameter store allows you to securely store configuration data and secrets for reusability. You can re-use configuration data that is well controlled and consistent. In this case, you need to store the configuration file for CloudWatch Agent on your EC2 instance. The CloudWatch agent configuration data specifies which logs and metrics will be sent to CloudWatch as well as the source of this data.\nOpen the Systems Manager console . Choose Parameter Store from the left side menu under Application Management. Choose Create parameter from that screen. Enter the parameter name AmazonCloudWatch-securitylab-cw-config. You may use a different name, but it must begin with AmazonCloudWatch``- in order to be recognized by CloudWatch as a valid configuration file. Give your parameter a description, such as “This is a CloudWatch Agent config file for use in the Well Architected security lab”. Set Tier to Standard. Set Type to String. Set Data type to text. In the Value field, copy and paste the contents of the config.json file found in the lab assets. This config file specifies which metrics and logs to collect. The agent section specifies which user to run the logs agent as, and how frequently to collect logs. The logs section specifies which log files to monitor and which log group and stream to place those logs in. This information can be seen in collect_list. For this lab, you are collecting SSH logs, Apache Web Server logs, and logs for the CloudWatch Agent itself. We will examine these logs more closely in a later step The metrics section specifies which metrics are collected (in metrics_collected), the frequency of collection, measurement, and other details. To learn more about creating config files, see this link . Click Create parameter. Recap: In this portion of the lab, you created a re-usable, centrally stored configuration file stored in Amazon Systems Manager Parameter Store. You can re-use configuration data stored in Parameter Store while ensuring that it is consistent and correct across uses. This becomes especially helpful when scaling, as you can re-use configuration files across fleets of instances. This highlights the Well-Architected best practice of “configuring services and resources centrally” by maintaining configuration files centrally in Parameter Store.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_detective_controls/3_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them.\nNote: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab.\nDelete the stack:\nSign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Select the DetectiveControls stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets:\nSign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/ . Select the CloudTrail bucket name you previously created without clicking the name. Click Empty bucket and enter the bucket name in the confirmation box. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. With the bucket now empty, click Delete bucket. Enter the bucket name in the confirmation box and click Confirm. Repeat steps 2 to 6 for the Config bucket you created. References \u0026amp; useful resources AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_iam_groups_and_roles/3_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them.\nDelete the IAM stack:\nSign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Select the baseline-iam stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button. References \u0026amp; useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/3_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nDelete the CloudFront distribution:\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack:\nSign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. References \u0026amp; useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_updates_and_ingestion/3_cleanup/","title":"Teardown","tags":[],"description":"","content":" Delete the Glue database, select the database name, click Action and click Delete database: Delete the CloudFormation stack, select the stack, click Actions and click Delete stack: X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/3_test_replication/","title":"Test bi-directional cross-region replication (CRR)","tags":[],"description":"","content":"To test bi-directional replication using the two rules your created, you will upload another object into each of the east and west S3 buckets and observe it is replicated across to the other bucket. For this step you will need two more test objects:\nThese are files that you will upload into each S3 bucket. They should not be too big, as this will increase the time to upload it from your computer. If you do not have files to use, you can download file #1 and download file #2 File #1 File #2 3.1 Upload objects to their respective Amazon S3 buckets 3.1.1 Upload object #1 to the east S3 bucket Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner Click on the name of the east bucket if you used Ohio the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Click on ⬆ Upload Upload the file you will use as object #1 Drag and drop the file or click Add files Click Upload (note there is a Next button, but you do not need to click it) 3.1.2 Upload object #2 to the west S3 bucket Click on Amazon S3 in the upper left corner of the Amazon S3 console Click on the name of the west bucket if you used Oregon the name will be \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Click on ⬆ Upload Upload the file you will use as object #2 Drag and drop the file or click Add files Click Upload (note there is a Next button, but you do not need to click it) 3.2 Verify bi-directional replication You are already looking at the objects in the west bucket Verify that object #1, that you uploaded to the east bucket is present here also Note the Replication status is REPLICA Click on Amazon S3 in the upper left corner Click on the name of the east bucket Verify that object #2, that you uploaded to the west bucket is present here also Note the Replication status is REPLICA 3.3 Explore which Amazon S3 events trigger replication and which do not 3.3.1 Use CloudWatch Logs Insights to query the CloudTrail logs AWS CloudTrail is a service that provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. You will use AWS CloudTrail to explore which Amazon S3 events trigger replication to occur.\nChange back to the east AWS region\nIf you used the directions in this lab, then this is Ohio (us-east-2) The CloudFormation template you deployed configured CloudTrail to deliver a trail to CloudWatch Logs. Therefore:\nGo to the CloudWatch console Click on Insights (under Logs) on the left Where it says Select log group(s) select the one named CloudTrail/logs/\u0026lt;your_prefix_name\u0026gt;\nRight below that is where you can enter a query\nDelete the query that is there\nand enter the following query. It returns all PutObject requests on S3 buckets\nfields @timestamp, requestParameters.key AS key, | requestParameters.bucketName AS bucket, | userIdentity.invokedBy AS invokedBy, | userIdentity.arn AS arn, | userIdentity.sessionContext.sessionIssuer.userName AS UserName | filter eventName ='PutObject' | sort @timestamp desc | limit 20 Click Run query\nLook at the results at the bottom of the screen\n3.3.2 Difference between uploaded and replicated objects in S3 bucket You are looking for three results, one for each of the test objects you uploaded. Use the key field to see the test object names.\nTroubleshooting: If your query returned less or more than three results then consult this guide to tuning your Insights query For these events look at the tabular attributes returned by the query at the bottom of the page\nHowever, if you want to see all the attributes, you can click to the left of each event The three events correspond to each of the objects you put into the S3 buckets\nThe object you put into the east bucket testing rule #1 The object you put into the east bucket testing bi-directional replication The object you put into the west bucket testing bi-directional replication Look at the bucket for this event. This event is for the east bucket This is actually the replication event for the object you put into the west bucket What is different between events where you uploaded the object into the bucket and events where the object was put into the bucket by replication? Replicated objects have a userIdentity.invokedBy value of \u0026ldquo;AWS Internal\u0026rdquo;\nThe userIdentity is different - see the arn and username\nThe CloudWatch Logs Insights page should look like this:\nThe result is:\nFor an object uploaded by you Amazon S3 triggers the rule you configured to replicate it to another bucket And sets Replication status to COMPLETED For an object replicated from another bucket Amazon S3 knows not to re-replicate the object And sets Replication status to REPLICA 3.4 Additional exercises These are optional. They help you to explore and understand bi-direction cross-region replication on Amazon S3.\nLook at the Permissions on the \u0026lt;your-naming-prefix\u0026gt;-S3-Replication-Role-\u0026hellip; IAM Roles\nWhy do they have the policies that they do? What happens when you rename an object in one of the buckets?\nHint: if you cannot figure it out consider that versioning is enabled (and must be enabled for replication to work) Switch to the west AWS region and run the same CloudWatch Insights Query there.\nWhat do you expect? 3.5 Summary You created two S3 buckets in two different AWS regions. You then setup bi-directional cross-region replication (CRR) between the two Amazon S3 buckets. Putting an object in either bucket resulted in the object asynchronously being backed up to the other bucket. Objects encrypted in their original bucket are also encrypted in their replication bucket. Objects are replicated once \u0026ndash; replication \u0026ldquo;looping\u0026rdquo; is prevented.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/3_failure_injection/","title":"Test Resiliency Using Failure Injection","tags":[],"description":"","content":"Failure injection (also known as chaos testing) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.\nPreparation Before testing, please prepare the following:\nRegion must be the one you selected when you deployed your WebApp\nWe will be using the AWS Console to assess the impact of our testing\nThroughout this lab, make sure you are in the correct region. For example the following screen shot shows the desired region assuming your WebApp was deployed to Ohio region\nGet VPC ID\nA VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to WebApp1-VPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever \u0026lt;vpc-id\u0026gt; is indicated in a command Get familiar with the service website\nPoint a web browser at the URL you saved from earlier If you do not recall this, then in the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created) Note the instance_id (begins with i-) - this is the EC2 instance serving this request Refresh the website several times watching these values Note the values change. You have deployed two web servers per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. 3.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service.\nNavigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane.\nThere are three EC2 instances with a name beginning with WebApp1. For these EC2 instances note:\nEach has a unique Instance ID There is two instances per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open\nTo fail one of the EC2 instances, use the VPC ID as the command line argument replacing \u0026lt;vpc-id\u0026gt; in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for)\nLanguage Command Bash ./fail_instance.sh \u0026lt;vpc-id\u0026gt; Python python fail_instance.py \u0026lt;vpc-id\u0026gt; Java java -jar app-resiliency-1.0.jar EC2 \u0026lt;vpc-id\u0026gt; C# .\\AppResiliency EC2 \u0026lt;vpc-id\u0026gt; PowerShell .\\fail_instance.ps1 \u0026lt;vpc-id\u0026gt; The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down\n$ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \u0026quot;TerminatingInstances\u0026quot;: [ { \u0026quot;CurrentState\u0026quot;: { \u0026quot;Code\u0026quot;: 32, \u0026quot;Name\u0026quot;: \u0026quot;shutting-down\u0026quot; }, \u0026quot;InstanceId\u0026quot;: \u0026quot;i-0710435abc631eab3\u0026quot;, \u0026quot;PreviousState\u0026quot;: { \u0026quot;Code\u0026quot;: 16, \u0026quot;Name\u0026quot;: \u0026quot;running\u0026quot; } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one )\nRefresh it. (Note: it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated.\n3.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n3.2.1 System availability Refresh the service website several times. Note the following:\nWebsite remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id) 3.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance.\nGo to the Target Groups console you already have open (or click here to open a new one )\nIf there is more than one target group, select the one with whose name begins with WebAp Click on the Targets tab and observe:\nStatus of the instances in the group. The load balancer will only send traffic to healthy instances.\nWhen the auto scaling launches a new instance, it is automatically added to the load balancer target group.\nIn the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic.\nNote the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify.\nFrom the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts\n3.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS.\nGo to the Auto Scaling Groups console you already have open (or click here to open a new one )\nIf there is more than one auto scaling group, select the one with the name that starts with WebApp1 Click on the Activity History tab and observe:\nThe screen cap below shows that instances were successfully started at 17:25\nAt 19:29 the instance targeted by the script was put in draining state and a new instance ending in \u0026hellip;62640 was started, but was still initializing. The new instance will ultimately transition to Successful status\nDraining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more: After the lab see this blog post for more information on draining.\nLearn more: After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand\n3.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.\nAvailability Zones (AZs) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more: After the lab see this whitepaper on regions and availability zones X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/3_test_role/","title":"Test Role","tags":[],"description":"","content":"3.1 Assume ec2-admin-team-alpha Role Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role.\nSign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role. Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user.\nTip\nThe last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.\n3.2 Launch Instance With \u0026amp; Without Tags Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized. This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details. Accept default details by clicking Next: Add Storage. Accept default storage options by clicking Next: Add Tags. Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example. Repeat to add Key of Team and Value of Beta. Note: Keys and values are case sensitive! Click Next: Configure Security Group. Click Select an existing security group, click the check box next to security group with name default, then click Review and Launch. Click Launch then click the option to Proceed without a key pair. Tick the I acknowledge box then click Launch Instances. The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch. On the review launch page once again click Launch then click the option to Proceed without a key pair. Tick the I acknowledge box then click Launch Instances. You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet. 3.3 Modify Tags On Instances Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags, try changing the Team key to a value of Test then click Save. An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save. The request should succeed. 3.4 Manage Instances Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test. Click Actions button then expand out Instance State then Terminate. Check the instance is the one you wish to terminate by it\u0026rsquo;s name and click Yes, Terminate. The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2! "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/3_sp_coverage/","title":"View your Savings Plan coverage","tags":[],"description":"","content":"You can view your Savings Plan coverage to look for anomalies or changes in coverage.\nIn Cost Explorer, click on Saved reports on the left: Click on Coverage report: You can see the coverage is 0%: Scroll down to the table, click on the arrow next to On-demand spend to sort from the largest spend to the lowest. This helps show your opportunity for cost savings: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/","title":"Level 100: Pricing Models","tags":[],"description":"","content":"Last Updated August 2021\nYour browser doesnt support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Authors Nathan Besh, Cost Lead Well-Architected Contributor Cy Hopkins, Enterprise Solutions Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to perform an analysis of Savings Plans for your AWS cost and usage. The skills you learn will help you implement the correct pricing models for your workloads, in alignment with the AWS Well-Architected Framework.\nNOTE: There is also a 200 level lab on building a Savings Plan analysis dashbord. Goals Perform basic analysis of your recommended Savings Plans Prerequisites AWS Account Setup has been completed Consistent usage in your account for more than 1 month Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Costs https://aws.amazon.com/aws-cost-management/pricing/ There are no costs for this lab Time to complete The lab should take approximately 15 minutes to complete Steps: View your Savings Plan recommendations Understand your usage trend Analyze your Savings Plan recommendations Visualize your Savings Plan recommendations Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/","title":"Level 200: Pricing Models","tags":[],"description":"","content":"Last Updated May 2020\nAuthors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction Commitment discounts offer lower prices when you make a commitment to a minimum amount of usage. Reserved Instances (RI\u0026rsquo;s) are available for RDS, RedShift, DynamoDB, Elasticache, and Elasticsearch (For EC2 - Savings Plans should be purchased). This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework.\nGoals Perform a Reserved Instance analysis Make low risk, high return RI recommendations Prerequisites AWS Account Setup has been completed Consistent usage in your account for more than 1 month to trigger a recommendation Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Costs There are no associated costs for this lab Steps: View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_playbook_with_jupyter-aws_iam/","title":"Level 300: Incident Response Playbook with Jupyter - AWS IAM","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Byron Pogson, Solutions Architect, AWS Introduction This hands-on lab will guide you through running a basic incident response playbook using Jupyter. It is a best practice to be prepared for an incident, and practice your investigation and response tools and processes. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable. Steps: Install Python \u0026amp; AWS CLI Playbook Run References \u0026amp; useful resources AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax Jupyter "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_200_incident_response_day/","title":"Quest: AWS Incident Response Day","tags":[],"description":"This quest is the guide for incident response workshop often ran at AWS led events.","content":"About this Guide This quest is the guide for an AWS led event including incident response day. Using an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nStart now! Lab 2 - Protecting workloads on AWS from the instance to the edge In this workshop, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them.\nStart now! Lab 3 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.\nStart now! Lab 4 - Getting Hands on with Amazon GuardDuty Walks you through a scenario covering threat detection and automated remediation using Amazon GuardDuty; a managed threat detection service. The scenario simulates an attack that spans a few threat vectors, representing just a small sample of the threats that GuardDuty is able to detect.\nStart now! Lab 5 - Open Source AWS Memory Forensics This lab consists of using an open source python module for orchestrating memory acquisitions and analysis using AWS Systems Manager . It analyzes the memory dump using Rekall with the most common plugins: psaux, pstree, netstat, ifconfig, pidhashtable.\nStart now! Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Authors Ben Potter, Security Lead, Well-Architected "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_reinvent2020_automate_with_weinvest/","title":"Quest: re:Invent 2020 - Automate The Well-Architected Way With WeInvest","tags":[],"description":"This quest is a collection of lab patterns which are covered in the upcoming session at re:Invent 2020: Automate The Well-Architected Way with WeInvest","content":"About this Guide This quest is a collection of featured lab patterns with are covered in the re:Invent 2020 session: Automate The Well-Architected Way with WeInvest.\nUsing this collection of labs, the user will be able to walk through the featured patterns from the session which WeInvest have worked with AWS to implement within their business to build an improved and effective security posture.\nUsing either an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nLab 1 - Autonomous Montoring Of Cryptographic Activity With KMS. In this lab we will walk you through an example scenario of monitoring our KMS service for encryption and decryption activity. We will autonomously detect abormal activity beyond a predefined threshold and respond accordingly, using the following services:\nAWS CloudTrail - Used for capturing API events within the environment. Amazon CloudWatch Log Groups - Used to log our CloudTrail API events. Amazon CloudWatch Metric Filter to create apply filter so we can measure the only the events that matters for us. Amazon CloudWatch Alarms to allow our system to react against pre created events Amazon Simple Notification Service to allow us to send email notification when an event occurs. Start now! Lab 2 - Autonomous Patching With EC2 Image Builder and Systems Manager. In this lab we will walk you through a blue/green deployment methodology to build an entirely new Amazon Machine Image (AMI) that contains the latest operating system patch, which can be deployed into an application cluster. We will use the following services to complete the workload deployment:\nEC2 Image Builder to automate creation of the AMI Systems Manager Automated Document to orchestrate the execution. CloudFormation with AutoScalingReplacingUpdate update policy, to gracefully deploy the newly created AMI into the workload with minimal interruption to the application availability. Start now! Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Authors Tim Robinson - Well-Architected Geo Solution Architect (Asia) "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/","title":"300 Labs","tags":[],"description":"","content":"List of labs available Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability Improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors\nLevel 300: Testing for Resiliency of EC2, RDS, and AZ Use code to inject faults simulating EC2, RDS, and Availability Zone failures. These are used as part of Chaos Engineering to test workload resiliency\nLevel 300: Fault Isolation with Shuffle Sharding Implement shuffle sharding to minimize scope of impact of failures\n"},{"uri":"https://wellarchitectedlabs.com/cost/costeffectiveresources/","title":"Cost Effective Resources","tags":[],"description":"","content":"About cost effective resources Using the appropriate services, instances and resources for your workload is key to cost savings. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. AWS offers a variety of flexible and cost-effective pricing options to acquire instances from EC2 and other services in a way that best fits your needs.\nStep 1 - Pricing Models By using the right pricing model for your workload resources, you pay the lowest price for that resource.\n100 Level Lab: This lab will introduce you to working with Savings Plans (SP\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab: This lab will will create a Savings Plan analysis dashboard. Using this you can completely analyze your usage and select the best commitment for your business. 200 Level Lab: This lab will introduce you to working with Reserved Instances (RI\u0026rsquo;s), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. Step 2 - Right Sizing This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. This lab will show you how to install the memory agent. "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/3_monitor_cost_usage/","title":"Monitor Cost and Usage","tags":[],"description":"","content":"Monitor Cost and Usage Allocate costs based on workload metrics Measure efficiency Goal: Measure workload efficiency Target: Within 6 months all Tier1 workloads must have efficiency dashboards, within 12months all Tier2 workloads must have efficiency dashboards. Best Practice: Allocate costs based on workload metrics Measures: % of workloads with dashboards Good/Bad: Good Why? When does it work well or not?: Ensures the organization is focusing on the correct metric (efficiency), and not the bill. Contact/Contributor: natbesh@amazon.com Costs Goal: Reduction in bill Target: Reduce the bill by x% in the next billing cycle Best Practice: Decommission Resources Measures: Reduction in bill Good/Bad: Bad Why? When does it work well or not?: Does not drive improvement or capability building, can stifle innovation and positive growth. Does not factor in outcomes - ignores efficiency. Can have false positives - goal can be achieved by doing nothing if next months usage decreases, or impossible to achieve if next months usage is significantly higher Contact/Contributor: natbesh@amazon.com Assign business value to costs \u0026amp; usage Tagging Goal: Add tags to all of our bill (where possible) Target: No more than $100 a month of taggable spend, is to be un-tagged Best Practice: Assign organization meaning to cost and usage Measures: % of tag-able bill untagged Good/Bad: Good Why? When does it work well or not?: Helps add meaning to cost and usage, allows for adjustment between effort/reward with the % of bill approach, instead of using resource count. Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_cost_estimation/3_review_estimate/","title":"Review Estimate","tags":[],"description":"","content":"Review Estimate Review the overall costs (for all 3 services) in the My Estimate page. Pricing Calculator provides total cost for first 12 months. If needed, you can edit/delete the configuration of the services added by clicking on the Action button in each service tile.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/4_failure_injection_ec2/","title":"Test Resiliency Using EC2 Failure Injection","tags":[],"description":"","content":"4.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service.\nIn Chaos Engineering we always start with a hypothesis. For this experiment the hypothesis is:\nHypothesis: If one EC2 instance dies, then availability will not be impacted\n[Optional] Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing:\nsingle region: WaitForWebApp shows completed (green) multi region: WaitForWebApp1 shows completed (green) Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane.\nThere are three EC2 instances with a name beginning with WebServerforResiliency. For these EC2 instances note:\nEach has a unique Instance ID All instances are running and healthy There is one instance per each Availability Zone Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open\nTo fail one of the EC2 instances, use the VPC ID as the command line argument replacing \u0026lt;vpc-id\u0026gt; in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for)\nLanguage Command Bash ./fail_instance.sh \u0026lt;vpc-id\u0026gt; Python python3 fail_instance.py \u0026lt;vpc-id\u0026gt; Java java -jar app-resiliency-1.0.jar EC2 \u0026lt;vpc-id\u0026gt; C# .\\AppResiliency EC2 \u0026lt;vpc-id\u0026gt; PowerShell .\\fail_instance.ps1 \u0026lt;vpc-id\u0026gt; The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down\n$ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \u0026quot;TerminatingInstances\u0026quot;: [ { \u0026quot;CurrentState\u0026quot;: { \u0026quot;Code\u0026quot;: 32, \u0026quot;Name\u0026quot;: \u0026quot;shutting-down\u0026quot; }, \u0026quot;InstanceId\u0026quot;: \u0026quot;i-0710435abc631eab3\u0026quot;, \u0026quot;PreviousState\u0026quot;: { \u0026quot;Code\u0026quot;: 16, \u0026quot;Name\u0026quot;: \u0026quot;running\u0026quot; } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one )\nRefresh it. (Note: it is usually more efficient to use the refresh button in the console, than to refresh the browser)\nObserve the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated.\n4.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n4.2.1 System availability Refresh the service website several times. Note the following:\nWebsite remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id) Also note the availability_zone value when you refresh. You can see that requests are being handled by the EC2 instances in only two Availability Zones, while the EC2 instance in the third zone is being replaced Availability can also be measured programmatically using Amazon CloudWatch Synthetics canaries. A canary has already been created as part of the WebServersforResiliencyTesting stack. The canary has been configured to send a simple GET request to the application endpoint at 1 minute intervals.\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation click on the WebServersforResiliencyTesting stack click on the Outputs tab Open the URL for WorkloadAvailability in a new window View canary run data to see if there are any failed runs due to workload unavailability 4.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance.\nGo to the Target Groups console you already have open (or click here to open a new one )\nIf there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer Click on the Targets tab and observe:\nStatus of the instances in the group. The load balancer will only send traffic to healthy instances.\nWhen the auto scaling launches a new instance, it is automatically added to the load balancer target group.\nClick here to see an example of what you might expect to see: In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic.\nNote the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify.\nFrom the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts.\n4.2.3 Auto scaling Auto scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS.\nGo to the Auto Scaling Groups console you already have open (or click here to open a new one )\nIf there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting Click on the Activity tab and observe the sequence of events\nClick here to see an example of what you might expect to see: The screen cap below shows that all three instances were successfully started at 22:48\nAt 23:44 the instance targeted by the script was put in draining state and a new instance ending in \u0026hellip;55ee was started to replace it.\nDraining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance.\nLearn more: After the lab see this blog post for more information on draining. Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your workload.\nLearn more: After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances 4.3 [Optional] EC2 failure injection using AWS Fault Injection Simulator (FIS) You can also use AWS FIS to simulate failure of an EC2 instance. This step is optional. You will get experience using AWS FIS later during the RDS failure experiment and application failure experiments.\nIf you are running this lab as part of a live workshop, then skip this step and come back to it later if you wish\nClick here for instructions to simulate EC2 instance failure using AWS FIS: As in section 4.1, you will simulate a critical problem with one of the three web servers used by your service, but using AWS FIS. You can create an experiment template and use this template to run failure injection experiments on your resources.\n4.3.1 Create experiment template Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane.\nThere are three EC2 instances with a name beginning with WebServerforResiliency. For these EC2 instances note:\nEach has a unique Instance ID There is one instance per each Availability Zone All instances are healthy Select the checkbox next to any one of the WebServerforResiliency EC2 instances, then click the Tags tab.\nVerify that there is a tag with Key = Workshop and Value = AWSWellArchitectedReliability300-ResiliencyofEC2RDSandS3 Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open\nNavigate to the AWS FIS console at http://console.aws.amazon.com/fis and click Experiment templates in the left pane.\nClick on Create expermient template to define the type of failure you want to inject.\nEnter Experiment template for EC2 resiliency testing for Description and EC2-resiliency-testing for Name. For IAM role select WALab-FIS-role.\nScroll down to Actions and click Add action.\nEnter terminate-instance for the Name. Under Action type select aws:ec2:terminate-instances and click Save.\nScroll down to Targets and click Edit next to Instances-Target-1 (aws:ec2:instance).\nUnder Target method, select Resource tags and filters. Select Count for Selection mode and enter 1 under Number of resources. This ensures that FIS will only terminate one instance.\nScroll down to Resource tags and click Add new tag. Enter Workshop for Key and AWSWellArchitectedReliability300-ResiliencyofEC2RDSandS3 for Value. These are the same tags that are on the EC2 instances used in this lab. Click Save. This ensures that EC2 instances launched as part of this lab are the only ones selected for failure injection.\nYou can choose to stop running an experiment when certain thresholds are met, in this case, using CloudWatch Alarms under Stop condition. For this lab, you can leave this blank.\nClick Create experiment template.\nIn the warning pop-up, confirm that you want to create the experiment template without a stop condition by entering create in the text box. Click Create experiment template.\n4.3.4 Run the experiment Click on Experiment templates from the menu on the left.\nSelect the experiment template EC2-resiliency-testing and click Actions. Select Start experiment.\nYou can choose to add a tag to the experiment if you wish to do so.\nClick Start experiment.\nIn the pop-up, type start and click Start experiment.\nGo to the EC2 Instances console which you already have open (or click here to open a new one )\nPeriodically refresh it. (Note: it is usually more efficient to use the refresh button in the console, than to refresh the browser)\nObserve the status of the instances listed. As shown in the screen cap below, one of the instances will show shutting down and will ultimately transition to terminated.\nRevisit section 4.2 to observe the system response to the EC2 failure.\n4.4 EC2 failure injection - conclusion By deploying multiple servers and using Elastic Load Balancing, the workload can suffer the loss of a server but experience no availability disruption. This is because user traffic is automatically routed to the healthy servers and Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.\nOur hypothesis is confirmed:\nHypothesis: If one EC2 instance dies, then availability will not be impacted\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_manage_workload_risks_with_opscenter/4_cleanup/","title":"Teardown","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nCleaning up the CloudFormation Stack Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack WA-risk-management, and delete the stack. Cleaning up Lambda functions Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/lambda/ Select the wa-risk-tracking function, click on Actions and Delete. Select the wa-update-workload function, click on Actions and Delete. Cleaning up OpsItems Depending on the number of workloads defined and documented in the AWS WA Tool and the number of best practices missing in each workload, the walkthrough of the solution in this lab could potentially create a large number of OpsItems. You can set the status of the OpsItems to Resolved manually from the console, or you can use this Python script that uses Boto3 to set the status of all OpsItems with the source of Well-Architected to Resolved.\nTo use the cleanup script, make sure you have Python 3 and the AWS SDK for Python (Boto3) installed. The environment must also be configured with AWS credentials to make API calls to Systems Manager. The script requires one argument - the AWS Region used to walk through this lab.\npython3 clear_OpsItems.py us-east-1 NOTE: In the command above, replace us-east-1 with the AWS Region you used for this lab.\nThank you for using this lab. "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_vpc_flow_logs_analysis_dashboard/4_teardown/","title":"Teardown","tags":[],"description":"","content":"To perform a teardown for this lab, perform the following steps:\nDelete QuickSight dashboard, Athena dataset\nGo to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack you crated with below template to delete and click Delete Parquet: vpc_flowlogs_quicksight_multi_view_template.yaml CSV: vpc_flowlogs_quicksight_template.yaml In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete Athena DB, Table, View, Lambda Function and Cloudwatchwatch rule\nGo to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack you crated with below template to delete and click Delete Parquet: vpc_athena_db_table_view_lambda_parquet.yaml, OR CSV: vpc_athena_db_table_view_lambda.yaml In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete VPC Flow Logs from VPC:\nCSV: Please login to the AWS account where you have executed cloudformation template to create VPC Flow Logs Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack you crated with template CSV: vpc-flow-logs-custom.yaml to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events OR\nParquet: Please login to the AWS account where you have enabled VPC flow logs on the desired VPC. Navigate to the CloudShell service and execute below api command to delete the flow logs. Replace \u0026lt;fl-11223344556677889\u0026gt; with your flow log id.\naws ec2 delete-flow-logs --flow-log-id \u0026lt;fl-11223344556677889\u0026gt; Please login to the AWS account where you have created S3 bucket via AWS Console . Delete S3 bucket you created for storing VPC Flow Logs.\nX Congratulations! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/4_application_layer_defence/","title":"Application layer defence","tags":[],"description":"","content":"In this section we will tighten security using AWS WAF further to mitigate the risk of vulnerabilities such as SQL Injection, Distributed denial of service (DDoS) and other common attacks. WAF allows you to create your own custom rules to decide whether to block or allow HTTP requests before they reach your application.\n4.1. Identify the risk of vulnerabilities. A SQL Injection attack consists of insertion of a SQL query via the input data to the application. A successful SQL injection exploit can be capable or reading sensitive data from a database, or in extreme cases data modification/deletion.\nOur current API retrieves data from RDS for MySQL and relies on the user interacting via CloudFront. However, it is still possible for malicious SQL code to be injected into a web request, which could result in unwanted data extraction.\nAs a simple example, simply add \u0026lsquo;or 1=1\u0026rsquo; at the end of your CloudFront domain name as shown:\nhttps://Your_CloudFront_Domain_Name/?id=1 or 1=1 As you can see from the output, using this simple SQL injection could result in an attacker gaining access to all the data in our database:\nThis section of the lab will focus on some techniques which work to block web requests that contain malicious SQL code or SQL injection using AWS WAF.\n4.2. Working with SQL injection match conditions. From CloudFormation, locate the second stack which you deployed. On the stack Outputs tab, click WAFWebACLG to go to Web ACLs to review Rules in WAF. This web ACL is associated with CloudFront. Go to the Rules tab and select add managed rule groups as shown: Expand the AWS managed rule groups section and enable the SQL database rules as shown: By doing this we are adding rules that allow you to block request patterns associated with exploitation specific to SQL databases, such as SQL injection attacks. Make sure you select Add rules at the bottom of the screen to proceed to the next stage.\nIt is possible to assign priorities based on the rules which you specify. As you only have one rule at this moment, we can skip this configuration. Click Save: You should now be able to see the AWS-AWSManagedRulesSQLiRuleSet added to web ACL as shown: We can now rerun our test again, which should hopefully produce a different output. Use your CloudFrontEndpoint to run the same query as before, inclusive of the injection attack at the end. This can be done in either a web-browser or your Cloud9 IDE environment using the script that we have provided previously:\nIf your configuration is correct, you should now see a Response code: 403. This means that WAF has blocked this request as malicious code has been detected in the input.\nWe can now check that the blocked request has registered in our ACL metrics. To do this, go to the Overview in WAF console to see the metrics of ACL. You can confirm your request was blocked by WAF from this metrics. Click AWS-AWSManagedRulesSQLiRuleSet BlockedRequests to see only blocked request by SQL database. Note that your output may differ from the screenshot below depending on the amount of blocked requests that you sent. END OF SECTION 4\n"},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/100_labs/100_automating_serverless_best_practices_with_dashbird/4_teardown/","title":"Tear down","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\n1. Delete CloudFormation stacks Go to aws-well-architected-labs/static/wapartners/100_Automating_Serverless_Best_Practices_with_Dashbird/Code/oncall-health-sample-app directory path and run teardown_script.sh. cd aws-well-architected-labs/static/wapartners/100_Automating_Serverless_Best_Practices_with_Dashbird/Code/oncall-health-sample-app bash teardown_script.sh END OF SECTION 4\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_8_tag_policies/4_teardown/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\nCreated a Tag Policy (Cost Allocation) Created Tagged and Untagged EC2 Resources Navigate to the AWS Organizations service using the navigation bar and click the link for the account in which you applied the policy Select “Policies” beneath the Account details box. Underneath Tag policies, within “Attached policies” you should see the name of the tag policy you created. Select the radio button next to the name of the tag policy, and click “Detach” In the left-hand panel navigate to Policies and select Tag Policies Click the box next to the tag policy you created. Under actions select “Delete policy” Navigate to the EC2 console, select the EC2 instances you created and select \u0026ldquo;Terminate instance\u0026rdquo; X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/exportimport_utility/","title":"Export and Import Workload Utility","tags":[],"description":"","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction The purpose of this lab is to teach you how to use the AWS SDK for Python (Boto3) to retrieve all of the questions, best practices, and improvement plan links for each lens and export the results to a JSON file or import all of the workload properties from a JSON file into the Well-Architected tool.\nPrerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to use Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Python 3.9+ AWS SDK for Python (Boto3) installed Costs: There are no costs for exporting or importing Well-Architected Reviews Steps: Configure Environment Python Code Script usage examples X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/configure-websites/","title":"Configure Websites","tags":[],"description":"","content":"Configure the Active-Primary Website You will need the Amazon CloudFormation output parameter values from the Active-Primary stack to complete this section. For help, refer to the CloudFormation Outputs Primary Region section of the workshop.\n1.1 Using your favorite editor, create a new file named config.json file. Initialize the document to the template provided below. Next, set the host property equal to the APIGURL output value from the Active-Primary CloudFormation stack. Remove the trailing slash (/) if one is present. Remove the curly braces. Finally, set the region property to us-east-1.\n{ \u0026#34;host\u0026#34;: \u0026#34;{{Replace with your APIGURL copied from above}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34; } Your final config.json should look similar to this example.\n{ \u0026#34;host\u0026#34;: \u0026#34;https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/Production\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34; } Upload the configuration to Amazon S3 2.1 Click S3 to navigate to the dashboard.\n2.2 Click the bucket link that begins with active-primary-uibucket- .\n2.3 Click the Upload button.\n2.4 Click the Add Files button and specify the config.json file from the previous step.\n2.5 In the Permissions Section section. Select the Specify Individual ACL permissions radio button. Enable the Read checkbox next to Everyone (public access) grantee.\n2.6 Enable the I understand the effects of these changes on the specified objects. checkbox. Click the Upload button.\nConfigure the Passive-Secondary Website You will need the Amazon CloudFormation output parameter values from the Passive-Secondary stack to complete this section. For help, refer to the CloudFormation Outputs Secondary Region section of the workshop.\n3.1 Using your favorite editor, open the config.json file you just created on your local machine. Modify the document to set the host property equal to the APIGURL output value from the Passive-Secondary CloudFormation stack. Remove the trailing slash (/) if one is present. Remove the curly braces. Finally, set the region property to us-west-1.\n{ \u0026#34;host\u0026#34;: \u0026#34;{{Replace with your APIGURL copied from above}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-1\u0026#34; } Your final config.json should look similar to this example.\n{ \u0026#34;host\u0026#34;: \u0026#34;https://xxxxxxxx.execute-api.us-west-1.amazonaws.com/Production\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-1\u0026#34; } Upload the configuration to Amazon S3 4.1 Click S3 to navigate to the dashboard.\n4.2 Click the bucket name that begins with passive-secondary-uibucket-.\n4.3 Click the Upload button.\n4.4 Click the Add Files button and specify the config.json file from the previous step.\n4.5 In the Permissions Section section. Select the Specify Individual ACL permissions radio button. Enable the Read checkbox next to Everyone (public access) grantee.\n4.6 Enable the I understand the effects of these changes on the specified objects. checkbox. Click the Upload button.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/failover/","title":"Failover to Secondary","tags":[],"description":"","content":"When a regional service event affects the Unicorn application in the primary region N. Virginia (us-east-1), you want to fail-over to the secondary region N. California (us-west-1).\nWe assume a regional service event has occurred. In this section, we will manually fail over the application to the secondary region, N. California (us-west-1). You can consider using Amazon Route53 or other DNS (Domain Name Services) fail-over routing in a real-world scenario. You can further automate by subscribing to application notifications.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/failover/","title":"Failover to Secondary","tags":[],"description":"","content":"When a regional service event affects the Unicorn application in the primary region N. Virginia (us-east-1), you want to fail-over to the secondary region N. California (us-west-1).\nWe assume a regional service event has occurred. In this section, we will manually fail over the application to the secondary region, N. California (us-west-1). You can consider using Amazon Route53 or other DNS (Domain Name Services) fail-over routing in a real-world scenario. You can further automate by subscribing to application notifications.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/failover/promote-app/","title":"Promote App in Secondary Region","tags":[],"description":"","content":"1.1 Navigate to CloudFormation Stacks in N. California (us-west-1) region.\n1.2 Select the Pilot-Secondary stack and click Update.\n1.3 Chose Use current template and click Next to continue.\n1.4 Update the IsPromote parameter to yes and click Next to continue.\n1.5 Scroll to the bottom of the page, click the checkbox to acknowledge IAM role creation, and then click Update stack.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/failover/promote-app/","title":"Promote App in Secondary Region","tags":[],"description":"","content":"1.1 Navigate to CloudFormation Stacks in N. California (us-west-1) region.\n1.2 Select the Warm-Secondary stack and click Update.\n1.3 Chose Use current template and click Next to continue.\n1.4 Update the IsPromote parameter to yes and click Next to continue.\n1.5 Scroll to the bottom of the page, click the checkbox to acknowledge IAM role creation, and then click Update stack.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/verify-website/","title":"Verify Website","tags":[],"description":"","content":"Primary Region 1.1 Click CloudFormation Stacks to navigate to the dashbaord in the N. Virginia (us-east-1) region.\n1.2 Select BackupAndRestore under Stacks.\n1.3 Click on the Outputs link.\n1.4 Click on the WebsiteURL link. You will see The Unicorn Shop - us-east-1.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/4_limit_network_access/","title":"Step 4 - Limit Network Access","tags":[],"description":"","content":"In this exercise we will use AWS Trusted Advisor\u0026rsquo;s basic security checks to identify remote access risks associated with the EC2 instance and fix them. Furthermore, we will use AWS Systems Manager\u0026rsquo;s feature to secure our remote access.\nNote: For this lab, it is assumed that Microsoft Windows based EC2 instance is already created with default settings. For instructions to create EC2 Instance please follow the link .\nFrom the AWS console, click Services and select Trusted Advisor.\nOn the Trusted Advisor console click on Refresh All icon on the right side as shown below.\nYou will notice that there are few risks identified by Trusted Advisor. Click on Security tab. You will notice findings for security groups about open network access. Now let\u0026rsquo;s fix these issues. Click on one of the findings, which expand with more details. You will also see the list of security group names that have this particular security issue. Click on the Security Group Name in the list. It will open a Security Group console on a new browser tab. On Security Groups page, click on the Inbound rules. You will notice that there is one rule allowing open access to port 3389 from the internet, which is not a good practice. Therefore, we need to remove this rule. Click on Edit inbound rules.\nClick Delete associated with the open port of 3389. Click Save rules, which will remove the rule permanently.\nNow go back to Trusted Advisor tab and click on the Refresh this check icon associated with the security risk. Trusted Advisor will re-run the check and will show green once it finds that the issue is fixed. Now to access the EC2 instance securely, we will be using AWS System\u0026rsquo;s Manager capability called Session Manager.\nFrom the AWS console, click Services and select Systems Manager.\nClick on Fleet Manager under Node Management on the menu at the left side of Systems Manager console.\nClick on Get Started.\nYou will see your instance(s) listed which are managed by Systems Manager.\nClick on the Settings tab.\nClick on Change account setting button in order to use an Advanced Instance Tier which allows you to use Session Manager capability.\nAccept the changes by clicking on the checkbox and click on Change setting.\nClick on Session Manager under Node Management on the menu at the left side of Systems Manager console.\nClick on Start Session on upper right.\nSelect the instance that you want to have access and then click Start session.\nA new tab will be opened and you will have the Microsoft Windows command prompt or Linux terminal window. To exit the session simply use relevant OS command or click Terminate on upper right side of the page. For more information please read the AWS User Guide: https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/4_teardown/","title":"Teardown","tags":[],"description":"","content":"If you\u0026rsquo;d like to disable the Monthly Reports, please follow the following steps:\nDisabling Monthly Reports Go to the billing dashboard: Click on the Billing Preferences from the left menu: Click Detailed Billing Reports [Legacy] and uncheck the box. Then Click Save Preferences. If you configured Save to S3 Bucket you can delete either the bucket or files to prevent any further charges.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST1 - \u0026ldquo;How do you implement cloud financial management?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/helpful_resources/","title":"Helpful Resources","tags":[],"description":"","content":"List of helpful resources available Copy a workload from one account or region to another Generate a custom WellArchitected Framework HTML Report Export Well-Architected content to XLSX Spreadsheet Export and Import Workload Utility "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_enable_security_hub/","title":"Enable Security Hub","tags":[],"description":"","content":"Last Updated: February 2021\nAuthor: Pierre Liddle, Principal Security Architect\nAWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There is a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. Costs Typically less than $1 per month if the account is only used for personal testing or training, and the tear down is not performed. AWS Config Pricing AWS Security Hub pricing AWS Pricing Steps: Enable AWS Security Hub Tear Down References \u0026amp; useful resources AWS Security Hub AWS Security Hub Documentation AWS Config AWS Config Documentation "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/4_deploying_sample_lambda_app/","title":"Deploying Sample Lambda application along with Well-Architected review","tags":[],"description":"","content":" The CloudFormation template that accompanies this lab requires the ability to create IAM Roles and AWS Lambda functions. If the account you are using does not have these capabilities, you will not be able to complete this lab.\nDeploy Sample Application CloudFormation Template Download the SampleLambdaAPIGWDeploy.yaml CloudFormation template to your machine. For this lab, you will need to use us-east-2 Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: SampleLambdaAPIGWDeploy.yaml Click Next\nFor Stack name use WALabDemoApp\nParameters\nLook over the Parameters and their default values.\nNone of these parameters need to be changed, but are available if you wish to try different settings\nLambdaStackName - Name of the Role Stack to reference outputs. This should be the same as the stack you deployed in the second step.\nWAWorkloadType - Example list of Workload types that could be used in your environment. For this lab we only allow these values:\nAPIGWLambda EC2WebApp EKSWebApp ECSWebApp WAWorkloadDescription - Description for WA Workload field\nWAWorkloadOwner - Who owns the WA workload\nWAEnvironment - The environment in which your workload runs. You must select either PRODUCTION or PREPRODUCTION\nAPIGWName - Name for the API Gateway\napiGatewayStageName - The stage name for the API Gateway\nAPIGWHTTPMethod - The method type for the deployed API\nSampleLambdaFunctionName - The name for the sample lambda function\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nThis template will take between 2-5 minutes to fully deploy.\nReview CloudFormation Outputs from stack creation Once deployed, you can click on the Outputs tab and find the various outputs from the Cloudformation. The WAWorkloadId is the WorkloadId from the Well-Architected Tool.\nThe link next to apiGatewayInvokeURL will show you the sample Lambda function responding via Amazon API Gateway. If you click on the link, it will show your IP address as reported by API Gateway headers as well as a link to the Well-Architected labs website.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_building_custom_aws_well-architected_reports_with_amazon_athena_and_amazon_quicksight/","title":"Level 300: Build custom reports of AWS Well-Architected Reviews","tags":[],"description":"","content":"Authors Tom McMeekin, Solutions Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: tommcm@amazon.com Introduction You can use the AWS Well-Architected Tool (AWS WA Tool) to review the state of your workloads and compare them to the latest AWS architectural best practices. Its API allows customers, ISVs, and AWS Partner Network to extend AWS Well-Architected functionality, best practices, measurements, and lessons learned into their existing architecture governance processes, applications, and workflows.\nBy building your own integrations with the AWS WA Tool, your enterprise can support a range of use cases, such as integrating AWS Well-Architected data into centralized reporting tools or ticketing and management solutions. The API can also be used to create automation for specific use cases.\nThis lab presents a simple approach for aggregating the data of the workload reviews into a central data lake repository. It helps teams to analyze their organization\u0026rsquo;s Well-Architected maturity across multiple AWS accounts and workloads and perform centralized reporting on high-risk issues (HRIs).\nArchitecture overview Many customers use multiple AWS accounts to provide administrative autonomy for their teams. The AWS WA Tool offers a simple way to share workloads with other AWS accounts. You can share a workload that you own in your account with AWS accounts used by other members of your review team or with a centralized AWS account. For more information, see sharing a Workload .\nAfter workloads are defined in the AWS WA Tool, AWS Lambda can poll the AWS Well-Architected Tool API to extract the raw data and store it in an Amazon Simple Storage Service (Amazon S3) bucket. AWS Glue crawlers are used to discover schema and store it in the AWS Glue Data Catalog. Amazon Athena is then used for data preparation tasks like building views of the workload report data. Amazon QuickSight can now query and build visualizations to discover insights into your Well-Architected Reviews. This pattern can extend the visibility of HRIs identified in the AWS WA Tool to enable custom visualizations and more insights. The central management account would typically be managed by a Cloud Center of Excellence (CCoE) team, who can advise and act on emerging HRIs across their entire AWS application portfolio.\nGoals Automatically extract Well-Architected workload data via the API Query and visualise the extracted Well-Architected workload data using Amazon QuickSight Prerequisites Define and document a workload in the AWS WA Tool. For better understanding, refer to AWS documentation. Create or use an existing S3 bucket where you will store the extracted AWS Well-Architected data. Permissions required Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler Create and publish QuickSight dashboards Steps: Extract workload data Catalog the workload data Query the workload data Visualize the workload data Clean up the deployment X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_building_custom_aws_well-architected_reports_with_amazon_athena_and_amazon_quicksight/4_visualize/","title":"Visualize the workload data","tags":[],"description":"","content":"Now that you can query your data in Amazon Athena, you can use Amazon QuickSight to visualize the results.\nGrant Amazon QuickSight access to Amazon Athena and your S3 bucket First, grant Amazon QuickSight access to the S3 bucket where your Well-Architected data is stored.\nSign in to the Amazon QuickSight console. In the upper right corner of the console, choose Admin/username, and then choose Manage QuickSight Choose Security and permissions. Under QuickSight access to AWS services, choose Add or remove. Choose Amazon Athena, and then choose Next. Give QuickSight access to the S3 bucket where you will store the extracted AWS Well-Architected data, e.g. \u0026ldquo;well-architected-reporting-blog\u0026rdquo;. Create your datasets Before you can analyze and visualize the data in QuickSight, you must create datasets for your Athena views and tables for each of the Athena views. Create QuickSight datasets for:\nwell_architected_workload_milestone_view well_architected_workload_lens_risk_view well_architected_reports_view To create a dataset, on the Datasets page, choose New dataset, and then choose Athena. Choose the Glue database that was created as part of the Glue Crawler creation step. Choose Directly Query your data. * Create your analysis Now create a single-page dashboard using the three Athena datasets you just created.\nIn the QuickSight console, choose Create Analysis. Choose one of the Athena datasets, and then choose Create Analysis. Choose the pencil icon to add the other Athena datasets. After you have added the datasets, they should be available for analysis. Build your dashboard Each dataset can be used to build visualizations for the following:\nNumber of conducted reviews Number of HRIs Top high-risk questions Top high-risk workloads Mix of risk across reviews, lenses, and pillars Trends of risks across reviews, milestones, lenses, and pillars This list not exhaustive. At AWS, we look forward to seeing the visualizations you\u0026rsquo;ll build for your organization. Below shows an example of a dashboard:\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/4_test_restore/","title":"Test Restore","tags":[],"description":"","content":"A backup of a data source is useful only if data can be restored from it. If backups aren\u0026rsquo;t tested, you might find yourself in a situation where your workload has been impacted by an event and you need to recover data from your backups, but the backups are faulty and restoring data is not possible, or exceeds your RTO. To avoid such situations, backups taken should always be tested to ensure they can be used to recover data.\nIn this lab, you will leverage AWS Lambda to automatically test all backups created to ensure recovery is successful, and clean up any resources that were created during the restore test process to save on cost. This will ensure you are aware of any faulty backups that might be unusable to recover data from. Automating this process with notifications enabled will ensure there is minimal operational overhead and that the Operations teams are aware of backup and restore statuses.\nWhen testing recovery, it is important to define the criteria for successful data recovery from the restored resource. This will depend on a variety of factors such as the data source, the type of data, the margin for error, etc. Organizations and workload owners are responsible for defining this success criteria.\nThe EC2 Instance that was created as part of this lab is running a simple web application. For this use-case, I have determined that data recovery is successful if the application is running on the restored resource as well. If the restored resource is missing any application critical files, the healthchecks made against the restored resource will fail, indicating an issue with the backup.\nTesting Recovery For the purpose of this lab, we will simulate the action performed by AWS Backup when creating backups of data sources by creating an on-demand backup to see if the backup is successful. Once the backup is completed, you will receive a notification stating that the backup job has completed and the lambda function will get invoked. The Lambda function will make API calls to start restoring data from the backup that was created. This will help ascertain that the backup is good. Once the restore process has been completed, you will receive another notification confirming this, and the lambda function will get invoked again to clean up new resources that were created as part of the restore. Once the cleanup has been completed, you will receive one last notification confirming cleanup.\nUse your administrator account to access the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on CREATE AN ON-DEMAND BACKUP in the middle of the screen.\nUnder RESOURCE TYPE, select EC2. Paste in the Instance ID obtained from the Output of the CloudFormation Stack.\nUnder BACKUP WINDOW, ensure that the CREATE BACKUP NOW option is selected.\nUnder EXPIRE, select the option DAYS AFTER CREATION and enter 1 for the value for this lab. This will ensure that the backup is deleted after 1 day.\nUnder Backup Vault, select the BACKUP-LAB-VAULT.\nLeave the default IAM role selected.\nClick CREATE ON-DEMAND BACKUP.\nClick on JOBS from the menu on the left and select BACKUP JOBS. You should see a new backup job started with the status of RUNNING. Click on the RESTORE JOBS tab, there shouldn\u0026rsquo;t be any restore jobs running.\nPeriodically refresh the console until the STATUS changes to COMPLETED. It should take about 5-10 minutes to complete.\nAfter the job is completed, click on the JOB ID and view the DETAILS. You should see the Recovery Point ARN that was created, the RESOURCE ID for which the backup was created, and the RESOURCE TYPE for which the backup was created.\nMonitor your email to see if you receive a Notification from AWS Backup. Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the backup job has completed.\nThe SNS Topic that was created as part of the CloudFormation stack has been configured as a trigger for the Lambda function. When AWS Backup publishes a new message when a backup or restore job is completed, the Lambda function gets invoked.\nLet\u0026rsquo;s take a look at the relevant section of the Lambda function code to understand what will happen once the backup job is completed and a notification has been received. You can view the full Lambda function code here .\nThe Lambda function first obtains the recovery point restore metadata for the recovery point that was created when the on-demand backup job was initiated.\nmetadata = backup.get_recovery_point_restore_metadata( BackupVaultName=backup_vault_name, RecoveryPointArn=recovery_point_arn ) Once the recovery point restore metadata has been retrieved, the function will then use this to make an API call to AWS Backup to start a restore job.\nrestore_request = backup.start_restore_job( RecoveryPointArn=recovery_point_arn, IamRoleArn=iam_role_arn, Metadata=metadata[\u0026#39;RestoreMetadata\u0026#39;] ) Go back to JOBS and switch to the RESTORE JOBS tab. You should see a RESTORE JOB running. The lambda function that was created as part of this lab has requested a restore job from AWS Backup. This is to ensure data recovery from the backup is successful.\nPeriodically refresh the console until the STATUS changes to COMPLETED. It should take about 5-10 minutes to complete.\nAfter the job is completed, click on the JOB ID and view the DETAILS. You should see the Recovery Point ARN from which the restore was tested, the RESOURCE ID of the newly created EC2 Instance, and the RESOURCE TYPE for which the restore was created.\nNote down the RESOURCE ID of the newly created EC2 Instance and verify that it exists from the EC2 Console - https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:sort=instanceState . Note down the public IP of the new EC2 Instance.\nMonitor your email to see if you have received a Notification from AWS Backup confirming the restore job was successful. Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the restore job has completed.\nWhile waiting for the notification, let\u0026rsquo;s take a look at the relevant sections of the Lambda function code to see what happens when a restore job is completed. You can view the full Lambda function code here .\nAfter receiving confirmation from AWS Backup that the restore job has completed successfully, the Lambda function will verify data recovery. To do this, an API call is made to retrieve the public IP address of the new EC2 Instance. It makes an HTTP GET request to the new EC2 Instance to check if the application is running. If a valid response (200 in this case) is received, it is ascertained that data recovery was successful as per the recovery success criteria that was established earlier in this section. The Lambda function will then make an API call to EC2 to terminate the new EC2 Instance to save on cost. You can manually verify this as well by visiting the following URL:\nhttp://\u0026lt;PUBLIC_IP_OF_THE_NEW_INSTANCE\u0026gt;/\ninstance_details = ec2.describe_instances( InstanceIds=[ instance_id ] ) public_ip = instance_details[\u0026#39;Reservations\u0026#39;][0][\u0026#39;Instances\u0026#39;][0][\u0026#39;PublicIpAddress\u0026#39;] http = urllib3.PoolManager() url = public_ip resp = http.request(\u0026#39;GET\u0026#39;, url) print(resp.status) if resp.status == 200: print(\u0026#39;Valid response received. Data recovery validated. Proceeding with deletion.\u0026#39;) print(\u0026#39;Deleting: \u0026#39; + instance_id) delete_request = ec2.terminate_instances( InstanceIds=[ instance_id ] ) else: print(\u0026#39;Invalid response. Data recovery is questionable.\u0026#39;) Monitor your email to see if you have received a Restore Test Status notification confirming the deletion of the newly created resource. Check the EC2 Console to verify that the new EC2 Instance has been terminated. - https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Volumes:sort=size Use your administrator account to access the AWS CloudWatch console - https://console.aws.amazon.com/cloudwatch/home?region=us-east-1 Click on LOGS from the menu on the left side.\nFor filter, paste the following string after replacing the value for the name of the CloudFormation stack that was created as part of this lab.\n/aws/lambda/RestoreTestFunction-\u0026lt;YOUR CLOUDFORMATION STACK NAME\u0026gt;\nClick on the LOG STREAM and view the output of the Lambda function\u0026rsquo;s execution to understand the different steps performed by the function to automate this process.\nReview of Best Practices Implemented Identify all data that needs to be backed up and perform backups or reproduce the data from sources: Back up important data using Amazon S3, Amazon EBS snapshots, or third-party software. Alternatively, if the data can be reproduced from sources to meet RPO, you may not require a backup.\nPerform data backup automatically or reproduce the data from sources automatically: Automate backups or the reproduction from sources using AWS features (for example, snapshots of Amazon RDS and Amazon EBS, versions on Amazon S3, etc.), AWS Marketplace solutions, or third-party solutions.\nPerform periodic recovery of the data to verify backup integrity and processes: Validate that your backup process implementation meets Recovery Time Objective and Recovery Point Objective through a recovery test.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/trusted-advisor-dashboards/dashboards/4_update-dashboard/","title":"Optional Steps","tags":[],"description":"","content":"Optional Step 1: Update dashboard template Pull latest dashboard version from template:\naws quicksight update-dashboard --aws-account-id {account} --region {region} --cli-input-json file://update-dashboard-input.json Update published version of dashboard:\naws quicksight list-dashboard-versions --aws-account-id {account} --region {region} --dashboard-id ta-organizational-view --query \u0026#39;DashboardVersionSummaryList[-1].VersionNumber\u0026#39; | xargs -I {} aws quicksight update-dashboard-published-version --aws-account-id {account} --dashboard-id ta-organizational-view --version-number {} Apply the latest pulled changes to the deployed dashboard with this CLI command:\naws quicksight update-dashboard-published-version --region {region} --aws-account-id {account} --dashboard-id ta-organizational-view --version-number {version} Alternatively automation script can be used for update\nThe Cloud Intelligence Dashboards automation repo is an optional way to create the Cloud Intelligence Dashboards using a collection of setup automation scripts. The supplied scripts allow you to complete the workshops in less than half the time as the standard manual setup.\nFollow the How to use steps for installation and dashboard deployment. We recommend to use AWS CloudShell for automated deployment\nOptional Step 2: Add new TA Organizational view report By default Trusted Advisor (TA) refreshes checks on weekly basis. To get historical progress and trends visualized on TAO Dashboard we recommend to upload new TA Organizational View reports regularly, for example bi-weekly or monthly\nCreate Organizational View report\nFor the step by step guide please follow the documentation Please choose JSON format for report You can select either all accounts and Trusted Advisor checks or filter by particular checks or Organizational Unit (OU). There is no limitation from dashboard deployment point of view You can select certain accounts but please ensure you maintain consistency in following reports for periodic refreshes to avoid data mismatch.\nDownload Organizational View report\nFor step by step guide please follow the documentation Unzip downloaded report\nUpload downloaded report to the reports folder in the S3 bucket\nMake sure you upload unzipped folder to S3 bucket Open and Refresh ta-organizational-view dataset in QuickSight Optional Step 3: Switch data collection method There are 2 supported data collection methods:\nTrusted Advisor Organizational View - provides an easy way to collect Trusted Advisor checks for all accounts in your AWS Organizations without need to provision any additional resources. Only manual data refresh is supported. Trusted Advisor API via deployment of Optimization Data Collection lab - provides an automated way to collect Trusted Advisor checks for all accounts in your AWS Organizations via deployment of required AWS resources from provided AWS CloudFormation templates. Supports automated data refresh. If you deployed TAO Dashboard with Trusted Advisor Organizational View as data collection method you can switch at any time to Trusted Advisor API via deployment of Optimization Data Collection lab . For that:\nMakes sure you\u0026rsquo;ve deployed Optimization Data Collection lab as prerequisite step Depending on which deployment option you used during initial deployment of TAO Dashboard follow the steps below: Automated or CloudFormation deployment Launch AWS CloudShell Clone repository (if haven\u0026rsquo;t done before) and navigate to legacy/tao folder: git clone https://github.com/aws-samples/aws-cudos-framework-deployment cd aws-cudos-framework-deployment/legacy/tao Run the following script: ./shell-script/tao.sh --action=change-source-location Manual deployment Change {s3FolderPath} in athena-table.json file created in Prepare configuration step to S3 URI path to ta-data folder in optimization data bucket created in the Optimization Data Collection lab . The path should be similar to s3://costoptimizationdata{account_id}/optics-data-collector/ta-data/ Update Glue table is the metadata definition with following command: aws glue update-table --region {region} --cli-input-json file://athena-table.json Open and Refresh ta-organizational-view dataset in QuickSight X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/4_perform_review/","title":"Performing a data-driven review","tags":[],"description":"","content":"\nOverview Now that we have defined a workload, we will review the question COST 6. How do you meet cost targets when you select resource type, size and number. Defining a workload generates an event called CreateWorkload that Amazon EventBridge receives, which will invoke AWS Lambda function that collects cost optimization data from AWS Compute Optimizer and AWS Trusted Advisor. Then this data will be automatically available before the review, reviewer and customer could have a data-driven cost optimization review.\nTo start AWS Well-Architected Framework Review, click Continue reviewing and select AWS Well-Architected Framework Lens. Choose Cost Optimization pillar and click question COST 6. How do you meet cost targets when you select resource type, size and number? For COST 6 question, the following best practices are what a reviewer is going to validate with customers. Now you assume that you are a reviewer and may create your own question to evaluate if customers have impletemented them. Perform cost modeling Select resource type, size, and number based on data Select resource type, size, and number automatically based on metrics Without having any data, you can ask \u0026ldquo;How do you match instance types and sizes to your workload at the lowest cost?\u0026rdquo;. If so, customer may be able to answer \u0026ldquo;We refer to vCPU mapping for each Amazon EC2 instance type and then select the lowest priced instance.\u0026rdquo;.\nThe mechanism that customer shared is not too bad but it does not tell whether the current resources are actually optimized based on the current usage from a cost and performance standpoint. Then, the reviewer will try to understand the current state from this customer by asking a follow-up question like ”How do you know if vCPU, memory, storage are currently optimized for your workload?”. To answer this question, customer should have collected the historical utilization data prior to the review and the conversation end.\nScroll down to the bottom of Well-Architected Tool. Now data is automatically available for the reviwer and customers. The reviewer can rephrase the previous question based on the data thta AWS Compute Optimizer and AWS Trusted Advisor provided. Rather than asking how do you know if your resources are currently optimized, reviewer can have a data-driven cost optimization review with customer.\nThe reviewer may ask \u0026ldquo;Your largest instance is CPU over-provisioned and we could potentially save you 74% cost on that with t4g.xlarge instance type. Customer, how do you feel about us working with you to make it happen?\u0026rdquo;. Now that customer can see an immediate cost benefit from data, this will enable customer to prioritize saving opportunities depending on business criticality. With these insights prior to a review, the reviewer managed to take the previous conversation to the next level.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/4_save_milestone/","title":"Saving a milestone","tags":[],"description":"","content":"Overview A milestone records the state of a workload at a particular point in time.\nSave a milestone after you initially complete all the questions associated with a workload. As you change your workload based on items in your improvement plan, you can save additional milestones to measure progress.\nA best practice is to save a milestone when you first do a new W-A review, or every time you make improvements to a workload.\n1. Create a Milestone Using the create-milestone API we will save our current progress as our first milestone aws wellarchitected create-milestone --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --milestone-name Rev1 The return value will be the WorkloadId and the Milestone number assigned. 2. List all milestones If we want to see all milestones associated with a workload Using the list-milestones API you can see all milestones associated with a workload aws wellarchitected list-milestones --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --max-results 50 This will return a summary of the milestones you have created for the workload. 3. Retrieve the results from a milestone Using the get-milestone API , you can see all of the metadata for a specific milestone: aws wellarchitected get-milestone --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --milestone-number 1 This will return the complete workload information for the workload milestone (such as when it was created and what lenses were included) 4. List all question answers based from a milestone If you want to see all of the best practices for when the milestone was created, you can use Using the list-answers API , you can see all of the best practices for when the milestone was created: aws wellarchitected list-answers --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --milestone-number 1 This will return a large json structure with all of the questions and best practices for each pillar. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/4_impact_of_failures_sharding/","title":"Impact of failures with sharding","tags":[],"description":"","content":"Break the application You will now introduce the poison pill into the workload by including the bug query-string with your requests and see how the updated workload architecture handles it. As in the previous case, imagine that customer Alpha triggered the bug in the application again.\nInclude the query-string bug with a value of true and make a request as customer Alpha. The modified URL should look like this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\u0026bug=true (but using your own URL from the CloudFormation stack Outputs)\nThis should result in an Internal Server Error response on the browser indicating that the application has stopped working as expected on the instance that processed this request\nAt this point, there is one healthy instance still available on the shard so other customers mapped to that shard are not impacted. You can verify this by opening another browser tab and using the URL for customer Bravo and without the bug query string.\nJust like before, customer Alpha, not aware of this bug in the application, will retry the request.\nRefresh the page to simulate this as you did before. This request is routed to the other healthy instance in the shard. The bug is triggered again and the other instance goes down as well. The entire shard is now affected. All requests to this shard will now fail because there are no healthy instances in the shard. You can verify this by sending requests for customers Alpha or Bravo. No matter how many times the page is refreshed, you will see a 502 Bad Gateway.\nSince customers can only make requests to the shard they are assigned to, customers Charlie, Delta, Echo, Foxtrot, Golf, and Hotel are not affected by customer Alpha’s actions.\nVerify this by making requests using the URLs for these customers (obtained from the CloudFormation stack Outputs). The impact is localized to a specific shard, shard 1 in this case, and only customers Alpha and Bravo are affected. The scope of impact has now been reduced so that only 25% of customers are affected by the failure induced by the poison pill.\nIn a sharded system, the scope of impact of failures can be calculated using the following formula:\nFor example if there were 100 customers, and the workload was divided into 10 shards, then the failure of any 1 shard will only impact 10% of customers.\nWith this sharded architecture, the scope of impact is reduced by the number of shards. Here with four shards, if a customer experiences a problem, then the shard hosting them might be impacted, as well as all of the other customers on that shard. However, that shard represents only one fourth of the overall service. Since this is just a lab we kept it simple with only four shards, but with more shards, the scope of impact decreases further. Adding more shards requires adding more capacity (more workers). But in the next steps of this lab you will learn how with shuffle sharding, we can do exponentially better without adding extra capacity.\nVerify workload availability You can look at the AvailabilityDashboard to see the impact of the failure introduced by customer Alpha across all customers.\nSwitch to the tab that the AvailabilityDashboard opened. (You can also retrieve the URL from the CloudFormation stack Outputs).\nYou can see that the introduction of the poison-pill and subsequent retries by customer Alpha has only impacted customer Bravo as the canaries for these two customers are unable to make successful requests to the workload.\nNotice that the impact is localized to a specific shard and the other customers are not impacted by this. With sharding, only 25% of customers are impacted. NOTE: You might have to wait a few minutes for the dashboard to get updated. Fix the application As in the previous section, Systems Manager will be used to fix the application and return functionality to the users that are affected - Alpha and Bravo. The Systems Manager Document restarts the application on the selected instances.\nGo to the Outputs section of the CloudFormation stack and open the link for “SSMDocument”. This will take you to the Systems Manager console.\nClick on Run command which will open a new tab on the browser\nScroll down to the Targets section and select Specify instance tags\nEnter Workload for the tag key and WALab-shuffle-sharding for the tag value. Click Add.\nScroll down to the Output options section and uncheck the box next to Enable an S3 bucket. This will prevent Systems Manager from writing log files based on the command execution to S3.\nClick on Run\nYou should see the command execution succeed in a few seconds\nOnce the command has finished execution, you can go back to the application and test it to verify it is working as expected.\nUse the URL for customer Alpha (obtained from the CloudFormation stack Outputs) and make sure that the query-string bug is not included in the request. Refresh the page a few times to make sure responses are being received from 2 different EC2 instances. Repeat this process for the other customer that was affected - Bravo. Review the AvailabilityDashboard to make sure canary requests are succeeding and normal functionality has returned to customers Alpha and Bravo. You should see that SuccessPercent has returned to 100 for both customers.\nNOTE: You might have to wait a few minutes for the dashboard to get updated. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/4_adding_metrics_to_dashboard/","title":"Add metrics to Dashboard","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Now that we have a dashboard for our Linux EC2 instance, we can add an additional metric for available memory.\nLet\u0026rsquo;s add a new widget to our CloudWatch Dashboard. Click on the \u0026ldquo;Add Widget\u0026rdquo; button in the upper right corner. Click Line, then next Click Metrics and then Configure Select \u0026ldquo;PerfLab\u0026rdquo; under Custom Namespaces and then select the second metric group “ImageId, InstanceId, InstanceType” Make sure to search only for the InstanceId if you have multiple machines reporting metrics Find the “mem_used” and \u0026ldquo;mem_total\u0026rdquo; and click the check box next to it Click on “Graphed Metrics (2)” and then select “5 seconds” as the period Click on Create widget You should now see two widgets on your Dashboard You can drag the widgets around the screen and re-size them if you wish You can also change the time period, select 1h to show just the most recent metrics You can also set the auto-refresh rate for the Dashboard by using this pull-down. Select 10s Click the “Save Dashboard” button before proceeding to the next step. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/","title":"Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"How to configure an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of a Amazon Linux EC2 instance.","content":"Authors Eric Pullen, Performance Efficiency Lead Well-Architected Introduction This hands-on lab will guide you through creating an Amazon EC2 instance running Amazon Linux and then configuring a Amazon CloudWatch Dashboard to get aggregated views of the health and performance information for that instance. This lab should enables you to quickly get started with CloudWatch monitoring and explore account and resource-based views of metrics. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Monitor a Amazon Linux EC2 machine to identify CPU and memory bottlenecks Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes An IAM role in your AWS account Costs https://aws.amazon.com/cloudwatch/pricing/ You can create 3 dashboards for up to 50 metrics per month on the free tier and then it is $3.00 per dashboard per month This lab creates one dashboard, so the maximum cost would be $3.00 per month if you have already consumed the free tier. The default lab uses a t3.large EC2 instance which will consume approximately $3.00 for every day the lab is running The VPC that is created for this lab will build a Nat Gateway, and will consume $5.50 per day when deployed. Using defaults, the total cost of the lab would be at least $8.50 per day NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploying the infrastructure Deploying an instance Create CloudWatch Dashboard Add metrics to Dashboard Generate CPU and Memory load Teardown "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/4_adding_metrics_to_dashboard/","title":"Add metrics to Dashboard","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Now that we have a dashboard for our Windows EC2 instance, we can add an additional metric for available memory.\nLet\u0026rsquo;s add a new widget to our CloudWatch Dashboard. Click on the \u0026ldquo;Add Widget\u0026rdquo; button in the upper right corner. Click Line, then next Click Metrics and then Configure Select CWAgent and then select the second metric group \u0026ldquo;ImageId, InstanceId, InstanceType, objectname\u0026rdquo; Make sure to search only for the InstanceId if you have multiple machines reporting metrics Find the \u0026ldquo;Memory available Mbytes\u0026rdquo; and click the check box next to it. Click on \u0026ldquo;Graphed Metrics (1)\u0026rdquo; and then select \u0026ldquo;5 seconds\u0026rdquo; as the period Click on Create widget You should now see two widgets on your Dashboard. You can drag the widgets around the screen and re-size them if you wish. You can also change the time period, select 1h to show just the most recent metrics You can also set the auto-refresh rate for the Dashboard by using this pull-down. Select 10s Click the \u0026ldquo;Save Dashboard\u0026rdquo; button before proceeding to the next step. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/3_prepare_redshift_consumer_cluster/","title":"Prepare Amazon Redshift Consumer Cluster","tags":[],"description":"","content":"Lab 3 Now, let’s create the consumer Amazon Redshift cluster (we will refer this as consumer cluster throughout the lab) in us-west-1 region, and remember, we will not load the sample dataset in this cluster.\nStep-1: Create Redshift Consumer Cluster Login into AWS Console (make sure us-west-1 region is selected in top right corner), and click Create Cluster.\nProvide Cluster name as redshift-cluster-west, and select ra3.xlplus node type.\nNOTE: If you get access error launching cluster with ra3.xlplus node type, then select ra3.4xlarge node type. Please note, Amazon Redshift Data Sharing feature is not supported for previous generation dc2 node types, and Amazon Redshift only supports data sharing on the ra3.16xlarge, ra3.4xlarge, and ra3.xlplus instance types for producer and consumer clusters. Amazon Redshift ra3 nodes incurs cost as these nodes are not part of the Amazon Redshift free trial, or AWS Free Tier.\nDo not select “Load Sample data”.\nSupply a password for Admin user.\nOther configuration settings can be left as default.\nClick the Create Cluster button – it will take few minutes to create the cluster. Step-2: Connect to database using query editor Once the cluster is created (Status = Available), using one of the Amazon Redshift query editors is the easiest way to query the Amazon Redshift database. After creating your cluster, use the query editor v2 to connect to newly created database.\nStep-3: Validate database In the query editor, click on the newly created cluster, and it will establish connection to the database. You will then see two databases created automatically – dev, sample_data_dev. The dev database has one schema called public, which will not have any tables as we did not select “Load Sample Data” during cluster creation, unlike produce cluster in us-east-1 region. We will refer this as consumer database throughout the lab. Expand the public schema under dev database:\nWe now have both producer and consumer clusters installed, configured, and loaded sample dataset in Producer database. Next, we will baseline existing metrics and KPIs.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/3_prepare_redshift_consumer_cluster/","title":"Prepare Amazon Redshift Consumer Cluster","tags":[],"description":"","content":"Lab 3 Now, let’s create the consumer Amazon Redshift cluster (we will refer this as consumer cluster throughout the lab) in us-west-1 region, and remember, we will not load the sample dataset in this cluster.\nStep-1: Create Redshift Consumer Cluster Login into AWS Console (make sure us-west-1 region is selected in top right corner), and click Create Cluster.\nProvide Cluster name as redshift-cluster-west, and select ra3.xlplus node type.\nNOTE: If you get access error launching cluster with ra3.xlplus node type, then select ra3.4xlarge node type. Please note, Amazon Redshift Data Sharing feature is not supported for previous generation dc2 node types, and Amazon Redshift only supports data sharing on the ra3.16xlarge, ra3.4xlarge, and ra3.xlplus instance types for producer and consumer clusters. Amazon Redshift ra3 nodes incurs cost as these nodes are not part of the Amazon Redshift free trial, or AWS Free Tier.\nDo not select “Load Sample data”.\nSupply a password for Admin user.\nOther configuration settings can be left as default.\nClick the Create Cluster button – it will take few minutes to create the cluster. Step-2: Connect to database using query editor Once the cluster is created (Status = Available), using one of the Amazon Redshift query editors is the easiest way to query the Amazon Redshift database. After creating your cluster, use the query editor v2 to connect to newly created database.\nStep-3: Validate database In the query editor, click on the newly created cluster, and it will establish connection to the database. You will then see two databases created automatically – dev, sample_data_dev. The dev database has one schema called public, which will not have any tables as we did not select “Load Sample Data” during cluster creation, unlike produce cluster in us-east-1 region. We will refer this as consumer database throughout the lab. Expand the public schema under dev database:\nWe now have both producer and consumer clusters installed, configured, and loaded sample dataset in Producer database. Next, we will baseline existing metrics and KPIs.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/1-3_query_s3_usage_by_class/","title":"Query your Amazon S3 usage by storage class","tags":[],"description":"","content":"Lab 1.3 In the previous steps, you learned how you can run SQL queries on your AWS Cost \u0026amp; Usage Report data with Amazon Athena. In this step you will learn how you can aggregate the usage data by type and AWS account. This way you can calculate proxy metrics for sustainability and Key Performance Indicators (KPI) of your application teams.\nLet\u0026rsquo;s query the cur_hourly table we just created to get the Amazon S3 storage by account and storage class.\nGo to the Amazon Athena console in the same region as before. Choose the Database proxy_metrics_lab. Fill the New Query field with the following query. SELECT line_item_usage_account_id account_id, product_volume_type storage_class, sum(line_item_usage_amount) as usage_gb, year, month FROM cur_hourly WHERE product_servicename LIKE \u0026#39;%Amazon Simple Storage Service%\u0026#39; AND product_volume_type not like \u0026#39;\u0026#39; AND product_volume_type not like \u0026#39;Tags\u0026#39; GROUP BY line_item_usage_account_id, product_volume_type, year, month Click Run query You have now the Amazon S3 storage grouped by month, year, storage class, and account.\nYou can also save the query now as a view to refer to it later. This way you don\u0026rsquo;t need to remember the query and can also handle the view like a normal table, e.g. for joins with other tables.\nLeave the previously executed query in the query editor. Choose Create. Choose View from query. Enter storage_by_class as Name. Choose Create. You can now see new view called storage_by_class on the left menu. Choose the three dots next to the storage_by_class table and choose Preview view to test if it works. Congratulations, by the end of this step you learned to query AWS Cost \u0026amp; Usage Report data with Amazon Athena. You have run SQL queries on that data to aggregate it by usage type and AWS account to calculate proxy metrics for sustainability. And finally, you saved this query as a view. Now you can use this view\u0026rsquo;s data to establish KPIs for your AWS account owners and application teams.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/4_utilize_data/","title":"Utilize Data","tags":[],"description":"","content":"Utilizing Your Data Now you have pulled together optimization data there different ways in which you can analyze and visualize it and use to make infrastructure optimization decisions\nVisualization of Trusted Advisor data with Amazon QuickSight You can visualize Trusted Advisor Data with TAO Dashboard. To deploy TAO Dashboard please follow either automated or manual deployment steps and specify organizational data collection bucket created in this lab as a source\nVisualization of Compute Optimizer data with Amazon QuickSight You can visualize Compute Optimizer Data with Compute Optimizer Dashboard. .\nAWS Organisation Data and The Cost Intelligence Dashboard This video shows you how to use the Optimization Data Collection Lab to pull in AWS Organisation data such as Account names and Tags into the Cost And Usage report so it can be used in the CID.\nSnapshots and AMIs When a AMI gets created it takes a Snapshot of the volume. This is then needed to be kept in the account whilst the AMI is used. Once the AMI is released the Snapshot can no longer be used but it still incurs costs. Using this query we can identify Snapshots that have the \u0026lsquo;AMI Available\u0026rsquo;, those where the \u0026lsquo;AMI Removed\u0026rsquo; and those that fall outside of this scope and are \u0026lsquo;NOT AMI\u0026rsquo;. Data must be collected and the crawler finished running before this query can be run.\nOptimization Data Snapshots and AMIs Query SELECT *, CASE WHEN snap_ami_id = imageid THEN 'AMI Avalible' WHEN snap_ami_id LIKE 'ami%' THEN 'AMI Removed' ELSE 'Not AMI' END AS status FROM ( (SELECT snapshotid AS snap_id, volumeid as volume, volumesize, starttime, Description AS snapdescription, year, month, ownerid, CASE WHEN substr(Description, 1, 22) = 'Created by CreateImage' THEN split_part(Description,' ', 5) WHEN substr(Description, 2, 11) = 'Copied snap' THEN split_part(Description,' ', 9) WHEN substr(Description, 1, 22) = 'Copied for Destination' THEN split_part(Description,' ', 4) ELSE '' END AS \u0026quot;snap_ami_id\u0026quot; FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;snapshot_data\u0026quot; ) AS snapshots LEFT JOIN (SELECT imageid, name, description, state, rootdevicetype, virtualizationtype FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;ami_data\u0026quot;) AS ami ON snapshots.snap_ami_id = ami.imageid ) There is an option to add pricing data to this query. This assumes you have already run the accounts collector lambda.\nOptimization Data Snapshots and AMIs with pricing data Lambda\nGo to AWS Lambda Find the pricing-Lambda-Function function Test the function Athena\nGo to AWS Athena\nGo to Saved queries at the top of the screen\nRun the ec2_pricing Query to create a pricing table\nIn Saved queries Run the region_names Query to create a normalized region name table\nIn Saved queries run snapshot-ami-query to create a view\nRun the below to see your data\nSELECT * FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;snapshot_ami_quicksight_view\u0026quot; limit 10; EBS Volumes and Trusted Advisor Recommendations Trusted advisor identifies idle and underutilized volumes. This query joins together the data so you can see what portion of your volumes are flagged. Data must be collected and the crawler finished running before this query can be run.\nThis section requires you to have the Inventory Module and the Trusted Advisor Module deployed.\nOptimization Data EBS Volumes and Trusted Advisors Query SELECT * FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;ebs_data\u0026quot; LEFT JOIN (select \u0026quot;volume id\u0026quot;,\u0026quot;volume name\u0026quot;, \u0026quot;volume type\u0026quot;,\u0026quot;volume size\u0026quot;,\t\u0026quot;monthly storage cost\u0026quot; ,accountid, category, region, year,month from \u0026quot;optimization_data\u0026quot;.ta_data ) ta ON \u0026quot;ebs_data\u0026quot;.\u0026quot;volumeid\u0026quot; = \u0026quot;ta\u0026quot;.\u0026quot;volume id\u0026quot; and \u0026quot;ebs_data\u0026quot;.\u0026quot;year\u0026quot; = \u0026quot;ta\u0026quot;.\u0026quot;year\u0026quot; and \u0026quot;ebs_data\u0026quot;.\u0026quot;month\u0026quot; = \u0026quot;ta\u0026quot;.\u0026quot;month\u0026quot; There is an option to add pricing data to this query.\nOptimization Data EBS Volumes and Trusted Advisor with pricing data Lambda\nGo to AWS Lambda Find the pricing-Lambda-Function function Test the function Athena\nGo to AWS Athena and run the below\nGo to Saved queries at the top of the screen\nRun the ec2-view Query to create a view of ebs and ta data\nRun the ec2_pricing Query to create a pricing table\nIn Saved queries run the region_names Query to create a normalized region name table\nIn Saved queries run ebs-ta-query-pricing to create a view\nRun the below to see your data\nSELECT * FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;ebs_quicksight_view\u0026quot; limit 10; The section below will bring in opportunities to move EBS volumes to gp3\nEBS Volumes and Trusted Advisor moving to gp3 Go to AWS Athena and run the below Go to Saved queries at the top of the screen Run the ec2-view Query to create a view of ebs and ta data Run the ec2_pricing Query to create a pricing table In Saved queries run the region_names Query to create a normalized region name table In Saved queries run gp3-opportunity to create a view EBS Volumes and Trusted Advisor Recommendations Trusted advisor identifies idle and underutilized volumes. This query joins together the data so you can see what portion of your volumes are flagged. Data must be collected and the crawler finished running before this query can be run.\nThis section requires you to have the Inventory Module and the Trusted Advisor Module deployed.\nOptimization Data EBS Volumes and Trusted Advisors Query CREATE OR REPLACE VIEW \u0026quot;ebs_view\u0026quot; AS SELECT *FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;ebs_data\u0026quot; LEFT JOIN (select \u0026quot;volume id\u0026quot;,\u0026quot;volume name\u0026quot;, \u0026quot;volume type\u0026quot;,\u0026quot;volume size\u0026quot;,\t\u0026quot;monthly storage cost\u0026quot; ,accountid as ta_accountid, status, category, region as ta_region, year as ta_year ,month as ta_month from \u0026quot;optimization_data\u0026quot;.ta_data where category = 'cost_optimizing') ta ON \u0026quot;ebs_data\u0026quot;.\u0026quot;volumeid\u0026quot; = \u0026quot;ta\u0026quot;.\u0026quot;volume id\u0026quot; and \u0026quot;ebs_data\u0026quot;.\u0026quot;year\u0026quot; = \u0026quot;ta\u0026quot;.\u0026quot;ta_year\u0026quot; and \u0026quot;ebs_data\u0026quot;.\u0026quot;month\u0026quot; = \u0026quot;ta\u0026quot;.\u0026quot;ta_month\u0026quot; LEFT JOIN ( SELECT \u0026quot;region\u0026quot; \u0026quot;region_code\u0026quot; , \u0026quot;regionname\u0026quot; FROM storage.region_names ) region ON (\u0026quot;ebs_data\u0026quot;.\u0026quot;region\u0026quot; = \u0026quot;region\u0026quot;.\u0026quot;region_code\u0026quot;) AWS Budgets into Cost Dashboard In these labs we have a couple of amazing cost dashboards that can be found here . If you would like to add your budget data into these dashboard please follow the below steps. Data must be collected and the crawler finished running before this query can be run. There is a saved query called aws_budgets created in the CloudFormation. This is used when connecting to dashboard.\nGuide to add AWS Budgets into Cost Dashboard Ensure you have budget data in your Amazon Athena table\nCreate a Amazon Athena View of this data to extract the relevant information. The costfilters identifies if this is just an account spend budget or filtered. As this if for CUDOS we have filtered these out for the query but all budgets data is in the table.\nCREATE OR REPLACE VIEW aws_budgets_view AS SELECT budgetname budget_name , CAST(budgetlimit.amount AS decimal) budget_amount , CAST(calculatedspend.actualspend.amount AS decimal) actualspend , CAST(calculatedspend.forecastedspend.amount AS decimal) forecastedspend , timeunit , account_id , budgettype budget_type , year budget_year , month budget_month FROM \u0026quot;optimization_data\u0026quot;.\u0026quot;budgets\u0026quot; WHERE (budgettype = 'COST') AND costfilters.filter[1] = 'None' Go to the Amazon QuickSight service homepage\nIn QuickSight, select the summary_view Data Set\nSelect Edit data set\nSelect Add data: Select your Amazon Athena aws_budgets_view table and click Select Click on the join and choose month, year from summary_view and budget_month, budget_year to join. Click Save. In your Analysis you can now add a Budget figure to your lines. Make sure to change to Average. AWS EBS Volumes and Snapshots If you wish to see whats volumes have what snapshots attached to them from a holistic view then this query can combine these two data sources. This could provide information into which snapshots you could archive using Elastic Block Storage Snapshots Archive Optimization Data Snapshots with EBS WITH data as ( Select volumeid, snapshotid, ownerid \u0026quot;account_id\u0026quot;, cast( replace(split(split(starttime, '+') [ 1 ], '.') [ 1 ], 'T', ' ') as timestamp) as start_time, CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026quot;data_date\u0026quot;, sum(volumesize) \u0026quot;volume_size\u0026quot; from \u0026quot;optimization_data\u0026quot;.\u0026quot;snapshot_data\u0026quot; group by 1,2,3,4,5 ), latest AS( Select max(data_date) \u0026quot;latest_date\u0026quot; from data ), ratio AS( Select distinct volumeid, data_date, latest_date, count(distinct snapshotid) AS \u0026quot;snapshot_count_per_volume\u0026quot; from data LEFT JOIN latest ON latest.latest_date = data_date WHERE volumeid like 'vol%' and data_date = latest_date group by 1,2,3 ) select data.volumeid, data.snapshotid, account_id, data.data_date, start_time, volume_size, snapshot_count_per_volume, CASE WHEN data.volumeid NOT LIKE 'vol%' THEN 1 ELSE dense_rank() OVER (partition by data.volumeid ORDER by start_time) END AS \u0026quot;snapshot_lineage\u0026quot; from data Left JOIN ratio ON ratio.volumeid = data.volumeid ORDER by volumeid, snapshot_lineage If you wish to connect to your Cost and Usage report for snapshot costs please use the below:\nOptimization Data Snapshots with EBS and CUR WITH cur_mapping AS ( SELECT DISTINCT split_part(line_item_resource_id,'/',2) AS \u0026quot;snapshot_id\u0026quot;, line_item_usage_account_id AS \u0026quot;linked_account_id\u0026quot;, CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026quot;billing_period\u0026quot;, sum(line_item_usage_amount) \u0026quot;snapshot_size\u0026quot;, sum(line_item_unblended_cost) \u0026quot;snapshot_cost\u0026quot; FROM \u0026quot;athenacurcfn_mybillingreport\u0026quot;.\u0026quot;mybillingreport\u0026quot; WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) = (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '1' MONTH)) AND (line_item_resource_id \u0026lt;\u0026gt; '') AND (line_item_line_item_type LIKE '%Usage%') AND (line_item_product_code = 'AmazonEC2') AND (line_item_usage_type LIKE '%EBS:Snapshot%') group by 1,2,3 ), snapshot_data AS ( Select volumeid, snapshotid, ownerid \u0026quot;account_id\u0026quot;, cast( replace(split(split(starttime, '+') [ 1 ], '.') [ 1 ], 'T', ' ') as timestamp ) as start_time, CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026quot;data_date\u0026quot;, sum(volumesize) \u0026quot;volume_size\u0026quot; from \u0026quot;optimization_data\u0026quot;.\u0026quot;snapshot_data\u0026quot; group by 1,2,3,4,5 ), data AS ( SELECT DISTINCT volumeid, snapshotid, account_id, billing_period, data_date, start_time, sum(snapshot_size) AS snapshot_size, sum(snapshot_cost) AS snapshot_cost, sum(volume_size) AS \u0026quot;volume_size\u0026quot; FROM snapshot_data LEFT JOIN cur_mapping ON cur_mapping.snapshot_id = snapshotid AND cur_mapping.linked_account_id = account_id group by 1,2,3,4,5,6 ), latest AS( Select max(data_date) \u0026quot;latest_date\u0026quot; from data ), ratio AS( Select distinct volumeid, data_date, latest_date, count(distinct snapshotid) AS \u0026quot;snapshot_count_per_volume\u0026quot;, sum(snapshot_cost) AS \u0026quot;all_snapshot_cost_per_volume\u0026quot;, sum(snapshot_size) AS \u0026quot;all_snapshot_size_per_volume\u0026quot; from data LEFT JOIN latest ON latest.latest_date = data_date WHERE volumeid like 'vol%' and data_date = latest_date group by 1,2,3 ) select data.volumeid, data.snapshotid, account_id, data.data_date, start_time, billing_period, snapshot_size, volume_size, all_snapshot_cost_per_volume all_snapshot_size_per_volume, snapshot_count_per_volume, CASE WHEN data.volumeid NOT LIKE 'vol%' THEN 1 ELSE dense_rank() OVER (partition by data.volumeid ORDER by start_time) END AS \u0026quot;snapshot_lineage\u0026quot; from data LEFT JOIN ratio ON ratio.volumeid = data.volumeid ECS Chargeback Report to show costs associated with ECS Tasks leveraging EC2 instances within a Cluster Athena Configuration Navigate to the Athena service Select the \u0026ldquo;optimization data\u0026rdquo; database In Saved Queries find \u0026ldquo;cluster_metadata_view\u0026rdquo;\u0026quot; Change \u0026lsquo;BU\u0026rsquo; to the tag you wish to do chargeback for Click the Run button In Saved Queries find \u0026ldquo;ec2_cluster_costs_view\u0026rdquo;\u0026quot; \u0026ndash; Replace ${CUR} in the \u0026ldquo;FROM\u0026rdquo; clause with your CUR table name \u0026ndash; For example, \u0026ldquo;curdb\u0026rdquo;.\u0026ldquo;ecs_services_clusters_data\u0026rdquo; Click the Run button In Saved Queries find \u0026ldquo;bu_usage_view\u0026rdquo;\u0026quot; \u0026ndash; Replace ${CUR} in the \u0026ldquo;FROM\u0026rdquo; clause with your CUR table name \u0026ndash; For example, \u0026ldquo;curdb\u0026rdquo;.\u0026ldquo;ecs_services_clusters_data\u0026rdquo; Click the Run button Now your views are created you can run your report\nManually execute billing report In Saved Queries find \u0026ldquo;ecs_chargeback_report\u0026rdquo; \u0026ndash; Replace \u0026ldquo;bu_usage_view.month\u0026rdquo; value with the appropriate month desired for the report \u0026ndash; For example, a value of \u0026lsquo;2\u0026rsquo; returns the charges for February Click the Run button Example Output Breakdown:\ntask_usage: total memory resources reserved (in GBs) by all tasks over the billing period (i.e. – monthly) percent: task_usage / total_usage ec2_cost: monthly cost for EC2 instance in $ Services: Name of service servicearn: Arn of service Value: Value of specified tag for the ECS service (could be App, TeamID, etc?) X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/4_test_fail_condition/","title":"Test Fail Condition","tags":[],"description":"","content":"Now that an alarm has been created to alert and send out notifications when the external service is experiencing an outage, it is time to test it. To do this, an outage can be simulated so that the external service is no longer able to write data into S3. This can be achieved in a few different ways. These are also a few different failure modes for the dependent service, which could cause the Lambda function to not get invoked. While the alarm will provide visibility into the outage itself, it will not identify the root cause.\nExternal service no longer has permission to write to S3 (role used by the service has been removed/modified) S3 bucket policy has been changed so that the external service no longer has access to write to it S3 bucket configuration has been modified and S3 notifications removed so that notifications are no longer sent to Lambda to invoke the function External service is experiencing network connectivity issues preventing it from writing to S3 In this lab, the last failure mode - Loss of connectivity will be simulated. To do this, the default route for the subnet can be removed so that the external service running on EC2 will no longer have a path to reach the internet.\nGo to the VPC console at https://console.aws.amazon.com/vpc and click on Route Tables\nSearch for the route table WA-Lab-RouteTable\nClick on the Routes tab and then click Edit routes\nOn the Edit routes page, find the route with the destination of 0.0.0.0/0 and click on the REMOVE button at the end of that row\nClick on Save routes\nThe external service running on EC2 no longer has a path to reach the internet, which means it cannot write data to S3. Now that an \u0026ldquo;outage\u0026rdquo; has occurred, it is time to see if the alarm is able to identify this and send out notifications.\nGo to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Alarms\nSearch for the alarm WA-Lab-Dependency-Alarm and click on it\nMonitor the alarm state to see if it changes (NOTE: It will take a few minutes for the alarm state to change since CloudWatch attempts to retrieve a higher number of data points than specified by Evaluation Periods when evaluating a metric with missing data)\nOnce the alarm state changes from OK to In alarm, CloudWatch will execute the action specified, in this case, sends a message to the SNS Topic specified\nMonitor the email address that was specified during the CloudFormation Stack creation process in section 1 Deploy the Infrastructure\nSNS will send a notification providing details of the alarm, the change in state, reason for change, and additional data\nOnce the notification has been received, the team responsible for the workload can start investigating to identify the cause of failure. This will ensure a timely response to dependent service outages, and allow for improved business continuity.\nThe alarm will go back to an OK state once the metric is no longer breaching the threshold defined, in this case, at least 1 Lambda invocation every minute. This can be achieved by adding the default route back to the route table so that the external service running on EC2 is able to reach the internet again.\nGo to the VPC console at https://console.aws.amazon.com/vpc and click on Route Tables\nSearch for the route table WA-Lab-RouteTable\nClick on the Routes tab and then click Edit routes\nOn the Edit routes page, click on Add route and enter the following:\nDestination - 0.0.0.0/0 Target - Click on the dropdown, select Internet Gateway, and click on WA-Lab-InternetGateway Click on Save routes\nNow that internet connectivity has been re-established, the external service should start writing data to S3 again, which will in turn invoke the Lambda function. Since Lambda functions will now start getting invoked periodically again, the WA-Lab-Dependency-Alarm should go back to an OK state.\nGo to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Alarms Search for the alarm WA-Lab-Dependency-Alarm and click on it The alarm should be in an OK state confirming that the fix worked and the external service is functioning normally again You have now configured an alarm and tested it to ensure dependency monitoring has been established and notifications will be sent out in the event of an outage. While it is important to be notified of events affecting dependent resources, responses to these events are just as important. Once a notification has been received, the event has to be effectively tracked to ensure the right resources are assigned to it and to avoid duplication of effort. The Bonus Content in the next section talks about how this can be achieved and how it can be automated. With this approach responses to events will be manual. If there is a known and codified procedure (runbook) for the event, SNS can be used to trigger the execution of the runbook in response to the event.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/contributing/04_updatingexisting/","title":"Updating an Existing Lab","tags":[],"description":"","content":"Make the required updates to the content.\nChange to your local lab directory aws-well-architected-labs Navigate to the content folder then to the pillar and lab you are looking to edit (i.e. select the folder of the pillar you are looking to edit: Open the portion of the lab you are looking to edit and make the changes For more information on lab formats and best practices, see the create a new lab section\nVerify your edits and/or additions After making the changes or additions test and verify locally\nNavigate back to the aws-well-architected-labs parent folder\nServe the content locally:\nhugo serve -D Open a browser and navigate to http://localhost:1313/ Verify the change you made was correct and there were no problems introduced\nPush your changes to the remote repository: Add, Commit and Push your changes to GitHub using the following commands:\ngit add -A git commit -m \u0026quot;your comment here\u0026quot; git push Please write a descriptive commit message following this guidance. Smaller meaningful commits are better than one large commit with lots of changes\nAll your changes will be in the remote repository in GitHub, which can now be merged into the Well-Architected Labs repository\nPicture Updates When you update a lab picture please ensure it had the following:\nBlack boarder Orange box\u0026rsquo;s to show the item the customer is looking for Role/AccountID hidden using the same colour as the section Image needs to be 800 wide to avoid wrap The description above the image much match the image e.g. if you say use 2688Mb then the picture must have 2688Mb An example can be seen below:\nStep example :\nRole name LambdaOrgRole, click Create role: "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/compute/","title":"Compute","tags":[],"description":"","content":"These are queries for AWS Services under the Compute product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents EC2 Total Spend EC2 Hours a Day EC2 Effective Savings Plans Compute with Savings Plans Account Spend of Shared Savings Plan Lambda Elastic Load Balancing - Idle ELB EC2 Savings Plans Inventory EC2 Reserved Instance Coverage AWS Outposts - EC2 Hours a Day EC2 Total Spend Query Description This query will display the top costs for all spend with the product code of \u0026lsquo;AmazonEC2\u0026rsquo;. This will include all pricing categories (i.e. OnDemand, Reserved etc..) as well as charges for storage on EC2 (i.e. gp2). The query will output the product code as well as the product description to provide context. It is ordered by largest to smallest spend.\nPricing Please refer to the EC2 pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Sample Output Download SQL File Link to Code Copy Query SELECT line_item_product_code, line_item_line_item_description, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND line_item_line_item_type NOT IN (\u0026#39;Tax\u0026#39;,\u0026#39;Refund\u0026#39;,\u0026#39;Credit\u0026#39;) GROUP BY line_item_product_code, line_item_line_item_description ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents EC2 Hours a Day Query Description This query will provide the EC2 usage quantity measured in hours for each purchase option and each instance type. The output will include detailed information about the instance type, amortized cost, purchase option, and usage quantity. The output will be ordered by usage quantity in descending order.\nThis query will not run against CUR data from accounts which have purchased EC2 Reserved Instances or Savings Plans.\nPricing Page Please refer to the EC2 pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_billing_period_start_date, line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, CASE WHEN (line_item_usage_type LIKE \u0026#39;%SpotUsage%\u0026#39;) THEN SPLIT_PART(line_item_usage_type, \u0026#39;:\u0026#39;, 2) ELSE product_instance_type END AS case_product_instance_type, CASE WHEN (savings_plan_savings_plan_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;) THEN \u0026#39;SavingsPlan\u0026#39; WHEN (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;) THEN \u0026#39;Reserved\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%Spot%\u0026#39;) THEN \u0026#39;Spot\u0026#39; ELSE \u0026#39;OnDemand\u0026#39; END AS case_purchase_option, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN line_item_unblended_cost ELSE 0 END) AS sum_amortized_cost, SUM(line_item_usage_amount) AS sum_line_item_usage_amount FROM ${table_name} WHERE ${date_filter} AND (line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND product_servicecode \u0026lt;\u0026gt; \u0026#39;AWSDataTransfer\u0026#39; AND line_item_operation LIKE \u0026#39;%RunInstances%\u0026#39; AND line_item_usage_type NOT LIKE \u0026#39;%DataXfer%\u0026#39; ) AND (line_item_line_item_type = \u0026#39;Usage\u0026#39; OR (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) OR (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) ) GROUP BY bill_billing_period_start_date, line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, 5, --refers to case_product_instance_type 6 --refers to case_purchase_option ORDER BY sum_line_item_usage_amount DESC; Help \u0026amp; Feedback Back to Table of Contents EC2 Effective Savings Plans Query Description This query will provide EC2 consumption of Savings Plans across Compute resources by linked accounts. It also provides you with the savings received from these Savings Plans and which Savings Plans its connected to. The output is ordered by date.\nPricing Please refer to the EC2 pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(savings_plan_savings_plan_a_r_n, \u0026#39;/\u0026#39;, 2) AS savings_plan_savings_plan_a_r_n, savings_plan_offering_type, savings_plan_region, CASE WHEN line_item_product_code = \u0026#39;AmazonECS\u0026#39; THEN \u0026#39;Fargate\u0026#39; WHEN line_item_product_code = \u0026#39;AWSLambda\u0026#39; THEN \u0026#39;Lambda\u0026#39; ELSE product_instance_type_family END AS case_instance_type_family, savings_plan_end_time, SUM(TRY_CAST(line_item_unblended_cost AS DECIMAL(16, 8))) AS sum_line_item_unblended_cost, SUM(TRY_CAST(savings_plan_savings_plan_effective_cost AS DECIMAL(16, 8))) AS sum_savings_plan_savings_plan_effective_cost FROM ${table_name} WHERE ${date_filter} AND savings_plan_savings_plan_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), SPLIT_PART(savings_plan_savings_plan_a_r_n, \u0026#39;/\u0026#39;, 2), savings_plan_offering_type, savings_plan_region, 7, -- refers to case_instance_type_family savings_plan_end_time ORDER BY day_line_item_usage_start_date; Help \u0026amp; Feedback Back to Table of Contents Compute with Savings Plans Query Description This query will provide details about Compute usage that is covered by Savings Plans. The output will include detailed information about the usage type, usage amount, Savings Plans ARN, line item description, and Savings Plans effective savings as compared to On-Demand pricing. The public pricing on-demand cost will be summed and in descending order.\nPricing Please refer to the Savings Plans pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, bill_billing_period_start_date, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, savings_plan_savings_plan_a_r_n, line_item_product_code, line_item_usage_type, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, line_item_line_item_description, pricing_public_on_demand_rate, SUM(pricing_public_on_demand_cost) AS sum_pricing_public_on_demand_cost, savings_plan_savings_plan_rate, SUM(savings_plan_savings_plan_effective_cost) AS sum_savings_plan_savings_plan_effective_cost FROM ${table_name} WHERE ${date_filter} AND line_item_line_item_type LIKE \u0026#39;SavingsPlanCoveredUsage\u0026#39; GROUP BY bill_payer_account_id, bill_billing_period_start_date, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), savings_plan_savings_plan_a_r_n, line_item_product_code, line_item_usage_type, line_item_unblended_rate, line_item_line_item_description, pricing_public_on_demand_rate, savings_plan_savings_plan_rate ORDER BY sum_pricing_public_on_demand_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Account Spend of Shared Savings Plan Query Description This query focuses on surfacing accounts which have utilized AWS Savings Plans for which they are not a buyer.\nPricing Please refer to the Savings Plans pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, savings_plan_offering_type, line_item_resource_id, SUM(CAST(line_item_unblended_cost AS DECIMAL(16, 8))) AS sum_line_item_unblended_cost, SUM(CAST(savings_plan_savings_plan_effective_cost AS DECIMAL(16, 8))) AS sum_savings_plan_savings_plan_effective_cost FROM ${table_name} WHERE ${date_filter} AND bill_payer_account_id = \u0026#39;111122223333\u0026#39; AND line_item_usage_account_id = \u0026#39;444455556666\u0026#39; AND line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; AND savings_plan_savings_plan_a_r_n NOT LIKE \u0026#39;%444455556666%\u0026#39; GROUP BY DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_resource_id, line_item_usage_account_id, bill_payer_account_id, savings_plan_offering_type ORDER BY sum_savings_plan_savings_plan_effective_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Lambda Query Description This query focuses on Lambda and the breakdown of its costs by different usage element. Split by Resource IDs you can view the usage, unblended costs and amortized cost broken down by different pricing plans. These results will be ordered by date and costs.\nPricing Please refer to the Lambda pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT * FROM ( ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE \u0026#39;%Lambda-Edge-GB-Second%\u0026#39; THEN \u0026#39;Lambda EDGE GB x Sec.\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Lambda-Edge-Request%\u0026#39; THEN \u0026#39;Lambda EDGE Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Lambda-GB-Second%\u0026#39; THEN \u0026#39;Lambda GB x Sec.\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Request%\u0026#39; THEN \u0026#39;Lambda Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%In-Bytes%\u0026#39; THEN \u0026#39;Data Transfer (IN)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Out-Bytes%\u0026#39; THEN \u0026#39;Data Transfer (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Regional-Bytes%\u0026#39; THEN \u0026#39;Data Transfer (Regional)\u0026#39; ELSE \u0026#39;Other\u0026#39; END AS case_line_item_usage_type, line_item_resource_id, pricing_term, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN savings_plan_total_commitment_to_date - savings_plan_used_commitment WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 ELSE line_item_unblended_cost END) AS sum_amortized_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;AWS Lambda\u0026#39; AND line_item_line_item_type LIKE \u0026#39;%Usage%\u0026#39; AND product_product_family IN (\u0026#39;Data Transfer\u0026#39;, \u0026#39;Serverless\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_region, 6, -- refers to case_line_item_usage_type line_item_resource_id, pricing_term ) UNION ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE \u0026#39;%Lambda-Edge-GB-Second%\u0026#39; THEN \u0026#39;Lambda EDGE GB x Sec.\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Lambda-Edge-Request%\u0026#39; THEN \u0026#39;Lambda EDGE Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Lambda-GB-Second%\u0026#39; THEN \u0026#39;Lambda GB x Sec.\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Request%\u0026#39; THEN \u0026#39;Lambda Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%In-Bytes%\u0026#39; THEN \u0026#39;Data Transfer (IN)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Out-Bytes%\u0026#39; THEN \u0026#39;Data Transfer (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Regional-Bytes%\u0026#39; THEN \u0026#39;Data Transfer (Regional)\u0026#39; ELSE \u0026#39;Other\u0026#39; END AS case_line_item_usage_type, line_item_resource_id, savings_plan_offering_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(savings_plan_savings_plan_effective_cost AS DECIMAL(16,8))) AS sum_savings_plan_savings_plan_effective_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN savings_plan_total_commitment_to_date - savings_plan_used_commitment WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 ELSE line_item_unblended_cost END) AS sum_amortized_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;AWS Lambda\u0026#39; AND product_product_family IN (\u0026#39;Data Transfer\u0026#39;, \u0026#39;Serverless\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_region, 6, --refers to case_line_item_usage_type line_item_resource_id, savings_plan_offering_type ) ) AS aggregatedTable ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost; Help \u0026amp; Feedback Back to Table of Contents Elastic Load Balancing - Idle ELB Query Description This query will display cost and usage of Elastic Load Balancers which didn\u0026rsquo;t receive any traffic last month and ran for more than 336 hours (14 days). Resources returned by this query could be considered for deletion.\nPricing Please refer to the Elastic Load Balancing pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 6) split_line_item_resource_id, product_region, pricing_unit, sum_line_item_usage_amount, CAST(cost_per_resource AS DECIMAL(16, 8)) AS sum_line_item_unblended_cost FROM ( SELECT line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_per_resource_and_pricing_unit, COUNT(pricing_unit) OVER (PARTITION BY line_item_resource_id) AS pricing_unit_per_resource FROM ${table_name} WHERE line_item_product_code = \u0026#39;AWSELB\u0026#39; -- get previous month AND month = CAST(month(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS VARCHAR) -- get year for previous month AND year = CAST(year(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS VARCHAR) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id ) WHERE -- filter only resources which ran more than half month (336 hrs) usage_per_resource_and_pricing_unit \u0026gt; 336 AND pricing_unit_per_resource = 1 ORDER BY cost_per_resource DESC; Help \u0026amp; Feedback Back to Table of Contents EC2 Savings Plans Inventory Query Description This query will provide an inventory for EC2 Savings Plans. It will show useful information about the Savings Plans purchased including ID (ARN), type, term length, commitment (used, hourly, etc\u0026hellip;), and utilization. Cost Explorer can also provide this information in the Inventory and Utilization reports however, this combines elements from both into a single report.\nPricing Please refer to the Savings Plans pricing page .\nCost Explorer Links Savings Plans Utilization Report Savings Plans Inventory Report Sample Output Download SQL File Link to Code Copy Query SELECT SPLIT_PART(savings_plan_savings_plan_a_r_n, \u0026#39;/\u0026#39;, 2) AS split_savings_plan_savings_plan_a_r_n, bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, savings_plan_offering_type, savings_plan_region, DATE_FORMAT(FROM_ISO8601_TIMESTAMP(savings_plan_start_time),\u0026#39;%Y-%m-%d\u0026#39;) AS day_savings_plan_start_time, DATE_FORMAT(FROM_ISO8601_TIMESTAMP(savings_plan_end_time),\u0026#39;%Y-%m-%d\u0026#39;) AS day_savings_plan_end_time, savings_plan_payment_option, savings_plan_purchase_term, SUM(TRY_CAST(savings_plan_recurring_commitment_for_billing_period AS DECIMAL(16, 8))) AS sum_savings_plan_recurring_committment_for_billing_period, SUM(TRY_CAST(savings_plan_total_commitment_to_date AS DECIMAL(16, 8))) AS sum_savings_plan_total_commitment_to_date, SUM(TRY_CAST(savings_plan_used_commitment AS DECIMAL(16, 8))) AS sum_savings_plan_used_commitment, AVG(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN TRY_CAST(savings_plan_total_commitment_to_date AS DECIMAL(8, 2)) END) AS \u0026#34;Hourly Commitment\u0026#34;, -- (used commitment / total commitment) * 100 = utilization % TRY_CAST(((SUM(TRY_CAST(savings_plan_used_commitment AS DECIMAL(16, 8))) / SUM(TRY_CAST(savings_plan_total_commitment_to_date AS DECIMAL(16, 8)))) * 100) AS DECIMAL(3, 0)) AS calc_savings_plan_utilization_percent FROM ${table_name} WHERE ${date_filter} AND savings_plan_savings_plan_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; GROUP BY SPLIT_PART(savings_plan_savings_plan_a_r_n, \u0026#39;/\u0026#39;, 2), bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m\u0026#39;), savings_plan_offering_type, savings_plan_region, DATE_FORMAT(FROM_ISO8601_TIMESTAMP(savings_plan_start_time),\u0026#39;%Y-%m-%d\u0026#39;), DATE_FORMAT(FROM_ISO8601_TIMESTAMP(savings_plan_end_time),\u0026#39;%Y-%m-%d\u0026#39;), savings_plan_payment_option, savings_plan_purchase_term ORDER BY split_savings_plan_savings_plan_a_r_n, month_line_item_usage_start_date; Help \u0026amp; Feedback Back to Table of Contents EC2 Reserved Instance Coverage Query Description The Reserved Instance Utilization and Coverage reports are available out-of-the-box in AWS Cost Explorer. This query provides coverage for EC2 Reserved Instances. It shows useful information about the Reserved Instances purchased including Lease ID, instance type and family, used and unused amounts, and On-Demand usage that could be covered by additional Savings Plans if this is your preferred savings method.\nCost and Usage columns are dynamic and their visibility in the Athena tables depends on usage. This query will only work if you have reserved instance usage in the table/view you are querying. If you do not have usage you will receive an error that reservation_reservation_a_r_n is an invalid column name.\nPricing Please refer to the EC2 reserved instances pricing page .\nSample Output Sample output includes a subset of query columns Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, CASE WHEN line_item_line_item_type IN (\u0026#39;Usage\u0026#39;) THEN \u0026#39;OnDemand\u0026#39; WHEN line_item_line_item_type IN (\u0026#39;Fee\u0026#39;,\u0026#39;RIFee\u0026#39;,\u0026#39;DiscountedUsage\u0026#39;) THEN \u0026#39;ReservedInstance\u0026#39; END AS case_purchase_option, SPLIT_PART(SPLIT_PART(reservation_reservation_a_r_n,\u0026#39;:\u0026#39;,6),\u0026#39;/\u0026#39;,2) AS split_reservation_reservation_a_r_n, SPLIT_PART(line_item_usage_type ,\u0026#39;:\u0026#39;,2) AS split_line_item_usage_type_instance_type, SPLIT_PART(SPLIT_PART(line_item_usage_type ,\u0026#39;:\u0026#39;,2), \u0026#39;.\u0026#39;, 1) AS split_line_item_usage_type_instance_family, CASE product_region WHEN NULL THEN \u0026#39;Global\u0026#39; WHEN \u0026#39;\u0026#39; THEN \u0026#39;Global\u0026#39; ELSE product_region END AS case_product_region, line_item_line_item_type, SUM(TRY_CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(TRY_CAST(reservation_unused_quantity AS DOUBLE)) AS sum_reservation_unused_quantity, SUM(TRY_CAST(line_item_normalized_usage_amount AS DOUBLE)) AS sum_line_item_normalized_usage_amount, SUM(TRY_CAST(reservation_unused_normalized_unit_quantity AS DOUBLE)) AS sum_reservation_unused_normalized_unit_quantity, SUM(CAST(reservation_effective_cost AS DECIMAL(16,8))) AS sum_reservation_effective_cost, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic Compute Cloud\u0026#39; AND line_item_operation LIKE \u0026#39;%RunInstance%\u0026#39; AND line_item_line_item_type IN (\u0026#39;Usage\u0026#39;,\u0026#39;Fee\u0026#39;,\u0026#39;RIFee\u0026#39;,\u0026#39;DiscountedUsage\u0026#39;) AND product_product_family NOT IN (\u0026#39;Data Transfer\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), 4, -- refers to case_purchase_option SPLIT_PART(SPLIT_PART(reservation_reservation_a_r_n,\u0026#39;:\u0026#39;,6),\u0026#39;/\u0026#39;,2), SPLIT_PART(line_item_usage_type ,\u0026#39;:\u0026#39;,2), SPLIT_PART(SPLIT_PART(line_item_usage_type ,\u0026#39;:\u0026#39;,2), \u0026#39;.\u0026#39;, 1), 8, -- refers to case_product_region line_item_line_item_type ORDER BY day_line_item_usage_start_date, split_line_item_usage_type_instance_type, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents AWS Outposts - EC2 Hours a Day Query Description AWS Services running locally on AWS Outposts will be charged on usage only. Operating system charges are billed based on usage as an uplift to cover the license fee and no minimum fee required. This query will provide the Amazon EC2 software costs (like Windows, RHEL or others) on AWS Outposts. These software fees are not included with the cost of the AWS Outposts racks and are billed separately here. The output will include detailed information about the instance type, pre-installed softwares, operating system, description, amortized cost, and usage quantity. The output will be ordered by amortized cost in descending order.\nPricing Please refer to the AWS Outposts pricing page .\nSample Output Sample output includes a subset of query columns Download SQL File Link to Code Copy Query SELECT DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, product_instance_type, product_operating_system, product_pre_installed_sw, line_item_line_item_description, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN line_item_unblended_cost ELSE 0 END) AS sum_amortized_cost FROM ${table_name} WHERE ${date_filter} AND product_location_type=\u0026#39;AWS Outposts\u0026#39; AND product_product_family=\u0026#39;Compute Instance\u0026#39; AND line_item_operation LIKE \u0026#39;%RunInstance%\u0026#39; AND (line_item_line_item_type = \u0026#39;Usage\u0026#39; OR (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) OR (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) ) GROUP BY DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, product_instance_type, product_operating_system, product_pre_installed_sw, line_item_line_item_description ORDER BY day_line_item_usage_start_date ASC, sum_amortized_cost DESC; Help \u0026amp; Feedback Back to Table of Contents "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/4_configure_sso/","title":"Enable Single Sign On (SSO)","tags":[],"description":"","content":"You will create an AWS Organization, and join two or more accounts to the management account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a management account that is used for security and administration, with access provided for limited billing tasks. A dedicated member account will be created for the Cost Optimization team or function, and another (or multiple) member account/s created to contain workload resources.\nYou will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you join a member account to a management account, it will contain all billing information for that member account. Member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data before joining accounts to a management account.\nConfigure SSO You will create an AWS Organization with the management account.\nLogin to the AWS console as an IAM user with the required permissions, start typing SSO into the Find Services box and click on AWS Single Sign-On: Click Enable AWS SSO: Select Groups: Click Create group: Enter a Group name of Cost_Optimization and a description, click Create group: Click Users: Click Add user: Enter the following details:\nUsername Password - Email address First name Last name Display name Configure the optional fields as required click Next: Select the Cost_Optimization group and click Next: Review user details and click Add User The user will receive an email, with a link to Accept invitation, the Portal URL and their Username: When the user goes to the portal, they will enter in a Password and click Set new password: Enter the new SSO Username and Password click Sign In: Users will not have permissions until you complete the rest of this step. A management and member permission set will be created\nCreate the management permission set. Click on Permission sets, and click Create permission set: Select Custom permission set and click Next: Select Inline Policy. Use the policy below as a starting point, modify it to your requirements and paste it in the policy field, click Next.\nYou MUST work with your security team/specialist to ensure you create the policies inline with least privileges for your organization.\nClick here for Custom permissions policy {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;budgets:*\u0026quot;,\r\u0026quot;ce:*\u0026quot;,\r\u0026quot;aws-portal:*Usage\u0026quot;,\r\u0026quot;aws-portal:*PaymentMethods\u0026quot;,\r\u0026quot;aws-portal:*Billing\u0026quot;,\r\u0026quot;cur:DescribeReportDefinitions\u0026quot;,\r\u0026quot;cur:PutReportDefinition\u0026quot;,\r\u0026quot;cur:DeleteReportDefinition\u0026quot;,\r\u0026quot;cur:ModifyReportDefinition\u0026quot;,\r\u0026quot;pricing:DescribeServices\u0026quot;,\r\u0026quot;wellarchitected:*\u0026quot;,\r\u0026quot;savingsplans:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r}\r]\r}\rEnter a Permission set name of management_CostOptimization, enter a Description, set the Session duration, click Next. Review and Create the custom permissions policy. Create the member permission set. Click on Permission sets, and click Create permission set: Select Custom permission set and click Next: Select Inline Policy. Use the policy below as a starting point, replace (management CUR bucket) and (Cost Optimization Member Account ID) click Next.\nYou MUST work with your security team/specialist to ensure you create the policies inline with least privileges for your organization.\nClick here for Custom permissions policy {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;CostServices\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;ce:*\u0026quot;,\r\u0026quot;budgets:*\u0026quot;,\r\u0026quot;aws-portal:*Usage\u0026quot;,\r\u0026quot;aws-portal:*PaymentMethods\u0026quot;,\r\u0026quot;aws-portal:*Billing\u0026quot;,\r\u0026quot;pricing:DescribeServices\u0026quot;,\r\u0026quot;wellarchitected:*\u0026quot;,\r\u0026quot;savingsplans:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;S3ManagementCUR\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::(management CUR bucket)\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;AthenaGlueAndServiceReadAccess\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;athena:*\u0026quot;,\r\u0026quot;glue:*\u0026quot;,\r\u0026quot;iam:ListRoles\u0026quot;,\r\u0026quot;iam:ListPolicies\u0026quot;,\r\u0026quot;s3:GetBucketLocation\u0026quot;,\r\u0026quot;s3:ListAllMyBuckets\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;QuickSightAccess\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;quicksight:CreateUser\u0026quot;,\r\u0026quot;quicksight:Subscribe\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;IAMAccessForGlue\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;,\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):role/service-role/AWSGlueServiceRole-Cost*\u0026quot;,\r\u0026quot;arn:aws:iam::(Cost Optimization Member Account ID):policy/service-role/AWSGlueServiceRole-Cost*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;S3AccessForAthena\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:GetBucketLocation\u0026quot;,\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;,\r\u0026quot;s3:ListBucketMultipartUploads\u0026quot;,\r\u0026quot;s3:ListMultipartUploadParts\u0026quot;,\r\u0026quot;s3:AbortMultipartUpload\u0026quot;,\r\u0026quot;s3:CreateBucket\u0026quot;,\r\u0026quot;s3:PutObject\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::aws-athena-query-results-*\u0026quot;\r]\r},\r{\r\u0026quot;Sid\u0026quot;: \u0026quot;FullS3AccessForBucketsWithSpecificPrefix\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;,\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::cost*\u0026quot;\r]\r}\r]\r}\rEnter a Permission set name of member_CostOptimization, enter a Description, set the Session duration, click Next. Review and Create the custom permissions policy. Setup the Cost Optimization management account. Click AWS accounts, select the management account, click Assign users or groups: Select Groups, select the Cost_Optimization group, click Next: Select the management_CostOptimization Permission set, click Next: Review and Submit: Verify account was updated with permission set: Setup the Cost Optimization member account. Click AWS accounts, select the member account, click Assign users or groups: Select Groups, select the Cost_Optimization group, click Next: Select the member_CostOptimization Permission set, click Next: Review and Submit: Verify account was updated with permission set: You have now setup your Cost Optimization users, group and their permissions.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/4_distribute_dashboards/","title":"Distribute Dashboards","tags":[],"description":"","content":" This Lab has moved under the Level 200 Cloud Intelligence Dashboards.Click this link to navigate to the updated Lab Authors Nathan Besh, Cost Lead Well-Architected (AWS) You now have a set of dashboards to provide insight and assist with analysis. The most effective way to allow users to access and work with the dashboards is to create users in QuickSight, and provide access to the dashboard. This will provide full access to all the most recent data, enable the use of features such as filters to dive deep into the data and perform analysis work..\nThis step will look at ways to distribute the content to users, which can be an effective reminder - for example, weekly updates to ensure people are checking their dashboards and tracking progress towards goals.\nConfigure email reports We will configure a weekly email report of the Cost Intelligence dashboard. This will ensure that all relevant parties within your organization have the access and visibility to the information they need.\nLogin to QuickSight\nSelect All dashboards and click on the Cost Intelligence dashboard\nClick on Share, then Share dashboard Ensure all the required users have access to the dashboard: Click on Share, then Email report: Create the Schedule, the Text and report preferences, add recipients and click Save report: If required, modify the analysis and re-save it as a dashboard, and create additional email reports.\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/teardown/4_teardown/","title":"Teardown","tags":[],"description":"","content":"To perform a teardown for this lab, perform the following steps:\nRemove QuickSight email reports\nGo into QuickSight Select All dashboards Click on the dashboard name Click Reports Select Unsubscribe Click Update Remove any created QuickSight dashboards\nGo into QuickSight Select All dashboards Click the 3 dots next to the dashboard name Click Delete Click Delete Remove any QuickSight analyses\nGo into QuickSight Select All analyses Click the 3 dots next to the analysis name Click Delete Click Delete Remove QuickSight Datasets\nGo into QuickSight Click Manage data Click on the dataset, we created summary_view s3_view compute_savings_plan_eligible_spend ec2_running_cost data_transfer_view Click Delete data set Click Delete Remove the Athena views\nGo into Athena Execute the following commands to remove the Cost Intelligence views: drop view costmaster.compute_savings_plan_eligible_spend drop view costmaster.ec2_running_cost drop view costmaster.ri_sp_mapping drop view costmaster.s3_view drop view costmaster.summary_view drop view costmaster.data_transfer_view drop view costmaster.account_map drop view costmaster.customer_all To disable Trusted Advisor Organizational View\nFollow the documentation X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/4_visualize_organization_data_in_quicksight/","title":"Visualize Organization Data in QuickSight","tags":[],"description":"","content":"Join your AWS Organizations data with the AWS Cost \u0026amp; Usage Report in Amazon Quicksight In Amazon Quicksight we will add to the existing AWS Cost \u0026amp; Usage Report from the Cost Visualization lab. Please make sure you have completed the 100 AWS Account Setup QuickSight part of the lab as this will allow QuickSight to have access to your S3 bucket that starts with \u0026lsquo;cost\u0026rsquo;.\nLog on to the console via SSO, go to the QuickSight service, Enter your email address and click Continue: Click Manage data in the top right: In your data set click on the Cost \u0026amp; Usage dataset you have already created. Select Edit Data Set On the Top Left of the screen click Add Data In the pop up select your organisation_data and click Select Now your data has been added we can setup the join. Click on the two circles connecting the two data sets. Below you will see two down downs where we can select the joins. The left box should be your Cost and Usage Report. Under that select the drop down bock and choose line_item_usage_account_id. In the right box under the Organizations_data select id. On the right select Full Join. Then click Apply. You can save this as a new data set by changing the name at the top to Cost_and_Usage_Data_with_Org. You then click on Save. Let\u0026rsquo;s setup a Schedule refresh for the data. Click on the QuickSight Icon in the top left to bring you back to the home page. Then click on Datasets on the left and find your dataset you just created. Click on the name then select Schedule refresh the click Create. Choose your Time zone and select Weekly. Selecting the Starting day as tomorrow so you match the 7 days set in the Amazon CloudWatch Event. Click Create and then the Exit button. Click on your dataset again and click Create analysis. Now you will be taken to a new dashboard. On the left you can see some of the column names we have added such as account_name.\nOn the top of the screen click on Field wells and pull in the following:\nDrag in account_name into Y axis line_item_unblended_cost into Value (ensure the Aggregate is Sum). The visual below will show you your spend by account name.\nCongratulations - QuickSight is now setup for your users to see the account names and other details in your dashboards.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_vpc/","title":"Level 200: Automated Deployment of VPC","tags":[],"description":"","content":"Last Updated: July 2020\nAuthors: Ben Potter, Security Lead, Well-Architected\nIntroduction This hands-on lab will use AWS CloudFormation to create an Amazon VPC to outline some of the AWS security features available. Using CloudFormation to automate the deployment provides a repeatable way to create and update, and you can re-use the template after this lab.\nThe example template will deploy a completely new VPC incorporating a number of AWS security best practices which include:\nNetworking subnets created in 3 availability zones for the following network tiers:\nApplication Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Database - named DB1 VPC Architecture: VPC endpoints are created for private connectivity to AWS services. Additional endpoints can be enabled for the application tier using the App1SubnetsPrivateLinkEndpoints CloudFormation parameter. NAT Gateways are created to allow subnets in the VPC to connect to the internet, without any direct ingress access as defined by the Route Table . Network ACLs control access at each subnet tier. VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Requirements An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with access to CloudFormation, EC2, VPC, IAM. Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . It is recommended to delete the CloudFormation stack when you have completed the lab.\nSteps: Create VPC Stack Tear Down References \u0026amp; useful resources Well-Architected: Protecting Networks AWS CloudFormation User Guide Amazon VPC User Guide Security in Amazon Virtual Private Cloud "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_vpc/2_cleanup/","title":"Tear Down","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nNote: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC.\nDelete the VPC CloudFormation stack:\nSign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-VPC stack. Click the Actions button then click Delete stack. Confirm the stack and then click Delete button. Delete the CloudWatch Logs:\nSign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\u0026lt;some unique ID\u0026gt;. Click the Actions Button then click Delete Log Group. Verify the log group name then click Yes, Delete. "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_clock_source_performance/4_cleanup/","title":"Teardown","tags":[],"description":"","content":"In this lab, you created two different EC2 instances and tested gettime system calls to each on to test the performance for each clocksource type. You were able to set a new clocksource for a Xen based instance type and see a dramatic improvement in the time it takes for these kinds of system calls.\nRemove all the resources via CloudFormation In order to remove the lab, go into the CloudFormation console, select the deployed template, click the drop down next to “Create Stack” and then click “Delete Stack”. This should remove all components created for this Lab.\nReferences \u0026amp; useful resources AWS Systems Manager Nitro How to manage the clock source for EC2 instances running Linux X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF2 - \u0026ldquo;Understand the available compute configuration options.\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/4_add_s3/","title":"Add an Amazon S3 Bucket to the Stack","tags":[],"description":"","content":"In this task, you will gain experience in editing a CloudFormation template and updating your CloudFormation stack\nYour objective is to deploy a new Amazon S3 bucket 4.1 Edit the CloudFormation template file From the Amazon S3 Template Snippets documentation page, copy the YAML example for Creating an Amazon S3 Bucket with Defaults Edit the simple_stack.yaml file you downloaded earlier to include an Amazon S3 bucket Under the Resources section add the snippet you copied You do not require any Properties for this new S3 bucket resource Indents are important in YAML \u0026ndash; use two spaces for each indent. Look at the other resources for guidance The correct solution only needs two lines \u0026ndash; one for the Logical ID and one for the Type Save the template Once you have edited the template, continue with the following steps to update the stack.\n4.2 Update the CloudFormation stack - specify updated template Go to the AWS CloudFormation console Click on Stacks Click on the CloudFormationLab stack Click Update Now click Replace current template selected. You are replacing the template again. Click Upload a template file Click Choose file Select simple_stack.yaml, your edited CloudFormation template file Click Next At this point you may see an error where you remain on the Update stack screen and a red banner across the top of the page displays an error message If you see Template format error then: Check the indentation and punctuation in your simple_stack.yaml file Once you have corrected the error, click Choose file again to reload you new corrected file If you did not see an error you may proceed\n4.3 Update the CloudFormation stack - complete the deployment On the Specify stack details click Next Click Next again, until you arrive at the Review CloudFormationLab screen Scroll down to Change set preview and note your S3 bucket is the only resource being added At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack It takes about a minute for the stack update to finish and the stack status is UPDATE_COMPLETE Click the Resources tab Note your new S3 bucket is listed among the resources deployed Click on the Physical ID of the S3 bucket to view the bucket on the S3 console Note the name is cloudformationlab-mys3bucket-\u0026lt;some_random_string\u0026gt;. The name for the S3 bucket was auto-generated by CloudFormation based on your CloudFormation stack name (converted to lowercase), plus the string \u0026ldquo;mys3bucket\u0026rdquo;, plus a randomly generated string.\nThe name for an S3 bucket must be unique across all S3 buckets in AWS Your bucket was assigned an auto-generated name because you did not specify a name in the S3 bucket properties in your CloudFormation template In the next exercise you will add a bucket name property for your S3 bucket and update the deployment 4.4 Assign name property for the S3 bucket For this task you are going to specify a Parameter where you can set the bucket name. To do this you will add a property on the S3 bucket resource that uses this parameter.\nUnder the Parameters section of your simple_stack.yaml template look at the S3BucketName parameter\nIt is not currently used in the template\n# S3 Bucket S3BucketName: Type: String Description: The name for the S3 bucket - must be unique across all of AWS (3-63 lowercase letters or numbers) Default: replaceme AllowedPattern: '^[a-z0-9]{5,40}$' ConstraintDescription: 3-63 characters; must contain only lowercase letters or numbers It is a string for which we have configured certain constraints\nThe AllowedPattern is a regular expression specifying only lowercase letters or numbers and a string length between 3-63 characters\nThis satisfies the constraints on what is allowed in an S3 bucket name\nIt is actually more constrictive than what is allowed. See Rules for Bucket Naming under Bucket Restrictions and Limitations for more details.\nAdd a few more lines to your S3 bucket under in the Resources section of your template so it looks like this\nBe cautious to maintain the two-space indents where indicated\nMyS3Bucket: Type: 'AWS::S3::Bucket' Properties: BucketName: !Join - '-' - - !Ref S3BucketName - !Ref 'AWS::Region' The Properties label defines that the items that follow (indented underneath) are properties of the S3 bucket\nFor the BucketName property you are specifying a reference to another value in the template. Specifically you are indicating that the string entered as the S3BucketName parameter should be used as the name of the bucket\nThe !Join function concatenates strings in a CloudFormation template. Use that to add the AWS Region to make the bucket more unique. AWS::Region is a pseudo-parameter available within CloudFormation.\nSave the file\nGo to the AWS CloudFormation console Click on Stacks\nClick on the CloudFormationLab stack\nClick Update\nNow click Replace current template selected. This is different from what you did for the last update.\nClick Upload a template file\nClick Choose file\nSelect simple_stack.yaml, your edited CloudFormation template file Click Next \u0026ndash; Look for any errors reported\nOn the Specify stack details look at the Parameters\nYou must enter a value for S3BucketName (you must replace the default value) Remember it must be a name that no other bucket in all of AWS is already using (try appending the name you choose with the date on which you are going through this lab to increase uniqueness and reduce chances of stack update failures) Click Next again, until you arrive at the Review CloudFormationLab screen\nScroll down to Change set preview and note your S3 bucket will be modified Note where it says Replacement is True. This means it will actually delete the current bucket and replace it with a new one with the newly specified name At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack It takes about a minute for the stack update to finish and the stack status is UPDATE_COMPLETE\nUnder the resources tab see your newly named S3 bucket Troubleshooting\nIf when trying to upload your new template you see Invalid template resource property Check that the properties you specified for the resource you added match the properties in the documentation. Once you have corrected the error, click Choose file again to reload you new corrected file If your CloudFormation stack fails, then click on the Events tab and scroll down to find the source of the error If you see a message like \u0026lt;your_chosen_bucket_name\u0026gt; already exists then re-do the CloudFormation update steps, but specify a more unique bucket name X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_loft_introduction_to_security/4_automated_detective_controls/","title":"Automated Deployment of Detective Controls","tags":[],"description":"","content":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail.\nWalkthrough Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/4_build_run_remediation_runbook/","title":"Build &amp; Run Remediation Runbook","tags":[],"description":"","content":"In contrast to playbooks, runbooks are procedures that accomplish specific tasks to achieve an outcome. In the previous section, you have identified an issue with CPU utilization, which occurs because there is only 1 ECS task running in the cluster. This could be remediated through the use of auto-scaling.\nHowever, implementing this requires preparation and planning. When an incident occurs, operations teams should have a defined escalation path for the issue. Depending on the criticality of the system they should also be equipped to do what is necessary to ensure system availability is protected while the escalation occurs.\nIn this section, you will build an automated runbook to remediate the CPU utilization issue by increasing the number of tasks in the ECS cluster. Your automated runbook, will notify the owner of the workload and give them the option to be able to intercept the scale-up action should they choose not to proceed.\nActions items in this section: You will build a runbook to scale up the ECS cluster, with the approval mechanism. You will execute the runbook and observe the recovery of your application. 4.0 Building the \u0026ldquo;Approval-Gate\u0026rdquo; Runbooks. In this section you will build a reusable runbook, which provides the owner with the ability to deny or approve remediation actions within a defined waiting period. If the wait time is exceeded and a decision has has not been made, the runbook will automatically approve the action as shown.\nWe will achieve this through the use of a Systems Manager Automation document, which we will build using the following steps:\nThe Approval-Gate runbook executes a separate document called the Approve-Timer.\nThe Approve-Timer runbook will then wait for a preconfigured amount of time and send an approve signal to the Approval-Gate runbook.\nMeanwhile, the Approval-Gate runbook then sends an approval request to the workload owner via a designated SNS topic.\nIf the owner choose to approve, the Approval-Gate runbook will continue to the next step. If the owner declines the approval, the runbook will fail, blocking further steps. However, if the owner does not response within the preconfigured wait time, the Approve-Timer runbook will automatically approve the request. Follow the instructions below to build the runbook:\nNote: Select a step-by-step guide below to build the runbook using either the AWS console or CloudFormation template.\nClick here for Console step by step Go to the AWS Systems Manager console. Click Documents under Shared Resources on the left menu. Then click Create Automation as show in the screen shot below:\nEnter Approval-Timer in the Name field and copy the notes shown below into the Document description field.\n# What does this automation do?\rAutomatically trigger \u0026#39;Approval\u0026#39; Signal to an execution, after a timer lapse\r## Steps 1. Sleep for X time specified on the parameter input\r2. Automatically signal \u0026#39;Approval\u0026#39; to the Execution specified in parameter input In the Assume role field, enter the IAM role ARN we created in the previous section 3.0 Prepare Automation Document IAM Role.\nExpand the Input Parameters section and enter Timer as the Parameter name. Set the type as String and Required as Yes.\nThen add another parameter this time called AutomationExecutionId, of type String and set Required to Yes. Once you are done, your configuration should look like the screenshot below.\nUnder Step 1 section specify SleepTimer as Step name, select aws::sleep as the Action type.\nExpand the Inputs section of the step, and specify {{Timer}} as the Duration\nClick on Add step and specify ApproveExecution as Step name, select aws::executeAwsApi as the Action type.\nExpand the Inputs section of the step, and specify ssm in the Service field and SendAutomationSignal in the API field.\nUnder Additional inputs specify below values.\nApprove as the SignalType {{AutomationExecutionId}} as the AutomationExecutionId. Once you are done, your configuration should look like the screenshot below.\n6 . Click on Create automation once you are done.\nNext, you will create the Approval-Gate runbook responsible for running the Approval-Timer runbook asynchronously. Follow below steps to complete the configuration:\nFrom the AWS Systems Manager console, select Documents under Shared Resources on the left menu. Then click Create Automation as show in the screen shot below:\nNext, enter Approval-Gate in the Name field and add the notes shown below to the Document description field.\n# What does this automation do?\rPlace a gate before your desired step to create approval mechanism.\rAutomation will trigger an asynchronously timer that will automatically approve once the time has lapsed.\rAutomation will then send approval / deny request to the designated SNS Topic.\rWhen deny is triggered by approver, the step will fail and block the following step from executing.\rNote: Please ensure to have onFailure set to abort in your automation document.\r## Steps 1. Trigger an asynchronously timer that will automatically approve once the time has lapsed.\r2. Send approval / deny request to the designated SNS Topic. In the Assume role field, enter the IAM role ARN we created in the previous section 3.0 Prepare Automation Document IAM Role.\nExpand the Input Parameters section and enter the following:\nTimer as the Parameter name, set the type as String and Required as Yes. NotificationMessage as the Parameter name, set the type as type String and Required is Yes. NotificationTopicArn as the Parameter name, set the type as type String and Required is Yes. ApproverRoleArn as the Parameter name, set the type as type String and Required is Yes. Expand Step 1 create a step named executeAutoApproveTimer and action type aws:executeScript.\nExpand Inputs, then set the Runtime as Python3.6 and paste in below code into the script section. Note that code snippet will execute the Approval-Timer runbook you created asyncronously.\nimport boto3\rdef script_handler(event, context):\rclient = boto3.client(\u0026#39;ssm\u0026#39;)\rresponse = client.start_automation_execution(\rDocumentName=\u0026#39;Approval-Timer\u0026#39;,\rParameters={\r\u0026#39;Timer\u0026#39;: [ event[\u0026#39;Timer\u0026#39;] ],\r\u0026#39;AutomationExecutionId\u0026#39; : [ event[\u0026#39;AutomationExecutionId\u0026#39;] ]\r}\r)\rreturn None Expand Additional Inputs, then select InputPayload under Input Name, and add the text shown below to Input Value:\nAutomationExecutionId: \u0026#39;{{automation:EXECUTION_ID}}\u0026#39;\rTimer: \u0026#39;{{Timer}}\u0026#39; Once you have completed this step, your Step 1 configuration should look like below screenshot.\nClick Add step to create Step 2\nCreate a step named ApproveOrDeny and action type aws:approve.\nExpand Inputs and specify below values under Approvers, replacing the AutomationRoleArn with the Arn of AutomationRole you took note of in section 3.0 Prepare Automation Document IAM Role.\n[ \u0026#39;{{ApproverRoleArn}}\u0026#39;, \u0026#39;AutomationRoleArn\u0026#39; ] Example:\n[ \u0026#39;{{ApproverRoleArn}}\u0026#39;, \u0026#39;arn:aws:iam::xxxxx:role/AutomationRole\u0026#39; ] Expand Additional Inputs and specify the following values:\nNotificationArn as the Input name, and {{NotificationTopicArn}} as the Input value Message as the Input name, and {{NotificationMessage}} as the Input value MinRequiredApprovals as the Input name, and 1 as the Input value Expand Common properties and change the following properties to below values (keep the remaining as it is):\nContinue for On failure false for Is critical Once you have completed this step, your Step 2 configuration should look like below screenshot.\nClick Add step to create Step 3\nCreate a step named getApprovalStatus and action type aws:executeAwsApi\nExpand Inputs and specify ssm in the Service field, and DescribeAutomationStepExecutions in the API field.\nExpand Additional Inputs and specify below values:\nAutomationExecutionId as the Input Name, and {{automation:EXECUTION_ID}} as the Input value\nFilters as the Input Name, and copy below values as the Input value\n- Key: StepName\rValues:\r- requestApproval Expand Outputs and specify below values:\napprovalStatusVariable as the Name $.StepExecutions[0].Outputs.ApprovalStatus[0] as the Selector String as the Type Once you have completed this step, your Step 3 configuration should look like below screenshot.\nClick on Create automation to complete the configuation.\nClick here for CloudFormation deployment steps Download the template here. If you decide to deploy the stack from the console, ensure that you follow below requirements \u0026amp; step:\nPlease follow this guide for information on how to deploy the CloudFormation template. Use waopslab-runbook-approval-gate as the Stack Name, as this is referenced by other stacks later in the lab. Click here for CloudFormation CLI deployment step From the Cloud9 terminal, change to the templates folder as shown:\ncd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/templates Run the below commands, replacing the AutomationRoleArn with the Arn of AutomationRole you took note of in section 3.0 Prepare Automation Document IAM Role.\naws cloudformation create-stack --stack-name waopslab-runbook-approval-gate \\\r--parameters ParameterKey=PlaybookIAMRole,ParameterValue=AutomationRoleArn \\\r--template-body file://runbook_approval_gate.yml With your AutomationRole Arn in place your command will look similar to the following example:\naws cloudformation create-stack --stack-name waopslab-runbook-approval-gate \\\r--parameters ParameterKey=PlaybookIAMRole,ParameterValue=arn:aws:iam::000000000000:role/xxxx-runbook-role \\\r--template-body file://runbook_approval_gate.yml Confirm that the stack has installed correctly. You can do this by running the describe-stacks command below, locate the StackStatus and confirm it is set to CREATE_COMPLETE.\naws cloudformation describe-stacks --stack-name waopslab-runbook-approval-gate 4.1 Building the \u0026ldquo;ECS-Scale-Up\u0026rdquo; runbook. Next, you are going to build the ECS-Scale-Up runbook which will complete the following:\nRun the Approval-Gate runbook which you created previously. Wait for the Approval-Gate runbook to complete. Once the Approval-Gate runbook completes successfully, the runbook will increase the number of ECS tasks in the cluster. Please follow below steps to build the runbook.\nNote: Select a step-by-step guide below to build the runbook using either the AWS console or CloudFormation template.\nClick here for Console step by step Go to the AWS Systems Manager console. Click Documents under Shared Resources on the left menu. Then click Create Automation as show in the screen shot below.\nNext, enter Runbook-ECS-Scale-Up in the Name field and add the notes shown below to the Document description field:\n# What does this automation do?\rScale up a given ECS service task desired count to certain number, with approval process.\rThe automation will trigger Approval-Gate runbook, before executing.\r## Steps 1. Trigger Approval-Gate\r2. Scale ECS Service by number of service In the Assume role field, enter the IAM role ARN we created in the previous section 3.0 Prepare Automation Document IAM Role.\nExpand the Input Parameters section and enter the following.\nECSDesiredCount as the Parameter name, set the type as Integer and Required as Yes. ECSClusterName as the Parameter name, set the type as String and Required is Yes. ECSServiceName, as the Parameter name, set the type as String and Required is Yes. NotificationTopicArn, as the Parameter name, set the type as String and Required is Yes. NotificationMessage, as the Parameter name, set the type as String and Required is Yes. ApproverRoleArn, as the Parameter name, set the type as String and Required is Yes. Timer, as the Parameter name, set the type as String and Required is Yes. Expand Step 1 create a step named executeApprovalGate and action type aws:executeAutomation.\nExpand Inputs, then set the Document name as Approval-Gate.\nExpand Additional inputs and select RuntimeParameters as the Input Name\nPaste in below as the Input Value\n{\r\u0026#34;Timer\u0026#34;:\u0026#39;{{Timer}}\u0026#39;,\r\u0026#34;NotificationMessage\u0026#34;:\u0026#39;{{NotificationMessage}}\u0026#39;,\r\u0026#34;NotificationTopicArn\u0026#34;:\u0026#39;{{NotificationTopicArn}}\u0026#39;,\r\u0026#34;ApproverRoleArn\u0026#34;:\u0026#39;{{ApproverRoleArn}}\u0026#39;\r} Click Add Step to create the second step.\nSpecify updateECSServiceDesiredCount as Step Name and select aws:executeAwsApi as Action type.\nExpand Inputs and configure the following values:\necs as Service UpdateService as Api Expand Additional inputs and configure the following values:\nforceNewDeployment as the Input Name and true as Input Value desiredCountas the Input Name and {{ECSDesiredCount}} as Input Value service as the Input Name and {{ECSServiceName}} as Input Value cluster as the Input Name and {{ECSClusterName}} as Input Value 13 . Click on Create automation once complete\nClick here for CloudFormation Console deployment step Download the template here. If you decide to deploy the stack from the console, ensure that you complete the following steps:\nPlease follow this guide for information on how to deploy the CloudFormation template. Use waopslab-runbook-scale-ecs-service as the Stack Name, as this is referenced by other stacks later in the lab. Click here for CloudFormation CLI deployment step From the Cloud9 terminal, run the command to get into the working script folder.\ncd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/templates Then run below commands, replacing the \u0026lsquo;AutomationRoleArn\u0026rsquo; with the Arn of AutomationRole you took note in previous step 3.0 Prepare Automation Document IAM Role.\naws cloudformation create-stack --stack-name waopslab-runbook-scale-ecs-service \\\r--parameters ParameterKey=PlaybookIAMRole,ParameterValue=AutomationRoleArn \\\r--template-body file://runbook_scale_ecs_service.yml Example:\naws cloudformation create-stack --stack-name waopslab-runbook-scale-ecs-service \\\r--parameters ParameterKey=PlaybookIAMRole,ParameterValue=arn:aws:iam::000000000000:role/AutomationRole \\\r--template-body file://runbook_scale_ecs_service.yml Confirm that the stack has installed correctly. You can do this by running the describe-stacks command below, locate the StackStatus and confirm it is set to CREATE_COMPLETE.\naws cloudformation describe-stacks --stack-name waopslab-runbook-scale-ecs-service 4.2 Executing remediation Runbook. Now, lets run the runbook you created above to remediate the issue.\nGo to the AWS CloudFormation console.\nClick on the stack named walab-ops-sample-application.\nClick on the Output tab, and take note following output values. You will need these values to execute the runbook.\nOutputECSCluster OutputECSService OutputSystemOwnersTopicArn If you are currently using an IAM user or role to log into your AWS Console, take note of the ARN. You will need this ARN when executing the runbook to restrict access to approve or deny request capability.\nTo find your current IAM user ARN, go to the IAM console and click Users on the left side menu, then click on your User name. For IAM role, go to the IAM console and click Roles on the left side menu, then click on the Role name, you are using.\nYou will see something similar to the example below. Take note of the ARN value,and proceed to the next step.\nGo to the Systems Manager Automation console, click on Document under Shared Resources, locate and click an automation document called Runbook-ECS-Scale-Up.\nThen click *Execute automation.\nFill in the Input parameters with values below.\nFor ECSServiceName, place the value of OutputECSService you took note on step 3.\nFor ECSClusterName, Place the value of OutputECSCluster you took note on step 3.\nFor ApproverArn, place the ARN value you took note on step 4.\nFor ECSDesiredCount, place in 100 to increase the task number to 100.\nFor NotificationMessage, place in any message that can help the approver make an informed decision when approving or denying the requested action.\nFor example:\nHello, your mysecretword app is experiencing performance degradation. To maintain quality customer experience we will manually scale up the supporting cluster. This action will be approximately 10 minutes after this message is generated unless you do not consent and deny the action within the period. For NotificationTopicArn, place the value of OutputSystemOwnersTopicArn you took note on step 3.\nFor Timer, you can specify PT5M or specify a value defined in ISO 8601 duration format.\nClick Execute to run the runbook.\nOnce the runbook is running, you will receive an email with instructions approve or deny, on the email address subscribed to the owners SNS topic ARN. Follow the link in the email using the User of the ApproverArn you placed in the Input parameters. The link will take you to the SSM Console where you can approve or deny the request.\nIf you approve, or ignore the email, the request will be automatically be approved after the Timer set in the runbook expires. If you deny, the runbook will fail and no action will be taken.\nOnce the runbook completes, you can see that the ECS task count increased to the value specified.\nGo to ECS console and click on Clusters and select mysecretword-cluster.\nClick on the mysecretword-service Service, and you will see the number of running tasks increasing to 100 and the average CPUUtilization decrease.\nSubsequently, you will see the API response time returns to normal and the CloudWatch Alarm returns to an OK state.\nYou can check both using your CloudWatch Console, following the steps you ran in section 2.1 Observing the alarm being triggered.\nCongratulations ! You have now completed the Automating operations with Playbooks and Runbooks lab, click on the link below to cleanup the lab resources.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/4_memory_plugin/","title":"CloudWatch Agent Manual Install","tags":[],"description":"","content":" There are multiple ways to install the CloudWatch agent. This lab will walk through a manual install on a single instance. Please visit the CloudWatch installation documentation for a comprehensive list of ways to install CloudWatch.\nOn the left bar in the EC2 console, click on Instances and select the EC2 Instance with the CloudWatchAgentServerRole IAM role. Connect into the EC2 Instance using the browser-based SSH connection tool. Download the Amazon Cloudwatch agent package, the instructions below are for Amazon Linux, for other OS please check here wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip Unzip and Install the package unzip AmazonCloudWatchAgent.zip sudo ./install.sh Configure the AmazonCloudWatchAgent profile Before running the CloudWatch agent on any servers, you must create a CloudWatch agent configuration file, which is a JSON file that specifies the metrics and logs that the agent is to collect, including custom metrics. You can create it by using the wizard or by writting it yourself from scratch. Any time you change the agent configuration file, you must then restart the agent to have the changes take effect.\nThe wizard can autodetect the credentials and AWS Region to use if you have the AWS credentials and configuration files in place. For more information about these files, see Configuration and Credential Files in the AWS Systems Manager User Guide and the AWS documentation page .\nFor now, let\u0026rsquo;s start the CloudWatch agent configuration file wizard executing the command below at the selected EC2 instance.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard For this lab we want to keep the following structure:\nCloudWatch Agent Configutation File Wizard Parameter On which OS are you planning to use the agent? 1. Linux Are you using EC2 or On-Premises hosts? 1. EC2 Which user are you planning to run the agent? 2. cwagent Do you want to turn on StatsD daemon? 2. No Do you want to monitor metrics from CollectD? 2. No Do you want to monitor any host metrics? 1. Yes Do you want to monitor cpu metrics per core? 2. No Do you want to add ec2 dimensions? 1. Yes Would you like to collect your metrics at high resolution? 4. 60s Which default metrics config do you want? 1. Basic Are you satisfied with the above config? 1. Yes Do you have any existing CloudWatch Log Agent? 2. No Do you want to monitor any log files? 2. No Do you want to store the config in the SSM parameter store? 2. No The CloudWatch Agent config file should look like the following:\n{ \u0026#34;agent\u0026#34;: { \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;run_as_user\u0026#34;: \u0026#34;cwagent\u0026#34; }, \u0026#34;metrics\u0026#34;: { \u0026#34;append_dimensions\u0026#34;: { \u0026#34;AutoScalingGroupName\u0026#34;: \u0026#34;${aws:AutoScalingGroupName}\u0026#34;, \u0026#34;ImageId\u0026#34;: \u0026#34;${aws:ImageId}\u0026#34;, \u0026#34;InstanceId\u0026#34;: \u0026#34;${aws:InstanceId}\u0026#34;, \u0026#34;InstanceType\u0026#34;: \u0026#34;${aws:InstanceType}\u0026#34; }, \u0026#34;metrics_collected\u0026#34;: { \u0026#34;disk\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 } } } } Start the CloudWatch Agent sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s It may take up to 5 minutes for the metrics to become available, go back to the Amazon CloudWatch console page, under the Metrics session to validate that you are getting Memory information.\nClick CWAgent: Click ImageID,InstanceID,InstanceType: Select the Instance from the list below: You have now completed the CloudWatch agent installation and will be able to monitor on Amazon CloudWatch the memory utilization of that instance.\nAutomation Option The next step is not mandatory to complete this lab.\nIf you have to install and start the CloudWatch agent on several instances at once doing it manually might not be a scalable option. Consider using AWS Systems Manager or a pre-configured AWS CloudFormation template to automatically install the CloudWatch agent by default on all your stacks.\nAWS Systems Manager Steps:\nCreate IAM Roles and Users for Use with CloudWatch Agent Download and Configure the CloudWatch Agent Install the CloudWatch Agent on EC2 Instances Using Your Agent Configuration Install the CloudWatch Agent on On-Premises Servers CloudFormation Steps:\nRight-click and save link as: here to download the AWS Cloudformation template Go to the AWS CloudFormation console Click to Create Stack and select Upload a template file and point to the downloaded file Enter a Stack Name and select a KeyName Enter the tag Key: Event | Value: myStackforWACostLab Click Next and Create stack X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/4_configure_function_parameters/","title":"Configure parameters of function code and upload code to S3","tags":[],"description":"","content":"This step is used to edit parameters (CUR database name and table, SES sender and recipient etc) in the Lambda function code, which is then uploaded to S3 for Lambda execution.\nDownload function code https://d3h9zoi3eqyz7s.cloudfront.net/Cost/AutoCURDelivery.zip to your local disk. This zip file includes: - auto_cur_delivery.py - Lambda function code\nconfig.yml - Configuration file package/ - All dependencies, libraries, including pandas, numpy, Xlrd, Openpyxl, Xlsxwriter, pyyaml Unzip config.yml from within AutoCURDelivery.zip, and open it into a text editor.\nConfigure the following parameters in config.yml:\nCUR_Output_Location: Your S3 bucket created previously, i.e. S3://my-cur-bucket/out-put/ CUR_DB: CUR database and table name defined in Athena, i.e. \u0026lsquo;\u0026ldquo;athenacurcfn_my_athena_report\u0026rdquo;.\u0026ldquo;myathenareport\u0026rdquo;\u0026rsquo; CUR_Report_Name: Report filename that is sent with SES as an attachment, i.e. cost_utilization_report.xlsx Region: The region where SES service is called, i.e. us-east-1 Subject: SES mail subject, i.e. Cost and Utilization Report Sender: Your sender e-mail address, i.e. john@example.com Recipient: Your recipient e-mail addresses. If there are multiple recipients, separate them by comma, i.e. john@example.com , alice@example.com Keep other configuration unchanged and save config.yml.\nAdd the updated config.yml back to AutoCURDelivery.zip.\nUpload AutoCURDelivery.zip to your S3 bucket. Make sure this S3 path is in the same region as Lambda function created in next step. NOTE this is a large 30+MB file, so it may take a little time.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/4_configure_the_workload_logging_and_alarm/","title":"Configure The Workload Logging and Alarm","tags":[],"description":"","content":"4.1. We are now going to create a filter within our CloudWatch Log Group. This filter will generate a CloudWatch metric which we will use as to create our alarm.\nTo create your filter, complete the following configuration steps:\n4.1.1. Navigate to CloudWatch in your console and click on Log Groups on the side menu.\n4.1.2. Locate the pattern1-logging-loggroup you created in the previous section and click on the the log group as shown:\n4.1.3. Select the tick box beside the log groups, click on Actions and then Create metric filter as shown:\n4.1.4. Enter below filter under Filter pattern\n{ $.errorCode = \u0026#34;*\u0026#34; \u0026amp;\u0026amp; $.eventSource= \u0026#34;kms.amazonaws.com\u0026#34; \u0026amp;\u0026amp; $.userIdentity.sessionContext.sessionIssuer.arn = \u0026#34;\u0026lt;ECS Task Role ARN\u0026gt;\u0026#34; } Note: Replace \u0026lt; ECS Task Role ARN \u0026gt; with the value of OutputPattern1ECSTaskRole. This value was provided in the Output section in the pattern1-app. If you need a reminder, you can refer to section 2.3.\nWhen you have completed this, you can click Next.\n4.1.5. It is important at this stage to understand the importance of filtering using this rule. The filter which we created in the previous step will look for all error codes which come from an eventSource of kms.amazonaws.com where the identity of the request matches the ECS Task role ARN.\nThis means that When KMS triggers an event by our application, the event registered within CloudTrail will look like this:\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, ... \u0026#34;sessionContext\u0026#34;: { \u0026#34;sessionIssuer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Role\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAQKTRYBJEYHGY4HLFO\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::xxxxxxxxxxx:role/pattern1-application-Pattern1ECSTaskRole\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;xxxxxxxxxxx\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;pattern1-application-Pattern1ECSTaskRole\u0026#34; }, ... } }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-11-16T22:25:39Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;kms.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;Decrypt\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;ap-southeast-2\u0026#34;, \u0026#34;errorCode\u0026#34;: \u0026#34;IncorrectKeyException\u0026#34;, \u0026#34;errorMessage\u0026#34;: \u0026#34;The key ID in the request does not identify a CMK that can perform this operation.\u0026#34;, ..... \u0026#34;responseElements\u0026#34;: null, \u0026#34;requestID\u0026#34;: \u0026#34;11748bbd-ddcd-4ee2-9f42-9cec69f414b1\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;1f620618-46e5-4f78-93cc-0b7bccfff5d2\u0026#34;, \u0026#34;readOnly\u0026#34;: true, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;xxxxxxxxxxx\u0026#34; } Our configured filter rule will perform filtering based on the JSON keys which are presented by the event as follows:\n$.eventSource : Describes the EventSource of \u0026ldquo;kms.amazon.com\u0026rdquo; signifying that it is a KMS event. $.errorCode : Describes any value with key \u0026ldquo;ErrorCode\u0026rdquo; signifying that an error event is being generated. $.userIdentity.sessionContext.sessionIssuer.arn: filters for the the userIdentity that executes the event. This is the assumed role that is used by ECS, which indicates that this call was made from our application running in the container. Now that we have explained the details of how our filter operates, we can complete the configuration.\n4.1.5. In the Assign Metric form, enter the following configuration detail:\nEnter pattern1-logging-metricfilter as the Filter name. Enter Pattern1Application/KMSSecurity as the Metric namespace. Enter KMSSecurityError as the Metric name. Enter 1 as the Metric Value. Your completed configuration should match the following screenshot:\nWhen you have verified your configuration, click Next and Create metric filter\n4.2 Create The Metric Alarm. Once your Metric filter has been created, you should be able to view it under the Metric filters tab of your LogGroups. We will now create the Metric Alarm from this filter.\nComplete the following steps:\n4.2.1. Select the Metric filter you just created, then click on CreateAlarm as shown:\n4.2.2. Change the name of the metric to KMSsecurityError and set the Period to 10 seconds as shown:\n4.2.3. Within the conditions dialog box, configure the following:\nSet the Threshold type as Static Set the condition as Greater \u0026gt; threshold Set the threshold value as 1 Under Additional Configuration set Missing data treatment as Treat missing data as Ignore(maintain the alarm state) Your configuration should match the following screenshot:\nWhen your configuration is complete, click Next\n4.2.4. In the Notification dialog box, configure the following:\nSelect In alarm as alarm trigger state. Select Create new topic and enter pattern1-logging-topic as the topic name. Enter an email address where you would like to receive notification. When your configuration is complete, click Create topic then click Next\n4.2.5. Complete the following configuration to complete the alarm setup:\nEnter pattern1-logging-alarm as the Alarm name and click Next Review the setting and click Create Alarm Wait for an email to arrive in your mailbox, and confirm subscription to you the topic once it arrives as shown here: This completes the creation of the filter and alarm for the lab. Proceed to Section 5 to test functionality.\nEND OF SECTION 4\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/4_budget_report/","title":"Create and implement an AWS Budget Report","tags":[],"description":"","content":"AWS Budgets Reports allow you to create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budgets.\nFrom the Budgets dashboard, Click on Budgets Reports: Click Create budget report: Create a report with the following details:\nSelect all budgets Report frequency: Weekly Day of week: Monday Email recipients: Input your email address Report name: WeeklyBudgets Select the Create budget report button when finished: Your budget report should now be complete: You should receive an email similar to the one below: You have created a budget report. Use reports to regularly track your progress against defined budgets.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/4_lambda_function/","title":"Create Lambda function to run the Saved Queries","tags":[],"description":"","content":"This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself.\n1 - Go to the IAM service dashboard\n2 - Create a policy named LambdaSubAcctSplit\n3 - Edit the following policy inline with security best practices, and add it to the policy:\n./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services\n5 - Attach the LambdaSubAcctSplit policy\n6 - Name the role LambdaSubAcctSplit\n7 - Go into the Lambda service dashboard\n8 - Create a function named SubAcctSplit, Author from scratch using the Python 3.7 Runtime and role LambdaSubAcctSplit:\n9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top.\n11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end\n12 - Change the Execution role to LambdaSubAcctSplit\n13 - Save the function\n14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test.\nYou have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/4_create_lambda/","title":"Create Lambda in account 1","tags":[],"description":"","content":" Open the Lambda console https://console.aws.amazon.com/lambda Click Create a function\nAccept the default Author from scratch\nEnter function name as Lambda-List-S3\nSelect Python 3.7 runtime\nExpand Permissions, click Use an existing role, then select the Lambda-List-S3-Role\nClick Create function\nReplace the example function code with the following\nReplace bucketname with the S3 bucket name from account 2\nimport json import boto3 import os import uuid def lambda_handler(event, context): try: # Create an S3 client s3 = boto3.client('s3') # Call S3 to list current buckets objlist = s3.list_objects( Bucket='bucketname', MaxKeys = 10) print (objlist['Contents']) return str(objlist['Contents']) except Exception as e: print(e) raise e Click Save.\nClick Test, accept the default event template, enter an event name for the test, then click Create\nClick Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/4_recommendation_dashboard/","title":"Create the Recommendation Dashboard","tags":[],"description":"","content":" Go to the QuickSight service homepage: Go to the sp_usage analysis: Create a line chart, add line_item_usage_start_date to the X axis, aggregate day. Add spprice to the Value and set the aggregate to min. Drag the product_instance_type to Colour field well. Change the title to Usage in Savings Plan Rates: Click Parameters, and click Create one: Parameter name OperatingSystem, Data type String, click Set a dynamic default: Select your dataset, and select product_operating_system for the columns, click Apply: Click Create: Click Control: Enter OperatingSystem as the display name, style Single select drop down, values Link to a data set field, dataset your data set, column product_operating_system, click Add: Using the process above, Add the parameter Region:\nName: Region Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_location, product_location Add as: Control Control Display Name: Region Style: Single select drop down Values: link to data set field Data set: your data set Column: product_location Using the process above, Add the parameter Tenancy:\nName: Tenancy Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_tenancy, product_tenancy Add as: Control Control Display Name: Tenancy Style: Single select drop down Values: link to data set field Data set: your data set Column: product_tenancy Create an InstanceType parameter, datatype String, Single value, Static default value of . (a full stop): Click Control, Display name InstanceType, style Text box, click Add: Click Filter and click Create one, select product_instance_type: Edit the filter, Filter type:\nAll visuals Custom filter Contains Use Parameters InstanceType click Apply: Create a Parameter DaysToDisplay:\nName: DaysToDisplay Data type: Integer Values: Single value Static default value: 90 Click Create: Click Control: Enter a Display name DaysToDisplay, Style Text box and click Add: Click on Filter, click +, and select line_item_usage_start_date: Click on the new filter: Select a filter type of:\nAll visuals Relative dates Days Last N days select Use parameters, and accept to change the scope of the filter select the parameter DaysToDisplay click Apply: Create a filter for product_operating_system:\nAll visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: OperatingSystem Create a filter for product_location:\nAll visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Region Create a filter for product_tenancy:\nAll visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Tenancy Click on Visualize, click Add, select Add calculated field: Field name HoursDisplayed, add the formula below and click Create:\ndistinct_count({line_item_usage_start_date}) Create a calculated field HoursRun, formula:\nHoursDisplayed / (${DaysToDisplay} * 24) Create a calculated field PayOffMonth, formula:\nifelse(((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))) \u0026lt; 12,((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))),12) Create a calculated field SavingsPlanReco, formula:\nifelse(PayOffMonth \u0026lt; 12,percentile(spprice,10),0.00) Create a calculated field StartSPPrice, formula:\nlag(min(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} - 2,[{product_instance_type}]) Create a calculated field Trend, formula:\n(min(spprice) - {StartSPPrice}) / min(spprice) Create a calculated field First3QtrAvg, formula:\nwindowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay},${DaysToDisplay} / 4,[{product_instance_type}]) Create a calculated field LastQtrAvg, formula:\nwindowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} / 4,1,[{product_instance_type}]) Create a calculated field TrendAvg, formula:\n(LastQtrAvg- First3QtrAvg) / First3QtrAvg Add a Visual, click Add, select Add visual: Select a Table visualization, Group by product_instance_type, Add the values:\nSavingsPlanReco PayOffMonth discountrate, aggregate: average HoursRun, show as percent Label it Recommendations Add a Pivot Table visual, Rows: product_instance_type and line_item_usage_start_date aggreate: day, Add the values:\ninstancecount aggregate: average Trend TrendAvg (show as percent) Label it Trends, Add a filter to this visual only:\nFilter on: StartSPPrice Type: Custom filter Operation: Greater than Value: -1 Nulls: Exclude nulls Decrease the width of the date column as much as possible, its not needed\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/4_deploy_the_build_automation_with_ssm/","title":"Deploy The Build Automation With SSM","tags":[],"description":"","content":"Now that our AMI Builder Pipeline is built, we can now work on the final automation stage with Systems Manager.\nIn this section we will orchestrate the build of a newly patched AMI and its associated deployment into our application cluster.\nTo automate this activities we will leverage AWS Systems Manager Automation Document .\nUsing our SSM Automation document we will execute the following activities:\nAutomate the execution of the EC2 Image Builder Pipeline. Wait for the pipeline to complete the build, and capture the newly created AMI with updated OS patch. Then it will Update the CloudFormation application stack with the new patched Amazon Machine Image. This AMI update to the stack will in turn trigger the CloudFormation AutoScalingReplacingUpdate policy to perform a simple equivalent of a blue/green deployment of the new Autoscaling group. Note: Using this approach, we streamline the creation of our AMI, and at the same time minimize interruption to applications within the environment.\nAdditionally, by leveraging the automation built in Cloudformation through autoscaling update policy, we reduce the complexity associated with building out a blue/green deployment structure manually. Lets look at how this works in detail:\nFirstly, CloudFormation detects the need to update the LaunchConfiguration with a new Amazon Machine Image. Then, CloudFormation will launch a new AutoScalingGroup, along with it\u0026rsquo;s compute resource (EC2 Instance) with the newly patched AMI. CloudFormation will then wait until all instances are detected healthy by the Load balancer, before terminating the old AutoScaling Group, ultimately achieving a blue/green model of deployment. Should the new compute resource failed to deploy, cloudformation will rollback and keep the old compute resource running. For details about how this is implemented in the CloudFormation template, please review the pattern3-application.yml template deployed in section 2.\nOnce we complete this section our architecture will reflect the following diagram:\nIn this section you have the option to build the resources manually using AWS console. If however you are keen to complete the lab quickly, you can simply deploy from the CloudFormation template and take a look at the deployed architecture. Select the appropriate section:\nBuild with a CloudFormation template on the command-line 4.1. Get The Template The template for Section 4 which can be found here .\n4.2. Deploy From The Command Line To deploy from the command line, ensure that you have installed and configured AWS CLI with the appropriate credentials. When you are ready, execute the following command:\naws cloudformation create-stack --stack-name pattern3-automate \\ --template-body file://pattern3-automate.yml \\ --parameters ParameterKey=ApplicationStack,ParameterValue=pattern3-app \\ ParameterKey=ImageBuilderPipelineARN,ParameterValue=\u0026lt;enter image builder pipeline arn\u0026gt; 4.3. Record The CloudFormation Output. Once the template is finished execution, note the Automation Document Name from the Cloudformation output specified under Pattern3CreateImageOutput.\nBuild with a CloudFormation template in the console 4.1. Get The Template The template for Section 4 which can be found here .\n4.2. Deploy From The Console To deploy the template from console please follow this guide for information on how to deploy the cloudformation template.\nUse pattern3-automate as the Stack Name. Provide the ARN of the pipeline you created in section 3.2.6 as ImageBuilderPipelineARN parameter value. Provide the cloudfromation stack name you created in section 2.1 as ApplicationStack parameter value. 4.3. Record The CloudFormation Output. Once the template is finished execution, note the Automation Document Name from the Cloudformation output specified under Pattern3CreateImageOutput.\nBuild Automation Document Manually 4.1. Access Systems Manager From The Console. From the AWS console, select \u0026lsquo;Systems Manager\u0026rsquo;.\nWhen you get to the front page of the service, use the left hand panel and go down to the bottom of the menu to select Documents from the Shared Resources as follows:\n.\n4.2. Create Automation Document In this section we will go through steps to create the automation document, explaining the automation document configuration detail interactively as we walk through. To ensure that you get the formatting correct when you insert the automation document we have provided a full copy for you to download here .\n4.2.1. Firstly access Systems Manager from the AWS Console.\n4.2.2. When you get to the front page of the service, use the left hand panel and go down to the bottom of the menu to select Documents from the Shared Resources as follows:\n.\n4.2.3. From the main page, select the Create Automation button to build an automation document.\n4.2.4. Enter the name of the automation document and select the Editor option to enter a the document directly into the console.\n4.2.5. Next we need to add the document specification below into the editor. Add the document which you downloaded at the start of section 4.2.. The following steps will explain the document configuration in stages.\n4.2.6. Firstly, we need to specify the schemaVersion and parameters which our document will take as an Input.\nIn this case we will take the ImageBuilderPipeline ARN as well as the name of the Application Stack (default: pattern3-app)\ndescription: CreateImage schemaVersion: \u0026#39;0.3\u0026#39; parameters: ImageBuilderPipelineARN: description: (Required) Corresponding EC2 Image Builder Pipeline to execute. type: String ApplicationStack: description: (Required) Corresponding Application Stack to Deploy the Image to. type: String 4.2.7. Next we will specify the first step which is to execute image builder pipeline we created in previous section. Passing the parameter inputs we specified before. This execution is achieved by calling the AWS service API directly leveraging aws:executeAwsApi action type in SSM Automation Document.\nmainSteps: - name: ExecuteImageCreation action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: StartImagePipelineExecution imagePipelineArn: \u0026#39;{{ ImageBuilderPipelineARN }}\u0026#39; outputs: - Name: imageBuildVersionArn Selector: $.imageBuildVersionArn Type: String 4.2.8. In the next we will specify aws:waitForAwsResourceProperty action wait for the Image to complete building.\n- name: WaitImageComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: \u0026#39;{{ ExecuteImageCreation.imageBuildVersionArn }}\u0026#39; PropertySelector: image.state.status DesiredValues: - AVAILABLE 4.2.9. Once the wait is complete, and the Image is ready, we will then call another aws:executeAwsApi to capture the AMI Id and pass the value into the next step.\n- name: GetBuiltImage action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: \u0026#39;{{ ExecuteImageCreation.imageBuildVersionArn }}\u0026#39; outputs: - Name: image Selector: $.image.outputResources.amis[0].image Type: String 4.2.10. With the AMI id we received in previous step, we will then pass the id to our Application CloudFormation Stack and trigger an update using aws:executeAwsApi action.\n- name: UpdateCluster action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: UpdateStack StackName: \u0026#39;{{ ApplicationStack }}\u0026#39; UsePreviousTemplate: true Parameters: - ParameterKey: BaselineVpcStack UsePreviousValue: true - ParameterKey: AmazonMachineImage ParameterValue: \u0026#39;{{ GetBuiltImage.image }}\u0026#39; Capabilities: - CAPABILITY_IAM 4.2.11. Once the update executes we will once again wait for the Cloudformation update to complete, and return with the UPDATE_COMPLETE status.\n- name: WaitDeploymentComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: DescribeStacks StackName: \u0026#39;{{ ApplicationStack }}\u0026#39; PropertySelector: Stacks[0].StackStatus DesiredValues: - UPDATE_COMPLETE 4.2.12. We have provided commentary above, to give you a picture of what is being executed in this automation document. As a whole your Automation Document should look as below. Please copy and paste below, and make that the indentation is correct as this document is specified in YAML format. Alternatively you can download the file here description: CreateImage schemaVersion: \u0026#39;0.3\u0026#39; parameters: ImageBuilderPipelineARN: description: (Required) Corresponding EC2 Image Builder Pipeline to execute. type: String ApplicationStack: description: (Required) Corresponding Application Stack to Deploy the Image to. type: String mainSteps: - name: ExecuteImageCreation action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: StartImagePipelineExecution imagePipelineArn: \u0026#39;{{ ImageBuilderPipelineARN }}\u0026#39; outputs: - Name: imageBuildVersionArn Selector: $.imageBuildVersionArn Type: String - name: WaitImageComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: \u0026#39;{{ ExecuteImageCreation.imageBuildVersionArn }}\u0026#39; PropertySelector: image.state.status DesiredValues: - AVAILABLE - name: GetBuiltImage action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: imagebuilder Api: GetImage imageBuildVersionArn: \u0026#39;{{ ExecuteImageCreation.imageBuildVersionArn }}\u0026#39; outputs: - Name: image Selector: $.image.outputResources.amis[0].image Type: String - name: UpdateCluster action: aws:executeAwsApi maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: UpdateStack StackName: \u0026#39;{{ ApplicationStack }}\u0026#39; UsePreviousTemplate: true Parameters: - ParameterKey: BaselineVpcStack UsePreviousValue: true - ParameterKey: AmazonMachineImage ParameterValue: \u0026#39;{{ GetBuiltImage.image }}\u0026#39; Capabilities: - CAPABILITY_IAM - name: WaitDeploymentComplete action: aws:waitForAwsResourceProperty maxAttempts: 10 timeoutSeconds: 3600 onFailure: Abort inputs: Service: cloudformation Api: DescribeStacks StackName: \u0026#39;{{ ApplicationStack }}\u0026#39; PropertySelector: Stacks[0].StackStatus DesiredValues: - UPDATE_COMPLETE 4.2.13. Once that\u0026rsquo;s done click Create Automation\nNow that we have created the Automation Document, let\u0026rsquo;s go ahead and execute it.\n4.3. Start The Monitor Script. Before we execute the document, we have provided a simple script for you to continuously query the Application Load Balancer http address during the document execution. This is to show that the load balancer remains available throughout the deployment.\n4.3.1. Firstly, download the monitor script here .\n4.3.2. Now change permissions of the script if required and execute passing in the application load balancer DNS address. Note that the DNS address is provided in the output of the application CloudFormation stack in section 2.2 under OutputPattern3ALBDNSName.\nExecute the script as follows:\n./watchscript.sh http://\u0026lt;enter DNS address for the Application Load Balancer\u0026gt; As mentioned above, the script will run a continuous poll of the ALB throughout the next few steps to demonstrate that there is no interruption to traffic during the patch process.\nFor clarity, you might want to run this in a separate dedicated terminal as it will continue to poll the ALB in a loop.\nYou can leave this script running, and monitor to see if there is any failed response to the application. Your output should look similar to this:\n4.4 Start the Automation Document. Once your monitor script is running a continous poll of the ALB, you can execute the SSM automation document.\nTo Execute the automation document, you can run the following command:\naws ssm start-automation-execution \\ --document-name \u0026#34;\u0026lt;enter_document_name\u0026gt;\u0026#34; \\ --parameters \u0026#34;ApplicationStack=\u0026lt;enter_application_stack_name\u0026gt;,imageBuilderPipeline=\u0026lt;enter_image_builder_pipeline_arn\u0026gt;\u0026#34; Note: The value of \u0026lt;enter_document_name\u0026gt; is provided as output to the CloudFormation template which you noted in section 4.1.1, or in section 4.2 if you are building it manually. The value of \u0026lt;enter_application_stack_name\u0026gt; is the name that you provided to the application stack in Section 2 (default is pattern3-app). The value of \u0026lt;enter_image_builder_pipeline_arn\u0026gt; is the ARN of the Image Builder Pipeline. You can get this from the output to the pipeline stack from Section 3.1.2 or 3.2.6 if you are building it manually via the console.. When you have successfully executed the command you will be provided with an AutomationExecutionID.\nTo check the status of the currently running automation executions, you can use the following command:\naws ssm describe-automation-executions Note that you can pass a filter to the command with the AutomationExecutionID which you were provided from the automation execution as follows:\naws ssm describe-automation-executions --filter \u0026#34;Key=ExecutionId,Values=\u0026lt;enter_execution_id\u0026gt;\u0026#34; 4.5. Confirm that the AMI has been Updated Via the Load Balancer DNS Name. When the automation execution is completed, use your web browser to access your application load balancer DNS name, together with the \u0026lsquo;details.php\u0026rsquo; script added to the end of the address. You will now find that the AMI-ID has been updated with a new one, indicating that your original autoscaling group has been replaced with an updated group which is configured to use the patched AMI. as follows:\nThis concludes our lab.\nEND OF SECTION 4 END OF LAB\n"},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/4_explore_cloudformation/","title":"Explore the CloudFormation Template","tags":[],"description":"","content":"In this section you will explore the CloudFormation template and learn how you were able to deploy the web application infrastructure using it\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation Click on the CloudFormation stack that you deployed Click on the Template tab Alternate: You previously downloaded the CloudFormation Template staticwebapp.yaml. You can also view it in the text editor of your choice\nThe template is written in a format called YAML , which is commonly used for configuration files. CloudFormation templates can also be written in JSON.\nLook through the template. You will notice several sections:\nThe Parameters section is used to prompt for inputs that can be used elsewhere in the template. The template is asking for several inputs, but also provides default values for each one.\nLook through these and start to reason about what some are used for. For example InstanceType is a parameter where the user can choose that Amazon EC2 instance type to deploy for the servers used in this Web App. Search the file for !Ref InstanceType. !Ref! is a built-in function that refrences the value of a parameter. Here you can see it is used to provide a value to the Auto Scaling Launch Configuration, which is used to launch new EC2 instances. The Conditions section is where you can setup if/then-like control of what happens during template deployment. It defines the circumstances under which entities are created or configured.\nThe Resources section is the \u0026ldquo;heart\u0026rdquo; of the template. It is where you define the infrastructure to be deployed. Look at the first resource defined.\nIt is the Amazon DynamoDB table used as the mock for the RecommendationService It has a logical ID which in this case is DynamoDBServiceMockTable. This logical ID is how we refer to the DynamoDB table resource within the CloudFormation template. It has a Type which tells CloudFormation which type of resource to create. In this case a AWS::DynamoDB::Table And it has Properties that define the values used to create the VPC The Outputs section is used to display selective information about resources in the stack.\nIn this case it uses the built-in function !GetAtt to get the DNS Name for the Application Load Balancer. This URL is what you used to access the WebApp The Metadata section here is used to group and order how the CloudFormation parameters are displayed when you deploy the template using the AWS Console\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/4_ec2_restrict_size/","title":"Extend an IAM Policy to restrict EC2 usage by instance size","tags":[],"description":"","content":"We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed.\nExtend the EC2Family_Restrict IAM Policy Log on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left menu: Click on Filter policies, then select Customer managed: Click on EC2_FamilyRestrict to modify it: Click on Edit policy: Click on the JSON tab: Modify the policy by adding in the sizes, add in nano, medium, large, be careful not to change the syntax and not remove the quote characters. Click on Review policy: Click on Save changes: Log out from the console\nYou have successfully modified the policy to restrict usage by instance size.\nVerify the policy is in effect Logon to the console as the TestUser1 user, click on Services and go to the EC2 dashboard: Try to launch an instance by clicking Launch Instance, select Launch Instance: Click on Select next to the Amazon Linux 2 AMI: We will attempt to launch a t3.micro which was successful before. Click on Review and Launch: Review the configuration and take note of the security group created, click Launch: Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances: You will get a failure, as it wasn\u0026rsquo;t a size we allowed in the policy. Click Back to Review Screen: Click Edit instance type: We will now select a t3.nano which will succeed. Click Review and Launch: Select Yes, I want to continue with this instance type (t3.nano), and click Next: Review the configuration and click Launch: Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances: It will succeed. Click on the Instance ID and terminate the instance as above: Log out of the console as TestUser1.\nYou have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/4_fail_open/","title":"Fail open when appropriate","tags":[],"description":"","content":"4.1 Disable RecommendationService again Confirm the service is healthy Refresh the web service multiple times and note that personalized recommendations are being served from all three servers You will now simulate another complete failure of the RecommendationService. Every request will fail for every request on every server Return to the AWS Systems Manager \u0026gt; Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled once again to false and Save changes What is the expected behavior? The previous time you simulated a complete failure of the RecommendationService\nThe web service failed with a http 502 error Then you implemented error handling and the following were observed The service returned a static response (as per the error handling code) Since the healthcheck code at that time was configured to only return http 200, it reported healthy status for all servers Now, with the new deep health check in place\u0026hellip;\nWhat status do you expect the elastic load balancer to report for the servers? How will the AWS Elastic Load Balancer handle traffic routing to the servers? 4.2 Observe fail-open behavior Refresh the web service multiple times\nLook at which servers (and Availability Zones) are serving requests Note that the service does not fail But as expected (without access to RecommendationServiceEnabled) it always serves static responses Refresh the health check URL multiple times\nThe deep health detects that RecommendationServiceEnabled is not available and returns a failure code for all servers From the Target Groups console Targets tab note the health check status of all the servers (you may need ot refresh)\nThey all report unhealthy with http code 503. This is the code the deep health check is configured to return when the dependency is not available\nNote the message at the top of the tab (if you do not see a message, try refreshing the entire page using the browser refresh function)\nThe Amazon Builders\u0026rsquo; Library: Implementing health checks When an individual server fails a health check, the load balancer stops sending it traffic. But when all servers fail health checks at the same time, the load balancer fails open, allowing traffic to all servers. When we rely on fail-open behavior, we make sure to test the failure modes of the dependency heath check. A system set to fail-open does not shut down when failure conditions are present. Instead, the system remains “open” and operations continue. The AWS Application Load Balancer here exhibits this fail-open behavior and the service continues to serve requests sent to it by the load balancer.\nReset the value of RecommendationServiceEnabled to true and observe that the service resumes serving personalized recommendations. The RecommendationServiceEnabled parameter was initially intended to simulate the failure of RecommendationService for this lab But now that we have implemented fail-open behavior and graceful degradation we could use the RecommendationServiceEnabled parameter as an emergency lever to cuto-off traffic to RecommendationService if there was a serious problem with it. Well-Architected for Reliability: Best practice Implement emergency levers: These are rapid processes that may mitigate availability impact on your workload. They can be operated in the absence of a root cause. An ideal emergency lever reduces the cognitive burden on the resolvers to zero by providing fully deterministic activation and deactivation criteria. Example levers include blocking all robot traffic or serving a static response. Levers are often manual, but they can also be automated. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/4_inventory_mgmt/","title":"Inventory Management using Operations as Code","tags":[],"description":"","content":"Management Tools: Systems Manager AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab.\nThere are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments .\nYou must use a supported operating system Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS The SSM Agent must be installed The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents Your EC2 instances must have outbound internet access You must access Systems Manager in a supported region Systems Manager requires IAM roles for instances that will process commands for users executing commands SSM Agent is installed by default on:\nAmazon Linux base AMIs dated 2017.09 and later Windows Server 2016 instances Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments.\n4.1 Setting up Systems Manager Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Fleet Manager from the navigation bar in the Node Management menu. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles. Then choose Create role. In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 (EC2 Allows EC2 instances to call AWS services on your behalf) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for AWS Systems Manager to select it. Then choose Next: Permissions. Under Attached permissions policy, verify that AmazonEC2RoleforSSM is listed, and then choose Next: Tags. In the Tags section: Add one or more keys and values, then choose Next: Review. In the Review section: Enter a Role name, such as ManagedInstancesRole. Accept the default in the Role description. Choose Create role. Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances. Select the first instance and then choose Actions, Security, and Modify IAM Role. Under Modify IAM Role, select ManagedInstancesRole from the drop down list and choose Save. Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Fleet Manager from the navigation bar in the Node Management menu. Periodically choose Fleet Manager until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager.\n4.2 Create a Second CloudFormation Stack Create a second CloudFormation stack using the procedure in 3.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2. Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod. Systems Manager: Inventory You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated.\n4.3 Using Systems Manager Inventory to Track Your Instances Under Node Management in the AWS Systems Manager navigation bar, choose Inventory. Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by, select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory.\nSchedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every, accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory, choose Setup inventory. To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit. Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager.\nSystems Manager: State Manager In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state.\nAn association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components:\nA document that defines the state Target(s) A schedule (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager.\n4.4 Review Association Status Under Node Management in the navigation bar, select State Manager. At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit. Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name). Inventory is accomplished through the following:\nThe activities defined in the AWS-GatherSoftwareInventory command document. The parameters provided in the Parameters section are passed to the document at execution. The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets.\nThe schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. There is the option to specify Output options. Note If you change the command document, the Parameters section will change to be appropriate to the new command document.\nNavigate to Fleet Manager under Node Management in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Node Management in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Resources Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section.\nSystems Manager: Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren’t compliant.\nBy default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/4_knowledge_check/","title":"Knowledge Check","tags":[],"description":"","content":" The security best practices followed in this lab are:\nManage credentials and authentication Use of MFA for access to provide additional access control. Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/4_knowledge_check/","title":"Knowledge Check","tags":[],"description":"","content":" The security best practices followed in this lab are:\nGrant least privileges: The roles are scoped with minimum privileges to accomplish the task. "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/4_monitoring_and_alerting/","title":"Monitoring and Alerting","tags":[],"description":"","content":"Lastly, we will setup your foundations for monitoring the security status of your AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across your AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs.\nBoth Security Hub and Guard Duty have a concept of a \u0026ldquo;management\u0026rdquo; and \u0026ldquo;member\u0026rdquo; account. The management account will receive data for all member accounts that are enrolled in it. A best practice is to enable your security audit account to be the management where your security team has read-only access to it. In addition to enabling these tools, setup notifications to ensure that you receive alerts as they occur. Develop a process per alert to handle incident response and over time automate your responses.\nWalk through Note that these steps need to be repeated in each region you wish to monitor.\nAmazon GuardDuty has the ability to delegate administration. Follow the instructions in the documentation to designate a Delegated Administrator and add Member Accounts . Use the audit account as your delegated administrator account. This will enable GuardDuty for all accounts within your organization and make the findings accessible from the Audit account. Enable AWS Security Hub . Follow the steps outlined in the blog post Automating AWS Security Hub Alerts with AWS Control Tower lifecycle events If you are not using Control Tower you can leverage the AWS Security Hub Multi-account Scripts to enable it across accounts. In your audit account, send AWS Security Findings to Email to ensure that your security team is alerted as findings are triggered. "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/4_other_rs_tools/","title":"Other Rightsizing Tools","tags":[],"description":"","content":"Other AWS Rightsizing tools Launched during re:Invent 2019, AWS Compute Optimizer is another tool that can help identify EC2 rightsizing opportunities. AWS Compute Optimizer recommends optimal AWS resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Over-provisioning resources can lead to unnecessary infrastructure cost and under-provisioning resources can lead to poor application performance. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, as well as optimal Amazon EBS volume configurations, based on your utilization data.\nCompute Optimizer is available at no additional cost. To get started, you just need to opt in using the AWS Compute Optimizer Console .\nComparing AWS Compute Optimizer vs Cost Explorer Rightsizing recommendations Both AWS Compute Optimizer and Cost Explorer Rightsizing recommendations use the same engine to provide rightsizing recommendations. Amazon EC2 Resource Optimization focuses on cost reduction providing recommendations only for over-provisioned resources. AWS Compute Optimizer aggregates over and under provisioned resources on their recommendations. In addition to EC2 it also covers Auto-Scaling Groups, Lambda functions, and EBS volumes.\nIf you are a cloud engineer looking for more detailed metrics and a technical approach to rightsizing check out our 200 level Rightsizing recommendations lab . If you are a cloud financial analyst trying to measure the potential savings for your company then Cost Explorer Rightsizing recommendations will give you a good starting point to assess and evangelize the teams around the potential financial benefits.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/4_save_milestone/","title":"Saving a milestone","tags":[],"description":"","content":" From the detail page for the workload, click the Save milestone button: Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/4_start_cw_agent/","title":"Start the CloudWatch Agent","tags":[],"description":"","content":"Now that your CloudWatch agent is installed on your EC2 Instance, we need to load the configuration file and restart the CloudWatch agent in order to begin collecting logs. This can be done remotely from the Systems Manager console using Run Command.\nOpen the Systems Manager console . Choose Run command from the left side menu under Instances \u0026amp; Nodes. Click Run Command on the page that opens up. In the Command document box, click in the search bar. Select “Document name prefix”, then “equals”, and enter AmazonCloudWatch-ManageAgent. Select the command that appears in the results. This command sends commands directly to the CloudWatch agent on your instances by remotely running scripts on the instance. You will be sending a “configure” command with the created parameter from Parameter Store to instruct the CloudWatch agent installed on the EC2 instance to use this configuration and start collecting logs. Under Command parameters: Set Action to Configure. Set Mode to ec2. Set Optional Configuration Source to ssm. Set Optional Configuration Location to the name of the parameter you created in Parameter Store. If you used the name provided above, it should be called AmazonCloudWatch-securitylab-cw-config. Set Optional Restart to yes. Under Targets: Select Choose instances manually. You should see a list of running instances. Select the instance that was launched by the CloudFormation template you deployed for this lab. This will be named Security-CW-Lab-Instance. Under Output Options, deselect Enable writing to an S3 bucket. Choose Run. Optionally, in the Targets and outputs areas, select the button next to an instance name and choose View output. Systems Manager should show that the agent was successfully installed in a few seconds. Recap: In this section, you started the CloudWatch Agent on your EC2 instance using Systems Manager Run Command. The command ran a shell script on the EC2 instance. This script instructs the CloudWatch agent to use the configuration file stored in Parameter Store, which gives the agent information on where to collect logs from, how often to collect them, and how to store them in CloudWatch. The script instructs the agent to reboot and begin collecting logs. This “enables people to perform actions at a distance” by not directly accessing the instance.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_3_pricing_models/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"There are no resources or configuration items that are created during this workshop.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST7 - \u0026ldquo;How do you use pricing models to reduce cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. There may be some charges from AWS Glue if it is above the free tier limit.\nAs it is best practice to regularly analyze your usage and cost, so there is no teardown for this lab.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/4_tear_down/","title":"Tear down","tags":[],"description":"","content":"It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution.\nDelete your Dashboard Go to the QuickSight homepage, and select All dashboards: Click the 3 dots next to the dashboard you created: Click Delete: Click Delete: Delete your Analysis Click on All analyses: Click the 3 dots next to the analysis you created: Click Delete: Click Delete to confirm: Delete the dataset Click on Manage data: Select the dataset: Click Delete data set: Confirm by clicking Delete: X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_cloudfront_with_s3_bucket_origin/4_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the CloudFront distribution and S3 bucket created in this lab.\nDelete the CloudFront distribution:\nOpen the Amazon CloudFront console at (https://console.aws.amazon.com/cloudfront/home) . From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Disabled, select the distribution and click the Delete. button, and then to confirm click the Yes, Delete button. Delete the S3 bucket:\nOpen the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket you created previously, then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting. "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_iam_user_cleanup/2_cleanup/","title":"Tear down","tags":[],"description":"","content":"It is recommended to keep this lab in place to continuously audit your environment. To remove this stack execute:\naws cloudformation delete-stack --stack-name IAM-User-Cleanup\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/4_cleanup/","title":"Tear down","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nTerminate the instance:\nSign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the left console instance menu, select Instances. Select the instance you created to terminate. From the Actions button (or right click) select Instance State \u0026gt; Terminate. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer:\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . From the console dashboard, choose Load Balancers from the Load Balancing section. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. Confirm by clicking Yes, Delete. From the console dashboard, choose Target Groups from the Load Balancing section. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack:\nOpen the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Select the lab-waf-regional stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/4_cleanup/","title":"Tear down","tags":[],"description":"","content":"Remove the lambda function, then roles.\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/4_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nThere is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account:\nYou may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Empty the S3 buckets You cannot delete an Amazon S3 bucket unless it is empty, so you need to empty the buckets you created. There are a total of four buckets:\nReplication bucket in east region: \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-east-2 Replication bucket in west region: \u0026lt;your_naming_prefix\u0026gt;-crrlab-us-west-2 Logging bucket in east region: logging-\u0026lt;your_naming_prefix\u0026gt;-us-east-2 Logging bucket in west region: logging-\u0026lt;your_naming_prefix\u0026gt;-us-west-2 Go to the Amazon S3 console , or if you are already there click on Amazon S3 in the upper left corner\nFor each of the four buckets do the following:\nSelect the radio button next to the bucket Click Empty Type the bucket name in the confirmation box Click Empty After you see the message Successfully emptied bucket then click Exit For the logging buckets it is also recommended your delete the bucket now to prevent the logs from writing more data there after you empty it Follow the same steps as above, but click the Delete button (instead of Empty) Remove AWS CloudFormation provisioned resources If you are using an AWS supplied to you as part of an in-person AWS workshop\nHow to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks\nGo to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks First delete the S3-CRR-lab-east CloudFormation stack in Ohio (us-east-2) Then delete the S3-CRR-lab-west CloudFormation stack in Oregon (us-west-2) Troubleshooting: if your CloudFormation stack deletion fails with status DELETE_FAILED and error (from the Events tab) Cannot delete entity, must detach all policies first then see these additional instructions X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/4_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nIf you deployed the CloudFormation stacks as part of the prerequisites for this lab, then delete these stacks to remove all the AWS resources. If you need help with how to delete CloudFormation stacks then follow these instructions to tear down those resources:\nDelete the WebApp resources Wait for this stack deletion to complete Delete the VPC resources Otherwise, there were no additional new resources added as part of this lab.\nReferences \u0026amp; useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 12 How do you test reliability?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_ec2_web_application/2_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nDelete the WordPress or Static Web Application CloudFormation stack:\nSign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. Click the Actions button then click Delete stack. Confirm the stack and then click Delete button. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/ References \u0026amp; useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/4_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use.\nDelete the CloudFront distribution:\nOpen the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack:\nSign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. References \u0026amp; useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/4_tear_down/","title":"Teardown","tags":[],"description":"","content":"The follwoing resources were created during this lab and can be deleted:\nS3 bucket, name starting with costefficiency Glue Classifier WebLogs Glue Crawler ApplicationLogs IAM Role \u0026amp; Policy AWSGlueServiceRole-CostWebLogs Glue Database webserverlogs Crawler CostUsage IAM Role \u0026amp; Policy AWSGlueServiceRole-Costusage Glue Database CostUsage Athena table costusagefiles_workshop.hourlycost Athena table costusagefiles_workshop.efficiency QuickSight dataset efficiency QuickSight Analysis efficiency analysis X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost? \u0026ldquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/4_elasticity/","title":"View your Elasticity","tags":[],"description":"","content":"NOTE: This exercise requires you have enabled hourly granularity within Cost Explorer, this can be done by following the instructions here - AWS Account Setup , Step 6 - Enable Cost Explorer. There are additional costs to enable this granularity.\nA key part of cost optimization is ensuring that your systems scale with your usage. This visualization will show how your systems operate over time.\nClick on Cost Explorer to go back to the default view: Click the down arrow to change the period, select 14D and click Apply: Click on Monthly and change the granularity to Hourly: Click on Bar, then select Line: You will now have in depth insight to how your environment is operating. You can see in this example the EC2 Instances scaling every day, you can see a period of large ELB usage, and EC2-Other, which includes charges related to EC2 such as data transfer. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/4_visualize_recommendations/","title":"Visualize your Savings Plan recommendations","tags":[],"description":"","content":"A visualization of your recommendation can be used as a quick double check, and also assist to demonstrate the savings and risks to other job functions.\nWe will use the Cost Explorer hourly granularity feature to visualize Savings Plan recommendations. You need to have this enabled to view hourly usage, and there are associated costs.\nIn the console go to AWS Cost Management by opening Cost Explorer: Click on Recommendations: Select EC2 Instance Savings Plans, 1-year, All upfront and 7 days: If Payer is selected and you see few or no recommendations, select Linked account to display recommendations to the linked accounts within your AWS Organization.\nScroll down and identify an instance family within your recommendations for later in this lab. If it is displaying the same instance family in multiple accounts, also note the account number: Scroll up and click Cost Explorer: To show the last 7 Days, click on the date range dropdown and select 7D, then click Apply: From the granularity dropdown select Hourly: Click the Service filter, select EC2-Instances (Elastic Compute Cloud - Compute) and click Apply filters: In the Group by: options select Instance Type: From the visualization dropdown select Line: Apply a filter on the region if you use multiple regions: If you have multiple instance types for a single family, you can select them by using a filter and choosing a stack graph. This will show the total costs for that family in the required region.\nClick More filters: Click Purchase Option, select On Demand and click Apply: If you have the same instance family in multiple accounts, select Linked Acount from the filters menu. Then select the account number you chose earlier in the lab and apply the filter.\nHover over a recommendation that matches the family you chose earlier in the lab: Cost Explorer gives $0.58 hourly usage of our m5.xlarge. Our recommended commitment was $0.354/hour for m5, 1-year, all upfront. Now open the Savings Plan Pricing Page and select the EC2 Instance Savings Plans tab.\nSelect the correct parameters that match the recommendation and view the rates: The savings is 41%. We were using $0.58, so multiplied by 1.00-41% (0.59) = $0.344/hour.\nWe can see the recommendation is accurate and valid within $.01.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_4_cost_and_usage_analysis/","title":"Level 100: Cost and Usage Analysis","tags":[],"description":"","content":"Last Updated June 2021\nAuthors Nathan Besh, Cost Lead Well-Architected Matt Berk, Sr. Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals Perform basic analysis of your cost and usage Prerequisites AWS Account Setup has been completed An AWS account with usage for more than 1 month Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs S3: Storage for enabling your Monthly Report in the Detailed Billing Reports, refer to S3 pricing https://aws.amazon.com/s3/pricing/ Time to complete The lab should take approximately 10 minutes to complete Steps: View your AWS Invoices View your cost and usage in detail Download your Monthly Report CSV Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_4_cost_and_usage_analysis/","title":"Level 200: Cost and Usage Analysis","tags":[],"description":"","content":"Last Updated July 2021\nAuthors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Contributors Alee Whitman, Sr. Commercial Architect (AWS OPTICS) Duncan Bell, Solutions Architect, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage Prerequisites AWS Account Setup has been completed Have usage that is tagged (preferred, not mandatory) Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs Variable depending on bill size and analysis performed Approximately less than $5 for the supplied files and small accounts Time to complete The lab should take approximately 20 minutes to complete Steps: Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_for_resiliency_of_ec2/","title":"Level 200: Testing for Resiliency of EC2 instances","tags":["test_resiliency"],"description":"Use code to inject faults simulating EC2 failures. These are used as part of Chaos Engineering to test workload resiliency","content":"Authors Rodney Lester, Senior Solutions Architect Manager, AWS Well-Architected Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). It is also a key component of Chaos Engineering, which uses such failure injection to test hypotheses about workload resiliency. One primary capability that AWS provides is the ability to test your systems at a production scale, under load.\nIt is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner.\nIn this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2).\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. Note: This 200 level lab covers only EC2 failure injection. If you would prefer a more feature-rich 300 level lab that demonstrates EC2 failure, RDS failure, and AZ failure then see Level 300: Testing for Resiliency of EC2, RDS, and AZ . That 300 level lab includes everything in this 200 level lab, plus additional fault simulations.\nNOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Deploy the Infrastructure and Application Configure Execution Environment Test Resiliency Using Failure Injection Tear down this lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/","title":"Level 300: Incident Response with AWS Console and CLI","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable. Steps: Getting Started Identity \u0026amp; Access Management Amazon VPC "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/","title":"Level 300: Splitting the CUR and Sharing Access","tags":[],"description":"","content":"Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information.\nCommon use cases are:\nSeparate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement:\nCreate a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script Goals Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler Steps: Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear Down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_security_best_practices_workshop_ec2/","title":"Quest: AWS Security Best Practices Workshop","tags":[],"description":"This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits.","content":"Authors Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect About this Guide This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity \u0026amp; access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Identity \u0026amp; Access Management For Lab 1 choose one of labs to run based on your interest or experience:\nLab 1a - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy.\nIn this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation Lab 1b - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab.\nIn this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are:\nNetworking subnets created in multiple availability zones for the following network tiers:\nApplication Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.\nAutomated Deployment of VPC Lab 3 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach.\nThe WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.\nAutomated Deployment of EC2 Web Application Lab 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nAutomated Deployment of Detective Controls Lab 5 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nEnable Security Hub Lab 6 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nIncident Response with AWS Console and CLI "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/","title":"100 Labs","tags":[],"description":"","content":"List of labs available Level 100: AWS Account Setup: Lab Guide Level 100: Cost and Usage Governance Level 100: Pricing Models Level 100: Cost and Usage Analysis Level 100: Cost Visualization Level 100: Rightsizing Recommendations Level 100: Cost Estimation Level 100: Goals and Targets Level 100: Tag Policies "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/","title":"Disaster Recovery","tags":[],"description":"","content":" Introduction Background Unishop is THE one-stop-shop for all your Unicorn needs. You can find the best Unicorn selection online at the Unishop and get your Unicorn delivered in less than 24 hours! As a young startup Unishop built a great service which was focused on customers and business outcomes but less on technology and architecture. After a few years establishing a business model and securing the next round of venture capital funding, the business is looking to expand to other markets, such as Unicorn-Insurance, Unicorn-Banking and Unicorn-Ride-Sharing.\nModule 1: Backup and Restore In this module, you will go through the Backup and Restore DR strategy. To learn more about this DR strategy, you can review this Disaster Recovery blog . Our test application is Unishop. It is a Spring Boot Java application connected to a MySQL database with a frontend written using bootstrap. The app is deployed on a single EC2 instance (t3.small) within a dedicated VPC using a single public subnet. Note that this is not the ideal infrastructure architecture for running highly available production applications but suffices for this workshop.\nModule 2: Pilot Light In this module, you will go through the Pilot-Light Disaster Recovery (DR) strategy. To learn more about this DR strategy, you can review this Disaster Recovery blog . Our test application is Unishop. It is a Spring Boot Java application with a frontend written using bootstrap. The app uses an Amazon S3 bucket to host a static web interface. A single EC2 instance serves as a proxy for API calls to an Amazon Aurora MySQL database.\nModule 3: Warm Standby In this module, you will go through the Warm Standby Disaster Recovery (DR) strategy. To learn more about this DR strategy, you can review this Disaster Recovery blog . Our test application is Unishop. It is a Spring Boot Java application with a frontend written using bootstrap. The app uses an Amazon S3 bucket to host a static web interface. A single EC2 instance serves as a proxy for API calls to an Amazon Aurora MySQL database.\nModule 4: Hot Standby Our test application is Unishop. It is a Spring Boot Java application with a frontend written using bootstrap. The app uses an Amazon S3 bucket to host a static web interface. A single EC2 instance serves as a proxy for API calls to an Amazon Aurora MySQL database. The database contains mock user and product information. Amazon API Gateway is used to connect via AWS Lambda to a DynamoDB database storing shopping cart and session information.\nAWS Elastic Disaster Recovery Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication. You can perform non-disruptive tests to confirm that implementation is complete. During normal operation, maintain readiness by monitoring replication and periodically performing non-disruptive recovery and failback drills.\n"},{"uri":"https://wellarchitectedlabs.com/security/quests/","title":"Quests","tags":[],"description":"","content":"Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn.\nThe following quests are aligned to the security best practice questions in AWS Well-Architected.\nList of quests available Quest: Loft - Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events.\nQuest: Simplest Security Steps These are the six things we encourage all customers to do to improve their security in the cloud.\nQuest: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture.\nQuest: AWS Incident Response - Credential Misuse This quest is the guide for incident response workshop on credential misuse at AWS organized events.\nQuest: Reviewing Security Essential Best Practice - Well-Architected Webinar This quest is a collection of lab patterns which are covered in the June 2021 Webinar \u0026#39;Reviewing Security Essential Best Practice\u0026#39;\nQuest: AWS Incident Response Day This quest is the guide for incident response workshop often ran at AWS led events.\nQuest: re:Invent 2020 - Automate The Well-Architected Way With WeInvest This quest is a collection of lab patterns which are covered in the upcoming session at re:Invent 2020: Automate The Well-Architected Way with WeInvest\nQuest: AWS Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits.\nQuest: AWS Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity \u0026amp; access management, detective controls, infrastructure protection, data protection and incident response.\nQuest: Managing Credentials \u0026amp; Authentication Quest: Control Human Access Quest: Control Programmatic Access Quest: Detect \u0026amp; Investigate Events Quest: Defend Against New Threats Quest: Protect Networks Quest: Protect Compute Quest: Classify Data Quest: Protect Data at Rest Quest: Protect Data in Transit Quest: Incident Response "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/4_decommission_resources/","title":"Decommission Resources","tags":[],"description":"","content":"Decommission resources Decommission resources automatically Goal: Reduce decommission costs of workloads Target: All workloads are to have automatic decommission of non-ephemeral storage resources. Best Practice: Decommission resources automatically Measures: % of workloads with automatic decommission, cost of non-decommissioned resources Good/Bad: Good Why? When does it work well or not?: Reduce cost of manual decommission work Contact/Contributor: natbesh@amazon.com Goal: Reduce waste Target: Reduce waste by x% Best Practice: Decommission resources Measures: amount of waste removed Good/Bad: Bad Why? When does it work well or not?: Does not work long term as it requires waste to always exist, but it could be possibly used as a short term goal. Instead, set a positive goal of minimizing waste creation - such as having waste not exceed a set $ amount. Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_cost_estimation/4_export_estimate/","title":"Export Estimate","tags":[],"description":"","content":"Export Estimate On the My Estimate page, click on the Action button\nClick on Export estimate\nClick on \u0026ldquo;OK\u0026rdquo;\nThis downloads the estimate as a csv file. You can open the csv using a suitable application such as Excel:\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/5_failure_injection_rds/","title":"Test Resiliency Using RDS Failure Injection","tags":[],"description":"","content":"5.1 RDS failure injection This failure injection will simulate a critical failure of the Amazon RDS DB instance.\nIn Chaos Engineering we always start with a hypothesis. For this experiment the hypothesis is:\nHypothesis: If the primary RDS instance dies, then availability will not be impacted\n[Optional] Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing:\nsingle region: WaitForMultiAZDB shows completed (green) multi region: both WaitForRDSRRStack1 and CheckRDSRRStatus1 show completed (green) Before you initiate the failure simulation, refresh the service website several times. Every time the image is loaded, the website writes a record to the Amazon RDS database\nClick on click here to go to other page and it will show the latest ten entries in the Amazon RDS DB The DB table shows \u0026ldquo;hits\u0026rdquo; on your image web page These include requests you may make as well as load balancer health checks Refresh and note that new data is constantly being written to the table Click on click here to go to other page again to return to the image web page Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds From the RDS dashboard\nClick on \u0026ldquo;DB Instances (n/40)\u0026rdquo; Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) If running the multi-region deployment, select the DB instance with Role=Master Look at the configured values. Note the following:\nValue of the Status field is Available Region \u0026amp; AZ shows the AZ for your primary DB instance Select the Configuration tab: Multi-AZ. is enabled, and Secondary Zone shows the AZ for you standby DB instance To failover of the RDS instance, use the VPC ID as the command line argument replacing \u0026lt;vpc-id\u0026gt; in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for)\nLanguage Command Bash ./failover_rds.sh \u0026lt;vpc-id\u0026gt; Python python3 fail_rds.py \u0026lt;vpc-id\u0026gt; Java java -jar app-resiliency-1.0.jar RDS \u0026lt;vpc-id\u0026gt; C# .\\AppResiliency RDS \u0026lt;vpc-id\u0026gt; PowerShell .\\failover_rds.ps1 \u0026lt;vpc-id\u0026gt; The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt\n5.2 System response to RDS instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n5.2.1 System availability The website is not available. Some errors you might see reported:\nNo Response / Timeout: Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out: Amazon Elastic Load Balancer did not get a response from the server. This can happen when it has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway: The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site can’t be reached. This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests. This can also be verified by viewing the canary run data.\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation click on the WebServersforResiliencyTesting stack click on the Outputs tab Open the URL for WorkloadAvailability in a new window You will see that canary runs are failing because the website is not available. Continue on to the next steps, periodically returning to attempt to refresh the website or viewing the canary runs.\n5.2.2 Failover to standby On the database console Configuration tab Refresh and note the values of the Status field. It will ultimately return to Available when the failover is complete.\nNote the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. (After RDS failover it can take several minutes for the console to update as shown below. The failover has however completed)\nFrom the AWS RDS console, click on the Logs \u0026amp; events tab and scroll down to Recent events. You should see entries like those below. (Note: you may need to page over to the most recent events) .In this case failover took less than a minute.\nMon, 11 Oct 2021 19:53:37 GMT - Multi-AZ instance failover started. Mon, 11 Oct 2021 19:53:45 GMT - DB instance restarted Mon, 11 Oct 2021 19:54:21 GMT - Multi-AZ instance failover completed 5.2.3 EC2 server replacement From the AWS RDS console, click on the Monitoring tab and look at DB connections\nAs the failover happens the existing three servers all cannot connect to the DB\nAWS Auto Scaling detects this (any server not returning an http 200 status is deemed unhealthy), and replaces the three EC2 instances with new ones that establish new connections to the new RDS primary instance\nThe graph shows an unavailability period of about four minutes until at least one DB connection is re-established\n[optional] Go to the Auto scaling group and AWS Elastic Load Balancer Target group consoles to see how EC2 instance and traffic routing was handled\n5.2.4 RDS failure injection - results AWS RDS Database failover took less than a minute Time for AWS Auto Scaling to detect that the instances were unhealthy and to start up new ones took four minutes. This resulted in a four minute non-availability event. Our requirements for availability require that downtime be under one minute. Therefore our hypothesis is not confirmed:\nHypothesis: If the primary RDS instance dies, then availability will not be impacted\nChaos Engineering uses the scientific method. We ran the experiment, and in the verify step found that our hypothesis was not confirmed, therefore the next step is to improve and run the experiment again.\n5.3 RDS failure injection - improving resiliency In this section you reduce the unavailability time from four minutes to under one minute.\nYou observed before that failover of the RDS instance itself takes under one minute. However the servers you are running are configured such that they cannot recognize that the IP address for the RDS instance DNS name has changed from the primary to the standby. Availability is only regained once the servers fail to reach the primary, are marked unhealthy, and then are replaced. This accounts for the four minute delay. In this part of the lab you will update the server code to be more resilient to RDS failover. The new code will re-establish the connection to the database, and therefore uses the new DNS record to connect to the RDS instance.\nUse either the Express Steps or Detailed Steps below:\nExpress Steps Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation For the WebServersForResiliencyTesting Cloudformation stack Redeploy (Update) the stack and Use current template Change the BootObject parameter to server_with_reconnect.py Detailed Steps Click here for detailed steps for updating the Cloudformation stack: Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation Click on WebServersForResiliencyTesting Cloudformation stack Click the Update button Select Use current template then click Next On the Parameters page, find the BootObject parameter and replace the value there with server_with_reconnect.py Click Next Click Next Scroll to the bottom and under Changes (2) note that you are changing the WebServerAutoscalingGroup and WebServerLaunchConfiguration. This CloudFormation deployment will modify the launch configuration to use the improved server code. Check I acknowledge that AWS CloudFormation might create IAM resources. Click Update stack Go the Events tab for the WebServersForResiliencyTesting Cloudformation stack and observe the progress. When the status is UPDATE_COMPLETE or UPDATE_COMPLETE_CLEANUP_IN_PROGRESS you may continue. When you see UPDATE_COMPLETE_CLEANUP_IN_PROGRESS you may continue. There is no need to wait.\nThis update deploys three new EC2 instances in a new Auto Scaling group. There may be a period that you will still see the old three instances running, before they are drained and terminated. There may be a short period of unavailability. Make sure the web site is available before continuing. Now you will re-run the experiment as per the steps below:\nBefore we used a custom script. For this run of the experiment, we will show how to use AWS Fault Injection Simulator (FIS) 5.4 RDS failure injection using AWS Fault Injection Simulator (FIS) As in section 5.1, you will simulate a critical failure of the Amazon RDS DB instance, but using FIS.\nWe would not normally change our execution approach as part of the \u0026ldquo;improve / experiment\u0026rdquo; cycle. However, for this lab it is illustrative to see the different ways that the experiment can be executed.\n5.4.1 Create experiment template Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds From the RDS dashboard\nClick on \u0026ldquo;DB Instances (n/40)\u0026rdquo; Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) If running the multi-region deployment, select the DB instance with Role=Master Look at the configured values. Note the following:\nValue of the Status field is Available Region \u0026amp; AZ shows the AZ for your primary DB instance Select the Configuration tab: Multi-AZ. is enabled, and Secondary Zone shows the AZ for you standby DB instance Select the Tags tab: Note the Value for the Workshop tag Navigate to the FIS console and click Experiment templates in the left pane.\nClick on Create experiment template to define the type of failure you want to inject.\nEnter Experiment template for RDS resiliency testing for Description and RDS-resiliency-testing for Name. For IAM role select WALab-FIS-role.\nScroll down to Actions and click Add action.\nEnter reboot-database for the Name. Under Action type select aws:rds:reboot-db-instances. Enter true under forceFailover - optional and click Save.\nScroll down to Targets and click Edit next to DBInstances-Target-1 (aws:rds:db).\nUnder Target method, select Resource tags and filters. Select Count for Selection mode and enter 1 under Number of resources. This ensures that FIS will only reboot one RDS DB instance.\nScroll down to Resource tags and click Add new tag. Enter Workshop for Key and AWSWellArchitectedReliability300-ResiliencyofEC2RDSandS3 for Value. These are the same tags that are on the RDS DB instance used in this lab.\nYou can choose to stop running an experiment when certain thresholds are met, in this case, using CloudWatch Alarms under Stop condition. For this lab, this is a single point in time event (with no duration) so you can leave this blank.\nClick Create experiment template.\nIn the warning pop-up, confirm that you want to create the experiment template without a stop condition by entering create in the text box. Click Create experiment template.\n5.4.2 Run the experiment Click on Experiment templates from the menu on the left.\nSelect the experiment template RDS-resiliency-testing and click Actions. Select Start experiment.\nYou can choose to add a tag to the experiment if you wish to do so.\nClick Start experiment.\nIn the pop-up, type start and click Start experiment.\nCheck the website availability. Re-check every 20-30 seconds.\nRevisit section 5.2 to observe the system response to the RDS instance failure.\nAt a minimum, return to the RDS console, go the the Logs \u0026amp; events tab, and look at the most recent events to verify that a failover has occurred. 5.4.3 RDS failure injection, second experiment - results You will observe that the unavailability time is now under one minute What else is different compared to the previous time the RDS instance failed over? 5.5 RDS failure injection - conclusion After making the necessary improvements, now our hypothesis is confirmed:\nHypothesis: If the primary RDS instance dies, then availability will not be impacted\nResources Learn more: After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments.\nHigh Availability (Multi-AZ) for Amazon RDS\nThe primary DB instance switches over automatically to the standby replica if any of the following conditions occur:\nAn Availability Zone outage The primary DB instance fails The DB instance\u0026rsquo;s server type is changed The operating system of the DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/5_control_access_to_api/","title":"Control access to API","tags":[],"description":"","content":"In this section we will be building API Access Control with Amazon Cognito . This will extend our architecture to ensure that only identified users are permitted access to the API.\n5.1. Identify the risk of vulnerabilities. Even though we have controlled traffic at multiple layers, anyone who knows your CloudFront Domain Name can access your API. Furthermore we do not know who accessed your API, so the owner of the traffic remains anonymous. Ideally we should ensure that only legitimate users who we are aware of are permitted access. This will mean that a successful API call will have to be both identified as well as authorized.\nIn this lab we will build out our architecture using Amazon Cognito. This addition will allow a user to sign in to a user pool which we create, obtain an identity or access token and then call the API method with one of the tokens. These tokens are typically set to the request\u0026rsquo;s Authorization header.\n5.2. Sign up with Amazon Cognito user pools. The second CloudFormation template which you have already deployed already contains an Amazon Cognito User Pool. We will start by obtaining the App client secret of Cognito, which we will use to generate an ID token at later points in the lab.\nClick here for for SignUp instructions for Amazon Cognito Go to the Outputs section of the current cloudformation stack and locate the CognitoSignupURL. Click on the link as shown: Select the Sign up link with Cognito user pools. Provide your valid email address and password. You will receive an email from no-reply@verificationemail.com with a link to confirm your email address is valid. Once you click the link in email, you will see the following confirmation. Go to Amazon Cognito in AWS Console and click Manage User Pools Select your user pools. Select Users and groups under General settings in the left panel. Note that your account status should now be shown as CONFIRMED:\nSelect App clients under General settings in the left panel and click Show Details. Record the App client secret. We will need this secret to generate an ID Token that will be made temporarily available for 60 minutes by default. You can customize this later if needed. Take note of App client secret. Other required values such as user pool ID and App client ID are available in the Output section of the current cloudformation stack. Record these before moving to the next step.\n5.3. Create Cognito user pools as Authorizer in API Gateway Console. Amazon Cognito user pools are used to control who can invoke REST API methods. We now need to integrate the API with the Amazon Cognito user pool.\nGo to API Gateway in the AWS console and select API called wa-lab-rds-api. From the main navigation pane, choose Authorizers and click Create New Authorizer button.\nType an authorizer name in Name. Select Cognito as Authorizer Type. Select your Cognito user pool (this should start with WAUserPool). For Token source, type Authorization as the header name to pass the identity or access token. Your configuration should be similar to the screenshot below:\nWhen you have completed the configuration, click on Create. You have now successfully added an Authorizer: From the main navigation pane, choose Resource to configure a COGNITO_USER_POOLS authorizer on methods. Before you click Method Request, ensure that you refresh your browser. If you do not do this, our new Authorizer will not appear when trying to associate with Method Request.\nSelect Method Request as shown:\nChoose the pencil icon next to Authorization. You should be able to see the Authorizer we created in the drop-down list. To save the settings, choose the check mark icon Since we made a change, we need to re-deploy the API. Complete the following steps: Choose Action. Deploy API. Select Development stage as Dev from the drop-down list. Click Deploy. Your API is now using COGNITO_USER_POOLS as Authorizer. Only users registered in COGNITO_USER_POOLS can access your API. 5.4. Add Authorization header in CloudFront. By default, CloudFront doesn\u0026rsquo;t consider headers when caching your objects in edge locations. Now your request must have an Authorization header with a valid ID Token to access API. Therefore, we need to configure CloudFront to forward headers to the API Gateway. Further details on this can be found here Go to CloudFront in the AWS console and select your CloudFront distribution. From within the distribution, select the Behaviors tab. Select Cache Behavior associated with our API Gateway using the tick box. Click Edit. Select Authorization from the list of available headers and choose Add. To forward a custom header, enter the name of the header in the field, and choose Add Custom. Click Yes,Edit. If you test out without Authorization header now, you will still see data being returned as it\u0026rsquo;s served from CloudFront edge caches. Let\u0026rsquo;s invalidate files to prevent this from happening.\nInvalidate files from CloudFront edge. Select Invalidation tab. Select Create Invalidation Type /* , then click Invalidate button. 5.5. Generate an ID Token and send a request with ID Token. After successful completing authentication, Amazon Cognito returns user pool tokens to your app. You can use these tokens to grant your users access to the API Gateway. Amazon Cognito user pools implements ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.\nIn this lab, we will use an ID Token that is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number.\nIn Cloud9, we will test with both CloudFrontEndpoint and APIGatewayURL to see the difference. Execute the script called sendRequest.py with the argument of your CloudFrontEndpoint. python sendRequest.py \u0026#39;CloudFrontEndpoint\u0026#39; You will get \u0026ldquo;Unauthorized\u0026rdquo; response with 401 response code.\nYou must send Authorization header with valid ID token to access API now.\nLet\u0026rsquo;s generate an ID token with your username and password, Cognito user pool ID, App client ID, and App secret. We took note of App client secret when we signed up with Cognito. Other required values such as user pool ID, App client ID are available in Output section of the current cloudformation stack python getIDtoken.py \u0026lt;username\u0026gt; \u0026lt;user_password\u0026gt; \u0026lt;user_pool_id\u0026gt; \u0026lt;app_client_id\u0026gt; \u0026lt;app_client_secret\u0026gt; You should be able to generate ID Token successfully. This will be valid for 60 minutes. Copy your ID Token you generate above. Send a request with this valid ID Token. python sendRequest.py \u0026#39;CloudFrontEndpoint\u0026#39; ID_Token You should be seeing your data as expected with a 200 response code. Now you must have provided an ID Token as the Authorization header to access your API only through CloudFront.\nEnsure that you complete the tear down instructions in the final section to remove resources created in this lab.\nEND OF SECTION 5\n"},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/disaster/","title":"Failover to Secondary","tags":[],"description":"","content":"When a regional service event affects the Unicorn application in the primary region N. Virginia (us-east-1), we want to bring up the resources in the secondary region N. California (us-west-1).\nWe assume a regional service event has occurred. In this section, we will manually perform a series of tasks to bring up the application in the secondary region N. California (us-west-1). In a production environment, we would automate these steps using an AWS Cloudformation template or third-party tools.\nWe will perform the following:\nLaunch an EC2 instance from the AMI (Amazon Machine Image) Restore the RDS database from backup Configure the application Simulating a Regional Service Event We will now simulate a regional service event affecting the S3 static website in N. Virginia (us-east-1) serving The Unicorn Shop website.\n1.1 Click S3 to navigate to the dashboard.\n1.2 Click on the backupandrestore-uibucket-xxxx link.\n1.3 Click the Permissions link. In the Block public access (bucket settings) section, click the Edit button.\n1.4 Enable the Block all public access checkbox, then click the Save button.\n1.5 Type confirm, then click the Confirm button.\n1.6 Click the Properties link.\n1.7 In the Static website hosting section. Click on the Bucket website endpoint link.\n1.8 You should get a 403 Forbidden error.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/verify-websites/","title":"Verify Websites","tags":[],"description":"","content":"Verify the Active-Primary Website All previous modifications to resources must be finished before continuing on to this section.\nYou will need the Amazon CloudFormation output parameter values from the Active-Primary stack to complete this section. For help, refer to the CloudFormation Outputs Primary Region section of the workshop.\n1.1 Copy the WebsiteURL url value into a browser window.\n1.2 Verify the website header says The Unicorn Shop - us-east-1.\nVerify the Passive-Secondary Website You will need the Amazon CloudFormation output parameter values from the Passive-Secondary stack to complete this section. For help, refer to the CloudFormation Outputs Secondary Region section of the workshop.\n2.1 Copy the WebsiteURL url value into a browser window.\n2.2 Verify the website header says The Unicorn Shop - us-west-1.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/5_apply_patches/","title":"Step 5 - Apply patches","tags":[],"description":"","content":"In this exercise we will perform vulnerability scanning and patching on a pre-install EC2 instance, Microsoft based Windows Operating System using Amazon Inspector and AWS Systems Manager respectively.\nNote: For this lab, it is assumed that Microsoft Windows based EC2 instance is already created. For instructions to create EC2 Instance please follow the link .\nFrom the AWS console, click Services and select Amazon Inspector.\nOn the Inspector console click on Get started.\nClick Advanced Setup on the welcome page with default options.\nOne \u0026lsquo;Define an assessment target\u0026rsquo; page, leave the values as is and click Next.\nOn \u0026lsquo;Define an assessment template\u0026rsquo; page leave the default values as is and click Next.\nOn the \u0026lsquo;Review\u0026rsquo; page click on Create.\nA success notification will appear once the template is created.\nOn \u0026lsquo;Assessment templates\u0026rsquo; page select the template that you just have created click on Run. Wait till the \u0026lsquo;Last run\u0026rsquo; status shows Analysis complete (it may take 5 -10 minutes). Click the refresh icon to view the latest status.\nClick on Dashboard on the menu at the left side of the console.\nThen click on Important findings, which will show the list of important issues i.e., missing patches (if the EC2 instance is from the latest AMI then you may not get the findings as the AMI is fully patched).\nNow let\u0026rsquo;s patch our machine.\nFrom the AWS console, click Services and select AWS Systems Manager.\nClick on Quick setup on menu at the left side of the console.\nMake sure that correct region is selected on the top right corner of the console. Click on Get started.\nOn the Quick Setup page click on Create.\nOn \u0026lsquo;Choose a configuration type\u0026rsquo; page, select Host Management settings and click Next. On the \u0026lsquo;Customize Host Management configuration options\u0026rsquo; page leave the default values as is and click on Create.\nNotification will appear on screen once the host management setup is completed successfully (may take up to 5 minutes).\nOne the menu at the left side, scroll down and click on the Compliance under Node Management.\nOne the \u0026lsquo;Compliance resources summary\u0026rsquo; page the non-compliance status against Patch will be visible if the systems manager detects missing patches within the EC2 instance. Click on the number showing against the missing patches.\nFor patching the Operating System, click on Patch Manager under Node Management.\nClick on Patch now on the upper right side.\nOn \u0026lsquo;Patch instances now\u0026rsquo; page select Scan and install. Leave the remaining options as is, scroll down and click Patch now.\nOn \u0026lsquo;Association execution summary\u0026rsquo; page the Status of the operation will become success after few minutes.\nNow go back to \u0026lsquo;Compliance\u0026rsquo; section under Node Management on the left side menu.\nOn the \u0026lsquo;Compliance resources summary\u0026rsquo; section, the Patch Compliance type will now show as Compliant.\nClick on the Instance ID showed under the resource, which will take you to the Fleet Manager console. Click on the Patch tab, which will show that no more updates required. For more information please read the AWS User Guide: https://docs.aws.amazon.com/inspector/latest/userguide/inspector_introduction.html https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/","title":"Level 200: Backup and Restore with Failback for Analytics Workload","tags":[],"description":"Implement the backup and restore DR pattern with failback capability for an analytics workload","content":"Authors Randy DeFauw, Principal Solutions Architect Neelam Koshiya, Senior Solutions Architect Introduction In this module, you will go through the Backup and Restore Disaster Recovery (DR) strategy for an analytics workload. To learn more about this DR strategy, you can review this Disaster Recovery blog .\nThe workload includes streaming ingest and batch processing.\nIn a nutshell, there are three primary data stores, Amazon Simple Storage Service (Amazon S3) , Amazon DynamoDB , and the AWS Glue catalog. The AWS Glue catalog only receives new partitions as updates, and we use a AWS Lambda function to create those when new partitions land in S3. One S3 bucket is receiving streaming data, and another is receiving the output of a nightly batch processing job. Both buckets use S3 cross-region replication (CRR) to replicate to the backup region. We use DynamoDB point-in-time-recovery (PITR) to handle the database.\nIn terms of RTO, the failover process involves detecting the failure, deploying the rest of the infrastructure in the backup region, and switching the AWS Global Accelerator endpoint for data producers. That entire process can be done in as little as 10 minutes once you make the decision to fail over.\nFor RPO, you will lose whatever data your producers were trying to send to the endpoint from the time it became unhealthy to the time the failover completed. You may also lose some data in the original region that was not successfully replicated by S3 cross-region replication or by DynamoDB point-in-time recovery.\nTo configure the infrastructure and deploy the application, we will use AWS CloudFormation . CloudFormation is an easy way to speed up cloud provisioning with infrastructure as code.\nThis workshop takes about two hours to complete. Prior experience with the AWS Console and Linux command line are helpful but not required. You should also have basic familiarity with batch and stream processing architectures on AWS.\nSteps: Getting Started Setting Up Failover Failback Next Steps X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/intro/second-content/","title":"Agenda","tags":[],"description":"","content":"We’ve broken the workshop down into easy to follow and digestible chunks, which walks you through the process of recovery from a disaster using a variety of AWS services.\nIn Module 1, we will deploy the Unishop in one AWS regions and then utilize AWS Backup and EC2 AMIs to create a backup of our shop in another AWS region. We will demonstrate DR by simulating a disaster in the primary region and recovering the shop in the backup region. This module takes about 90 minutes to complete.\nIn Module 2, we will demonstrate DR using a pilot light architecture and in Module 3 we will use a warm standby. This module takes about 60 minutes to complete.\nIn Module 4 we will deploy the shop in an Active-Active configuration thus ensuring minimal RTO\\RPO. This module takes about 60 minutes to complete.\nFinally, Module 5 introduces CloudEndure which is a DR as a Service offering from AWS. This module takes about 60 minutes to complete.\nAs you probably understand by now, one of the major benefits cloud architecture is range of options available for DR and the ease with which they can be implemented.\n"},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/5_explore_wareview/","title":"Explore Well-Architected Review","tags":[],"description":"","content":"Explore Well-Architected Review that was created by the sample application Go to the Well-Architected Tool Console and find the workload called \u0026ldquo;APIGWLambda - walabs-api - WALabsSampleLambdaFunction\u0026rdquo;.\nYou will notice that the tool reports that 4 questions have already been answered and lists 3 as High Risks. Click on the Workload link \u0026ldquo;APIGWLambda - walabs-api - WALabsSampleLambdaFunction\u0026rdquo;\nFrom here, you can click on the \u0026ldquo;Continue Reviewing\u0026rdquo; and then select \u0026ldquo;AWS Well-Architected Framework\u0026rdquo; drop-down to answer the rest of the questions. You will notice that two of the questions in the Operations Excellence pillar have already been answered and marked as \u0026ldquo;Done\u0026rdquo;\nAt this point, you could continue your standard Well-Architected review process and answer the rest of the questions that are outstanding. For the purpose of the lab, just continue to the next step.\nClick on the \u0026ldquo;APIGWLambda - walabs-api - WALabsSampleLambdaFunction\u0026rdquo; breadcrumb at the top of the screen to return back to the overview. Explore tags created on the Well-Architected Review From the Workload Detail page, click on the Properties tab at the top. Scroll to the bottom and see that there are 3 tags associated with this Workload. Explore tags created on the sample Lambda application Go to the Lambda console You should see 3 Lambda Functions that have been deployed. Click on the \u0026ldquo;WALabsSampleLambdaFunction\u0026rdquo; Click the Configuration tab Click on the Tags menu on the left navigation bar You should see the same 3 Tags that we also assigned to the Well-Architected Workload above. You can use these tags to generate reports or to query against the various AWS API\u0026rsquo;s to find all associated components for the workload. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_building_custom_aws_well-architected_reports_with_amazon_athena_and_amazon_quicksight/5_clean_up/","title":"Clean up the deployment","tags":[],"description":"","content":"This lab presented a simple approach for aggregating the data of the workload reviews into a central data lake repository. It helps teams to analyze their organization’s Well-Architected maturity across multiple AWS accounts and workloads and perform centralized reporting on high-risk issues (HRIs).\nTo clean up from this lab, manually delete the:\nS3 bucket and the data stored in the bucket, Lambda function, Glue crawler and database, Athena views, and the QuickSight resources. Other custom integrations It\u0026rsquo;s exciting to see the custom integrations with the AWS Well-Architected Tool made possible with the Well-Architected APIs. Here are a few more examples of the functionality available with the new AWS Well-Architected Tool APIs:\nIntegrate AWS Well-Architected data into centralized reporting tools, or integrate with ticketing and management solutions. Automation of best practice detection. Provide insights to AWS customers based on their AWS Well-Architected Review , and recommend remediation steps and guidance. Pre-populate information in the Well-Architected Tool for customers based on information that\u0026rsquo;s already known about them\u0026mdash;streamlining the review process. By leveraging the Well-Architected Tool APIs, companies can effectively govern workloads across many AWS accounts, stay up-to-date on the latest best practices, and scale Well-Architected principles across teams and systems.\nX Congratulations! It\u0026rsquo;s exciting to see the custom integrations with the AWS Well-Architected Tool made possible with the Well-Architected APIs. Let\u0026rsquo;s us know what other integrations you build.\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_testing_backup_and_restore_of_data/5_cleanup/","title":"Teardown","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nCleaning up AWS Backup Resources Sign in to the AWS Management Console and navigate to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on BACKUP VAULTS from the menu on the left side, and select BACKUP-LAB-VAULT. Under the section BACKUPS, delete all the RECOVERY POINTS. Once all the RECOVERY POINTS have been deleted, delete the Backup Vault by clicking on DELETE on the top right hand corner. Click on BACKUP PLANS from the menu on the left side, and select BACKUP-LAB. Scroll down to the section RESOURCE ASSIGNMENTS, and delete the resource assignment. Delete the BACKUP PLAN by clicking on DELETE on the upper right corner of the screen. Cleaning up the CloudFormation Stack Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack WA-Backup-Lab, and delete the stack. Cleaning up the CloudWatch Logs Sign in to the AWS Management Console, and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the /aws/lambda/RestoreTestFunction. Click the Actions Button then click Delete Log Group. Verify the log group name then click Yes, Delete. Thank you for using this lab X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 9 How do you back up data?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/container/","title":"Container","tags":[],"description":"","content":"These are queries for AWS Services under the Container product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon Elastic Container Services Amazon ECS - Daily Usage Hours and Cost by Usage Type and Purchase Option Amazon Elastic Container Services Query Description This query will output the daily cost and usage per resource, by operation and service, for Elastic Consainer Services, ECS and EKS, both unblended and amortized costs are shown. To provide you with a complete picture of the data, to match totals in cost explorer, if you are using Savings Plans you will likely see results with blank resource IDs, these represent the Savings Plans Negation values for compute cost already covered by Savings Plans.\nPricing Please refer to the Amazon ECS pricing page and the Amazon EKS pricing page .\nSample Output Sample output includes a subset of query columns Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6) AS split_line_item_resource_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_operation, line_item_product_code, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) sum_line_item_unblended_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost ELSE line_item_unblended_cost END) AS sum_amortized_cost FROM ${table_name} WHERE ${date_filter} and line_item_product_code IN (\u0026#39;AmazonECS\u0026#39;,\u0026#39;AmazonEKS\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6), DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_operation, line_item_product_code ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost, sum_line_item_usage_amount, line_item_operation; Help \u0026amp; Feedback Back to Table of Contents Amazon ECS - Daily Usage Hours and Cost by Usage Type and Purchase Option Query Description This query will output the daily ECS cost and usage per resource, by usage type and purchase option, both unblended and amortized costs are shown. To provide you with a complete picture of the data, to match totals in cost explorer, if you are using Savings Plans you will likely see results with blank resource IDs, these represent the Savings Plans Negation values for compute cost already covered by Savings Plans.\nPricing Please refer to the Amazon ECS pricing page .\nSample Output Sample output includes a subset of query columns Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6),\u0026#39;/\u0026#39;,2) AS split_line_item_resource_id, CASE WHEN line_item_usage_type LIKE \u0026#39;%Fargate-GB%\u0026#39; THEN \u0026#39;GB per hour\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Fargate-vCPU%\u0026#39; THEN \u0026#39;vCPU per hour\u0026#39; END AS case_line_item_usage_type, CASE line_item_line_item_type WHEN \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_offering_type WHEN \u0026#39;SavingsPlanNegation\u0026#39; THEN savings_plan_offering_type ELSE CASE pricing_term WHEN \u0026#39;OnDemand\u0026#39; THEN \u0026#39;OnDemand\u0026#39; WHEN \u0026#39;\u0026#39; THEN \u0026#39;Spot Instance\u0026#39; ELSE pricing_term END END AS case_purchase_option, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CASE pricing_term WHEN \u0026#39;OnDemand\u0026#39; THEN line_item_unblended_cost WHEN \u0026#39;\u0026#39; THEN line_item_unblended_cost END) AS sum_line_item_unblended_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN savings_plan_total_commitment_to_date - savings_plan_used_commitment WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost ELSE line_item_unblended_cost END) AS sum_amortized_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code IN (\u0026#39;AmazonECS\u0026#39;) AND line_item_operation != \u0026#39;ECSTask-EC2\u0026#39; AND product_product_family != \u0026#39;Data Transfer\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), SPLIT_PART(SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,6),\u0026#39;/\u0026#39;,2), 5, --refers to case_line_item_usage_type 6 -- refers to case_purchase_option ORDER BY day_line_item_usage_start_date ASC, case_purchase_option, sum_line_item_usage_amount DESC; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/5_ec2_computer_opt/","title":"Rightsizing with AWS Compute Optimizer and Memory Utilization Enabled","tags":[],"description":"","content":" In order to complete this step, you need to have AWS Compute Optimizer enabled.\nAWS Compute Optimizer uses machine learning to analyze the configuration and utilization data of your AWS resources. It reports whether your resources are optimal and generates recommendations (findings) to reduce the cost and improve the performance of your workloads.\nWhen it comes to EC2 instances there are three types of findings:\nOver-provisioned. An EC2 instance is considered over-provisioned when at least one of the utilization metrics (CPU, Memory, Network) can be sized down while still meeting the performance requirements of your workload, and when no specification is under-provisioned. Over-provisioned EC2 instances might lead to unnecessary infrastructure cost. Under-provisioned. An EC2 instance is considered under-provisioned when at least one of the utilization metrics (CPU, Memory, Network) does not meet the performance requirements of your workload. Under-provisioned EC2 instances might lead to poor application performance. Optimized. An EC2 instance is considered optimized when all the utilization metrics (CPU, Memory, Network) meet the performance requirements of your workload, and the instance is not over-provisioned. For optimized instances, Compute Optimizer might sometimes recommend a new generation instance type. You can enable AWS Compute Optimizer across your AWS Organization and filter the recommendations by AWS Accounts, AWS resource (EC2 instances, EBS volumes, Auto Scaling groups or Lambda functions), and AWS regions.\nIf you have just installed the CloudWatch agent on your instances it may take a couple of days for AWS Compute Optimizer to start providing updated recommendations. You may not see the memory data during the first checks.\nDuring the steps below we will review some AWS Computer Optimizer examples and how the recommendations are affected by having an additional data point from the EC2 instance memory utilization.\nNavigate to the AWS Compute Optimizer page. Ensure you are on the Dashboard view and have the desired region selected. Select Over-provisioned instances. Select an instance that is over-provisioned and has memory monitoring enabled by clicking the radio button to the left and select View Details. In this scenario, AWS Compute Optimizer recommends we resize to a t3.large. Our CPU utilization graph shows minimal CPU usage with some bursting. This is an ideal workload for the t3 family. Memory utilization is consistent at 90% and downsizing to a cheaper t3.large will keep our memory at the same size (8 GiB). In other words, we can get the same memory performance while reducing overall cost.\nA case could be made that this instance is under-provisioned and upsizing to an instance with more memory is a better solution. For this scenario, AWS Compute Optimizer views scaling down CPU and keeping the same memory the best decision for cost and performance. Having memory utilization enabled provides an additional data point for customers before making a final decision.\nSelect Under-provisioned instances. A list of under-provisioned instances will be present. Note the Recommended instance type. Select the radio button on the far left of the instance and then select View details in the top right. This view will give recommendations on rightsizing the current instance. Notice there can be multiple options for rightsizing (t3.xlarge, c5.xlarge, m5.xlarge). All options are recommending we move from 2 vCPUs to 4 vCPU’s. The first option will typically be the cheapest. There are multiple columns that allow you to compare and contrast the right choice for upsizing your instance, including price, performance risk , CPU, memory, storage, and network.\nClick through each option using the blue radio button on the left and notice how the below graphs change to show projections for the recommended instance. The CPU utilization graph shows our instance is maxed out on CPU and projects an orange dotted line where CPU will be for our new instance. On the example above you might have noticed that memory utilization is not currently enabled on this instance, therefore AWS Compute Optimizer will not report memory utilization. This could be an important datapoint when making a decision to which instance to upsize. For example, if memory is not an issue option 2 may be a better choice than option 3 from a cost perspective. AWS Compute Optimizer will not recommend instances with lower memory if memory monitoring is not available.\nLet’s take a look at that same instance with memory utilization enabled. Notice we can be more comfortable with our options for rightsizing. AWS Compute Optimizer will now take memory utilization into account. We can see that memory utilization is low and 8 GiB is sufficient. Choosing option 2 (c5.xlarge) will give us additional CPU power and keep our performance risk low by not having to switch to a burstable instance (t3.xlarge). Depending on your goals option 1 may be ideal for cost savings while option 2 would be ideal for overall performance and stability.\nConclusions Using AWS Compute Optimizer to get recommendations for rightsizing EC2 instances is a free and recommended routine. In this lab we explored recommendations for both an under-provisioned and over-provisioned EC2 instance and why having memory utilization enabled makes the recommendations more precise and gives a more holistic view of the EC2 instance performance. The knowledge gained from this lab should help you make rightsizing decisions to decrease cost and improve performance.\nVisit the Level 300 Optimization Data Collection Lab for opportunities to automate the collection of Compute Optimizer to identify resources that are underutilized for multiple lookback periods\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/5_expand_review/","title":"Integrate AWS Compute Optimizer and Trusted Advisor to Another Question","tags":[],"description":"","content":"\nOverview Now that we understood how to integrate AWS Compute Optimizer and AWS Trusted Advisor checks to review the question COST 6. How do you meet cost targets when you select resource type, size and number. In this section, we will learn how to include checks from AWS Compute Optimizer and AWS Trusted Advisor to another question. For example, COST 7. How do you use pricing models to reduce cost\nFinding your WorkloadID and QuestionID You can navigate to the Well-Architected Tool and select the workload created previously to retrieve the WorkloadId. WorkloadID can be found in the Properties Tab as part of the ARN. As an example, you can see that the WorkloadID for the workload called myapplication is f2f0bb92d2c9a0818d7254c71c516e98 (highlighted in the screenshot)\nWith the workloadID, we can now retrieve the questionID using the ListAnswers API .\nUpdate DynamoDB Mapping Table The next step is to update the mapping table with the questionID we\u0026rsquo;ve just retrieved and the Trusted Advisor Check ID that we would like to include in this question note. In this example, i\u0026rsquo;m going to include the Amazon EC2 Reserved Instance Optimzation check, this has check ID value as cX3c2R1chu You can find more about the other Trusted Advisor checks here With that, i\u0026rsquo;ll navigate to wa-mapping.json and update the mapping table as follows: { \u0026#34;tableName\u0026#34;: \u0026#34;wa-mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [ { \u0026#34;PillarNumber\u0026#34;: \u0026#34;COST-6\u0026#34;, \u0026#34;PillarId\u0026#34;: \u0026#34;costOptimization\u0026#34;, \u0026#34;QuestionTitle\u0026#34;: \u0026#34;How do you meet cost targets when you select resource type, size and number?\u0026#34;, \u0026#34;QuestionId\u0026#34;: \u0026#34;type-size-number-resources\u0026#34;, \u0026#34;ChoiceTitle\u0026#34;: \u0026#34;Select resource type, size, and number based on data\u0026#34;, \u0026#34;ChoiceId\u0026#34;: \u0026#34;cost_type_size_number_resources_data\u0026#34;, \u0026#34;TACheckId\u0026#34;: \u0026#34;Qch7DwouX1\u0026#34; }, { \u0026#34;PillarNumber\u0026#34;: \u0026#34;COST-7\u0026#34;, \u0026#34;PillarId\u0026#34;: \u0026#34;costOptimization\u0026#34;, \u0026#34;QuestionTitle\u0026#34;: \u0026#34;How do you use pricing models to reduce cost?\u0026#34;, \u0026#34;QuestionId\u0026#34;: \u0026#34;pricing-model\u0026#34;, \u0026#34;ChoiceTitle\u0026#34;: \u0026#34;Perform pricing model analysis at the master account level\u0026#34;, \u0026#34;ChoiceId\u0026#34;: \u0026#34;cost_pricing_model_master_analysis\u0026#34;, \u0026#34;TACheckId\u0026#34;: \u0026#34;cX3c2R1chu\u0026#34; } ] } Now we are going to update AWS DynamoDB table with the updated json file through API Gateway provision in the the previous step .\nReplace APIGWUrl with your APIGWUrl that you deployed previously. curl --header \u0026#34;Content-Type: application/json\u0026#34; -d @mappings/wa-mapping.json -v POST {APIGWUrl} Confirm that UnprocessedItems appear to be empty, which means you successfully put items into AWS DynamoDB. In AWS DynamoDB console, click wa-mapping you just deployed and click Explore table items. There are 2 Question IDs of Well-Architected questions and 2 AWS Trusted Advisor checks. Create a Well-Architected Workload with Tags Refer to the previous section to create new Well-Architected Workload With Tags\nPerform Review I created workload called demo3 and when i clicked on Continue reviewing, choose Cost Optimization pillar. I can see that both COST 6. How do you meet cost targets when you select resource type, size and number? and COST 7. How do you use pricing models to reduce cost? have been updated with the checks from AWS Compute Optimizer and AWS Trusted Advisor\nWith the mapping table updated, we can create a new review click Continue reviewing and select AWS Well-Architected Framework Lens. Throughout this lab, you have learned how to include AWS Compute Optimizer and AWS Trusted Advisor to prepare data before the review. This can be expanded further to prepare data for more questions and from more data sources\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/5_view_report/","title":"Viewing and downloading the report","tags":[],"description":"","content":"Overview You can generate a workload report for a lens. The report contains your responses to the workload questions, your notes, and the current number of high and medium risks identified.\n1. Gather pillar and risk data for a workload Using the get-lens-review API , you can retrieve the pillar review summaries for the workload: aws wellarchitected get-lens-review --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; This will return a summary of the workload review summaries for each pillar.\n1. Generate and download workload PDF Using the get-lens-review-report , you can retrieve the pillar review report in PDF format: aws wellarchitected get-lens-review-report --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; --lens-alias \u0026#34;wellarchitected\u0026#34; --query \u0026#39;LensReviewReport.Base64String\u0026#39; --output text | base64 --decode \u0026gt; WAReviewOutput.pdf This will export the object into a PDF report file.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/5_implement_shuffle_sharding/","title":"Implement shuffle sharding","tags":[],"description":"","content":"In this section you will update the architectural design of the workload and implement shuffle sharding. Shuffle sharding is a combinatorial implementation of a sharded architecture. With shuffle sharding you create virtual shards with a subset of the capacity of the workload ensuring that the virtual shards are mapped to a unique subset of customers with no overlap. By minimizing the number of Workers a single customer is able to interact with within the workload, and spreading resources in a combinatorial way, you will be able to further reduce the impact of a potential posion pill.\nThe following diagram shows the updated architecture you will deploy. This architecture implements shuffle sharding (Note: There are still only eight workers. Workers are shown logically twice, once for each shard they participate in): Update the workload architecture Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and select the stack that was created as part of this lab - Shuffle-sharding-lab\nClick on Update\nUnder Prerequisite - Prepare template, select Replace current template\nFor Template source select Amazon S3 URL In the text box under Amazon S3 URL specify https://aws-well-architected-labs-virginia.s3.amazonaws.com/Reliability/300_Fault_Isolation_with_Shuffle_Sharding/shuffle-sharding.yaml Click Next\nNo changes are required for Parameters. Click Next\nFor Configure stack options click Next\nOn the Review page:\nScroll to the end of the page and select I acknowledge that AWS CloudFormation might create IAM resources with custom names. This ensures CloudFormation has permission to create resources related to IAM. Additional information can be found here . Note: The template creates an IAM role and Instance Profile for EC2. These are the minimum permissions necessary for the instances to be managed by AWS Systems Manager. These permissions can be reviewed in the CloudFormation template under the \u0026ldquo;Resources\u0026rdquo; section - InstanceRole.\nClick Update stack This will take you to the CloudFormation stack status page, showing the stack update in progress. The stack takes about 1 minute to go through all the updates. Periodically refresh the CloudFormation stack events until you see that the Stack Status is in UPDATE_COMPLETE.\nWith this stack update, the architecture of the workload has been updated by introducing 8 Application Load Balancer listener rules and Target Groups. These listener rules have been configured to inspect the incoming request for the query-string name. Depending on the value provided, the request is routed to one of eight target groups where each target group consists of 2 EC2 instances.\nTest the shuffle sharded application Now that the application has been deployed, it is time to test it to understand how it works. The sample application used in this lab is the same as before, a simple web application that returns a message with the Worker that responded to the request. Customers pass in a query string as part of the request to identify themselves. The query string used here is name.\nVisit the Outputs section of the CloudFormation stack created in the previous step. You will see a list of URLs next to customer names.\nOpen the link for customer Alpha in a new browser tab. Refresh the web browser a few times to see that responses are being returned from different EC2 instances on the back-end.\nNotice that after implementing shuffle sharding, you are seeing responses being returned from only 2 instances for customer Alpha\u0026rsquo;s requests. No matter how many times you refresh the page or try a different browser, customer Alpha will only receive responses from 2 EC2 instances. This is because you have created Application Load Balancer listener rules that divert traffic to a specific subset of the overall capacity of the workload, also known as a shard. Each customer has a unique combination of EC2 instances that will respond to requests with no 2 customers having the same combination. The following diagram provides a breakdown of how customers are mapped to EC2 instances. Open the links for a few other customers and verify that they are able to get responses from only 2 EC2 instances. The different customers are - Alpha, Bravo, Charlie, Delta, Echo, Foxtrot, Golf, and Hotel and their corresponding URLs can be obtained from the CloudFormation stack Outputs.\nRefresh the web browser multiple times to verify that customers are receiving responses only from EC2 instances in the shard they are mapped to. The customer to shard/workers mapping can be found in the table below.\nCustomer Name Workers Alpha Worker-1 and Worker-2 Bravo Worker-2 and Worker-3 Charlie Worker-3 and Worker-4 Delta Worker-4 and Worker-5 Echo Worker-5 and Worker-6 Foxtrot Worker-6 and Worker-7 Golf Worker-7 and Worker-8 Hotel Worker-8 and Worker-1 X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/5_generating_load/","title":"Generate CPU and Memory load","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have a CloudWatch dashboard to show us CPU and Memory statistics for the deployed EC2 instance. In order to showcase the dashboards, lets add a synthetic load to the machine.\nStress For this lab, the EC2 instance will install a utility called stress. This tool is designed to subject your system to a configurable measure of CPU, memory, I/O and disk stress.\nStress tools is not installed by default, you will need to install the package in advance:\nsudo amazon-linux-extras install epel -y sudo yum install stress -y To use this command we will use:\nsudo stress --cpu 8 --vm-bytes $(awk \u0026#39;/MemAvailable/{printf \u0026#34;%d\\n\u0026#34;, $2 * 0.9;}\u0026#39; \u0026lt; /proc/meminfo)k --vm-keep -m 1 \u0026ndash;cpu This will spawn 8 CPU workers spinning on a square root task (sqrt(x)) \u0026ndash;vm-bytes This will use 90% of the available memory from /proc/meminfo \u0026ndash;vm-keep This will re-dirty memory instead of freeing and reallocating. -m 1 This will spawn 1 worker spinning on malloc()/free() Generate Load Open a new tab for the AWS console with this link: https://console.aws.amazon.com/ec2/v2/home?r#Instances:instanceState=running;tag:Name=LinuxMachineDeploy You should see the EC2 instance we have deployed. Troubleshooting: If you do not see the instance, and you changed the CloudFormation stack name when deploying, then delete the Name: LinuxMachineDeploy filter and search for the instance with the same name as you used for your stack Click the checkbox next to the machine, and then click \u0026ldquo;Connect\u0026rdquo; Select \u0026ldquo;Session Manager\u0026rdquo; and then click Connect. This will open a new Linux bash shell to run commands on the EC2 instance. Type sudo stress --cpu 8 --vm-bytes $(awk '/MemAvailable/{printf \u0026quot;%d\\n\u0026quot;, $2 * 0.9;}' \u0026lt; /proc/meminfo)k --vm-keep -m 1 This will start to consume all of the available memory as well as all CPU\u0026rsquo;s within the instance Go back to your browser tab that contains the CloudWatch Dashboard. You should see the CPU and Memory graphs change within 10-15 seconds. cpu_usage_user goes up as the test script consumes CPU mem_used goes up as the script consumes all of it except for a small reserve and should level off right below mem_total As time goes on, it will continue to update the graph. In order to remove the load, press CTRL-C to stop the stress script. Go back to your browser tab that contains the CloudWatch Dashboard to watch as the CPU load goes down and the amount of free memory increases. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/5_generating_load/","title":"Generate CPU and Memory load","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"We have a CloudWatch dashboard to show us CPU and Memory statistics for the deployed EC2 instance. In order to showcase the dashboards, lets add a synthetic load to the machine. We have 2 PowerShell scripts that have already been deployed to the instance to facilitate this.\ncpu_stress.ps1 This script will start multiple threads (one per CPU in the machine) to keep the processor busy doing a simple math computation. We set the thread priority to \u0026ldquo;Lowest\u0026rdquo; so it should still allow system processes to continue.\nView cpu_stress.ps1 Code \u0026lt;# .EXAMPLE .\\cpu_stress.ps1 This will execute the script against all cores .DESCRIPTION #\u0026gt; # CPUs in the machine $cpus=$env:NUMBER_OF_PROCESSORS # Lower the thread so it won\u0026#39;t overwhelm the system for other things [System.Threading.Thread]::CurrentThread.Priority = \u0026#39;Lowest\u0026#39; ##################### # perfmon counters for CPU $Global:psPerfCPU = new-object System.Diagnostics.PerformanceCounter(\u0026#34;Processor\u0026#34;,\u0026#34;% Processor Time\u0026#34;,\u0026#34;_Total\u0026#34;) $psPerfCPU.NextValue() | Out-Null $StartDate = Get-Date Write-Output \u0026#34;=-=-=-=-=-=-=-=-=-= Stress Machine Started: $StartDate =-=-=-=-=-=-=-=-=-=\u0026#34; Write-Warning \u0026#34;This script will saturate all available CPUs in the machine\u0026#34; Write-Warning \u0026#34;To cancel execution of all jobs, close the PowerShell Host Window (or terminate the remote session)\u0026#34; Write-Output \u0026#34;=-=-=-=-=-=-=-=-=-= CPUs in box: $cpus =-=-=-=-=-=-=-=-=-= \u0026#34; # This will stress the CPU foreach ($loopnumber in 1..$cpus){ Start-Job -ScriptBlock{ $result = 1 foreach ($number in 1..0x7FFFFFFF){ $result = $result * $number }# end foreach }# end Start-Job }# end foreach Write-Output \u0026#34;Created sub-jobs to consume the CPU\u0026#34; # Ask the user if they want to clear out RAM, if so we will continue Read-Host -Prompt \u0026#34;Press any key to stop the JOBs CTRL+C to quit\u0026#34; Write-Output \u0026#34;Clearing CPU Jobs\u0026#34; Receive-Job * Stop-Job * Remove-Job * $EndDate = Get-Date Write-Output \u0026#34;=-=-=-=-=-=-=-=-=-= Stress Machine Complete: $EndDate =-=-=-=-=-=-=-=-=-=\u0026#34; mem_stress.ps1 This script will create an ever expanding array in RAM to attempt to consume as much as possible. We do reserve 512Mb of ram for the OS to continue to operate.\nView mem_stress.ps1 Code \u0026lt;# .EXAMPLE .\\mem_stress.ps1 This will execute the script to consume all of the memory (less 512 for the OS to survive) .DESCRIPTION #\u0026gt; # RAM in box $box=get-WMIobject Win32_ComputerSystem $Global:physMB=$box.TotalPhysicalMemory / 1024 /1024 # Create object to get current memory available $Global:psPerfMEM = new-object System.Diagnostics.PerformanceCounter(\u0026#34;Memory\u0026#34;,\u0026#34;Available Mbytes\u0026#34;) $psPerfMEM.NextValue() | Out-Null # leave 512Mb for the OS to survive. $HEADROOM=512 $ram = $physMB - $psPerfMEM.NextValue() $maxRAM=$physMB - $HEADROOM $progress = ($ram / $maxRAM) * 100 $completed = [int]$progress $StartDate = Get-Date Write-Output \u0026#34;=-=-=-=-=-=-=-=-=-= Memory Stress Started: $StartDate =-=-=-=-=-=-=-=-=-=\u0026#34; Write-Output \u0026#34;mem_stress - This script will consume all but 512MB of RAM available on the machine\u0026#34; Write-Output \u0026#34;Starting consumed RAM: $ram out of $maxRAM ($completed% Full)\u0026#34; Write-Output \u0026#34;=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\u0026#34; # If you increase the size of the array the GC seems to do quicker cleanups # Not sure why, but 200MB seems to be the suite spot $a = \u0026#34;a\u0026#34; * 200MB # These are the arrays we will create to consume all of the RAM $growArray = @() $growArray += $a $bigArray = @() $k=0 $lastCompleted = 900 # This loop will continue until we have consumed all of the RAM minus the headroom while ($ram -lt $maxRAM) { $bigArray += ,@($k,$growArray) $k += 1 $growArray += $a # Find out how much RAM we are now consuming $ram = $physMB - $psPerfMEM.NextValue() $progress = ($ram / $maxRAM) * 100 $completed = [int]$progress $status_string = -join([int]$ram,\u0026#34; of \u0026#34;,[int]$maxRAM, \u0026#34;MB ($completed% Complete)\u0026#34;) # Only show the message when we have a change in percentage if ($completed -ne $lastCompleted) { Write-Output \u0026#34;$status_string\u0026#34; $lastCompleted = $completed } } Write-Output \u0026#34;=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\u0026#34; # Do a final check of RAM after consuming it all $ram = $physMB - $psPerfMEM.NextValue() Write-Output \u0026#34;FINAL $ram / $maxRAM\u0026#34; # Ask the user if they want to clear out RAM, if so we will continue Read-Host -Prompt \u0026#34;Press any key to clear out RAM or CTRL+C to quit\u0026#34; Write-Output \u0026#34;Clearing RAM\u0026#34; ##################### # and now release it all. $bigArray.clear() #remove-variable bigArray $growArray.clear() #remove-variable growArray [System.GC]::Collect() ##################### $ram = $physMB - $psPerfMEM.NextValue() Write-Output \u0026#34;RAM HAS BEEN CLEARED: $ram / $maxRAM\u0026#34; Generate Load Open a new tab for the AWS console with this link: https://console.aws.amazon.com/ec2/v2/home?r#Instances:instanceState=running;tag:Name=WindowsMachineDeploy You should see the EC2 instance we have deployed.\nTroubleshooting: If you do not see the instance, and you changed the CloudFormation stack name when deploying, then delete the Name: WindowsMachineDeploy filter and search for the instance with the same name as you used for your stack Click the checkbox next to the machine, and then click \u0026ldquo;Connect\u0026rdquo; Select \u0026ldquo;Session Manager\u0026rdquo; and then click Connect. This will open a new tab with a PowerShell console for the instance. Type C:\\mem_stress.ps1 at the console and it will start to consume memory resources Go back to the previous broswer tab that has the EC2 console connect screen and click Connect again. This will open another PowerShell console. Type C:\\cpu_stress.ps1 at the console and it will start to consume CPU resources Go back to your browser tab that contains the CloudWatch Dashboard. You should see the CPU and Memory graphs change within 10-15 seconds.\nProcessor % User Time goes up as the test script consumes CPU Memory Available goes down, as the script consumes all of it except for a small reserve As time goes on, it will continue to update the graph. In order to remove the load, go back to each of the console windows and simply press any key. This will cause the script to reclaim all resources it has consumed.\nGo back to your browser tab that contains the CloudWatch Dashboard to watch as the CPU load goes down and the amount of free RAM increases. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/4_baseline_sustainability_kpi/","title":"Baseline Sustainability KPI","tags":[],"description":"","content":"Lab 4 Recall, our sustainability improvement goal is:\nTo eliminate waste, low utilization, and idle or unused resources. To maximize the value from resources you consume. Let’s baseline the metrics which we can use to measure sustainability improvement once workload optimization is completed - in this case, AWS resources provisioned (storage, network traffic) to support the business outcome (number of events).\nConnect to producer cluster (dev database, public schema) in us-east-1 region, and follow below steps.\nStep-1: Proxy metrics Let\u0026rsquo;s baseline below proxy metrics (provisioned resources) by executing the following SQL query in producer database:\nTotal data storage used for storing all tables: SELECT SUM(size) FROM SVV_TABLE_INFO WHERE \u0026#34;table\u0026#34; NOT LIKE \u0026#39;%auto%\u0026#39;; Above query will return data storage size (in MB) of all tables provisioned for all business events (business outcome) held by AnyCompany (stored in lab_event table). Data storage consumed in producer cluster = 640MB.\nNote - if you had selected ra3.4xlarge node type for Producer cluster creation in Lab-2, then you will receive 1252MB as output from above query. Please adjust rest of the lab steps accordingly for calculation\nWe will assume that the consumer cluster had the same amount of storage consumed in the current deployment, as it was built, and being refreshed from the producer cluster periodically. With that assumption, running the above query against the consumer cluster will provide similar result for data storage size (in MB).\nSo, total data storage consumed (provisioned) by two clusters (producer and consumer) = 640+640 = 1280MB\nTotal data transfer over network to nightly refresh consumer cluster in us-west-1 region from producer cluster in us-east-1 region: Assuming 10% daily data change rate in Producer cluster, Data transfer over network (every night) = 64MB (10% of 640MB - Producer cluster storage)\nStep-2: Business metrics Let’s now find out number of marketing events held by AnyCompany. The above proxy metrics (resources consumed) are consumed for those events. These events are business metrics, and we can find this out by running SQL query in producer database:\nSELECT COUNT(*) FROM lab_event; Above query will return the number of events held by AnyCompany = 8798. This will be the business metric we will be using for our sustainability KPI.\nStep-3: Sustainability KPI Baseline Let’s baseline the KPI:\nper event data storage = 1280MB / 8798 events = 0.14MB per event daily data transfer = 64MB / 8798 events = 0.007MB Our improvement goal is to reduce per event provisioned resources, and in this case:\nper event total storage used per event data transfer over network Now, let’s start optimizing this workload by implementing Well-Architected Sustainability Pillar best practices for Data Pattern, and Amazon Redshift Data Sharing feature. Our objective is to reduce the per event data storage, and data transfer between us-east-1 \u0026amp; us-west-1 region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/4_baseline_sustainability_kpi/","title":"Baseline Sustainability KPI","tags":[],"description":"","content":"Lab 4 Recall, our sustainability improvement goal is:\nTo eliminate waste, low utilization, and idle or unused resources. To maximize the value from resources you consume. Let’s baseline the metrics which we can use to measure sustainability improvement once workload optimization is completed - in this case, AWS resources provisioned (storage, network traffic) to support the business outcome (number of events).\nConnect to producer cluster (dev database, public schema) in us-east-1 region, and follow below steps.\nStep-1: Proxy metrics Let\u0026rsquo;s baseline below proxy metrics (provisioned resources) by executing the following SQL query in producer database:\nTotal data storage used for storing all tables: SELECT SUM(size) FROM SVV_TABLE_INFO WHERE \u0026#34;table\u0026#34; NOT LIKE \u0026#39;%auto%\u0026#39;; Above query will return data storage size (in MB) of all tables provisioned for all business events (business outcome) held by AnyCompany (stored in lab_event table). Data storage consumed in producer cluster = 640MB.\nNote - if you had selected ra3.4xlarge node type for Producer cluster creation in Lab-2, then you will receive 1252MB as output from above query. Please adjust rest of the lab steps accordingly for calculation\nWe will assume that the consumer cluster had the same amount of storage consumed in the current deployment, as it was built, and being refreshed from the producer cluster periodically. With that assumption, running the above query against the consumer cluster will provide similar result for data storage size (in MB).\nSo, total data storage consumed (provisioned) by two clusters (producer and consumer) = 640+640 = 1280MB\nTotal data transfer over network to nightly refresh consumer cluster in us-west-1 region from producer cluster in us-east-1 region: Assuming 10% daily data change rate in Producer cluster, Data transfer over network (every night) = 64MB (10% of 640MB - Producer cluster storage)\nStep-2: Business metrics Let’s now find out number of marketing events held by AnyCompany. The above proxy metrics (resources consumed) are consumed for those events. These events are business metrics, and we can find this out by running SQL query in producer database:\nSELECT COUNT(*) FROM lab_event; Above query will return the number of events held by AnyCompany = 8798. This will be the business metric we will be using for our sustainability KPI.\nStep-3: Sustainability KPI Baseline Let’s baseline the KPI:\nper event data storage = 1280MB / 8798 events = 0.14MB per event daily data transfer = 64MB / 8798 events = 0.007MB Our improvement goal is to reduce per event provisioned resources, and in this case:\nper event total storage used per event data transfer over network Now, let’s start optimizing this workload by implementing Well-Architected Sustainability Pillar best practices for Data Pattern, and Amazon Redshift Data Sharing feature. Our objective is to reduce the per event data storage, and data transfer between us-east-1 \u0026amp; us-west-1 region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/1-4_queries_from_sar/","title":"Install the AWS Cost and Usage Queries from the AWS Serverless Application Repository","tags":[],"description":"","content":"Lab 1.4 In the previous steps you\u0026rsquo;ve made the AWS Cost \u0026amp; Usage Report data available to Amazon Athena with a lot of manual steps. In this lab you will use sample code to automate this setup and deploy a set of pre-canned queries automatically.\nThe sample code is packaged and available in the AWS Serverless Application Repository and called AWS Usage Queries (find the source code on GitHub ).\nLet\u0026rsquo;s deploy this from the AWS Serverless Application Repository:\nGo to the AWS Usage Queries in the AWS Serverless Application Repository Choose Deploy. Change the AWS Management Console\u0026rsquo;s region to the region in which the bucket with the CUR data is, as set up in lab 1.1 . Fill the fields according to the configuration of your AWS Cost \u0026amp; Usage Report: CurBucketName: your bucket name ReportName: your report name ReportPathPrefix: your report path prefix Note If you chose Option C in lab 1.1 , use:\nCurBucketName: your-bucket-name ReportName: proxy-metrics-lab ReportPathPrefix: cur-data/hourly Select I acknowledge that this app creates custom IAM roles and resource policies. Click Deploy. AWS Lambda will initiate the deployment and redirect you to a page for the application you are deploying. Click Deployments and wait approximately 2 minutes until the application is deployed. When the deployment status is Create complete go to the Amazon Athena console. Choose the aws_usage_queries_database database from the drop down. As the tables are created now, they won\u0026rsquo;t query the partitions with the sample data from 2018. You need to change the table\u0026rsquo;s data range to also extend to 2018. Execute the following query: ALTER TABLE `cur_hourly` SET TBLPROPERTIES (\u0026#39;projection.year.range\u0026#39;=\u0026#39;2018,2022\u0026#39;) Explore the data. Choose the three dots next to the tables and views and choose Preview table. Congratulations! With a two-click deployment you can now query the CUR data, have pre-canned queries for proxy metrics for Amazon EC2 and Amazon S3, and additional reference data for the EC2 instance types in the table ref_instance_types.\nYou can now continue with Lab 2 in which you will learn to extend the queries by further data sources.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/5_create_custom_data_collection_module/","title":"Create Custom Data Collection Module (Optional)","tags":[],"description":"","content":"Example Module Explanation You can use this template to build your own data collection modules. In the next step, we will be providing you with modules you can add to your template. These are sets of Infrastructure as code that have all the resources you need and is easy to add to your template.\nDownload CloudFormation template by clicking here. This will be the foundation of the rest of this section and can be reused to build out the modules.\nThe first section we have Parameters which can be passed in from the main template. These are good for roles you will be using for reusable resources like the Amazon S3 Bucket or IAM Roles. There are also Outputs which declares output values that you can import into other stacks in the template.\nTo collect the data we have a lambda function which uses a role to have permissions for the resources its going to be utilizing. By default the role can currently:\nAssume Management Role we setup Access and place a file in S3 Start a Glue Crawler These are all needed actions for the basic lambda. If you need to add more to access the service you are collecting data from you need checkout documentation here and add to the policy.\nThe lambda function we have at the moment gives the base for your python code. In this we have an example of collecting S3 Object sizes. The key elements are: Collecting the data and storing them in a temporary json file (/tmp/file.json) Uploading the file into a partitioned folder in S3 (based on year and month) Starting the crawler to create/update the Athena Table There is a Glue Crawler which is the one we triggered in the Lambda which reads from your S3 bucket and creates an Athena table based on the data. It used the Glue Role we made in the template.\nTo trigger the Lambda we use a CloudWatch event which runs on a pre-defined schedule. You can find more options for scheduling here There is an example of a saved Athena query. This is useful if you know that you will be using the same query often and want to have it available.\nUsing the module in your Main CloudFormation Template Once you have your module created you can add it to your CloudFormation Template from the first stage.\nSave this file in your own S3 bucket which will be referred to as your Code Bucket in your Cost Optimization account where your template is deployed.\nOnce uploaded you can see your Object URL on the properties of the object. This will be used in the TemplateURL in the next step. In your Main file you can add a new CloudFormation Stack resource. In the example below you can see:\nName of the stack\nAn example link of the S3 object - Update with your url\nThe Parameters needed for the template\nDataStack: Type: AWS::CloudFormation::Stack Properties: TemplateURL: https://s3-eu-west-1.amazonaws.com/\u0026lt;mybucket\u0026gt;/lambda_s3_athen_cf_template.yaml TimeoutInMinutes: 2 Parameters: DestinationBucket: !Ref S3Bucket GlueRoleARN: !GetAtt GlueRole.Arn ManagementAccountRole: !Sub \u0026quot;arn:aws:iam::${ManagementAccountID}:role/${ManagementAccountRole}\u0026quot; Now you have added your new module you can update your CloudFormation stack in the console by selecting your stack and clicking Update If you would like more information on AWS CloudFormation checkout there website X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/100_labs/100_create_a_data_bunker/","title":"Create a Data Bunker Account","tags":[],"description":"","content":"Last Updated: September 2020\nAuthor: Byron Pogson, Solution Architect\nIntroduction In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.\nIf you are using AWS Control Tower the steps in this lab cover what has already been configured for the Control Tower Log Archive Account .\nPrerequisites A multi-account structure with AWS Organizations You have access to a role with administrative access to the management account for your AWS Organization Costs Typically less than $1 per month if the account is only used for personal testing or training, and the tear down is not performed Amazon S3 pricing Amazon CloudFront Pricing AWS CloudTrail pricing AWS Pricing Steps: Creating data bunker account in console "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/5_bonus_content/","title":"Bonus Content","tags":[],"description":"","content":"Now that dependency monitoring has been established by leveraging CloudWatch Metrics and CloudWatch alarms, the last piece of the \u0026ldquo;puzzle\u0026rdquo; is to ensure that events related to the external service are tracked effectively so that relevant stakeholders are aware of the status of resolution. Alarms and notifications are good to alert teams of potential issues, however, tracking an event such as this will ensure co-ordination of efforts towards resolution. AWS Systems Manager OpsCenter can be used to achieve this. An OpsItem can be created to track events and quickly understand the current status of an event and can help answer questions such as - what level of severity is the event? what resources are affected? what is the status of the event? are there other events similar to this?\nAutomating creation of an OpsItem, coupled with alarms and notifications will allow teams to quickly triage events and lead to faster, more organized resolution.\nThis process can be automated by using a Lambda function to create an OpsItem every time the dependency alarm goes into an In alarm state.\nGo to the Amazon SNS console at https://console.aws.amazon.com/sns/v3 and click on Topics\nClick on the SNS Topic that was created as part of this lab - WA-Lab-Dependency-Notification\nScroll down to the Subscriptions section and click on Create subscription\nOn the Create subscription page, make the following changes:\nTopic ARN - leave the default value that is already on there Protocol - select AWS Lambda from the dropdown Endpoint - paste the ARN of the OpsItemFunction copied from the Outputs section of the CloudFormation stack from section 1 Deploy the Infrastructure Click on Create subscription\nTo test this, follow the instructions in the previous section on testing a fail condition by deleting the default route. This time, when the alarm goes into an In alarm state, an OpsItem will be created in OpsCenter, in addition to the notification being sent to the email address specified.\nGo to the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager and click on OpsCenter\nClick on the OpsItems tab, search by Title, select contains, and enter the value as S3 Data Writes\nClick on the OpsItem that has been created with the title S3 Data Writes failing\nExpand the OpsItem details section by clicking on the triangle next to it, and view the information available there such as severity, category, etc.\nScroll down to the see the Related resources, in this case, the S3 bucket to which the writes are failing\nThe event can now be efficiently tracked using the OpsItem, and remediation work can be better co-ordinated. Additionally, you can choose to execute pre-created Runbooks which are listed under the Runbooks section and automate the remediation. You can create custom runbooks depending on the type of event.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/contributing/05_creatingnew/","title":"Creating a New lab","tags":[],"description":"","content":" Prior to starting work on a new lab Open an issue with your lab idea with a brief summary explaining how it fits into the Well-Architected labs, and an outline of your idea. Make sure to add a Label with the Well-Architected Pillar (i.e. COST) A Well-Architected Lead will reach out to confirm or get additional context on your idea For a description of how Hugo\u0026rsquo;s directory structure works, see this: Labs are built using Hugo and the Learn theme, which utilizes markdown. You can see the Markdown reference for Hugo here For a sample lab, look at the existing labs .md file File structure (note case sensitivity: All labs must belong to a pillar (if they can cross over, you may want to make them reusable via short codes). The pillar directories are located in /content/\u0026lt;Pillar Name\u0026gt;/\u0026lt;lab level\u0026gt;/\u0026lt;content folder for lab\u0026gt;. For instance, you would put a 100 level performance lab in /content/Performance Efficiency/100_Labs/100_my_new_lab No spaces in file names _index.md is required in the root folder for your lab, it’s the lab landing page (old README.md) and then each step is its own markdown file, prefaced with the order number. (For example: 1_starthere.md, 2_do_a_thing.md, 3_cleanup.md). You can also create a directory structure to hold additional markdown content as needed (MUST be markdown files, non-markdown files are in static) All files must have front-matter at the top of the file. See this for more information. For all front-matter, make sure you have a title defined and a weight. The weight is used to order the labs. Non-markdown Code must be stored in /static/(shortpillarname)/(content folder for lab)/code Images must be stored in /static/(shortpillarname)/(content folder for lab)/Images Image size should be that of a laptop screen. Roughly 1000px x 700px is good, if you use larger images they get scaled \u0026amp; are hard to read Image width MUST always be \u0026gt;800px, which stops images from being placed next to text. Create the image border and then resize with whitespace Ensure there is a black border around the images by formatting your images using the following structure ![Images/(your image.png)](/(shortpillarname)/(content folder for lab)/Images/(your image.png)?classes=lab_picture_small) Picture Formatting When you update a lab picture please ensure it had the following:\nBlack boarder Orange box\u0026rsquo;s to show the item the customer is looking for Role/AccountID hidden using the same colour as the section Image width MUST always be \u0026gt;800px, which stops images from being placed next to text. Create the image border and then resize with whitespace The description above the image much match the image e.g. if you say use 2688Mb then the picture must have 2688Mb An example can be seen below:\nStep example :\nRole name LambdaOrgRole, click Create role: Verify your edits and/or additions After making the changes or additions test and verify locally\nNavigate back to the aws-well-architected-labs parent folder\nServe the content locally:\nhugo serve -D Open a browser and navigate to http://localhost:1313/ Verify the change you made was correct and no problems were introduced\nPush your changes to the remote repository: Push your changes to GitHub, so that they are stored and backed up by GitHub. Use the following commands:\ngit add -A git commit -m \u0026quot;your comment here\u0026quot; git push Please write a descriptive commit message following this guidance. Smaller meaningful commits are better than one large commit with lots of changes\nAll your changes will be in the remote repository in GitHub, which can now be merged into the Well-Architected Labs repository.\nReview process New labs require an additional validation step of two peer reviews. Please have two peers review your lab step by step to confirm it is working as expected prior to submitting a pull request.\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/cost_optimization/","title":"Cost Optimization","tags":[],"description":"","content":"These are queries for AWS Services under the AWS Well-Architected Framework Cost Optimization Pillar .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nPrior to deleting resources, check with the application owner that your analysis is correct and the resources are no longer in use.\nTable of Contents Compute Elastic Load Balancing - Idle ELB Elastic Compute Cloud - Unallocated Elastic IPs Graviton Usage End User Computing Amazon WorkSpaces - Auto Stop Networking \u0026amp; Content Delivery NAT Gateway - Idle NATGW Storage EBS Volumes Modernize gp2 to gp3 EBS Snapshot Trends S3 Bucket Trends and Optimizations Elastic Load Balancing - Idle ELB Cost Optimization Technique This query will display cost and usage of Elastic Load Balancers which didn’t receive any traffic last month and ran for more than 336 hours (14 days). Resources returned by this query could be considered for deletion. AWS Trusted Advisor provides a check for idle load balancers but only covers Classic Load Balancers. This query will provide all Elastic Load Balancer types including Application Load Balancer, Network Load Balancer, and Classic Load Balancer.\nThe assumption is that if the Load Balancer has not received any traffic within 14 days, it is likely orphaned and can be deleted.\nCopy Query Click here - to expand the query Copy the query below or click to Download SQL File SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 6) AS split_line_item_resource_id, product_region, pricing_unit, sum_line_item_usage_amount, CAST(cost_per_resource AS DECIMAL(16, 8)) AS sum_line_item_unblended_cost FROM ( SELECT line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_per_resource_and_pricing_unit, COUNT(pricing_unit) OVER (PARTITION BY line_item_resource_id) AS pricing_unit_per_resource FROM ${table_name} WHERE line_item_product_code = \u0026#39;AWSELB\u0026#39; -- get previous month AND month = CAST(month(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS VARCHAR) -- get year for previous month AND year = CAST(year(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS VARCHAR) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id ) WHERE -- filter only resources which ran more than half month (336 hrs) usage_per_resource_and_pricing_unit \u0026gt; 336 AND pricing_unit_per_resource = 1 ORDER BY cost_per_resource DESC; Helpful Links Please refer to the ELB AWS CLI documentation for deletion instructions. The commands vary between the ELB types.\nClassic Application Network Help \u0026amp; Feedback Back to Table of Contents EC2 Unallocated Elastic IPs Cost Optimization Technique This query will return cost for unallocated Elastic IPs. Elastic IPs incur hourly charges when they are not allocated to a Network Load Balancer, NAT gateway or an EC2 instance (or when there are multiple Elastic IPs allocated to the same EC2 instance). The usage amount (in hours) and cost are summed and returned in descending order, along with the associated Account ID and Region.\nPricing Please refer to the EC2 Elastic IP pricing page .\nSample Output Download SQL File Copy the query below or click to Download SQL File SELECT line_item_usage_account_id, line_item_usage_type, product_location, line_item_line_item_description, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND line_item_usage_type LIKE \u0026#39;%ElasticIP:IdleAddress\u0026#39; GROUP BY line_item_usage_account_id, line_item_usage_type, product_location, line_item_line_item_description ORDER BY sum_line_item_unblended_cost DESC, sum_line_item_usage_amount DESC; Helpful Links Elastic IP Charges Help \u0026amp; Feedback Back to Table of Contents Graviton Usage Cost Optimization Technique AWS Graviton processors are designed by AWS to deliver the best price performance ratio for cloud workloads, delivering up to 40% improvement over comparable current gen x86 processors. Due to the improved price performance, many organizations track Graviton usage as a KPI to drive cost savings for their cloud workloads. Graviton-based EC2 instances are available, and many other AWS services such as Amazon Relational Database Service, Amazon ElastiCache, Amazon EMR, and Amazon OpenSearch also support Graviton-based instance types.\nThis query provides detail on Graviton-based usage. Amortized cost, usage hours, and a count of unique resources are summed. Output is grouped by day, payer account ID, linked account ID, service, instance type, and region. Output is sorted by day (descending) and amortized cost (descending).\nDownload SQL File Link to Code Copy Query SELECT DATE_TRUNC(\u0026#39;day\u0026#39;,line_item_usage_start_date) AS day_line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, line_item_product_code, product_instance_type, product_region, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN line_item_unblended_cost ELSE 0 END) AS sum_amortized_cost, SUM(line_item_usage_amount) as sum_line_item_usage_amount, COUNT(DISTINCT(line_item_resource_id)) AS count_line_item_resource_id FROM ${table_name} WHERE ${date_filter} AND REGEXP_LIKE(line_item_usage_type, \u0026#39;.[a-z]([1-9]|[1-9][0-9]).?.?[g]\\.\u0026#39;) AND line_item_usage_type NOT LIKE \u0026#39;%EBSOptimized%\u0026#39; AND (line_item_line_item_type = \u0026#39;Usage\u0026#39; OR line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; OR line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; ) GROUP BY DATE_TRUNC(\u0026#39;day\u0026#39;,line_item_usage_start_date), bill_payer_account_id, line_item_usage_account_id, line_item_product_code, line_item_usage_type, product_instance_type, product_region ORDER BY day_line_item_usage_start_date DESC, sum_amortized_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon WorkSpaces - Auto Stop Cost Optimization Technique AutoStop Workspaces are cost effective when used for several hours per day. If AutoStop Workspaces run for more than 80 hrs per month it is more cost effective to switch to AlwaysOn mode. This query shows AutoStop Workspaces which ran more that 80 hrs in previous month. If the usage pattern for these Workspaces is the same month over month it\u0026rsquo;s possible to optimize cost by switching to AlwaysOn mode. For example, Windows PowerPro (8 vCPU, 32GB RAM) bundle in eu-west-1 runs for 400 hrs per month. In AutoStop mode it costs $612/month ($8.00/month + 400 * $1.53/hour) while if used in AlwaysOn mode it would cost $141/month.\nCopy Query Click here - to expand the query Copy the query below or click to Download SQL File SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;/\u0026#39;, 2) AS split_line_item_resource_id, product_region, product_operating_system, product_bundle_description, product_software_included, product_license, product_rootvolume, product_uservolume, pricing_unit, sum_line_item_usage_amount, CAST(total_cost_per_resource AS DECIMAL(16, 8)) AS \u0026#34;sum_line_item_unblended_cost(incl monthly fee)\u0026#34; FROM ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, product_operating_system, pricing_unit, product_region, product_bundle_description, product_rootvolume, product_uservolume, product_software_included, product_license, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS total_cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_amount_per_resource_and_pricing_unit FROM ${table_name} WHERE line_item_product_code = \u0026#39;AmazonWorkSpaces\u0026#39; -- get previous month AND CAST(month AS INT) = CAST(month(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS INT) -- get year for previous month AND CAST(year AS INT) = CAST(year(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS INT) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; AND line_item_usage_type LIKE \u0026#39;%AutoStop%\u0026#39; GROUP BY line_item_usage_account_id, line_item_resource_id, product_operating_system, pricing_unit, product_region, product_bundle_description, product_rootvolume, product_uservolume, bill_payer_account_id, product_software_included, product_license ) WHERE -- return only workspaces which ran more than 80 hrs usage_amount_per_resource_and_pricing_unit \u0026gt; 80 ORDER BY total_cost_per_resource DESC, line_item_resource_id, line_item_usage_account_id, product_operating_system, pricing_unit; Helpful Links Please refer to the AWS Solution, Amazon WorkSpaces Cost Optimizer . This solution analyzes all of your Amazon WorkSpaces usage data and automatically converts the WorkSpace to the most cost-effective billing option (hourly or monthly), depending on your individual usage. This solution also helps you monitor your WorkSpace usage and optimize costs. This automates the manual process of running the above query and adjusting your WorkSpaces configuration.\nHelp \u0026amp; Feedback Back to Table of Contents NAT Gateway - Idle NATGW Cost Optimization Technique This query shows cost and usage of NAT Gateways which didn’t receive any traffic last month and ran for more than 336 hrs. Resources returned by this query could be considered for deletion.\nBesides deleting idle NATGWs you should also consider the following tips:\nDetermine What Types of Data Transfers Occur the Most - Deploy the CUDOS dashboard to help visualize top talkers Eliminate Costly Cross Availability Zone Transfer Charges - create new NAT Gateways in the same availability zone as your instances Consider Sending Amazon S3 and Dynamo Traffic Through Gateway VPC Endpoints Instead of NAT Gateways Consider Setting up Interface VPC Endpoints Instead of NAT Gateways for Other Intra-AWS Traffic Copy Query Click here - to expand the query Copy the query below or click to Download SQL File SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 6) AS split_line_item_resource_id, product_region, pricing_unit, sum_line_item_usage_amount, CAST(cost_per_resource AS DECIMAL(16, 8)) AS sum_line_item_unblended_cost FROM ( SELECT line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_per_resource_and_pricing_unit, COUNT(pricing_unit) OVER (PARTITION BY line_item_resource_id) AS pricing_unit_per_resource FROM ${table_name} WHERE line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND line_item_usage_type LIKE \u0026#39;%Nat%\u0026#39; -- get previous month AND month = CAST(month(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS VARCHAR) -- get year for previous month AND year = CAST(year(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS VARCHAR) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id ) WHERE -- filter only resources which ran more than half month (336 hrs) usage_per_resource_and_pricing_unit \u0026gt; 336 AND pricing_unit_per_resource = 1 ORDER BY cost_per_resource DESC; Helpful Links Data Transfer Costs Explained Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Volumes Modernize gp2 to gp3 Cost Optimization Technique This query will display cost and usage of general purpose Elastic Block Storage Volumes and provide the estimated cost savings for modernizing a gp2 volume to gp3 These resources returned by this query could be considered for upgrade to gp3 as with up to 20% cost savings, gp3 volumes help you achieve more control over your provisioned IOPS, giving the ability to provision storage with your unique applications in mind. This query assumes you would provision the max iops and throughput based on the volume size, but not all resources will require the max amount and should be validated by the resource owner.\nIf you are running this for all accounts in a large organization we recommend running the query below first to confirm export size is not over ~1M rows. If the count shown in the query is greater than 1M you will want to filter to groupings of accounts or feed this query into a BI tool such as QuickSight\nClick here - to expand the query SELECT COUNT(DISTINCT(line_item_resource_id)) FROM ${table_name} WHERE line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND line_item_line_item_type = \u0026#39;Usage\u0026#39; AND bill_payer_account_id \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_usage_account_id \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND (CAST(\u0026#34;concat\u0026#34;(\u0026#34;year\u0026#34;, \u0026#39;-\u0026#39;, \u0026#34;month\u0026#34;, \u0026#39;-01\u0026#39;) AS date) = (\u0026#34;date_trunc\u0026#34;(\u0026#39;month\u0026#39;, current_date) - INTERVAL \u0026#39;1\u0026#39; MONTH)) AND line_item_usage_type LIKE \u0026#39;%gp%\u0026#39; AND line_item_usage_type LIKE \u0026#39;%EBS%\u0026#39; Copy Query Click here - to expand the query Copy the query below or click to Download SQL File -- NOTE: If running this at a payer account level with millions of volumes we recommend filtering to specific accounts in line 20. You can also remove line 21 to view all EC2 EBS volumes. -- Step 1: Filter CUR to return all gp EC2 EBS storage usage WITH ebs_all AS ( SELECT bill_billing_period_start_date, line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, line_item_resource_id , product_volume_api_name, line_item_usage_type, pricing_unit, line_item_unblended_cost, line_item_usage_amount FROM ${table_name} WHERE (line_item_product_code = \u0026#39;AmazonEC2\u0026#39;) AND (line_item_line_item_type = \u0026#39;Usage\u0026#39;) AND (CAST(\u0026#34;concat\u0026#34;(\u0026#34;year\u0026#34;, \u0026#39;-\u0026#39;, \u0026#34;month\u0026#34;, \u0026#39;-01\u0026#39;) AS date) = (\u0026#34;date_trunc\u0026#34;(\u0026#39;month\u0026#39;, current_date) - INTERVAL \u0026#39;1\u0026#39; MONTH)) AND bill_payer_account_id \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_usage_account_id \u0026lt;\u0026gt; \u0026#39;\u0026#39;\tAND line_item_usage_type LIKE \u0026#39;%gp%\u0026#39;\tAND product_volume_api_name \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_usage_type NOT LIKE \u0026#39;%Snap%\u0026#39; AND line_item_usage_type LIKE \u0026#39;%EBS%\u0026#39; ), -- Step 2: Pivot table so storage types cost and usage into separate columns ebs_spend AS ( SELECT DISTINCT bill_billing_period_start_date AS billing_period, date_trunc(\u0026#39;month\u0026#39;,line_item_usage_start_date) AS usage_date, bill_payer_account_id AS payer_account_id, line_item_usage_account_id AS linked_account_id, line_item_resource_id AS resource_id, product_volume_api_name AS volume_api_name, SUM(CASE WHEN (((pricing_unit = \u0026#39;GB-Mo\u0026#39; or pricing_unit = \u0026#39;GB-month\u0026#39;) or pricing_unit = \u0026#39;GB-month\u0026#39;) AND line_item_usage_type LIKE \u0026#39;%EBS:VolumeUsage%\u0026#39;) THEN line_item_usage_amount ELSE 0 END) AS usage_storage_gb_mo, SUM(CASE WHEN (pricing_unit = \u0026#39;IOPS-Mo\u0026#39; AND line_item_usage_type LIKE \u0026#39;%IOPS%\u0026#39;) THEN line_item_usage_amount ELSE 0 END) AS usage_iops_mo, SUM(CASE WHEN (pricing_unit = \u0026#39;GiBps-mo\u0026#39; AND line_item_usage_type LIKE \u0026#39;%Throughput%\u0026#39;) THEN line_item_usage_amount ELSE 0 END) AS usage_throughput_gibps_mo, SUM(CASE WHEN ((pricing_unit = \u0026#39;GB-Mo\u0026#39; or pricing_unit = \u0026#39;GB-month\u0026#39;) AND line_item_usage_type LIKE \u0026#39;%EBS:VolumeUsage%\u0026#39;) THEN (line_item_unblended_cost) ELSE 0 END) AS cost_storage_gb_mo, SUM(CASE WHEN (pricing_unit = \u0026#39;IOPS-Mo\u0026#39; AND line_item_usage_type LIKE \u0026#39;%IOPS%\u0026#39;) THEN (line_item_unblended_cost) ELSE 0 END) AS cost_iops_mo, SUM(CASE WHEN (pricing_unit = \u0026#39;GiBps-mo\u0026#39; AND line_item_usage_type LIKE \u0026#39;%Throughput%\u0026#39;) THEN (line_item_unblended_cost) ELSE 0 END) AS cost_throughput_gibps_mo FROM ebs_all GROUP BY 1, 2, 3, 4, 5,6 ), ebs_spend_with_unit_cost AS ( SELECT *, cost_storage_gb_mo/usage_storage_gb_mo AS current_unit_cost, CASE WHEN usage_storage_gb_mo \u0026lt;= 150 THEN \u0026#39;under 150GB-Mo\u0026#39; WHEN usage_storage_gb_mo \u0026gt; 150 AND usage_storage_gb_mo \u0026lt;= 1000 THEN \u0026#39;between 150-1000GB-Mo\u0026#39; ELSE \u0026#39;over 1000GB-Mo\u0026#39; END AS storage_summary, CASE WHEN volume_api_name \u0026lt;\u0026gt; \u0026#39;gp2\u0026#39; THEN 0 WHEN usage_storage_gb_mo*3 \u0026lt; 3000 THEN 3000 - 3000 WHEN usage_storage_gb_mo*3 \u0026gt; 16000 THEN 16000 - 3000 ELSE usage_storage_gb_mo*3 - 3000 END AS gp2_usage_added_iops_mo, CASE WHEN volume_api_name \u0026lt;\u0026gt; \u0026#39;gp2\u0026#39; THEN 0 WHEN usage_storage_gb_mo \u0026lt;= 150 THEN 0 ELSE 125 END AS gp2_usage_added_throughput_gibps_mo, cost_storage_gb_mo + cost_iops_mo + cost_throughput_gibps_mo AS ebs_all_cost, CASE WHEN volume_api_name = \u0026#39;sc1\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_sc1_cost, CASE WHEN volume_api_name = \u0026#39;st1\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_st1_cost, CASE WHEN volume_api_name = \u0026#39;standard\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_standard_cost, CASE WHEN volume_api_name = \u0026#39;io1\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_io1_cost, CASE WHEN volume_api_name = \u0026#39;io2\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_io2_cost, CASE WHEN volume_api_name = \u0026#39;gp2\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_gp2_cost, CASE WHEN volume_api_name = \u0026#39;gp3\u0026#39; THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END AS ebs_gp3_cost, CASE WHEN volume_api_name = \u0026#39;gp2\u0026#39; THEN cost_storage_gb_mo*0.8/usage_storage_gb_mo ELSE 0 END AS estimated_gp3_unit_cost FROM ebs_spend ), ebs_before_map AS ( SELECT DISTINCT billing_period, payer_account_id, linked_account_id, resource_id, volume_api_name, storage_summary, SUM(usage_storage_gb_mo) AS usage_storage_gb_mo, SUM(usage_iops_mo) AS usage_iops_mo, SUM(usage_throughput_gibps_mo) AS usage_throughput_gibps_mo, SUM(gp2_usage_added_iops_mo) gp2_usage_added_iops_mo, SUM(gp2_usage_added_throughput_gibps_mo) AS gp2_usage_added_throughput_gibps_mo, SUM(ebs_all_cost) AS ebs_all_cost, SUM(ebs_sc1_cost) AS ebs_sc1_cost, SUM(ebs_st1_cost) AS ebs_st1_cost , SUM(ebs_standard_cost) AS ebs_standard_cost, SUM(ebs_io1_cost) AS ebs_io1_cost, SUM(ebs_io2_cost) AS ebs_io2_cost, SUM(ebs_gp2_cost) AS ebs_gp2_cost, SUM(ebs_gp3_cost) AS ebs_gp3_cost, /* Calculate cost for gp2 gp3 estimate using the following - Storage always 20% cheaper - Additional iops per iops-mo is 6% of the cost of 1 gp3 GB-mo - Additional throughput per gibps-mo is 50% of the cost of 1 gp3 GB-mo */ SUM(CASE /*ignore non gp2\u0026#39; */ WHEN volume_api_name = \u0026#39;gp2\u0026#39; THEN ebs_gp2_cost - (cost_storage_gb_mo*0.8 + estimated_gp3_unit_cost * 0.5 * gp2_usage_added_throughput_gibps_mo + estimated_gp3_unit_cost * 0.06 * gp2_usage_added_iops_mo) ELSE 0 END) AS ebs_gp3_potential_savings FROM ebs_spend_with_unit_cost GROUP BY 1, 2, 3, 4, 5, 6) SELECT DISTINCT billing_period, payer_account_id, linked_account_id, resource_id, volume_api_name, usage_storage_gb_mo, usage_iops_mo, usage_throughput_gibps_mo, storage_summary, gp2_usage_added_iops_mo, gp2_usage_added_throughput_gibps_mo, ebs_all_cost, ebs_sc1_cost, ebs_st1_cost , ebs_standard_cost, ebs_io1_cost, ebs_io2_cost, ebs_gp2_cost, ebs_gp3_cost, ebs_gp3_potential_savings FROM ebs_before_map; Helpful Links Migrate your Amazon EBS volumes from gp2 to gp3 and save up to 20% on costs gp2 to gp3 conversion blog discussion EBS Volume Modifications Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Snapshot Trends Cost Optimization Technique This query looks across your EC2 EBS Snapshots to identify all snapshots that still exist today with their previous month spend. It then provides the start date which is the first billing period the snapshot appeared in your CUR and groups them so you can see if they are over 1yr old. Snapshots over 1yr old should be tagged to keep, cleaned up, or archived.\nCopy Query Copy the query below or click to Download SQL File Click here - to expand the query -- Step 1: Filter CUR to return all ebs ec2 snapshot usage data WITH snapshot_usage_all_time AS ( SELECT year, month, bill_billing_period_start_date billing_period, line_item_usage_start_date usage_start_date, bill_payer_account_id payer_account_id, line_item_usage_account_id linked_account_id, line_item_resource_id resource_id, (CASE WHEN (line_item_usage_type LIKE \u0026#39;%EBS:SnapshotArchive%\u0026#39;) THEN \u0026#39;Snapshot_Archive\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%EBS:Snapshot%\u0026#39;) THEN \u0026#39;Snapshot\u0026#39; ELSE \u0026#34;line_item_operation\u0026#34; END) AS snapshot_type, line_item_usage_amount, line_item_unblended_cost, pricing_public_on_demand_cost FROM ${table_name} WHERE (((((bill_payer_account_id \u0026lt;\u0026gt; \u0026#39;\u0026#39;) AND (line_item_resource_id \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) AND (line_item_line_item_type LIKE \u0026#39;%Usage%\u0026#39;)) AND (line_item_product_code = \u0026#39;AmazonEC2\u0026#39;)) AND (line_item_usage_type LIKE \u0026#39;%EBS:Snapshot%\u0026#39;)) ),\t-- Step 2: Return most recent billing_period and the first billing_period request_dates AS ( SELECT DISTINCT resource_id AS request_dates_resource_id, MIN(usage_start_date) AS start_date FROM snapshot_usage_all_time WHERE (snapshot_type = \u0026#39;Snapshot\u0026#39;) GROUP BY 1 ), -- Step 3: Pivot table so looking at previous month filtered for only snapshots still available in the current month snapshot_usage_all_time_before_map AS ( SELECT DISTINCT billing_period, request_dates.start_date, payer_account_id, linked_account_id, snapshot_type, resource_id, SUM(line_item_usage_amount) usage_quantity, SUM(line_item_unblended_cost) ebs_snapshot_cost, SUM(pricing_public_on_demand_cost) public_cost, SUM((CASE WHEN ((request_dates.start_date \u0026gt; (billing_period - INTERVAL \u0026#39;12\u0026#39; MONTH)) AND (snapshot_type = \u0026#39;Snapshot\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END)) AS ebs_snapshots_under_1yr_cost, /*No savings estimate since it uses uses 100% of snapshot cost for snapshots over 6mos as savings estimate*/ SUM((CASE WHEN ((request_dates.start_date \u0026lt;= (billing_period - INTERVAL \u0026#39;12\u0026#39; MONTH)) AND (snapshot_type = \u0026#39;Snapshot\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END)) AS ebs_snapshots_over_1yr_cost FROM (snapshot_usage_all_time snapshot LEFT JOIN request_dates ON (request_dates.request_dates_resource_id = snapshot.resource_id)) WHERE (CAST(\u0026#34;concat\u0026#34;(snapshot.year, \u0026#39;-\u0026#39;, snapshot.month, \u0026#39;-01\u0026#39;) AS date) = (\u0026#34;date_trunc\u0026#34;(\u0026#39;month\u0026#39;, current_date) - INTERVAL \u0026#39;1\u0026#39; MONTH)) GROUP BY 1, 2, 3, 4, 5, 6 ) -- Step 4: Add all data SELECT billing_period, start_date, payer_account_id, linked_account_id, resource_id, snapshot_type, usage_quantity, ebs_snapshot_cost, public_cost, ebs_snapshots_under_1yr_cost, ebs_snapshots_over_1yr_cost FROM snapshot_usage_all_time_before_map; Helpful Links Amazon Data Lifecycle Manager Help \u0026amp; Feedback Back to Table of Contents Amazon S3 Bucket Trends and Optimizations Cost Optimization Technique This query breaks out the previous month\u0026rsquo;s costs and usage of each S3 bucket by storage class and includes and separates out identifiers that can be used to identify trends or potential areas to look into for optimization across Lifecycle Policies or Intelligent Tiering. The query uses this information to provide a variety of checks for each S3 bucket including:\nClick here - to see Bucket Trend Checks S3_all_cost: Provides a way to find your top spend buckets bucket_name_keywords: Checks if buckets contain any of the keywords for use of storage classes beyond S3 Standard and returns the first keyword that matches. If no keywords match it will show \u0026lsquo;other\u0026rsquo; last_requests: Looks back across all billing periods available in your CUR to identify the last usage date that there was any usage for \u0026lsquo;PutObject\u0026rsquo;, \u0026lsquo;PutObjectForRepl\u0026rsquo;, \u0026lsquo;GetObject\u0026rsquo;, and \u0026lsquo;CopyObject\u0026rsquo;. If this field is blank it means there have been no requests across these operations since your CUR was created and the last requests is older than your CUR\u0026rsquo;s first billing_period. s3_standard_underutilized_optimization: Checks if your bucket is only using S3 standard storage and has had no active requests (\u0026lsquo;PutObject\u0026rsquo;, \u0026lsquo;PutObjectForRepl\u0026rsquo;, \u0026lsquo;GetObject\u0026rsquo;, and \u0026lsquo;CopyObject\u0026rsquo;) in the last 6 months. If it meets this criteria it will show \u0026lsquo;Potential Underutilized S3 Bucket - S3 Standard only with no active requests in the last 6mo\u0026rsquo; and this will be something your teams should validate for moving to another storage class or deleting completely. s3_replication_bucket_optimization: Checks if a bucket has any usage across s3 transition, put object, get_object, and s3_copy. If it it doesn\u0026rsquo;t it returns \u0026lsquo;Potential Replication Bucket Optimization - Active Non-Replication Requests or Transitions\u0026rsquo;\u0026rsquo; s3_standard_only_bucket: Checks if a bucket is only using S3 standard s3_archive_in_use: Checks if a bucket is using any Archive storage (Glacier or Glacier Deep Archive) s3_inventory_in_use: Checks if the bucket is using S3 Inventory s3_analytics_in_use: Checks if the bucket is using S3 Analytics s3_int_in_use: Checks if the bucket is using Intelligent Tiering s3_standard_storage_potential_savings: Provides an estimated savings if you were to move your S3 Standard Storage to Infrequent Access. This query uses 30% as an assumption, but you can adjust to your preferred value. s3_glacier_instant_retrieval_potential_savings: Provides an estimated savings or additional cost if you were to move your S3 Standard-IA Storage to Glacier Instant Retrieval. This query uses a 68% storage savings, a 2x additional Tier 1 cost, a 10x additional Tier 2 cost, and a 3x retrieval cost, but you can adjust to your preferred value. Copy Query Click here - to expand the query Copy the query below or click to Download SQL File -- Step 1: Enter S3 standard savings savings assumption. Default is set to 0.3 for 30% savings WITH inputs AS ( SELECT * FROM ( VALUES (0.3,.68,2,10,3) ) t(s3_standard_savings, sia_to_glacier_instant_retrieval_storage_savings, sia_to_glacier_instant_retrieval_tier1_increase, sia_to_glacier_instant_retrieval_tier2_increase, sia_to_glacier_instant_retrieval_retriveal_increase ) ), -- Step 2: Filter CUR to return all storage usage data and update ${table_name} with your CUR table table s3_usage_all_time AS ( SELECT year, month, bill_billing_period_start_date AS billing_period, line_item_usage_start_date AS usage_start_date, bill_payer_account_id AS payer_account_id, line_item_usage_account_id AS linked_account_id, line_item_resource_id AS resource_id, s3_standard_savings, sia_to_glacier_instant_retrieval_storage_savings, sia_to_glacier_instant_retrieval_tier1_increase, sia_to_glacier_instant_retrieval_tier2_increase, sia_to_glacier_instant_retrieval_retriveal_increase, line_item_operation AS operation, line_item_usage_type AS usage_type, CASE WHEN line_item_usage_type LIKE \u0026#39;%EarlyDelete%\u0026#39; THEN \u0026#39;EarlyDelete\u0026#39; ELSE line_item_operation END AS early_delete_adjusted_operation, CASE WHEN line_item_product_code = \u0026#39;AmazonGlacier\u0026#39; AND line_item_operation = \u0026#39;Storage\u0026#39; THEN \u0026#39;Amazon Glacier\u0026#39; WHEN line_item_product_code = \u0026#39;AmazonS3\u0026#39; AND product_volume_type LIKE \u0026#39;%Intelligent%\u0026#39; AND line_item_operation LIKE \u0026#39;%IntelligentTiering%\u0026#39; THEN \u0026#39;Intelligent-Tiering\u0026#39;\tELSE product_volume_type END AS storage_class_type, pricing_unit, SUM(line_item_usage_amount) AS usage_quantity, SUM(line_item_unblended_cost) AS unblended_cost, SUM(CASE WHEN (pricing_unit = \u0026#39;GB-Mo\u0026#39; AND line_item_operation like \u0026#39;%Storage%\u0026#39; AND product_volume_type LIKE \u0026#39;%Glacier Deep Archive%\u0026#39;) THEN line_item_unblended_cost WHEN (pricing_unit = \u0026#39;GB-Mo\u0026#39; AND line_item_operation like \u0026#39;%Storage%\u0026#39;) THEN line_item_unblended_cost ELSE 0 END) AS s3_all_storage_cost, SUM(CASE WHEN (pricing_unit = \u0026#39;GB-Mo\u0026#39; AND line_item_operation like \u0026#39;%Storage%\u0026#39;) THEN line_item_usage_amount ELSE 0 END) AS s3_all_storage_usage_quantity FROM ${table_name}, inputs WHERE bill_payer_account_id \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_resource_id \u0026lt;\u0026gt; \u0026#39;\u0026#39; AND line_item_line_item_type LIKE \u0026#39;%Usage%\u0026#39; AND (line_item_product_code LIKE \u0026#39;%AmazonGlacier%\u0026#39; OR line_item_product_code LIKE \u0026#39;%AmazonS3%\u0026#39;) GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17 ), -- Step 3: Return most recent request date to understand if bucket is in active use most_recent_request AS ( SELECT DISTINCT resource_id, MAX(usage_start_date) AS last_request_date FROM s3_usage_all_time WHERE usage_quantity \u0026gt; 0 AND operation IN (\u0026#39;PutObject\u0026#39;, \u0026#39;PutObjectForRepl\u0026#39;, \u0026#39;GetObject\u0026#39;, \u0026#39;CopyObject\u0026#39;) AND pricing_unit = \u0026#39;Requests\u0026#39; GROUP BY 1 ), -- Step 4: Pivot table so storage classes into separate columns and filter for current month month_usage AS ( SELECT DISTINCT billing_period, date_trunc(\u0026#39;month\u0026#39;, usage_start_date) AS \u0026#34;usage_date\u0026#34;, payer_account_id, linked_account_id, s3.resource_id, most_recent_request.last_request_date AS \u0026#34;last_requests\u0026#34;, s3_standard_savings, sia_to_glacier_instant_retrieval_storage_savings, sia_to_glacier_instant_retrieval_tier1_increase, sia_to_glacier_instant_retrieval_tier2_increase, sia_to_glacier_instant_retrieval_retriveal_increase, SUM(unblended_cost) AS s3_all_cost, -- All Storage SUM(s3_all_storage_cost) AS s3_all_storage_cost, SUM(s3_all_storage_usage_quantity) AS s3_all_storage_usage_quantity, -- S3 Standard SUM(CASE WHEN storage_class_type = \u0026#39;Standard\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_standard_storage_cost, SUM(CASE WHEN storage_class_type = \u0026#39;Standard\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_standard_storage_usage_quantity, -- S3 Standard Infrequent Access SUM(CASE WHEN storage_class_type = \u0026#39;Standard - Infrequent Access\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_standard_ia_storage_cost, SUM(CASE WHEN storage_class_type = \u0026#39;Standard - Infrequent Access\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_standard_ia_storage_usage_quantity, SUM(CASE WHEN usage_type LIKE \u0026#39;%Requests-SIA-Tier1%\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_standard_ia_tier1_cost, SUM(CASE WHEN usage_type LIKE \u0026#39;%Requests-SIA-Tier2%\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_standard_ia_tier2_cost,\tSUM(CASE WHEN usage_type LIKE \u0026#39;%Retrieval-SIA%\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_standard_ia_retrieval_cost, -- S3 One Zone Infrequent Access SUM(CASE WHEN storage_class_type = \u0026#39;One Zone - Infrequent Access\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_onezone_ia_storage_cost, SUM(CASE WHEN storage_class_type = \u0026#39;One Zone - Infrequent Access\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_onezone_ia_storage_usage_quantity, -- S3 Reduced Redundancy SUM(CASE WHEN storage_class_type = \u0026#39;Reduced Redundancy\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_reduced_redundancy_storage_cost, SUM(CASE WHEN storage_class_type = \u0026#39;Reduced Redundancy\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_reduced_redundancy_storage_usage_quantity, -- S3 Intelligent-Tiering SUM(CASE WHEN storage_class_type LIKE \u0026#39;%Intelligent%\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_intelligent_tiering_storage_cost, SUM(CASE WHEN storage_class_type LIKE \u0026#39;%Intelligent%\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_intelligent_tiering_storage_usage_quantity, -- S3 Glacier Instant Retrieval SUM(CASE WHEN storage_class_type LIKE \u0026#39;%Instant%\u0026#39; AND storage_class_type NOT LIKE \u0026#39;%Intelligent%\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_glacier_instant_retrieval_storage_cost, SUM(CASE WHEN storage_class_type LIKE \u0026#39;%Instant%\u0026#39; AND storage_class_type NOT LIKE \u0026#39;%Intelligent%\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_glacier_instant_retrieval_storage_usage_quantity, SUM(CASE WHEN usage_type LIKE \u0026#39;%Requests-GIR-Tier1%\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_glacier_instant_retrieval_tier1_cost, SUM(CASE WHEN usage_type LIKE \u0026#39;%Requests-GIR-Tier2%\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_glacier_instant_retrieval_tier2_cost, SUM(CASE WHEN usage_type LIKE \u0026#39;%Retrieval-SIA-GIR%\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_glacier_instant_retrieval_retrieval_cost, -- S3 Glacier Flexible Retrieval SUM(CASE WHEN storage_class_type = \u0026#39;Amazon Glacier\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_glacier_flexible_retrieval_storage_cost, SUM(CASE WHEN storage_class_type = \u0026#39;Amazon Glacier\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_glacier_flexible_retrieval_storage_usage_quantity, -- Glacier Deep Archive SUM(CASE WHEN storage_class_type = \u0026#39;Glacier Deep Archive\u0026#39; THEN s3_all_storage_cost ELSE 0 END) AS s3_glacier_deep_archive_storage_storage_cost, SUM(CASE WHEN storage_class_type = \u0026#39;Glacier Deep Archive\u0026#39; THEN s3_all_storage_usage_quantity ELSE 0 END) AS s3_glacier_deep_archive_storage_usage_quantity, -- Operations SUM(CASE WHEN operation = \u0026#39;PutObject\u0026#39; AND pricing_unit = \u0026#39;Requests\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_put_object_usage_quantity, SUM(CASE WHEN operation = \u0026#39;PutObjectForRepl\u0026#39; AND pricing_unit = \u0026#39;Requests\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_put_object_replication_usage_quantity, SUM(CASE WHEN operation = \u0026#39;GetObject\u0026#39; AND pricing_unit = \u0026#39;Requests\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_get_object_usage_quantity, SUM(CASE WHEN operation = \u0026#39;CopyObject\u0026#39; AND pricing_unit = \u0026#39;Requests\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_copy_object_usage_quantity, SUM(CASE WHEN operation = \u0026#39;Inventory\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_inventory_usage_quantity, SUM(CASE WHEN operation = \u0026#39;S3.STORAGE_CLASS_ANALYSIS.OBJECT\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_analytics_usage_quantity, SUM(CASE WHEN operation like \u0026#39;%Transition%\u0026#39; THEN usage_quantity ELSE 0 END) AS s3_transition_usage_quantity, SUM(CASE WHEN early_delete_adjusted_operation = \u0026#39;EarlyDelete\u0026#39; THEN unblended_cost ELSE 0 END) AS s3_early_delete_cost\tFROM s3_usage_all_time s3 LEFT JOIN most_recent_request ON most_recent_request.resource_id = s3.resource_id WHERE CAST(concat(s3.year, \u0026#39;-\u0026#39;, s3.month, \u0026#39;-01\u0026#39;) AS date) = (date_trunc(\u0026#39;month\u0026#39;, current_date) - INTERVAL \u0026#39;1\u0026#39; MONTH) GROUP BY 1,2,3,4,5,6,7,8,9,10,11 ) -- Step 6: Apply KPI logic - Add or Adjust bucket name keywords based on your requirements SELECT DISTINCT billing_period, usage_date, payer_account_id, linked_account_id, resource_id, CASE WHEN resource_id LIKE \u0026#39;%backup%\u0026#39; THEN \u0026#39;backup\u0026#39; WHEN resource_id LIKE \u0026#39;%archive%\u0026#39; THEN \u0026#39;archive\u0026#39; WHEN resource_id LIKE \u0026#39;%historical%\u0026#39; THEN \u0026#39;historical\u0026#39;\tWHEN resource_id LIKE \u0026#39;%log%\u0026#39; THEN \u0026#39;log\u0026#39; WHEN resource_id LIKE \u0026#39;%compliance%\u0026#39; THEN \u0026#39;compliance\u0026#39; ELSE \u0026#39;Other\u0026#39; END AS bucket_name_keywords, last_requests, CASE WHEN last_requests \u0026gt;= (usage_date - INTERVAL \u0026#39;2\u0026#39; MONTH) THEN \u0026#39;No Action\u0026#39; WHEN s3_all_storage_cost = s3_standard_storage_cost THEN \u0026#39;Potential Action\u0026#39; ELSE \u0026#39;No Action\u0026#39; END AS s3_standard_underutilized_optimization, CASE WHEN ((s3_transition_usage_quantity)\u0026gt; 0 AND (last_requests \u0026gt;= (usage_date - INTERVAL \u0026#39;1\u0026#39; MONTH))) THEN \u0026#39;No Action\u0026#39; WHEN s3_put_object_replication_usage_quantity \u0026gt; 0 THEN \u0026#39;Potential Action\u0026#39; ELSE \u0026#39;No Action\u0026#39; END AS s3_replication_bucket_optimization, CASE WHEN s3_all_storage_cost = s3_standard_storage_cost THEN \u0026#39;Yes\u0026#39; ELSE \u0026#39;No\u0026#39; END AS s3_standard_only_bucket, CASE WHEN s3_glacier_deep_archive_storage_storage_cost \u0026gt; 0 THEN \u0026#39;in use\u0026#39; WHEN s3_glacier_flexible_retrieval_storage_cost \u0026gt; 0 THEN \u0026#39;in use\u0026#39; WHEN s3_glacier_instant_retrieval_storage_cost \u0026gt; 0 THEN \u0026#39;in use\u0026#39; ELSE \u0026#39;not in use\u0026#39; END AS s3_archive_in_use, CASE WHEN s3_inventory_usage_quantity \u0026gt; 0 THEN \u0026#39;in use\u0026#39; ELSE \u0026#39;not in use\u0026#39; END AS s3_inventory_in_use, CASE WHEN s3_analytics_usage_quantity \u0026gt; 0 THEN \u0026#39;in use\u0026#39; ELSE \u0026#39;not in use\u0026#39; END AS s3_analytics_in_use, CASE WHEN s3_intelligent_tiering_storage_usage_quantity \u0026gt; 0 THEN \u0026#39;in use\u0026#39; ELSE \u0026#39;not in use\u0026#39; END AS s3_int_in_use, s3_standard_storage_cost * s3_standard_savings AS s3_standard_storage_potential_savings, (s3_standard_ia_retrieval_cost + s3_standard_ia_tier1_cost + s3_standard_ia_tier2_cost + s3_standard_ia_storage_cost) -((sia_to_glacier_instant_retrieval_storage_savings * s3_standard_ia_storage_cost) +(sia_to_glacier_instant_retrieval_tier1_increase * s3_standard_ia_tier1_cost) +(sia_to_glacier_instant_retrieval_tier2_increase * s3_standard_ia_tier2_cost) +(sia_to_glacier_instant_retrieval_retriveal_increase * s3_standard_ia_retrieval_cost) ) AS s3_glacier_instant_retrieval_potential_savings, s3_all_cost, (s3_all_cost/s3_all_storage_usage_quantity) AS s3_all_unit_cost, s3_all_storage_cost, s3_all_storage_usage_quantity, (s3_all_storage_cost/s3_all_storage_usage_quantity) AS s3_all_storage_unit_cost, s3_standard_storage_cost, s3_standard_storage_usage_quantity, (s3_standard_storage_cost/s3_standard_storage_usage_quantity) AS s3_standard_storage_unit_cost, s3_intelligent_tiering_storage_cost, s3_intelligent_tiering_storage_usage_quantity, (s3_intelligent_tiering_storage_cost/s3_intelligent_tiering_storage_usage_quantity) AS s3_intelligent_tiering_storage_unit_cost, s3_standard_ia_storage_cost, s3_standard_ia_storage_usage_quantity, (s3_standard_ia_storage_cost/s3_standard_ia_storage_usage_quantity) AS s3_standard_ia_storage_unit_cost, s3_onezone_ia_storage_cost, s3_onezone_ia_storage_usage_quantity, (s3_onezone_ia_storage_cost/s3_onezone_ia_storage_usage_quantity) AS s3_onezone_ia_storage_unit_cost, s3_reduced_redundancy_storage_cost, s3_reduced_redundancy_storage_usage_quantity, (s3_reduced_redundancy_storage_cost/s3_reduced_redundancy_storage_usage_quantity) AS s3_reduced_redundancy_storage_unit_cost, s3_glacier_instant_retrieval_storage_cost, s3_glacier_instant_retrieval_storage_usage_quantity, (s3_glacier_instant_retrieval_storage_cost/s3_glacier_instant_retrieval_storage_usage_quantity) AS s3_glacier_instant_retrieval_storage_unit_cost, s3_glacier_flexible_retrieval_storage_cost, s3_glacier_flexible_retrieval_storage_usage_quantity, (s3_glacier_flexible_retrieval_storage_cost/s3_glacier_flexible_retrieval_storage_usage_quantity) AS s3_glacier_flexible_retrieval_storage_unit_cost, s3_glacier_deep_archive_storage_storage_cost, s3_glacier_deep_archive_storage_usage_quantity, (s3_glacier_deep_archive_storage_storage_cost/s3_glacier_deep_archive_storage_usage_quantity)\tAS s3_glacier_deep_archive_storage_unit_cost, s3_early_delete_cost, s3_transition_usage_quantity, s3_put_object_usage_quantity, s3_put_object_replication_usage_quantity, s3_get_object_usage_quantity, s3_copy_object_usage_quantity, s3_standard_ia_tier1_cost, s3_standard_ia_tier2_cost, s3_standard_ia_retrieval_cost, s3_glacier_instant_retrieval_tier1_cost, s3_glacier_instant_retrieval_tier2_cost, s3_glacier_instant_retrieval_retrieval_cost\tFROM month_usage; Helpful Links 5 Ways to reduce data storage costs using Amazon S3 Storage Lens Amazon S3 cost optimization for predictable and dynamic access patterns Simplify your data lifecycle by using object tags with Amazon S3 Lifecycle Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/customer_engagement/","title":"Customer Engagement","tags":[],"description":"","content":"These are queries for AWS Services under the Customer Engagement product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon Connect Amazon Connect Query Description This query will provide daily unblended cost and usage information per linked account for Amazon Connect. The output will include specific details about the usage type, usage description, and product usage region. The cost will be summed and in descending order.\nPricing Please refer to the Connect pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE \u0026#39;%end-customer-mins\u0026#39; THEN \u0026#39;End customer minutes\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%chat-message\u0026#39; THEN \u0026#39;Chat messages\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%did-numbers\u0026#39; THEN \u0026#39;DID days of use\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%tollfree-numbers\u0026#39; THEN \u0026#39;Toll free days of use\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%did-inbound-mins\u0026#39; THEN \u0026#39;Inbound DID minutes\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%outbound-mins\u0026#39; THEN \u0026#39;Outbound minutes\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%tollfree-inbound-mins\u0026#39; THEN \u0026#39;Inbound Toll Free minutes\u0026#39; ELSE \u0026#39;Others\u0026#39; END AS case_line_item_usage_type, line_item_line_item_description, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code IN (\u0026#39;AmazonConnect\u0026#39;, \u0026#39;ContactCenterTelecomm\u0026#39;) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_region, line_item_usage_type, line_item_line_item_description ORDER BY day_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/5_account_settings/","title":"Configure account settings","tags":[],"description":"","content":"It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by multiple team members for redudancy. Ensure the email accounts are actively monitored.\nLog in to your Management account with administrative privileges, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit: Enter information into each of the fields for Billing, Operations and Security, and click Update: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/5_join_cost_intelligence_dashboard/","title":"Join with the Enterprise Cost Intelligence Dashboard","tags":[],"description":"","content":"Join with the Enterprise Cost Intelligence Dashboard This section is optional and shows how you can add your AWS Organization Data to your Enterprise Dashboards - 200_Enterprise_Dashboards .\nThis example will show you how to map your Enterprise Dashboard linked_account_id to your Organizations account_number to add account information that is meaningful to your organization. This is to replace this step: https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/2_modify_cost_intelligence/ .\nGo to the Amazon QuickSight service homepage\nIn QuickSight, select the summary_view Data Set\nSelect Edit data set\nSelect Add data: Select your Amazon Athena organization_data table and click Select Select the two circles to open the join configuration then select Left to change your join type: Create following join clause :\nlinked_account_id = id Click Apply Select Save\nRepeat steps 2-9, creating mapping joins for your remaining QuickSight data sets:\ns3_view ec2_running_cost compute_savings_plan_eligible_spend You now have new fields that can be used on the visuals in the Cost Intelligence Dashboard - we will now use them\nGo to the Cost Intelligence Analysis\nEdit the calculated field Account: Change the formula from toString({linked_account_id}) to {name} You can now select a visual, select the Account field, and you will see the account names in your visuals, instead of the Account number: You now have successfully utilized Organization mapping data on your Cost Intelligence Dashboard\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/5_tear_down/","title":"Tear down","tags":[],"description":"","content":" This Lab has moved under the Level 200 Cloud Intelligence Dashboards.Click this link to navigate to the updated Lab To perform a teardown for this lab, perform the following steps:\nRemove QuickSight email reports\nGo into QuickSight Select All dashboards Click on the dashboard name Click Reports Select Unsubscribe Click Update Remove any created QuickSight dashboards\nGo into QuickSight Select All dashboards Click the 3 dots next to the dashboard name Click Delete Click Delete Remove any QuickSight analyses\nGo into QuickSight Select All analyses Click the 3 dots next to the analysis name Click Delete Click Delete Remove QuickSight Datasets\nGo into QuickSight Click Manage data Click on the dataset, we created summary_view s3_view compute_savings_plan_eligible_spend ec2_running_cost data_transfer_view Click Delete data set Click Delete Remove the Athena views\nGo into Athena Execute the following commands to remove the Cost Intelligence views: drop view costmaster.compute_savings_plan_eligible_spend drop view costmaster.ec2_running_cost drop view costmaster.ri_sp_mapping drop view costmaster.s3_view drop view costmaster.summary_view drop view costmaster.data_transfer_view "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/5_add_ec2/","title":"Add an Amazon EC2 Instance to the Stack","tags":[],"description":"","content":"In this task, your objective is to add an Amazon EC2 instance to the template, then update the stack with the revised template.\nWhereas the bucket definition was rather simple (just two to four lines), defining an Amazon EC2 instance is more complex because it needs to use associated resources, such as an AMI, security group and subnet.\nFor this exercise we will assume you now know how to edit your CloudFormation template and update your CloudFormation stack with the updated template.\n5.1 Get the latest AMI to use for your EC2 instance In the Parameters section of your template, look at the LatestAmiId parameter.\nLatestAmiId: Description: Gets the latest AMI from Systems Manager Parameter store Type: 'AWS::SSM::Parameter::Value\u0026lt;AWS::EC2::Image::Id\u0026gt;' Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2' This is a special parameter. This parameter uses the AWS Systems Manager Parameter Store to retrieve the latest AMI (specified in the Default parameter, which in this case is Amazon Linux 2) for the stack\u0026rsquo;s region. This makes it easy to deploy stacks in different regions without having to manually specify an AMI ID for every region.\nGo to the AWS CloudFormation console Click on Stacks Click on the CloudFormationLab stack Click on the Parameters tab Look at the Value and Resolved value for LatestAmiId You see here how it resolves to an AMI ID For more details of this method, see: AWS Compute Blog: Query for the latest Amazon Linux AMI IDs using AWS Systems Manager Parameter Store 5.2 Add the EC2 instance resource to your CloudFormation template and deploy it Edit the CloudFormation Template, adding a new resource for an EC2 instance\nUse this documentation page for assistance: AWS::EC2::Instance Use the YAML format For Logical ID (the line above Type) use MyEC2Instance You only need to specify these five properties:\nImageId: References LatestAmiId, which is the parameter discussed previously\nInstanceType: References InstanceType, another parameter\nSecurityGroupIds: References PublicSecurityGroup, which is defined elsewhere in the template\nSubnetId: References PublicSubnet1, which is defined elsewhere in the template\nTags: Use this YAML block:\nTags: - Key: Name Value: Simple Server Remember\nWhen referencing other resources in the same template, use !Ref. See the BucketName example you already implemented\nWhen referencing SecurityGroupIds, CloudFormation is expecting a list of security groups. You therefore need to list the security group like this:\nSecurityGroupIds: - !Ref PublicSecurityGroup Not sure what to do???\nTo download a sample solution, right-click and download this link: simple_stack_plus_s3_ec2.yaml Or click below to see exactly what to add to your CloudFormation template. Click here to see YAML for adding your EC2 instance: MyEC2Instance: Type: AWS::EC2::Instance Properties: ImageId: !Ref LatestAmiId InstanceType: !Ref InstanceType SecurityGroupIds: - !Ref PublicSecurityGroup SubnetId: !Ref PublicSubnet1 Tags: - Key: Name Value: Simple Server Once you have edited the template, update the stack deployment with your revised template file.\nOn the Parameters screen of the CloudFormation update switch EC2SecurityEnabledParam to true\nImportant Change EC2SecurityEnabledParam to true This will tell the template to create resources your EC2 instance will need such as the Security Group and IAM Role This deployment of the CloudFormation stack will take about three minutes\nAfter the stack status is UPDATE_COMPLETE, the instance will be displayed in the Resources tab.\nGo to the EC2 console to see the Simple Server that was created. Explore the properties of this EC2 instance.\nThe final deployment is now represented by this architecture diagram:\n5.3 Add a web server to the EC2 instance In this task you will update your CloudFormation template to modify the deployed EC2 instance so that it runs a simple web server\nModify the EC2 resource in the template\nDelete the following properties from the EC2 resource\nSecurityGroupIds SubnetId Add the following properties using the YAML below\nNetworkInterfaces: adds an external IP address (and DNS name) for the EC2 instance UserData: a simple bash script to install and run an Apache web server. This runs on EC2 instance creation only. Visually the diff for this looks like:\nThe final EC2 instance resource should look like this:\nMyEC2Instance: Type: AWS::EC2::Instance Properties: ImageId: !Ref LatestAmiId InstanceType: !Ref InstanceType Tags: - Key: Name Value: Simple Server NetworkInterfaces: - AssociatePublicIpAddress: \u0026quot;true\u0026quot; DeviceIndex: \u0026quot;0\u0026quot; GroupSet: - Ref: PublicSecurityGroup SubnetId: Ref: PublicSubnet1 UserData: Fn::Base64: !Sub | #!/bin/bash -xe yum -y update sudo yum install -y httpd sudo systemctl start httpd sudo echo '\u0026lt;h1\u0026gt;Hello from ${AWS::Region}\u0026lt;/h1\u0026gt;' \u0026gt; /var/www/html/index.html Add an output value so you can easily find the public DNS of the EC2 instance\nInsert the following YAML under the Outputs section of your CloudFormation template\nPublicServerDNS: Value: !GetAtt MyEC2Instance.PublicDnsName Use the other entry under Outputs to ensure your new entry has the right indentation\nThe !GetAtt function can return various attributes of the resource. In this case the public DNS name of the EC2 instance.\nNOTE: if you used a Logical ID other than MyEC2Instance when you added your EC2 resource, then you should use that name here\nTo download a sample solution, right-click and download this link: simple_stack_plus_s3_ec2_server.yaml Update the CloudFormation stack using the modified template\nAfter deployment is complete and stack status is UPDATE_COMPLETE, click on the Outputs tab for the CloudFormation stack\nClick on the public DNS name You should see the Apache HTTP server Test Page, indicating your EC2 instance is running the web server and is accessible from the Internet.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/5_create_lambda_function/","title":"Create a Lambda function","tags":[],"description":"","content":"We will now create a Lambda function which will run the code and produce the reports. NOTE: this Lambda function must be created in the same region as S3 bucket for CUR query results created earlier.\nGo to the Lambda console, click Create function.\nSelect Author from scratch, configure the following parameters:\nFunction name: Auto_CUR_Delivery Runtime: Python 3.7 Execution role: Use an existing role Existing role: Lambda_Auto_CUR_Delivery_Role click Create function. In the top right-hand corner of Lambda configuration page, click Select a test event drop-down box and choose Configure test events. Use the default event template Hello world, because this function does not need any input event parameters, set a event name AutoCURDeliveryTest, and click Create. In Function code section, configure the following:\nCode entry type: Upload a file from Amazon S3 Amazon S3 link URL: https://s3.amazonaws.com/bucket-name/AutoCURDelivery.zip Handler: auto_cur_delivery.lambda_handler Scroll down to Basic settings section, set Memory to 512 MB, and timeout to 5 min. Keep other configurations as default, scroll to the very top and click Save. Click the Actions drop-down box and choose Publish new version. Set the Version description to v1, and click Publish. We have finished the configuration and we will now test it. Make sure AutoCURDeliveryTest event is selected, click Test. It takes a few seconds to execute Lambda function, and you\u0026rsquo;ll see all logs after execution. Check your e-mail recipients, they should receive a mail for cost \u0026amp; utilization report with an excel file attached, similarly as below: By default, the cost \u0026amp; utilization report contains:\nCost_By_Service - Cost in the recent three months split by service (e.g. current month is Jul, the recent three months are Jul, Jun and May, same as below) Data_Cost_By_Service - Data cost in the recent three months split by service MoM_Inter_AZ_DT(with graph) - Month over months inter-AZ data transfer usage and change in the recent three months MTD_S3_By_Bucket - Month to date S3 cost and usage type split by bucket name MTD_ELB_By_Name - Month to date ELB cost split by ELB name and region MTD_CF_By_Distribution - Month to date Cloudfront cost and usage split by distribution id Now you have completed this auto CUR delivery solution with default CUR query. In the next step we will add an additional query, and a CloudWatch scheduled event to trigger Lambda function as required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/5_ec2_volume_type/","title":"Create an IAM policy to restrict EBS Volume creation by volume type","tags":[],"description":"","content":"Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimize cost.\nWe will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type.\nNOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.\nCreate the EBS type restrictive IAM Policy Log on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left menu: Click Create policy: Click on the JSON tab: Copy and paste the policy into the console: IAM Policy { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:VolumeType\u0026quot;: \u0026quot;io1\u0026quot; } } } ] } Click on Review Policy: Configure the following details:\nName: EC2EBS_Restrict Description: Dont allow EBS io1 volumes Click Create policy: You have successfully created an IAM policy to restrict EBS actions by volume type.\nApply the policy to your test group Click on Groups from the left menu: Click on the CostTest group: Click on Attach Policy: Click on Policy Type, then click Customer Managed: Select the checkbox next to EC2EBS_Restrict, and click Attach Policy: You have successfully attached the policy to the CostTest group.\nLog out from the console\nVerify the policy is in effect Logon to the console as the TestUser1 user, click on Services then click EC2: Try to launch an instance by clicking Launch Instance, select Launch Instance: Click Select next to Amazon Linux 2\u0026hellip;: Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details: Click Next Add Storage: Click on Add New Volume, click on the dropdown, then select Provisioned IOPS SSD (io1): Click Review and Launch: Take note of the security group created, and click Launch: Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances: The launch will fail, as it contained an io1 volume. Click Back to Review Screen: Scroll down and click Edit storage: Click the dropdown and change it to General Purpose SSD(gp2), click Review and Launch: Click Launch: Select Proceed without a key pair, and click I acknowledge that i will not be able to\u0026hellip;, then click Launch Instances: It will now succeed, as it doesn\u0026rsquo;t contain an io1 volume type. Click on the instance ID and terminate the instance as above: Log out of the console as TestUser1.\nYou have successfully implemented an IAM policy that denies operations if there is an EBS volume of type io1.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/5_format_dashboard/","title":"Format the Recommendation Dashboard","tags":[],"description":"","content":"We will format the recommendation dashboard, this will improve its appearance, and also includes some business rules.\nClick on Themes, then click on Midnight: Select the Recommendations table, click the three dots, click Conditional formatting: Column: PayOffMonth, Add background color: Enter the formatting:\nCondition: Greater than Value: 9 Color: red Click Apply, click Close: Using the same process, add formatting for the column discountrate:\nType: Background color Condition: Less than Value: 10 NOTE adjust this for your business rules, speak with your finance teams Color: red Click Apply, click Close Under the discountrate formatting, Click Add text color:\nCondition: Greater than Value: 20 Color: Green Click Apply, click Close Using the same process, add formatting for the column HoursRun:\nType: Add text color Condition: Less than Value: 0.6 Color: Red Click Add condition Condition#2: Less than Value: 0.85 Color: Orange Click Apply, click Close Add formatting for the column SavingsPlanReco:\nType: Add background color Format field based on: PayOffMonth Condition: Greater than Value: 9 Color: Red Click Apply, click Close Click Add text color Format field based on: discountrate Aggregation: Average Condition: greater than Value: 20 Color: Green Click Apply, click Close Click SavingsPlanReco, Sort by Descending:\nSelect the Trends table, select conditional formatting, Column instancecount:\nType: Add background color Condition: Less than Value: 5, speak with your team to set this at the appropriate level Color: red Click Add condition Condition #2: Less than Value: 10 Color: Orange Click Apply, click Close Using the process above on the Trends table, Select the Trend column:\nType: Add background color Condition: Less than Value 0 Color: Red Click Apply, click Close Click Add text color Condition: Greater than Value: 0 Color: Green Click Apply, Click Close Add the same formatting to the TrendAvg column as the Trend column.\nCongratulations - you now have an analytics dashboard for Savings Plan recommendations!\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/5_generate_logs/","title":"Generate Logs","tags":[],"description":"","content":"In order to populate the logs you are collecting, you need to interact with the deployed website. The Apache web server service being used to host your website generates access logs. In the following steps, you will visit the website to generate these access logs.\nGo to the CloudFormation console . Select the stack you deployed for this lab, called security-cw-lab. Click on Outputs, then click on WebsiteURL. Refresh the page a few time to generate some activity on your website. Repeat steps 1-4, but add /example to the end of the website url. This will generate a 404 error, which is expected. Generating these access logs will allow you to explore the ways in which you can inspect and view these logs, as shown in the following sections of this lab.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_quick_steps_to_security_success/5_operating/","title":"Operating","tags":[],"description":"","content":"Although there are instructions for Decommissioning a Landing Zone in the AWS Control Tower documentation it is strongly recommended that you keep them this quest in place unless you are decommissioning your AWS account structure. The steps in this quest are intended to improve your security posture and tearing down this quest will remove the audit logs and guard rails put in place.\nThere is no additional cost for AWS Control Tower, you only pay for the services used by Control Tower which scales with you. See the AWS Control Tower pricing page for detailed examples. As a baseline, the setup of just Control Tower has a US$5/month fee for a product in Service Catalogue in additional to a once off cost of US$0.011 for AWS Config to record it\u0026rsquo;s initial state. Security Hub and Guard Duty both have a 30 day free trial to give you an indicative cost. Security Hub has a cost associated with number of security checks and findings ingested which will scale with your AWS usage, see AWS Secuity Hub pricing for examples. Guard Duty pricing is based on the total amount of logs consumed by the service, this will also scale cost effectively with your AWS usage, see AWS Guard Duty pricing for examples. Note that you may also additionally incur tax based on your location.\nAdditional Resources and next steps Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well-Architected Framework and how applying it can improve your security posture "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/5_patch_mgmt/","title":"Patch Management","tags":[],"description":"","content":"Systems Manager: Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates.\nNote For Linux-based instances, you can also install patches for non-security updates.\nYou can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags.\nWarning\nAWS does not test patches for Windows or Linux before making them available in Patch Manager . If any updates are installed by Patch Manager the patched instance is rebooted. Always test patches thoroughly before deploying to production environments. Patch Baselines Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage.\nWarning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent.\n5.1 Create a Patch Baseline Under Node Management in the AWS Systems Manager navigation bar, choose Patch Manager. Click the View predefined patch baselines link under the Configure patching button on the upper right. Choose Create patch baseline. On the Create patch baseline page in the Patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline. Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches. Select Amazon Linux from the list. In the Approval rules for operating systems section: Examine the options in the lists and ensure that Product, Classification, and Severity have values of All. Leave the Auto approval delay at its default of 0 days. Change the value of Compliance reporting - optional to Critical. Choose Add another rule. In the new rule, change the value of Compliance reporting - optional to Medium. Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance reporting, such as Critical or Medium, determines the severity of the compliance violation reported in System Manager Compliance.\nIn the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed. Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.\nYou create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers) but the key must be Patch Group.\nNote An instance can only be in one patch group.\nAfter you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance.\nIf the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. If a patch baseline is found for that group, the system applies that patch baseline. If an instance isn\u0026rsquo;t assigned to a patch group, the system automatically uses the currently configured default patch baseline. 5.2 Assign a Patch Group Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups. In the Modify patch groups window under Patch groups, enter Critical, choose Add, and then choose Close to be returned to the Patch Baseline details screen. AWS-RunPatchBaseline AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are.\nFor Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems.\nAWS Systems Manager: Document An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including \u0026lsquo;AWS-RunPatchBaseline\u0026rsquo;. These documents use JavaScript Object Notation (JSON) or YAML (a recursive acronym for \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;), and they include steps and parameters that you specify.\nAll AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents. You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.\n5.3 Examine AWS-RunPatchBaseline in Documents To examine AWS-RunPatchBaseline in Documents:\nIn the AWS Systems Manager navigation bar under Shared Resources, choose Documents. Click in the search box, select Document name prefix, and then Equals. Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details. Review the content of each tab in the details page of the document. AWS Systems Manager: Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.\n5.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command Under Node Management in the AWS Systems Manager navigation bar, choose Run Command. In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document: Choose the search icon and select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Command parameters section, leave the Operation value as the default Scan. In the Targets section: Under Choose a method for selecting targets, choose Specify instance tags to reveal the Tags sub-section. Under Enter a tag key, enter Workload, and under Enter a tag value, enter Test and click Add. The remaining Run Command features enable you to:\nSpecify Rate control, limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix. Note Only the last 2500 characters of a command document\u0026rsquo;s output are displayed in the console.\nSpecify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it. 5.5 Review Initial Patch Compliance Under Node Management in the the AWS Systems Manager navigation bar, choose Compliance. On the Compliance page in the Compliance resources summary, you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details. 5.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command Under Node Management in the AWS Systems Manager navigation bar, choose Run Command. Choose Run Command in the top right of the window. In the Run a command window, under Command document: Choose the search icon, select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by, choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key, enter Workload and under Enter a tag value enter Test. In the Command parameters section, change the Operation value to Install. In the Targets section, choose Specify a tag using Workload and Test. Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually.\nNote There are multiple pages of instances. If manually selecting instances, individual selections must be made on each page.\nIn the Rate control section: For Concurrency, ensure that targets is selected and specify the value as 1. Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times.\nFor Error threshold, ensure that error is selected and specify the value as 1. Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted.\n5.7 Review Patch Compliance After Patching Under Node Management in the the AWS Systems Manager navigation bar, choose Compliance. The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches.\nThe Impact of Operations as Code In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems.\nOperations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \u0026ldquo;swivel chair\u0026rdquo; processes that require multiple interfaces and systems to complete a single operations activity.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/5_resources/","title":"References &amp; useful resources","tags":[],"description":"","content":" What Is AWS Backup? - For backing up AWS resources other than S3 AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2) AWS re:Invent 2019: Backup-and-restore and disaster-recovery solutions with AWS (STG208) S3: Cross-Region Replication Amazon S3 Replication Time Control for predictable replication time, backed by an SLA Well-Architected Framework Well-Architected best practices for reliability Our Friend Rufus Additional information on multi-region strategies for disaster recovery (DR) Recovery Time Objective (RTO) and Recovery Point Objective (RPO) These terms are most often associated with Disaster Recovery (DR), which are a set of objectives and strategies to recover workload availability in the case of a disaster\nRecovery time objective (RTO) is the overall length of time that a workload’s components can be in the recovery phase, and therefore not available, before negatively impacting the organization’s mission or mission/business processes. Recovery point objective (RPO) is the overall length of time that a workload’s data can be unavailable, before negatively impacting the organization’s mission or mission/business processes. Use defined recovery strategies to meet defined recovery objectives If necessary, when architecting a multi-region strategy for your workload, you should choose one of the following strategies. They are listed in increasing order of complexity, and decreasing order of RTO and RPO. DR Region refers to an AWS Region other than the one used for your workload (or any AWS Region if your workload is on premises).\nBackup and restore (RPO in hours, RTO in 24 hours or less): Back up your data and applications into the DR Region. Restore this data when necessary to recover from a disaster. Pilot light (RPO in minutes, RTO in hours): Maintain a minimal version of an environment always running the most critical core elements of your system in the DR Region. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core. Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down version of a fully functional environment always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load. Multi-region active-active (RPO is none or possibly seconds, RTO in seconds): Your workload is deployed to, and actively serving traffic from, multiple AWS Regions. This strategy requires you to synchronize users and data across the Regions that you are using. When the time comes for recovery, use services like Amazon Route 53 or AWS Global Accelerator to route your user traffic to where your workload is healthy. The bi-directional cross-region replication that you created in this lab is helpful for Pilot light, Warm standby, and Multi-region active-active strategies.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 13 How do you plan for disaster recovery (DR)?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_2_cost_and_usage_governance/5_tear_down/","title":"Tear down","tags":[],"description":"","content":"Delete a Budget Report We will delete the bugdet report we created in the previous step.\nFrom the Budgets Reports dashboard, check the box next to the Weekly Budgets budget report. Then select Delete from the Actions dropdown: Click on the Confirm button to confirm you want to delete the budget report. The dashboard should now tell you your budget has been deleted: Delete a Budget We will delete all three budgets that we configured during the lab.\nFrom the Budgets dashboard, check the box next to the CostBudget1 budget. Then select Delete from the Actions dropdown: Click on the Confirm button to confirm you want to delete the budget. Follow the same steps to remove the budgets EC2_actual and SP_Coverage\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_3_pricing_models/5_tear_down/","title":"Tear down","tags":[],"description":"","content":"There is no tear down required for this lab.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST7 - \u0026ldquo;How do you use pricing models to reduce cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/5_tear_down/","title":"Tear down","tags":[],"description":"","content":"No tear down is required for this lab.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST6 - \u0026ldquo;How do you meet cost targets when you select resource type, size and number?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_permission_boundaries_delegating_role_creation/5_cleanup/","title":"Tear down","tags":[],"description":"","content":"Please note that the changes you made to the users, groups, and roles have no charges associated with them.\nUsing the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role. The roles created are: app1-user-region-restricted-services developer-restricted-iam For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete. The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read References \u0026amp; useful resources Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_iam_tag_based_access_control_for_ec2/5_cleanup/","title":"Tear down","tags":[],"description":"","content":"Please note that the changes you made to the policies and roles have no charges associated with them.\nUsing the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete. The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/5_cleanup/","title":"Tear down","tags":[],"description":"","content":" Remove the lambda function, then roles If you created a new S3 bucket, then you may remove it "},{"uri":"https://wellarchitectedlabs.com/reliability/100_labs/100_deploy_cloudformation/5_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nThere is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account:\nYou may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions How to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks\nGo to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks First delete the CloudFormationLab CloudFormation stack Wait for the CloudFormationLab CloudFormation stack to complete (it will no longer be shown on the list of actice stacks) Then delete the WebApp1-VPC CloudFormation stack References \u0026amp; useful resources AWS CloudFormation\nWhat is AWS CloudFormation? CloudFormation AWS Resource and Property Types Reference AWS Resources that enable reliable architectures:\nWhat Is Amazon EC2 Auto Scaling? Elastic Load Balancing: What Is an Application Load Balancer? Availability Zones: AWS Global Infrastructure X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 8 How do you implement change?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_health_checks_and_dependencies/5_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nThere is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account:\nYou may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Remove AWS CloudFormation provisioned resources How to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks\nGo to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks First delete the HealthCheckLab CloudFormation stack Wait for the HealthCheckLab CloudFormation stack to complete (it will no longer be shown on the list of actice stacks) Then delete the WebApp1-VPC CloudFormation stack Remove CloudWatch logs After deletion of the WebApp1-VPC CloudFormation stack is complete then delete the CloudWatch Logs:\nOpen the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\u0026lt;some unique ID\u0026gt;. Click the Actions Button then click Delete Log Group. Verify the log group name then click Yes, Delete. References \u0026amp; useful resources Patterns for Resilient Architecture — Part 3 Amazon Builders\u0026rsquo; Library: Implementing health checks Well-Architected Framework (see the Reliability pillar) Well-Architected best practices for reliability Health Checks for Your Target Groups (for your Application Load Balancer) X Congratulations! With completion of this lab you have learned several best practices. Consider how you can implement these, and update the Well-Architected Review for your workloads: REL 5 How do you design interactions in a distributed system to mitigate or withstand failures? REL 11 How do you design your workload to withstand component failures? .\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/200_labs/200_automating_operations_with_playbooks_and_runbooks/5_cleanup/","title":"Teardown","tags":[],"description":"","content":"In this section you will delete all resources related to the lab environment.\nRun the following command to navigate to the script folder. cd ~/environment/aws-well-architected-labs/static/Operations/200_Automating_operations_with_playbooks_and_runbooks/Code/scripts/ Run the teardown_resources.sh script to delete all resources related to the lab. bash teardown_resources.sh "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_patching_with_ec2_image_builder_and_systems_manager/5_teardown/","title":"Teardown","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\nTeardown of CloudFormation Deployment 5.1. Remove the Automation Stack From the CloudFormation console, select the stack named pattern3-automate from the list and select Delete and confirm the deletion in the next dialog box.\n5.2. Remove the Pipeline Stack 5.2.1. Note: The stack will fail to remove unless the S3 bucket is empty. As a pre requisite, remove the contents of the bucket before continuing.\nFrom the CloudFormation console, click on the pattern3-pipeline stack name and examine the resources.\nFind the resource called Pattern3LoggingBucket and note the bucket name.\nProceed to the S3 console and remove the contents of the bucket, confirming the delete action.\n5.2.2. Now, from the CloudFormation console, select the stack named pattern3-pipeline from the list and select Delete and confirm the deletion in the next dialog box.\n5.3. Remove the Application Stack From the CloudFormation console, select the stack named pattern3-app from the list and select Delete and confirm the deletion in the next dialog box.\n5.4. Remove the Base Infrastructure Stack From the CloudFormation console, select the stack named pattern3-base from the list and select Delete and confirm the deletion in the next dialog box.\n"},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/5_testing_the_workload_functionality/","title":"Testing the Workload Functionality","tags":[],"description":"","content":"Following the completion of section 4, we can complete the lab by testing the workload. We will achieve this by running a decrypt API call to our application. This will trigger a failed decrypt event which should result in our alarm being triggered and an SNS notification sent to the email address which you specified as an endpoint in the previous section.\nComplete the following steps to test the system functionality:\n5.1. Initiate a successful decryption operation Run the command shown below within your Cloud9 IDE, replacing the \u0026lt; encrypt key \u0026gt; with the key value that you took note of in section 2.4 as well as the \u0026lt; Application endpoint URL \u0026gt; with the OutputPattern1ApplicationEndpoint url you took note on section 2.3.3\nALBURL=\u0026#34;http://\u0026lt; Application endpoint URL \u0026gt;\u0026#34; curl --header \u0026#34;Content-Type: application/json\u0026#34; --request GET --data \u0026#39;{\u0026#34;Name\u0026#34;:\u0026#34;Andy Jassy\u0026#34;,\u0026#34;Key\u0026#34;:\u0026#34;\u0026lt;encrypt key\u0026gt;\u0026#34;}\u0026#39; $ALBURL/decrypt Once that is successful, you should see out put like below:\n{\u0026#34;Text\u0026#34;:\u0026#34;Welcome to ReInvent 2020!\u0026#34;} 5.2. Initiate an unsuccessful decryption operation Now that we have confirmed that the decrypt API is operational, let\u0026rsquo;s trigger a deliberate decryption failure to invoke our alerting.\nRun below command once again, but this time, pass on a wrong key for the encrypt key (you can just use whatever value).\nALBURL=\u0026#34;http://\u0026lt; Application endpoint URL \u0026gt;\u0026#34; curl --header \u0026#34;Content-Type: application/json\u0026#34; --request GET --data \u0026#39;{\u0026#34;Name\u0026#34;:\u0026#34;Andy Jassy\u0026#34;,\u0026#34;Key\u0026#34;:\u0026#34;some-random-false-key\u0026#34;}\u0026#39; $ALBURL/decrypt Once it is triggered, you should see output like below signifying that the decrypt procedure has failed, and in the background a failed KMS API has been called. :\n{\u0026#34;Message\u0026#34;:\u0026#34;Data decryption failed, make sure you have the correct key\u0026#34;} Make sure that you repeat this several times in a row, to ensure you we are triggering the alarm. This will result in email notification to the endpoint you defined earlier in the lab.\nNote: CloudTrail can typically take up to 15 mins to pick up the API event and trigger your alarm. For more information about this, please visit Cloudtrail FAQ page 5.3. Observing the alarm. If all the components are configured correctly, you should receive an email notification triggered by the CloudWatch alarm similar to this:\n5.3.1. Click on the URL included in the email that will take you to the CloudWatch Alarm resource in AWS console.\n5.3.2. Observe the state changes under the History section, and notice each activity change as follows:\nCongratulations! you have completed the Pattern1 lab.\nEND OF SECTION 5\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/5_trigger_lambda/","title":"Trigger the Lambda When a CUR is Delivered","tags":[],"description":"","content":"It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function.\n1 - Go to the CloudFormation service Dashboard\n2 - Select the current stack which updates the Glue database\n3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required)\n4 - Open the template up in a text editor of your choice\n5 - A sample crawler file is below:\n./Code/crawler-cfn.md 6 - Update the ** AWSCURCrawlerLambdaExecutor** IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section:\nAdd the following Action:\n\u0026#39;lambda:InvokeFunction\u0026#39; Edit the following line, and add the following resource\n- \u0026#39;arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;accountID\u0026gt;:function:SubAcctSplit\u0026#39; 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section:\nvar lambda = new AWS.Lambda(); var params = { FunctionName: \u0026#39;SubAcctSplit\u0026#39; }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file\n9 - In the CloudFormation console update the stack\n10 - Replace the current template with the new one, and upload your modified template\n11 - After the stack has successfully updated, you can test the function\n12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original management/payer CUR file\n13 - Download the CUR file, and delete the object from the bucket\n14 - Re-upload the current CUR file back into its bucket\n15 - Navigate to the output bucket and folder for the current month\n16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions\nSetup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/5_ri_coverage/","title":"View your Reserved Instance coverage","tags":[],"description":"","content":"You can view your Reserved Instance coverage to look for anomalies or changes in coverage. For compute resources you should purchase Savings Plans instead of Reserved Instances.\nIn Cost Explorer, click on Saved reports on the left: Click on RI Coverage: You will see the default RI Coverage report. It is for the Last 3 Months, and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/5_view_report/","title":"Viewing and downloading the report","tags":[],"description":"","content":" From the detail page for the workload you can identify the number of high and medium risk items: You can also edit the improvement plan configuration. Scroll down and click on the Edit button next to the words Pillar priority: Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Properties tab: Scroll down to set the improvement status: From the detail page for the workload, click the AWS Well-Architected Framework navigation item (others navigation items would appear for any applied Well-Architected Lens): Click the Generate report button to generate and download the report: You can either open the file or save it to view it.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/","title":"Level 100: Cost Visualization","tags":[],"description":"","content":"Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals Perform basic analysis through visualization of your cost and usage Prerequisites AWS Account Setup has been completed An account with usage for more than 1 month Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records for hourly granularity reporting Time to complete The lab should take approximately 15 minutes to complete Steps: View your cost and usage by service View your cost and usage by account View your Savings Plan coverage View your Elasticity View your Reserved Instance coverage Create custom EC2 reports Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_deployment_of_web_application_firewall/","title":"Level 200: Automated Deployment of Web Application Firewall","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Steps: Configure AWS WAF Configure Amazon CloudFront Tear down "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_5_cost_visualization/","title":"Level 200: Cost Visualization","tags":[],"description":"","content":"Last Updated May 2020\nAuthors Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework.\nGoals Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage Prerequisites A management AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Cost_and_Usage_Analysis has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs QuickSight pricing Approx $9-$12 monthly for QuickSight Authors Estimated additional costs should be \u0026lt;$5 a month for small accounts Time to complete The lab should take approximately 20 minutes to complete Steps: Create a data set Create visualizations Share your Analysis and Dashboard Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_bucket_policy/","title":"Level 300: Lambda Cross Account Using Bucket Policy","tags":[],"description":"","content":"Authors Seth Eliot, Resiliency Lead, Well-Architected, AWS Introduction This lab demonstrates configuration of an S3 bucket policy (which is a type of resource based policy) in AWS account 2 (the destination) that enables a Lambda function in AWS account 1 (the origin) to list the objects in that bucket using Python boto SDK. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id.\nIf in classroom and you do not have 2 AWS accounts, buddy up to use each other\u0026rsquo;s accounts, agree who will be account #1 and who will be account #2.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals S3 bucket policies Resource based policies versus identity based policies Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Steps: Identify (or create) S3 bucket in account 2 Create role for Lambda in account 1 Create bucket policy for the S3 bucket in account 2 Create Lambda in account 1 Tear down References \u0026amp; useful resources https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_300_security_best_practices_day/","title":"Quest: AWS Security Best Practices Day","tags":[],"description":"This quest is the guide for an AWS led event including security best practices day. Includes identity &amp; access management, detective controls, infrastructure protection, data protection and incident response.","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity \u0026amp; access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1: Identity \u0026amp; Access Management For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed:\nIntroductory Lab 1.1: AWS Account and Root User This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.\nAWS Account and Root User Lab 1.2 Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nBasic Identity and Access Management User, Group, Role Advanced Lab 1.3 - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation Lab 1.4 - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nAutomated Deployment of Detective Controls Lab 3 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nEnable Security Hub Lab 4 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers:\nApplication Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 5 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.\nAutomated Deployment of EC2 Web Application Lab 6 - Automated Deployment of Web Application Firewall This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods.\nAutomated Deployment of Web Application Firewall Lab 7 - CloudFront for Web Application This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront.\nCloudFront for Web Application Lab 8 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .\nIncident Response with AWS Console and CLI "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/","title":"200 Labs","tags":[],"description":"","content":"List of labs available Level 200: Cost and Usage Governance Level 200: Pricing Models Level 200: Cost and Usage Analysis Level 200: Cost Visualization Level 200: Rightsizing with Compute Optimizer Level 200: Pricing Model Analysis Level 200: Cloud Intelligence Dashboards Level 200: Workload Efficiency Level 200: Licensing Level 200: Cost Journey "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_cost_estimation/5_save_share/","title":"Save and Share","tags":[],"description":"","content":"Save and Share On the My Estimate page, click on the Save and share button\nReview the notice, then click on Agree and continue\nPricing Calculator saves (for 3 yrs) your estimate and makes it available at the URL in the text box.\nTo share the link, click on Copy public link and share it.\nBonus You can deploy a LAMP stack on AWS using the CloudFormation template found here .\nX Congratulations! Now that you have completed the lab, use the Pricing Calculator to estimate the cost of a workload in your organization. This lab specifically helps you with COST 5 - \u0026ldquo;How do you evaluate cost when you select services?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/5_select_services/","title":"Service Selection","tags":[],"description":"","content":"Service Selection Evaluate Cost Licensing Goal: Reduce database licensing costs Target: Within 1 year, convert 10 workloads with VendorX databases to Open Source databases Best Practice: Licensing Costs Measures: Number of workloads converted Good/Bad: Good Why? When does it work well or not?: Works well to reduce licensing costs, significant effort in conversion may reduce overall savings. Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/6_failure_injection_app/","title":"Test Resiliency Using Application Failure Injection","tags":[],"description":"","content":"7.1 Web server failure injection This failure injection will simulate a critical failure of the web server running on the EC2 instances using FIS.\nIn Chaos Engineering we always start with a hypothesis. For this experiment the hypothesis is:\nHypothesis: If the server process on a single instance is killed, then availability will not be impacted\n[Optional] Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing: single region: WaitForWebApp shows completed (green) multi region: WaitForWebApp1 shows completed (green) 7.1.1 Create experiment template Navigate to the FIS console at http://console.aws.amazon.com/fis and click Experiment templates in the left pane.\nTroubleshooting: If screen is blank, then select the region US East (Ohio) Click on Create experiment template to define the type of failure you want to inject.\nEnter Experiment template for application resiliency testing for Description and App-resiliency-testing for Name. For IAM role select WALab-FIS-role.\nScroll down to Actions and click Add action.\nEnter kill-webserver for the Name. Under Action type select aws:ssm:send-command/AWSFIS-Run-Kill-Process. Under documentParameters enter {\u0026quot;ProcessName\u0026quot;:\u0026quot;python3\u0026quot;,\u0026quot;Signal\u0026quot;:\u0026quot;SIGKILL\u0026quot;}. For duration select Minutes and then enter 2 in the text box next to it. Click Save.\nScroll down to Targets and click Edit next to Instances-Target-1 (aws:ec2:instance).\nUnder Target method, select Resource tags and filters. Select Count for Selection mode and enter 1 under Number of resources. This ensures that FIS will only kill the web server on one instance.\nScroll down to Resource tags and click Add new tag. Enter Workshop for Key and AWSWellArchitectedReliability300-ResiliencyofEC2RDSandS3 for Value. These are the same tags that are on the EC2 instances used in this lab.\nFor Resource filters click Add new filter. Enter State.Name for Attribute path and running for Values. This ensures FIS targets a running instance. Click Save.\nYou can choose to stop running an experiment when certain thresholds are met, in this case, using CloudWatch Alarms under Stop condition. For this lab, you can leave this blank.\nClick Create experiment template.\nIn the warning pop-up, confirm that you want to create the experiment template without a stop condition by entering create in the text box. Click Create experiment template.\n7.1.2 Run the experiment Click on Experiment templates from the menu on the left.\nSelect the experiment template App-resiliency-testing and click Actions. Select Start experiment.\nYou can choose to add a tag to the experiment if you wish to do so.\nClick Start experiment.\nIn the pop-up, type start and click Start experiment.\n7.2 System response to web server failure The instances launched as part of this lab are running simple Python webservers. This experiment uses AWS Systems Manager to run a command on the selected instance(s). In this workshop, the command used is kill-process. When the experiment runs, the python3 web server process is terminated on one of the instances and it can no longer handle requests. Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n7.2.1 System availability Refresh the service website several times. Note the following:\nWebsite remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id) Also note the availability_zone value when you refresh. You can see that requests are being handled by the EC2 instances in only two Availability Zones, while the EC2 instance in the third zone is being replaced This can also be verified by viewing the canary run data.\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation click on the WebServersforResiliencyTesting stack click on the Outputs tab Open the URL for WorkloadAvailability in a new window Canary runs continue to be successful confirming that the website is available Load balancing and Auto Scaling work here much the way they did for the EC2 failure injection experiment .\n[Optional] If you want to review the Load balancing and Auto Scaling behavior again for this case, click here 7.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the EC2 instance where the web server process was killed.\nGo to the Target Groups console you already have open (or click here to open a new one )\nIf there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer Click on the Targets tab and observe:\nStatus of the instances in the group. The load balancer will only send traffic to healthy instances.\nWhen the auto scaling launches a new instance, it is automatically added to the load balancer target group.\nIn the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic.\nNote the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify.\nFrom the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts\n7.2.3 Auto scaling Auto Scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Auto scaling uses health checks to ensure that all instances that are part of the Auto Scaling group are running as expected. In this lab, the Auto scaling group\u0026rsquo;s health check is configured to use the Load Balancer\u0026rsquo;s health check. If the Load Balancer marks one of the EC2 instances as unhealthy, Auto Scaling will also consider the instance to be unhealthy and replace it.\nGo to the Auto Scaling Groups console you already have open (or click here to open a new one )\nIf there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting Click on the Activity History tab and observe:\nThe screen cap below shows that all three instances were successfully started at 17:25\nAt 19:29 the instance targeted by the script was put in draining state and a new instance ending in \u0026hellip;62640 was started, but was still initializing. The new instance will ultimately transition to Successful status\nDraining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance.\nLearn more: After the lab see this blog post for more information on draining. Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your workload.\nLearn more: After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances 7.3 Web server failure injection - conclusion In this section, you simulated an application level failure where the web server process running the application was killed using FIS and SSM. Although there was no infrastructure failure, your workload was able to detect and correct the issue by replacing the EC2 instance. Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.\nOur hypothesis is confirmed:\nHypothesis: If the server process on a single instance is killed, then availability will not be impacted\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_multilayered_api_security_with_cognito_and_waf/6_teardown/","title":"Tear down","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\n1. Delete CloudFormation stacks Simply delete 3 CloudFormation stack created in this lab.\nOne CloudFormation Stack for Cloud9 Two CloudFormation Stacks to build out the lab base infrastructure and enhanced security services. 2. Remove the CloudWatch Log Group From the CloudWatch console, select Log group under Logs and select the log group which you created in the lab.\nSelect the Actions button and delete the log group confirming the deletion.\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_aws_resource_optimization/","title":"Level 100: Rightsizing Recommendations","tags":[],"description":"","content":"Last Updated June 2021\nAuthors Arthur Basbaum, AWS Cloud Economics Travis Ketcherside, AWS Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com .\nIntroduction This hands-on lab will give you an overview on Rightsizing recommendations and how to prioritize your EC2 rightsizing efforts.\nGoals Enable and use Rightsizing recommendations Learn how to filter Rightsizing recommendations report and focus only on the less complex high saving cases Prerequisites Have at least one Amazon EC2 instance running Enable Rightsizing recommendations at AWS Cost Explorer \u0026gt; Rightsizing recommendations (no additional cost) Permissions required A minimum of read-only access to the AWS Billing Console Read-only access can be granted via the AWSBillingReadOnlyAccess IAM Policy There may be permission error messages during the lab, as the console may require additional privilege These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required Costs There are no costs for this lab but you need to have at least 1 Amazon EC2 instance running. If you don\u0026rsquo;t have any instance running please check the AWS Free Tier page for more information Click here to check the costs associated with other AWS Cost Management tools Steps Intro to Rightsizing on AWS Using AWS Cost Management Rightsizing Recommendations Prioritizing Rightsizing Recommendations Other Rightsizing Tools Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Amazon S3 1.1 Click S3 to navigate to the dashboard.\n1.2 Select the bucket with prefix backupandrestore-uibucket-xxxx and click the Empty button.\n1.3 Enter permanently delete into the confirmation box and then click Empty.\n1.4 Wait until you see the green banner across the top of the page, indicating the bucket is empty. Then click the Exit button.\nPlease repeat steps 1.1 through 1.4 for the following buckets: backupandrestore-uibucket-xxxx-dr\nAmazon CloudFormation 2.1 Click CloudFormation to navigate to the dashboard in the N. Virginia (us-east-1) region.\n2.2 Select the BackupAndRestore stack and click the Delete button.\n2.3 Click the Delete stack button to confirm the removal.\nAWS Backup 3.1 Click AWS Backup to navigate to the dashboard in the N. Virginia (us-east-1) region.\n3.2 Click Backup Vaults and select Default.\n3.3 Select all the backups and select Actions, then select Delete.\n3.4 Click the Delete button.\nPlease repeat steps 3.1 through 3.3 for AWS Backup in the N. California (us-west-1) region.\nAmazon RDS 4.1 Click RDS to navigate to the dashboard in the N. California (us-west-1) region.\n4.2 Select the backupandrestore-secondary-region database. Select Actions, then select Delete.\n4.3 Uncheck the Create final snapshot and Retain automated backups checkboxes. Next, enable the I acknowledgement \u0026hellip; checkbox. Enter delete me into the confirmation box. Click the Delete button.\nAmazon EC2 5.1 Click EC2 to navigate to the dashboard in the N. California (us-west-1) region.\n5.2 Select the instance and click Instance State, then click Terminate instance.\nIf you have more than one instance running you can verify you are selecting the correct one by checking Security group name.\n5.3 Click the AMIs link and select the AMI. Click Deregister under the Actions dropdown.\n5.4 Click the Confirm button.\n5.5 Click the Security groups link. Select all security groups created launch-wizard-1 and rds-secondary-sg. Click Actions, then click Delete security groups.\n5.6 Click the Delete button.\nYou must repeat steps 5.3 and 5.4 for AMI Deregistration in EC2 in the N. Virginia (us-east-1) region. You must repeat steps 5.5 and 5.6 for Security groups deletion in EC2 in the N. Virginia (us-east-1) region.\nX Congratulations! This lab specifically helps you with the best practices covered in question REL 13 How do you plan for disaster recovery (DR)\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/failover/promote-aurora/","title":"Promote Aurora","tags":[],"description":"","content":"Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. In disaster recovery situations, you can promote a secondary region to take full read-write responsibilities in under a minute.\nNow let us promote the Amazon Aurora MySQL Secondary instance to a standalone instance.\n1.1 Navigate to RDS in N. California (us-west-1) region.\n1.2 Click into DB Instances.\n1.3 Find dr-immersionday-secondary-pilot instance and click Actions. Next click the Remove from global database option to promote the instance to a standalone database.\n1.4 Click Remove and Promote to confirm the server promotion.\nCongratulations! Your Amazon Aurora Secondary Database is now a standalone database and can become a primary database! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/failover/promote-aurora/","title":"Promote Aurora","tags":[],"description":"","content":"Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. In disaster recovery situations, you can promote a secondary region to take full read-write responsibilities in under a minute.\nNow let us promote the Amazon Aurora MySQL Secondary instance to a standalone instance.\n1.1 Navigate to RDS in N. California (us-west-1) region.\n1.2 Click into DB Instances.\n1.3 Find dr-immersionday-secondary-warm instance and click Actions. Next click the Remove from global database option to promote the instance to a standalone database.\n1.4 Click Remove and Promote to confirm the server promotion.\n1.5 Grab a snack! It takes few minutes for the operation to complete.\nCongratulations! Your Amazon Aurora Secondary Database is now a standalone database and can become a primary database! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/disaster/promote-aurora/","title":"Promote Aurora","tags":[],"description":"","content":"Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. In disaster recovery situations, you can promote a secondary region to take full read-write responsibilities in under a minute.\nNow let us promote the Amazon Aurora MySQL Secondary instance to a standalone instance.\nPromote Aurora secondary database 1.1 Click RDS to navigate to the dashboard in the N. California (us-west-1) region.\n1.2 Click the DB Instances link.\n1.3 Select hot-standby-passive-secondary then click Remove from global database in the Actions dropdown.\n1.4 Click the Remove and Promote button.\nCongratulations! Your Amazon Aurora Secondary Database is now a standalone database and can become a primary database! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/setup-cloudfront/","title":"Setup for CloudFront","tags":[],"description":"","content":"You can improve resiliency and increase availability for specific scenarios by setting up CloudFront with origin failover. To get started, you create an origin group in which you designate a primary origin for CloudFront plus a second origin. CloudFront automatically switches to the second origin when the primary origin returns specific HTTP status code failure responses.\nWe are going to configure CloudFront with origin failover in the below steps using our active-primary-uibucket-xxx S3 static website as our primary origin and our passive-secondary-uibucket-xxxx S3 static website as our failover origin.\nYou will need the Amazon CloudFormation output parameter values from the Primary-Active and Passive-Secondary stacks to complete this section. For help, refer to the CloudFormation Outputs section of the workshop.\nCreate the Amazon CloudFront Distribution 1.1 Click CloudFront to navigate to the dashboard.\n1.2 Click the Create a CloudFront Distribution button.\nIn Step 1.3, DO NOT choose the Amazon S3 active-primary-uibucket-xxxx bucket in the dropdown for the Origin Domain. The Cloudfront distribution will not work if you do this.\n1.3 Enter the WebsiteURL value from the Active-Primary output values as the Origin Domain.\nOne of the purposes of using CloudFront is to reduce the number of requests that your origin server must respond to directly. With CloudFront caching, more objects are served from CloudFront edge locations, which are closer to your users. This reduces the load on your origin server and reduces latency. However, that behavior masks our mechanism (disabling the UI bucket) from properly simulating an outage. For more information, see Amazon CloudFront Optimizing caching and availability . In production, customers typically want to use the default value CachingOptimized.\nThe next section Failover to Secondary, will not work without completing Step 1.4.\n1.4 In the Cache key and origin requests section, select CachingDisabled for the Cache Policy to disable CloudFront caching.\n1.5 Click the Create Distribution button.\nConfigure an Additional Origin We will now add an additional Origin and use our secondary-passive-uibucket-xxxx.\n2.1 Click the Origins link, then click the Create origin button.\nIn Step 2.2, DO NOT choose the Amazon S3 passive-secondary-uibucket-xxxx bucket in the dropdown for the Origin Domain. The Cloudfront distribution will not work if you do this.\n2.2 Enter the WebsiteURL value from the Passive-Secondary output values as the Origin Domain. Click the Create origin button.\nConfigure the Origin Group 3.1 Click the Create Origin Group link.\n3.2 Select active-primary-uibucket-xxxx as the Origins, then click the Add button.\n3.3 Select passive-secondary-uibucket-xxxx as the Origins, then click the Add button.\n3.4 Enter hot-standby-origin-group as the Name. Enable all checkboxes for Failover criteria, then click the Create origin group.\nConfigure Behaviors 4.1 Click the Behaviors link. Select Default (*), then click the Edit button.\n4.2 Select hot-standby-origin-group as the Origin and Origin Groups.\n4.3 Click the Save changes button.\n4.4 Click the Distributions link.\n4.5 Wait for Status to be Enabled and for Last Modified to have a date.\nVerify the Distribution 5.1 Copy the CloudFront Distribution\u0026rsquo;s Domain Name into a new browser window.\n5.2 Confirm that the website\u0026rsquo;s header says The Unicorn Shop - us-east-1.\nCongratulations! Your CloudFront distribution is working! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/verify-failover/","title":"Verify Failover","tags":[],"description":"","content":"Secondary Region 1.1 Navigate to CloudFormation Stacks in N. California (us-west-1) region.\n1.2 Choose the Pilot-Secondary stack.\n1.3 Then navigate to the Outputs tab.\n1.4 Click on the WebsiteURL output link.\nVerify the Website 2.1 Log in to the application. You need to provide the registered email from the Pre-requisites \u0026gt; Primary Region section.\n2.2 You should see items in your shopping cart that you added from the primary region N. Virginia (us-east-1).\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/verify-failover/","title":"Verify Failover","tags":[],"description":"","content":"Secondary Region 1.1 Navigate to CloudFormation Stacks in N. California (us-west-1) region.\n1.2 Choose the Warm-Secondary stack.\n1.3 Then navigate to the Outputs tab.\n1.4 Click on the WebsiteURL output link.\nVerify the Website 2.1 Log in to the application. You need to provide the registered email from the Pre-requisites \u0026gt; Primary Region section.\n2.2 You should see items in your shopping cart that you added from the primary region N. Virginia (us-east-1).\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_100_simplest_security_steps/6_restrict_public_storage/","title":"Step 6 - Restrict public storage","tags":[],"description":"","content":"In this exercise we will configure S3 Block Public Access, an easy way to prevent public access to your S3 bucket.\nFrom the AWS console, click Services and select S3.\nClick the bucket name that you want to block public access.\nClick on the Permissions tab.\nClick Edit under the section \u0026lsquo;Block public access (bucket settings)\u0026rsquo;.\nSelect Block all public access to prevent all sort of public access to your bucket.\nClick on Save changes.\nConfirm the settings by typing confirm in the field of confirmation dialogue box and click on Confirm.\nThe buckets and objects will now have no public access as shown in the permission overview.\nYou can also configure the policy to block public access to all the existing and newly created buckets in the account by clicking on S3 menu bar on left side of the S3 management console.\nClick on Block Public Access setting for this account.\nClick on Block all public access on the right side of the S3 management console.\nClick on Save changes.\nConfirm the settings by typing confirm in the field of confirmation dialogue box and click on Confirm.\nClick on Buckets and note that all of the buckets in your account no longer have a public access.\nFor more information please read the AWS User Guide: https://docs.aws.amazon.com/AmazonS3/latest/userguide/security.html "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/300_labs/300_using_wat_with_cloudformation_and_custom_lambda/6_cleanup/","title":"Teardown","tags":[],"description":"","content":"Cleanup of CloudFormation In the AWS Management Console, navigate to the AWS CloudFormation console Select the stack WALabDemoApp, and delete it. Wait for that stack to full delete before moving on to the next step Select the stack WALambdaHelpers, and delete it. Remove the WA Review In the AWS Management Console, navigate to the Well-Architected Tool Console Select the radio button next to the workload APIGWLambda - walabs-api - WALabsSampleLambdaFunction Click the Delete button at the top page. References \u0026amp; useful resources Well-Architected Tool API CloudFormation custom resource type. Lambda Powertools Python X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/6_programmatic/","title":"Optional - Programmatic access via API","tags":[],"description":"","content":"Choose which programming language you wish to use Python PowerShell Python version using Boto3 Library The lab uses AWS CLI to perform all of the tasks, but you can also use the AWS SDK for Python (Boto3) to perform the same steps. As a reference, you can download the code LabExample.py which will perform all of steps from the lab in a single python file. This file assumes you have already setup your AWS credential file, and uses the default profile for all interactions.\nThe code has been broken up into functions which accept various parameters, so you can pull those out and place them into integration points in your environment. There is error checking for most of the various API calls, but the code should not be considered production ready. Please review before implementing in your environment.\nPython code #!/usr/bin/env python3 # This is a simple python app for use with the Well-Architected labs # This will simulate all of the steps in the 200-level lab on using the # Well-Architected API calls # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ import botocore import boto3 import json import datetime import logging import jmespath import base64 from pkg_resources import packaging __author__ = \u0026#34;Eric Pullen\u0026#34; __email__ = \u0026#34;eppullen@amazon.com\u0026#34; __copyright__ = \u0026#34;Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; __credits__ = [\u0026#34;Eric Pullen\u0026#34;] # Default region listed here REGION_NAME = \u0026#34;us-east-1\u0026#34; blankjson = {} response = \u0026#34;\u0026#34; # Setup Logging logging.basicConfig( level=logging.DEBUG, format=\u0026#39;%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, ) logger = logging.getLogger() logging.getLogger(\u0026#39;boto3\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;botocore\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;s3transfer\u0026#39;).setLevel(logging.CRITICAL) logging.getLogger(\u0026#39;urllib3\u0026#39;).setLevel(logging.CRITICAL) # Helper class to convert a datetime item to JSON. class DateTimeEncoder(json.JSONEncoder): def default(self, z): if isinstance(z, datetime.datetime): return (str(z)) else: return super().default(z) def CreateNewWorkload( waclient, workloadName, description, reviewOwner, environment, awsRegions, lenses ): # Create your workload try: waclient.create_workload( WorkloadName=workloadName, Description=description, ReviewOwner=reviewOwner, Environment=environment, AwsRegions=awsRegions, Lenses=lenses ) except waclient.exceptions.ConflictException as e: workloadId = FindWorkload(waclient,workloadName) logger.error(\u0026#34;ERROR - The workload name %s already exists as workloadId %s\u0026#34; % (workloadName, workloadId)) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def FindWorkload( waclient, workloadName ): # Finding your WorkloadId try: response=waclient.list_workloads( WorkloadNamePrefix=workloadName ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;WorkloadSummaries\u0026#39;], cls=DateTimeEncoder)) workloadId = response[\u0026#39;WorkloadSummaries\u0026#39;][0][\u0026#39;WorkloadId\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workloadId def DeleteWorkload( waclient, workloadId ): # Delete the WorkloadId try: response=waclient.delete_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def GetWorkload( waclient, workloadId ): # Get the WorkloadId try: response=waclient.get_workload( WorkloadId=workloadId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Workload\u0026#39;], cls=DateTimeEncoder)) workload = response[\u0026#39;Workload\u0026#39;] # print(\u0026#34;WorkloadId\u0026#34;,workloadId) return workload def disassociateLens( waclient, workloadId, lens ): # Disassociate the lens from the WorkloadId try: response=waclient.disassociate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def associateLens( waclient, workloadId, lens ): # Associate the lens from the WorkloadId try: response=waclient.associate_lenses( WorkloadId=workloadId, LensAliases=lens ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) def listLens( waclient ): # List all lenses currently available try: response=waclient.list_lenses() except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) lenses = jmespath.search(\u0026#34;LensSummaries[*].LensAlias\u0026#34;, response) return lenses def findQuestionId( waclient, workloadId, lensAlias, pillarId, questionTitle ): # Find a questionID using the questionTitle try: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, PillarId=pillarId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,PillarId=pillarId,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) jmesquery = \u0026#34;[?starts_with(QuestionTitle, `\u0026#34;+questionTitle+\u0026#34;`) == `true`].QuestionId\u0026#34; questionId = jmespath.search(jmesquery, answers) return questionId[0] def findChoiceId( waclient, workloadId, lensAlias, questionId, choiceTitle, ): # Find a choiceId using the choiceTitle try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) jmesquery = \u0026#34;Answer.Choices[?starts_with(Title, `\u0026#34;+choiceTitle+\u0026#34;`) == `true`].ChoiceId\u0026#34; choiceId = jmespath.search(jmesquery, response) return choiceId[0] def getAnswersForQuestion( waclient, workloadId, lensAlias, questionId ): # Find a answer for a questionId try: response=waclient.get_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) # print(answers) return answers def updateAnswersForQuestion( waclient, workloadId, lensAlias, questionId, selectedChoices, notes ): # Update a answer to a question try: response=waclient.update_answer( WorkloadId=workloadId, LensAlias=lensAlias, QuestionId=questionId, SelectedChoices=selectedChoices, Notes=notes ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(json.dumps(response)) jmesquery = \u0026#34;Answer.SelectedChoices\u0026#34; answers = jmespath.search(jmesquery, response) return answers def listMilestones( waclient, workloadId ): # Find a milestone for a workloadId try: response=waclient.list_milestones( WorkloadId=workloadId, MaxResults=50 # Need to check why I am having to pass this parameter ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;MilestoneSummaries\u0026#39;], cls=DateTimeEncoder)) milestoneNumber = response[\u0026#39;MilestoneSummaries\u0026#39;] return milestoneNumber def createMilestone( waclient, workloadId, milestoneName ): # Create a new milestone with milestoneName try: response=waclient.create_milestone( WorkloadId=workloadId, MilestoneName=milestoneName ) except waclient.exceptions.ConflictException as e: milestones = listMilestones(waclient,workloadId) jmesquery = \u0026#34;[?starts_with(MilestoneName,`\u0026#34;+milestoneName+\u0026#34;`) == `true`].MilestoneNumber\u0026#34; milestoneNumber = jmespath.search(jmesquery,milestones) logger.error(\u0026#34;ERROR - The milestone name %s already exists as milestone %s\u0026#34; % (milestoneName, milestoneNumber)) return milestoneNumber[0] except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;MilestoneSummaries\u0026#39;], cls=DateTimeEncoder)) milestoneNumber = response[\u0026#39;MilestoneNumber\u0026#39;] return milestoneNumber def getMilestone( waclient, workloadId, milestoneNumber ): # Use get_milestone to return the milestone structure try: response=waclient.get_milestone( WorkloadId=workloadId, MilestoneNumber=milestoneNumber ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;Milestone\u0026#39;], cls=DateTimeEncoder)) milestoneResponse = response[\u0026#39;Milestone\u0026#39;] return milestoneResponse def getMilestoneRiskCounts( waclient, workloadId, milestoneNumber ): # Return just the RiskCount for a particular milestoneNumber milestone = getMilestone(waclient,workloadId,milestoneNumber) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(milestone[\u0026#39;Workload\u0026#39;][\u0026#39;RiskCounts\u0026#39;], cls=DateTimeEncoder)) milestoneRiskCounts = milestone[\u0026#39;Workload\u0026#39;][\u0026#39;RiskCounts\u0026#39;] return milestoneRiskCounts def listAllAnswers( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Get a list of all answers try: if milestoneNumber: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.list_answers( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) answers = response[\u0026#39;AnswerSummaries\u0026#39;] while \u0026#34;NextToken\u0026#34; in response: if milestoneNumber: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,MilestoneNumber=milestoneNumber,NextToken=response[\u0026#34;NextToken\u0026#34;]) else: response = waclient.list_answers(WorkloadId=workloadId,LensAlias=lensAlias,NextToken=response[\u0026#34;NextToken\u0026#34;]) answers.extend(response[\u0026#34;AnswerSummaries\u0026#34;]) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(answers, cls=DateTimeEncoder)) return answers def getLensReview( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Use get_lens_review to return the lens review structure try: if milestoneNumber: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.get_lens_review( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReview\u0026#39;], cls=DateTimeEncoder)) lensReview = response[\u0026#39;LensReview\u0026#39;] return lensReview def getLensReviewPDFReport( waclient, workloadId, lensAlias, milestoneNumber=\u0026#34;\u0026#34; ): # Use get_lens_review_report to return the lens review PDF in base64 structure try: if milestoneNumber: response=waclient.get_lens_review_report( WorkloadId=workloadId, LensAlias=lensAlias, MilestoneNumber=milestoneNumber ) else: response=waclient.get_lens_review_report( WorkloadId=workloadId, LensAlias=lensAlias ) except botocore.exceptions.ParamValidationError as e: logger.error(\u0026#34;ERROR - Parameter validation error: %s\u0026#34; % e) except botocore.exceptions.ClientError as e: logger.error(\u0026#34;ERROR - Unexpected error: %s\u0026#34; % e) # print(\u0026#34;Full JSON:\u0026#34;,json.dumps(response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;], cls=DateTimeEncoder)) lensReviewPDF = response[\u0026#39;LensReviewReport\u0026#39;][\u0026#39;Base64String\u0026#39;] return lensReviewPDF def main(): boto3_min_version = \u0026#34;1.16.38\u0026#34; # Verify if the version of Boto3 we are running has the wellarchitected APIs included if (packaging.version.parse(boto3.__version__) \u0026lt; packaging.version.parse(boto3_min_version)): logger.error(\u0026#34;Your Boto3 version (%s) is less than %s. You must ugprade to run this script (pip3 upgrade boto3)\u0026#34; % (boto3.__version__, boto3_min_version)) exit() # STEP 1 - Configure environment logger.info(\u0026#34;1 - Starting Boto %s Session\u0026#34; % boto3.__version__) # Create a new boto3 session SESSION = boto3.session.Session() # Initiate the well-architected session using the region defined above WACLIENT = SESSION.client( service_name=\u0026#39;wellarchitected\u0026#39;, region_name=REGION_NAME, ) WORKLOADNAME = \u0026#39;WA Lab Test Workload\u0026#39; DESCRIPTION = \u0026#39;Test Workload for WA Lab\u0026#39; REVIEWOWNER = \u0026#39;WA Python Script\u0026#39; ENVIRONMENT= \u0026#39;PRODUCTION\u0026#39; AWSREGIONS = [REGION_NAME] LENSES = [\u0026#39;wellarchitected\u0026#39;, \u0026#39;serverless\u0026#39;] # STEP 2 - Creating a workload # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/2_create_workload/ logger.info(\u0026#34;2 - Creating a new workload\u0026#34;) CreateNewWorkload(WACLIENT,WORKLOADNAME,DESCRIPTION,REVIEWOWNER,ENVIRONMENT,AWSREGIONS,LENSES) logger.info(\u0026#34;2 - Finding your WorkloadId\u0026#34;) workloadId = FindWorkload(WACLIENT,WORKLOADNAME) logger.info(\u0026#34;New workload created with id %s\u0026#34; % workloadId) logger.info(\u0026#34;2 - Using WorkloadId to remove and add lenses\u0026#34;) listOfLenses = listLens(WACLIENT) logger.info(\u0026#34;Lenses currently available: %s\u0026#34; % listOfLenses) workloadJson = GetWorkload(WACLIENT,workloadId) logger.info(\u0026#34;Workload ID \u0026#39;%s\u0026#39; has lenses \u0026#39;%s\u0026#39;\u0026#34; % (workloadId, workloadJson[\u0026#39;Lenses\u0026#39;])) logger.info(\u0026#34;Removing the serverless lens\u0026#34;) disassociateLens(WACLIENT,workloadId,[\u0026#39;serverless\u0026#39;]) workloadJson = GetWorkload(WACLIENT,workloadId) logger.info(\u0026#34;Now workload ID \u0026#39;%s\u0026#39; has lenses \u0026#39;%s\u0026#39;\u0026#34; % (workloadId, workloadJson[\u0026#39;Lenses\u0026#39;])) logger.info(\u0026#34;Adding serverless lens back into the workload\u0026#34;) associateLens(WACLIENT,workloadId,[\u0026#39;serverless\u0026#39;]) workloadJson = GetWorkload(WACLIENT,workloadId) logger.info(\u0026#34;Now workload ID \u0026#39;%s\u0026#39; has lenses \u0026#39;%s\u0026#39;\u0026#34; % (workloadId, workloadJson[\u0026#39;Lenses\u0026#39;])) # STEP 3 - Performing a review # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/3_perform_review/ logger.info(\u0026#34;3 - Performing a review\u0026#34;) logger.info(\u0026#34;3 - STEP1 - Find the QuestionId and ChoiceID for a particular pillar question and best practice\u0026#34;) questionSearch = \u0026#34;How do you reduce defects, ease remediation, and improve flow into production\u0026#34; questionId = findQuestionId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,\u0026#39;operationalExcellence\u0026#39;,questionSearch) logger.info(\u0026#34;Found QuestionID of \u0026#39;%s\u0026#39; for the question text of \u0026#39;%s\u0026#39;\u0026#34; % (questionId, questionSearch)) choiceSet = [] choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use version control\u0026#34;)) logger.info(\u0026#34;Found choiceId of \u0026#39;%s\u0026#39; for the choice text of \u0026#39;Use version control\u0026#39;\u0026#34; % choiceSet) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use configuration management systems\u0026#34;)) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use build and deployment management systems\u0026#34;)) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Perform patch management\u0026#34;)) choiceSet.append(findChoiceId(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,\u0026#34;Use multiple environments\u0026#34;)) logger.info(\u0026#34;All choices we will select for questionId of %s is %s\u0026#34; % (questionId, choiceSet)) logger.info(\u0026#34;3 - STEP2 - Use the QuestionID and ChoiceID to update the answer in well-architected review\u0026#34;) answers = getAnswersForQuestion(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId) logger.info(\u0026#34;Current answer for questionId \u0026#39;%s\u0026#39; is \u0026#39;%s\u0026#39;\u0026#34; % (questionId, answers)) logger.info(\u0026#34;Adding answers found in choices above (%s)\u0026#34; % choiceSet) updateAnswersForQuestion(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId,choiceSet,\u0026#39;Added by Python\u0026#39;) answers = getAnswersForQuestion(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,questionId) logger.info(\u0026#34;Now the answer for questionId \u0026#39;%s\u0026#39; is \u0026#39;%s\u0026#39;\u0026#34; % (questionId, answers)) # STEP 4 - Saving a milestone # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/4_save_milestone/ logger.info(\u0026#34;4 - Saving a Milestone\u0026#34;) logger.info(\u0026#34;4 - STEP1 - Create a Milestone\u0026#34;) milestones = listMilestones(WACLIENT,workloadId) milestoneCount = jmespath.search(\u0026#34;length([*].MilestoneNumber)\u0026#34;,milestones) logger.info(\u0026#34;Workload %s has %s milestones\u0026#34; % (workloadId, milestoneCount)) milestoneNumber = createMilestone(WACLIENT,workloadId,\u0026#39;Rev1\u0026#39;) logger.info(\u0026#34;Created Milestone #%s called Rev1\u0026#34; % milestoneNumber) logger.info(\u0026#34;4 - STEP2 - List all Milestones\u0026#34;) milestones = listMilestones(WACLIENT,workloadId) milestoneCount = jmespath.search(\u0026#34;length([*].MilestoneNumber)\u0026#34;,milestones) logger.info(\u0026#34;Now workload %s has %s milestones\u0026#34; % (workloadId, milestoneCount)) logger.info(\u0026#34;4 - STEP3 - Retrieve the results from a milestone\u0026#34;) riskCounts = getMilestoneRiskCounts(WACLIENT,workloadId,milestoneNumber) logger.info(\u0026#34;Risk counts for all lenses for milestone %s are: %s \u0026#34; % (milestoneNumber,riskCounts)) logger.info(\u0026#34;4 - STEP4 - List all question and answers based from a milestone\u0026#34;) answers = listAllAnswers(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;,milestoneNumber) # STEP 5 - Viewing and downloading the report # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/5_view_report/ logger.info(\u0026#34;5 - Viewing and downloading the report\u0026#34;) logger.info(\u0026#34;5 - STEP1 - Gather pillar and risk data for a workload\u0026#34;) lensReview = getLensReview(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;) logger.info(\u0026#34;The Well-Architected base framework has the following RiskCounts %s \u0026#34; % lensReview[\u0026#39;RiskCounts\u0026#39;]) logger.info(\u0026#34;5 - STEP2 - Generate and download workload PDF\u0026#34;) lensReviewBase64PDF = getLensReviewPDFReport(WACLIENT,workloadId,\u0026#39;wellarchitected\u0026#39;) # lensReviewPDF = base64.b64decode(lensReviewBase64PDF) # We will write the PDF to a file in the same directory with open(\u0026#34;WAReviewOutput.pdf\u0026#34;, \u0026#34;wb\u0026#34;) as fh: fh.write(base64.b64decode(lensReviewBase64PDF)) # STEP 6 - Teardown # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/6_cleanup/ logger.info(\u0026#34;6 - Teardown\u0026#34;) # Allow user to keep the workload input(\u0026#34;\\n*** Press Enter to delete the workload or use ctrl-c to abort the script and keep the workload\u0026#34;) logger.info(\u0026#34;6 - STEP1 - Delete Workload\u0026#34;) DeleteWorkload(WACLIENT, workloadId) if __name__ == \u0026#34;__main__\u0026#34;: main() PowerShell version using AWS Tools for PowerShell The lab uses AWS CLI to perform all of the tasks, but you can also use the AWS Tools for PowerShell to perform the same steps. As a reference, you can download the code LabExample.ps1 which will perform all of steps from the lab in a single PowerShell script. This file assumes you have already installed the AWS.Tools PowerShell module as well as the AWS.Tools.WellArchitected module using Install-AWSToolsModule command. It also assumes you have setup your AWS credential file and it will uses the default profile for all interactions.\nThere is no error checking for most of the various API calls, so the code should not be considered production ready. Please review before implementing in your environment.\nPowerShell code #!/usr/bin/env pwsh # This is a simple Powershell app for use with the Well-Architected labs # This will simulate all of the steps in the 200-level lab on using the # Well-Architected API calls # # This code is only for use in Well-Architected labs # *** NOT FOR PRODUCTION USE *** # # # Licensed under the Apache 2.0 and MITnoAttr License. # # Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;). You may not use this file except in compliance with the License. A copy of the License is located at # https://aws.amazon.com/apache2.0/ # Requires -Modules @{ModuleName=\u0026#39;AWSPowerShell.NetCore\u0026#39;;ModuleVersion=\u0026#39;3.3.618.0\u0026#39;} $__author__ = \u0026#34;Eric Pullen\u0026#34; $__email__ = \u0026#34;eppullen@amazon.com\u0026#34; $__copyright__ = \u0026#34;Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u0026#34; $__credits__ = @({\u0026#34;Eric Pullen\u0026#34;}) # Default region listed here $REGION_NAME = \u0026#34;us-east-1\u0026#34; # Setup Log Message Routine to print current date/time to console function Log-Message { [CmdletBinding()] Param ( [Parameter(Mandatory=$true, Position=0)] [string]$LogMessage ) Write-Output (\u0026#34;{0} - {1}\u0026#34; -f (Get-Date), $LogMessage) } function Find-Workload { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$workloadName ) $response = Get-WATWorkloadList -WorkloadNamePrefix $workloadName if (!$response.WorkloadId) { Write-Warning (\u0026#34;Did not find a workload called \u0026#34;+$workloadName) -InformationAction Continue } return $response.WorkloadId } function Create-New-Workload { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$workloadName, [Parameter(Mandatory=$true)] [string]$description, [Parameter(Mandatory=$true)] [string]$reviewOwner, [Parameter(Mandatory=$true)] [string]$environment, [Parameter(Mandatory=$true)] [array]$awsRegions, [Parameter(Mandatory=$true)] [array]$lenses ) try { $response = New-WATWorkload -WorkloadName $workloadName -Description $description -ReviewOwner $reviewOwner -Environment $environment -AwsRegion $awsRegions -Lense $lenses $returnValue = $response.WorkloadId } catch [Amazon.WellArchitected.Model.ConflictException] { Write-Warning (\u0026#34;Conflict - Found a workload that already exists with the name \u0026#34;+$workloadName+\u0026#34;. Finding the workloadId to return\u0026#34;) -InformationAction Continue $returnValue = Find-Workload $workloadName } return $returnValue } function Create-New-Milestone { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$workloadId, [Parameter(Mandatory=$true)] [string]$milestoneName ) try { $response = New-WATMilestone -WorkloadId $workloadId -MilestoneName $milestoneName $returnValue = $response } catch [Amazon.WellArchitected.Model.ConflictException] { Write-Warning (\u0026#34;Conflict - Found a milestone that already exists with the name \u0026#34;+$milestoneName+\u0026#34;. Finding the milestone to return\u0026#34;) -InformationAction Continue $milestoneReturn = Get-WATMilestoneList -WorkloadId $workloadId $foundMilestone = $milestoneReturn.MilestoneSummaries | where {$_.MilestoneName -like $milestoneName+\u0026#34;*\u0026#34; } $returnValue = $foundMilestone } return $returnValue } $WORKLOADNAME = \u0026#39;WA Lab Test Workload\u0026#39; $DESCRIPTION = \u0026#39;Test Workload for WA Lab\u0026#39; $REVIEWOWNER = \u0026#39;WA Python Script\u0026#39; $ENVIRONMENT= \u0026#39;PRODUCTION\u0026#39; $AWSREGIONS = @($REGION_NAME) $LENSES = @(\u0026#39;wellarchitected\u0026#39;, \u0026#39;serverless\u0026#39;) Log-Message (\u0026#34;1 - Starting LabExample.ps1\u0026#34;) # STEP 2 - Creating a workload # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/2_create_workload/ Log-Message (\u0026#34;2 - Creating new workload\u0026#34;) $createId = Create-New-Workload $WORKLOADNAME $DESCRIPTION $REVIEWOWNER $ENVIRONMENT $AWSREGIONS $LENSES Log-Message (\u0026#34;Created with new workloadId: \u0026#34;+$createId) Log-Message (\u0026#34;2 - Finding your WorkloadId for name \u0026#34;+$WORKLOADNAME) $workloadId = Find-Workload $WORKLOADNAME Log-Message (\u0026#34;New workload created with id \u0026#34; + $workloadId) Log-Message (\u0026#34;2 - Using WorkloadId to remove and add lenses\u0026#34;) $listOfLenses = Get-WATLenseList Log-Message (\u0026#34;Lenses currently available: \u0026#34;+$listOfLenses.LensAlias) $workload = Get-WATWorkload -WorkloadId $workloadId Log-Message (\u0026#34;WorkloadId \u0026#34;+$workloadId+\u0026#34; has lenses \u0026#39;\u0026#34;+$workload.Lenses+\u0026#34;\u0026#39;\u0026#34;) Log-Message (\u0026#34;Removing the serverless lens\u0026#34;) Remove-WATLense -WorkloadId $workloadId -LensAlias \u0026#34;serverless\u0026#34; -Force $workload = Get-WATWorkload -WorkloadId $workloadId Log-Message (\u0026#34;WorkloadId \u0026#34;+$workloadId+\u0026#34; has lenses \u0026#39;\u0026#34;+$workload.Lenses+\u0026#34;\u0026#39;\u0026#34;) Log-Message (\u0026#34;Adding serverless lens back into the workload\u0026#34;) Add-WATLense -WorkloadId $workloadId -LensAlias \u0026#34;serverless\u0026#34; -Force $workload = Get-WATWorkload -WorkloadId $workloadId Log-Message (\u0026#34;WorkloadId \u0026#34;+$workloadId+\u0026#34; has lenses \u0026#39;\u0026#34;+$workload.Lenses+\u0026#34;\u0026#39;\u0026#34;) # STEP 3 - Performing a review # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/3_perform_review/ Log-Message (\u0026#34;3 - Performing a review\u0026#34;) Log-Message (\u0026#34;3 - STEP1 - Find the QuestionId and ChoiceID for a particular pillar question and best practice\u0026#34;) $questionSearch = \u0026#34;How do you reduce defects, ease remediation, and improve flow into production\u0026#34; $questionReturn = Get-WATAnswerList -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -PillarId \u0026#34;operationalExcellence\u0026#34; $foundQuestion = $questionReturn.AnswerSummaries | where { $_.QuestionTitle -like $questionSearch+\u0026#34;*\u0026#34; } $questionId = $foundQuestion.QuestionId Log-Message (\u0026#34;Found QuestionID of \u0026#34;+$questionId+\u0026#34; for the question text of \u0026#39;\u0026#34;+$questionSearch+\u0026#34;\u0026#39;\u0026#34;) $choiceSet = @() $answerReturn = Get-WATAnswer -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -QuestionId $questionId $answerSearch = \u0026#34;Use version control\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId Log-Message (\u0026#34;Found choiceId of \u0026#39;\u0026#34;+$choiceSet+\u0026#34;\u0026#39; for the choice text of \u0026#39;Use version control\u0026#39;\u0026#34;) $answerSearch = \u0026#34;Use configuration management systems\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId $answerSearch = \u0026#34;Use build and deployment management systems\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId $answerSearch = \u0026#34;Perform patch management\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId $answerSearch = \u0026#34;Use multiple environments\u0026#34; $foundAnswer = $answerReturn.Answer.Choices | where {$_.Title -like $answerSearch+\u0026#34;*\u0026#34; } $choiceSet += $foundAnswer.ChoiceId Log-Message (\u0026#34;All choices we will select for questionId of \u0026#34;+$questionId+\u0026#34; is \u0026#34; + $choiceSet) Log-Message (\u0026#34;3 - STEP2 - Use the QuestionID and ChoiceID to update the answer in well-architected review\u0026#34;) Log-Message (\u0026#34;Current answer for questionId \u0026#39;\u0026#34;+$questionId+\u0026#34;\u0026#39; is \u0026#39;\u0026#34;+$answerReturn.Answer.SelectedChoices+\u0026#34;\u0026#39;\u0026#34;) Log-Message (\u0026#34;Adding answers found in choices above \u0026#39;\u0026#34;+$choiceSet+\u0026#34;\u0026#39;\u0026#34;) $updateResponse = Update-WATAnswer -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -QuestionId $questionId -SelectedChoices $choiceSet -Notes \u0026#34;Question modified by PowerShell script\u0026#34; $answerReturn = Get-WATAnswer -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -QuestionId $questionId Log-Message (\u0026#34;Now the answer for questionId \u0026#39;\u0026#34;+$questionId+\u0026#34;\u0026#39; is \u0026#39;\u0026#34;+$answerReturn.Answer.SelectedChoices+\u0026#34;\u0026#39;\u0026#34;) # STEP 4 - Saving a milestone # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/4_save_milestone/ Log-Message (\u0026#34;4 - Saving a Milestone\u0026#34;) Log-Message (\u0026#34;4 - STEP1 - Create a Milestone\u0026#34;) $milestoneReturn = Get-WATMilestoneList -WorkloadId $workloadId Log-Message (\u0026#34;Workload \u0026#34;+$workloadId+\u0026#34; has \u0026#34;+($milestoneReturn.MilestoneSummaries | measure ).Count+\u0026#34; milestones\u0026#34;) $createMilestoneResponse = Create-New-Milestone $workloadId \u0026#34;Rev1\u0026#34; $createdmilestoneNumber = $createMilestoneResponse.MilestoneNumber Log-Message (\u0026#34;Created Milestone #\u0026#34;+$createMilestoneResponse.MilestoneNumber+\u0026#34; called Rev1\u0026#34;) Log-Message (\u0026#34;4 - STEP2 - List all Milestones\u0026#34;) $milestoneReturn = Get-WATMilestoneList -WorkloadId $workloadId Log-Message (\u0026#34;Now workload \u0026#34;+$workloadId+\u0026#34; has \u0026#34;+($milestoneReturn.MilestoneSummaries | measure ).Count+\u0026#34; milestones\u0026#34;) Log-Message (\u0026#34;4 - STEP3 - Retrieve the results from a milestone\u0026#34;) $milestoneDetailReturn = Get-WATMilestone -WorkloadId $workloadId -MilestoneNumber $createdmilestoneNumber $riskCounts = $milestoneDetailReturn.Milestone.Workload.RiskCounts Log-Message (\u0026#34;Risk counts for all lenses for milestone \u0026#34;+$createdmilestoneNumber+\u0026#34; are: \u0026#34;+($riskCounts | Out-String)) Log-Message (\u0026#34;4 - STEP4 - List all question and answers based from a milestone\u0026#34;) $questionReturn = Get-WATAnswerList -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; -MilestoneNumber $createdmilestoneNumber # STEP 5 - Viewing and downloading the report # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/5_view_report/ Log-Message (\u0026#34;5 - Viewing and downloading the report\u0026#34;) Log-Message (\u0026#34;5 - STEP1 - Gather pillar and risk data for a workload\u0026#34;) $LensReviewReturn = Get-WATLensReview -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; $LensReviewRiskCounts = $LensReviewReturn.LensReview.RiskCounts Log-Message (\u0026#34;The Well-Architected base framework has the following RiskCounts \u0026#34;+($LensReviewRiskCounts | Out-String)) Log-Message (\u0026#34;5 - STEP2 - Generate and download workload PDF\u0026#34;) $LensPDFReviewReturn = Get-WATLensReviewReport -WorkloadId $workloadId -LensAlias \u0026#34;wellarchitected\u0026#34; $filename = \u0026#39;WAReviewOutput.pdf\u0026#39; $bytes = [Convert]::FromBase64String($LensPDFReviewReturn.LensReviewReport.Base64String) [IO.File]::WriteAllBytes($filename, $bytes) # STEP 6 - Teardown # https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/6_cleanup/ Log-Message (\u0026#34;6 - Teardown\u0026#34;) # Allow user to keep the workload Read-Host -Prompt \u0026#34;Press any key to continue or CTRL+C to quit\u0026#34; Log-Message (\u0026#34;6 - STEP1 - Delete Workload\u0026#34;) Remove-WATWorkload -WorkloadId $workloadId -ClientRequestToken \u0026#34;ClientRequestToken1\u0026#34; -Force # ClientRequestToken is required at this time, but we are investigating if we can remove this in the future. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_integration_with_aws_compute_optimizer_and_aws_trusted_advisor/6_cleanup/","title":"Teardown","tags":[],"description":"","content":"Summary You have learned how to use the various AWS CLI commands to work with the AWS Well-Architected Tool.\nRemove all the resources Delete the stack aws cloudformation delete-stack --stack-name STACK_NAME Confirm the stack has been deleted aws cloudformation list-stacks --query \u0026#34;StackSummaries[?contains(StackName,\u0026#39;STACK_NAME\u0026#39;)].StackStatus\u0026#34; References \u0026amp; useful resources AWS CLI - wellarchitected AWS Well-Architected Tool Documentation AWS Well-Architected Boto3 Reference AWS Well-Architected API Reference X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/6_impact_of_failures_shuffle_sharding/","title":"Impact of failures with shuffle sharding","tags":[],"description":"","content":"Break the application You will now introduce the poison pill into the workload by including the bug query-string with your requests and see how the updated workload architecture handles it. As in the previous case, imagine that customer Alpha triggered the bug in the application again.\nInclude the query-string bug with a value of true and make a request as customer Alpha. The modified URL should look like this - http://shuffle-alb-1p2xbmzo541rr-1602891463.us-east-1.elb.amazonaws.com/?name=Alpha\u0026bug=true (but using your own URL from the CloudFormation stack Outputs)\nThis should result in an Internal Server Error response on the browser indicating that the application has stopped working as expected on the instance that processed this request\nJust like before, customer Alpha, not aware of this bug in the application, will retry the request.\nRefresh the page to simulate this as you did before. This request is routed to the other healthy instance in the shard for customer Alpha. The bug is triggered again and the other instance goes down as well. The entire shard is now affected. All requests to this shard will now fail because there are no healthy instances in the shard. No matter how many times the page is refreshed, you will see a 502 Bad Gateway for customer Alpha showing that customer Alpha is experiencing complete downtime. At this point, the overall capacity of the fleet has decreased from 8 EC2 instances to 6 EC2 instances.\nDue to shuffle sharding, the other customers are unaffected or have limited impact.\nVerify this by making requests using the URLs for these customers (obtained from the CloudFormation stack Outputs) - Bravo, Charlie, Delta, Echo, Foxtrot, Golf, and Hotel. Refresh each request multiple times. You should notice that all customers (except Alpha) will now receive a response Notice that customers Bravo and Hotel will only get responses from a single EC2 instance while other customers get it from 2 different EC2 instances. The impact is localized to a specific shard and only customer Alpha experiences unavailability.\nCustomers Bravo and Hotel that have a shared EC2 instance with customer Alpha will still have one EC2 instance available to respond to requests. While this might lead to some degree of degradation for those customers, it is still an improvement over complete downtime. The scope of impact has now been reduced so that only 12.5% of customers are affected by the failure induced by the poison pill. Note that this improvement to scope of impact was achieved without having to increase capacity or take any manual actions. With larger fleet and shard sizes, the number of combinations will increase resulting in customers having different degrees of degradation i.e. some customers will only have a fraction of their overall shard capacity affected instead of complete downtime. Customer Name Workers Alpha Worker-1 and Worker-2 Bravo Worker-2 and Worker-3 Charlie Worker-3 and Worker-4 Delta Worker-4 and Worker-5 Echo Worker-5 and Worker-6 Foxtrot Worker-6 and Worker-7 Golf Worker-7 and Worker-8 Hotel Worker-8 and Worker-1 In a shuffle sharded system, the scope of impact of failures can be calculated using the following formula:\nThe formula can be expanded to calculate the number of unique combinations that can exist given the number of workers and the number of workers per shard, also referred to as shard size. The calculation is performed using factorials.\nFor example if there were 100 workers, and we assign a unique combination of 5 workers to a shard, then the failure of any 1 shard will only impact 0.0000013% of customers.\nWith this shuffle sharded architecture, the scope of impact is further reduced by the combination of Workers used to generate shards. Here with eight shards, if a customer experiences a problem, then the shard hosting them as well as the Workers mapped to that shard might be impacted. However, that shard represents only a fraction of the overall service. Since this is just a lab we kept it simple with only eight shards, but with more shards, the scope of impact decreases further. Adding more shards requires adding more capacity (more workers). With higher number of Workers, it is possible to achieve a higher number of unique combinations resulting in exponential improvement of the scope of impact of failures.\nVerify workload availability You can look at the AvailabilityDashboard to see the impact of the failure introduced by customer Alpha across all customers.\nSwitch to the tab that the AvailabilityDashboard opened. (You can also retrieve the URL from the CloudFormation stack Outputs).\nYou can see that the introduction of the poison-pill and subsequent retries by customer Alpha has not impacted any other customer.\nNotice that the impact is localized to a specific shard and the other customers are not impacted by this. With sharding, only 12.5% of customers are impacted. NOTE: You might have to wait a few minutes for the dashboard to get updated. Fix the application Note: This is optional and does not need to be completed if you are planning on tearing down this lab as described in the next section. If you are planning on testing this lab further, please follow the instructions below to fix the application on the EC2 instances.\nClick here for instructions to fix the application: As in the previous sections, Systems Manager will be used to fix the application and return functionality to the users that are affected. The Systems Manager Document restarts the application on the selected instances.\nGo to the Outputs section of the CloudFormation stack and open the link for “SSMDocument”. This will take you to the Systems Manager console.\nClick on Run command which will open a new tab on the browser\nScroll down to the Targets section and select Specify instance tags\nEnter Workload for the tag key and WALab-shuffle-sharding for the tag value. Click Add.\nScroll down to the Output options section and uncheck the box next to Enable an S3 bucket. This will prevent Systems Manager from writing log files based on the command execution to S3.\nClick on Run\nYou should see the command execution succeed in a few seconds\nOnce the command has finished execution, you can go back to the application and test it to verify it is working as expected.\nUse the URL for customer Alpha (obtained from the CloudFormation stack Outputs) and make sure that the query-string bug is not included in the request. Refresh the page a few times to make sure responses are being received from 2 different EC2 instances. Repeat this process for the other customers that were affected - Bravo and Hotel. Review the AvailabilityDashboard to make sure canary requests are succeeding and normal functionality has returned to customer Alpha. You should see that SuccessPercent has returned to 100.\nNOTE: You might have to wait a few minutes for the dashboard to get updated. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_linux_ec2_cloudwatch/6_cleanup/","title":"Teardown","tags":["Linux","Amazon Linux","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Summary In order to monitor your compute resources to ensure they are performing properly, you must record performance related metrics. In this simple lab, you were able to create an EC2 instance and generate a simulated high-load situation to observe how CloudWatch can be used to monitor those resources. These are important concepts that you should use across all of your compute resources to ensure you are meeting PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026rdquo; Now that you know how to have an EC2 instance report its metrics to CloudWatch, you should define the Key Performance Indicators (KPIs) to measure workload performance. In some cases, such as a image rendering farm, a high-cpu may be a valid metric and not be indicative of performance. Once defined, you should establish CloudWatch alarms to ensure you are meeting your performance metrics.\nAdditional Tasks In addition to the lab tasks, feel free to use this lab deployment to test some additional CloudWatch features:\nCreate a CPU or Memory alarm and test triggering the alarm by re-running step 5 Create a CloudWatch alarm for an instance Zoom into various timeframes on the CloudWatch dashboard. Notice how it will link the two charts together when you zoom in. Use Metrics Explorer to look at other metrics deployed into the EC2 instance Explore the CloudFormation template to see how the various CloudWatch Agent configurations were deployed Remove all the resources via CloudFormation Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete the CloudWatch Dashboard Go to CloudWatch Dashboards Click the radio button next to the dashboard you created and then click Delete References \u0026amp; useful resources Using CloudWatch Dashboards Monitoring your instances using CloudWatch Connect to instance using Session Manager X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/100_labs/100_monitoring_windows_ec2_cloudwatch/6_cleanup/","title":"Teardown","tags":["Windows Server","Windows","EC2","CloudWatch","CloudWatch Dashboard"],"description":"","content":"Summary In order to monitor your compute resources to ensure they are performing properly, you must record performance related metrics. In this simple lab, you were able to create an EC2 instance and generate a simulated high-load situation to observe how CloudWatch can be used to monitor those resources. These are important concepts that you should use across all of your compute resources to ensure you are meeting PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026rdquo; Now that you know how to have an EC2 instance report its metrics to CloudWatch, you should define the Key Performance Indicators (KPIs) to measure workload performance. In some cases, such as a image rendering farm, a high-cpu may be a valid metric and not be indicative of performance. Once defined, you should establish CloudWatch alarms to ensure you are meeting your performance metrics.\nAdditional Tasks In addition to the lab tasks, feel free to use this lab deployment to test some additional CloudWatch features:\nCreate a CPU or Memory alarm and test triggering the alarm by running the cpu_stress.ps1 script https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/using-cloudwatch-createalarm.html Zoom into various timeframes on the CloudWatch dashboard. Notice how it will link the two charts together when you zoom in. Use Metrics Explorer to look at other metrics deployed into the EC2 instance Explore the CloudFormation template to see how the various CW Agent configurations were deployed Remove all the resources via CloudFormation Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete the CloudWatch Dashboard Go to CloudWatch Dashboards Click the radio button next to the dashboard you created and then click Delete References \u0026amp; useful resources Using CloudWatch Dashboards Monitoring your Windows instances using CloudWatch Connect to a Windows instance using Session Manager X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with PERF7 - \u0026ldquo;How do you monitor your resources to ensure they are performing?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/2_add_assumptions/","title":"Add your own assumptions (using the Amazon Athena console)","tags":[],"description":"","content":"Introduction By now you have queried AWS Cost \u0026amp; Usage reports with Amazon Athena. You drew proxy metrics for sustainability from AWS usage data. Calculating key performance indicators (KPIs), you might want to extend the queries with further assumptions/ data to reinforce sustainable best practices.\nSome examples for further assumptions:\npreference of one EC2 instance family over another (e.g. AWS Graviton based instances) AWS Region preference by user location or the published carbon intensity of electricity grids preference of EC2 Spot instances to increase the overall utilization of the cloud\u0026rsquo;s resources while decreasing cost In this lab you will learn to make that data available to your queries as a simple view in Amazon Athena.\nLab 2 Go the the Amazon Athena console in the region where you deployed earlier the AWS Serverless Application Repository application in lab 1.4 . Choose the Database aws_usage_queries_database, which you have deployed in lab 1.4 . Execute the following query to get a list of regions which you use for Amazon EC2 instances. It will return a list of regions, like us-east-1, ap-southeast-2, and eu-west-1. SELECT DISTINCT(region) FROM monthly_vcpu_hours_by_account Let\u0026rsquo;s create a table with weights to prefer regions where AWS purchases and retires environmental attributes , like Renewable Energy Credits and Guarantees of Origin, to cover the non-renewable energy used in these regions. Execute the following query: SELECT \u0026#39;eu-west-1\u0026#39; region, 1 points UNION SELECT \u0026#39;eu-central-1\u0026#39; region, 1 points UNION SELECT \u0026#39;ca-central-1\u0026#39; region, 1 points UNION SELECT \u0026#39;us-gov-west-1\u0026#39; region, 1 points UNION SELECT \u0026#39;us-west-2\u0026#39; region, 1 points UNION SELECT \u0026#39;ap-southeast-2\u0026#39; region, 2 points UNION SELECT \u0026#39;us-east-1\u0026#39; region, 2 points [add further regions returned in your previous query with 2 points here] To create a view from the select statement choose Create and then choose Create view from query. Enter region_points as Name. Choose Create. Now you can do the same with the instance families to prefer the Graviton2 processor, the ARM-based AWS-designed chip. This is currently the most power efficient processor AWS offers to customers as Graviton 2 processors provide better performance per watt than any other EC2 processor.\nExecute the following query to get a list of regions which you use for Amazon EC2 instances: SELECT DISTINCT(instance_family) FROM monthly_vcpu_hours_by_account Execute the following query to create a view list of instance families with their weights, like t2 (Intel), m5a (AMD), t4g (Graviton2), etc. CREATE OR REPLACE VIEW instance_family_points AS SELECT \u0026#39;t2\u0026#39; instance_family, 2 points UNION SELECT \u0026#39;c5\u0026#39; instance_family, 2 points UNION SELECT \u0026#39;m4\u0026#39; instance_family, 2 points UNION SELECT \u0026#39;m5\u0026#39; instance_family, 2 points UNION SELECT \u0026#39;t3\u0026#39; instance_family, 2 points UNION SELECT \u0026#39;m5a\u0026#39; instance_family, 2 points UNION SELECT \u0026#39;t4g\u0026#39; instance_family, 1 points Now you can use points to weight the instance vCPU hours table. Execute the following query: SELECT instance_family, region, account_id, purchase_option, SUM(vcpu_hours) vcpu_hours, year, month, SUM(f.points * r.points * vcpu_hours) points FROM monthly_vcpu_hours_by_account JOIN region_points r USING (region) JOIN instance_family_points f USING (instance_family) GROUP BY instance_family, region, account_id, purchase_option, year, month ORDER BY 8 DESC Congratulations! You have put additional assumptions into views to extend the usage data by weights. You can now continue with Lab 3 to see how you could add the additional views and tables in an Infrastructure as Code (IaC) approach.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/5_enable_data_sharing/","title":"Enable Amazon Redshift Data Sharing","tags":[],"description":"","content":"Lab 5 The producer cluster administrator, who wants to share data, first sets up the producer cluster for data sharing by running the below commands in the query editor:\nStep-1: Create a datashare on the producer cluster Make sure you are connected to the producer cluster as Admin in us-east-1 region. Then go to query editor to run below command:\nCREATE DATASHARE MarketingShare; Step-2: Add database objects to the datashare The producer cluster administrator then adds the needed database objects. These might be schemas, tables, and views to the datashare and specifies a list of consumers that the objects to be shared with:\nALTER DATASHARE MarketingShare ADD SCHEMA public; ALTER DATASHARE MarketingShare ADD TABLE public.lab_users; ALTER DATASHARE MarketingShare ADD TABLE public.lab_venue; ALTER DATASHARE MarketingShare ADD TABLE public.lab_category; ALTER DATASHARE MarketingShare ADD TABLE public.lab_date; ALTER DATASHARE MarketingShare ADD TABLE public.lab_event; ALTER DATASHARE MarketingShare ADD TABLE public.lab_sales; ALTER DATASHARE MarketingShare ADD TABLE public.lab_listing; Step-3: Grant access on datashare to the consumer cluster Go to the consumer cluster in us-west-1 and note down the cluster namespace from the Amazon Redshift cluster details page: Go to producer cluster in us-east-1 and grant access on datashare to the Consumer cluster namespace (noted from previous step) :\nGRANT USAGE ON DATASHARE MarketingShare TO NAMESPACE \u0026#39;replace-with-your-consumer-cluster-namespace\u0026#39;; Step-4: Data dictionary validation Let’s validate the steps performed in above steps by querying data dictionary of producer cluster:\nRun below SQL query to find MarketingShare datashare type: SELECT * FROM svv_datashares; You can see MarketingShare is an OUTBOUND datashare type.\nRun the below SQL query to list objects and types: SELECT * FROM svv_datashare_objects; You can see list of objects and types (schema, table etc.) shared, and all of them are as OUTBOUND share type.\nRun below query to verify which cluster namespace has been granted access for datashare: SELECT * FROM svv_datashare_consumers; You can see which namespace(s), or clusters have been granted access to the data shares.\nWe have now granted access on the producer cluster data share to the consumer cluster. Next, let\u0026rsquo;s validate if the consumer cluster can access this data share.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/5_enable_data_sharing/","title":"Enable Amazon Redshift Data Sharing","tags":[],"description":"","content":"Lab 5 The producer cluster administrator, who wants to share data, first sets up the producer cluster for data sharing by running the below commands in the query editor:\nStep-1: Create a datashare on the producer cluster Make sure you are connected to the producer cluster as Admin in us-east-1 region. Then go to query editor to run below command:\nCREATE DATASHARE MarketingShare; Step-2: Add database objects to the datashare The producer cluster administrator then adds the needed database objects. These might be schemas, tables, and views to the datashare and specifies a list of consumers that the objects to be shared with:\nALTER DATASHARE MarketingShare ADD SCHEMA public; ALTER DATASHARE MarketingShare ADD TABLE public.lab_users; ALTER DATASHARE MarketingShare ADD TABLE public.lab_venue; ALTER DATASHARE MarketingShare ADD TABLE public.lab_category; ALTER DATASHARE MarketingShare ADD TABLE public.lab_date; ALTER DATASHARE MarketingShare ADD TABLE public.lab_event; ALTER DATASHARE MarketingShare ADD TABLE public.lab_sales; ALTER DATASHARE MarketingShare ADD TABLE public.lab_listing; Step-3: Grant access on datashare to the consumer cluster Go to the consumer cluster in us-west-1 and note down the cluster namespace from the Amazon Redshift cluster details page: Go to producer cluster in us-east-1 and grant access on datashare to the Consumer cluster namespace (noted from previous step) :\nGRANT USAGE ON DATASHARE MarketingShare TO NAMESPACE \u0026#39;replace-with-your-consumer-cluster-namespace\u0026#39;; Step-4: Data dictionary validation Let’s validate the steps performed in above steps by querying data dictionary of producer cluster:\nRun below SQL query to find MarketingShare datashare type: SELECT * FROM svv_datashares; You can see MarketingShare is an OUTBOUND datashare type.\nRun the below SQL query to list objects and types: SELECT * FROM svv_datashare_objects; You can see list of objects and types (schema, table etc.) shared, and all of them are as OUTBOUND share type.\nRun below query to verify which cluster namespace has been granted access for datashare: SELECT * FROM svv_datashare_consumers; You can see which namespace(s), or clusters have been granted access to the data shares.\nWe have now granted access on the producer cluster data share to the consumer cluster. Next, let\u0026rsquo;s validate if the consumer cluster can access this data share.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/6_teardown/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\nCloudFormation stack in the Cost account - OptimizationDataCollectionStack CloudFormation stack in the Management account - OptimizationManagementDataRoleStack CloudFormation stackset in the Management account - OptimizationDataRoleStack Before deleting Stacks and StackSet please make sure all related buckets are empty.\nWhen deleting the OptimizationDataRoleStack stackset, if you deployed to all accounts in your organization then add your AWS Organization id where it asks for AWS OU ID. Please make sure you empty s3 buckets before deletion of OptimizationDataCollectionStack.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with\nOr if you would like to contribute a module please see our contribution page or email costoptimization@amazon.com COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo; Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/6_teardown/","title":"Teardown","tags":[],"description":"","content":"The following resources were created in this lab:\nAmazon QuickSight Dataset: organisation_data Amazon Athena Table: organisation_data Amazon CloudWatch Event, Rule: Lambda_Org_Data AWS Lambda Functions: Lambda_Org_Data IAM Policy: LambdaOrgPolicy IAM Role: LambdaOrgRole IAM Policy: ListOrganizations IAM Role: OrganizationLambdaAccessRole S3 Bucket: (custom name) X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_dependency_monitoring/6_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"The following instructions will remove the resources that you have created in this lab.\nCleaning up Amazon CloudWatch Resources Go to the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch and click on Alarms Search for the alarm WA-Lab-Dependency-Alarm and click on it Click on Delete on the top right hand corner Click Delete Cleaning up AWS Systems Manager OpsCenter Resources Go to the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager and click on OpsCenter Click on the OpsItems tab, search by Title, select contains, and enter the value as S3 Data Writes Click on the OpsItem that has been created with the title S3 Data Writes failing Click on Set status on the top right hand corner, and select Resolved Cleaning up the CloudFormation Stack Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click on the Dependency-Monitoring-Lab Click on Delete and then Delete stack Thank you for using this lab. X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment or workload, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with OPS4 - \u0026ldquo;How do you design your workload so that you can understand its state?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/contributing/06_createpr/","title":"Creating a Pull Request","tags":[],"description":"","content":"Create a Pull Request All the changes are now in your remote repository, let’s do a pull request to merge it into the public Well-Architected Labs repository:\nGo to the your GitHub Well-Architected Labs remote repository http://github.com/(username)/aws-well-architected-labs/pulls ) Make sure you update with your username Click Pull Request Click compare across forks Select your fork on the right side as head repository Review the changes, and click Create pull request Edit the info (this is public – be careful and add a brief description of the edit or addition) Make sure to add the label of the Well-Architected Pillar (i.e. COST) Click Create pull request GitHub provides additional document on forking a repository and creating a pull request .\nThank you for your contribution. The Well-Architected team will receive a notification of the pull request, we will review and commit the change or reach out with any questions.\nCleanup Clean up your local and remote repositories, delete them if there is not additional work.\nGo to your remote repository, modify the link: https://github.com/(username)/aws-well-architected-labs Click Settings Scroll down under Danger Zone and click Delete this repository Confirm \u0026amp; click delete Clean up your local repository by deleting the directory created previously "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/database/","title":"Database","tags":[],"description":"","content":"These are queries for AWS Services under the Database product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon Aurora Global Database Amazon RDS Amazon RDS - Monthly Cost grouped by Usage Type and Resource Tag Amazon RDS on AWS Outposts Amazon DynamoDB Amazon Redshift Amazon ElastiCache Amazon DocumentDB Amazon Aurora Global Database Query Description This query provides a breakdown of costs associated with an Aurora Global Database deployment, excluding backup costs. Output will be grouped by day, account, charge type, usage type, operation, description, and resource ID. Output will be sorted by day, then cost (descending).\nAurora Global Databases are comprised of multiple components, each of which appears as a separate line item in CUR. There is no overarching Aurora Global Database resource ID that can be used to filter query output. This means to get an accurate picture of a specific Aurora Global DB deployment, the relevant cluster and database instance resource IDs in each region where the database is replicated must be added to the WHERE filter. Placeholder variables indicated by a dollar sign and curly braces (${ }) appear where resource IDs should be inserted. Placeholder variables must be replaced before the query will run. Note that depending on your deployment model, you may need to add or remove lines from the WHERE filter as indicated.\nResource IDs can be retrieved through the console, CLI, SDK, or other tools, as normal. If you only have access to CUR, consider the simple query SELECT DISTINCT(line_item_resource_id) FROM ${table_name} WHERE line_item_product_code = 'AmazonRDS' which will turn up a list of RDS resource IDs. Even in smaller environments the number of resource IDs returned may make it challenging to identify the right resource IDs. In that case, consider adding additional columns to help differentiate and identify the correct resource, such as columns with user-defined cost allocation tags . For example, SELECT DISTINCT(line_item_resource_id), resource_tags_user_name, resource_tags_user_cost_center FROM ${table_name} WHERE line_item_product_code = 'AmazonRDS'.\nConsider the following Aurora Global DB deployment example:\nPrimary Region Primary writer instance One reader instance Secondary Region 1 Three reader instances Secondary Region 2 Zero instances (headless) In this example, the following resource IDs would be needed:\nPrimary region cluster ID Primary region writer instance name Primary region reader instance name Secondary region 1 cluster ID Secondary region 1 reader1 instance name Secondary region 1 reader2 instance name Secondary region 1 reader3 instance name Secondary region 2 cluster ID Pricing Please refer to the Amazon Aurora pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT DATE_TRUNC(\u0026#39;day\u0026#39;,line_item_usage_start_date) AS day_line_item_usage_start_date, line_item_usage_account_id, line_item_line_item_type, line_item_usage_type, line_item_operation, line_item_line_item_description, line_item_resource_id, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(line_item_usage_amount) AS sum_line_item_usage_amount FROM ${table_name} WHERE ${date_filter} AND (line_item_resource_id LIKE \u0026#39;%${primary_cluster_id}%\u0026#39; -- primary region cluster id in format \u0026#39;cluster-xxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; OR line_item_resource_id LIKE \u0026#39;%${secondary_cluster_id_1}%\u0026#39; -- secondary region cluster id in format \u0026#39;cluster-xxxxxxxxxxxxxxxxxxxxxxxx\u0026#39; OR line_item_resource_id LIKE \u0026#39;%${secondary_cluster_id_n}%\u0026#39; -- additional secondary region cluster id. copy and paste this line once per additional region/cluster OR line_item_resource_id LIKE \u0026#39;%${primary_cluster_db_instance_name_1}%\u0026#39; -- primary region database instance name. user defined string, e.g \u0026#39;team-a-mysql-db-1\u0026#39; OR line_item_resource_id LIKE \u0026#39;%${primary_cluster_db_instance_name_n}%\u0026#39; -- additional primary region database instance name. copy and paste this line once per additional instance OR line_item_resource_id LIKE \u0026#39;%${secondary_cluster_db_instance_name_1}%\u0026#39; -- secondary region database instance name. user defined string, e.g \u0026#39;team-a-mysql-db-2\u0026#39;. optional if running headless. OR line_item_resource_id LIKE \u0026#39;%${secondary_cluster_db_instance_name_n}%\u0026#39; -- additional secondary region database instance name. copy and paste this line once per additional instance ) AND line_item_usage_type NOT LIKE \u0026#39;%BackupUsage%\u0026#39; GROUP BY DATE_TRUNC(\u0026#39;day\u0026#39;, line_item_usage_start_date), line_item_usage_account_id, line_item_line_item_type, line_item_usage_type, line_item_operation, line_item_line_item_description, line_item_resource_id ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC ; Amazon RDS Query Description This query will output the daily sum per resource for all RDS purchase options across all RDS usage types.\nPricing Please refer to the Amazon RDS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_instance_type, line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_product_family, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7) AS split_line_item_resource_id, product_database_engine, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN line_item_usage_amount WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN line_item_usage_amount WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN line_item_usage_amount ELSE 0 END) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN savings_plan_total_commitment_to_date - savings_plan_used_commitment WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 ELSE line_item_unblended_cost END) AS sum_amortized_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN -savings_plan_amortized_upfront_commitment_for_billing_period WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN -reservation_amortized_upfront_fee_for_billing_period ELSE 0 END) AS sum_ri_sp_trueup, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN line_item_unblended_cost WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN line_item_unblended_cost ELSE 0 END) AS sum_ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Relational Database Service\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_instance_type, line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_product_family, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7), product_database_engine ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_amortized_cost; Help \u0026amp; Feedback Back to Table of Contents Amazon RDS - Monthly Cost grouped by Usage Type and Resource Tag Query Description This query will output the total monthly blended costs for RDS grouped by usage type and a specified tag (e.g. Environment:Test,Dev,Prod). The query can be modified to adjust the Cost dataset from Blended to Unblended by adjusting the specified cost column (line_item_blended_cost -\u0026gt; line_item_unblended_cost). This query would be helpful to visualize a quick monthly breakdown of cost components for RDS usage with a specific tag (Environment:Test,Dev,Prod).\nPricing Please refer to the Amazon RDS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT line_item_usage_type, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, resource_tags_user_environment, SUM(CAST(line_item_blended_cost AS DECIMAL(16,8))) AS sum_line_item_blended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code=\u0026#39;AmazonRDS\u0026#39; AND resource_tags_user_environment = \u0026#39;dev\u0026#39; GROUP BY line_item_usage_type, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), resource_tags_user_environment HAVING SUM(line_item_blended_cost) \u0026gt; 0 ORDER BY line_item_usage_type, month_line_item_usage_start_date, resource_tags_user_environment; Help \u0026amp; Feedback Back to Table of Contents Amazon RDS on AWS Outposts Query Description This query will output the total daily unblended costs for RDS Instances running on AWS Outposts racks. This query will be helpful to visualize a quick breakdown of cost components for RDS usage based on instance type, database engine and deployment option (Multi vs Single-AZ).\nPricing Please refer to the Amazon RDS on Outposts pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7) AS split_line_item_resource_id, product_instance_type, product_database_engine, product_deployment_option, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_location_type=\u0026#39;AWS Outposts\u0026#39; AND product_product_family=\u0026#39;Database Instance\u0026#39; AND line_item_product_code = \u0026#39;AmazonRDS\u0026#39; AND (line_item_line_item_type = \u0026#39;Usage\u0026#39; OR (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) OR (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) ) GROUP BY DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, product_instance_type, product_database_engine, product_deployment_option ORDER BY day_line_item_usage_start_date ASC, sum_line_item_usage_amount DESC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon DynamoDB Query Description This query will output the total monthly sum per resource for all DynamoDB purchase options (including reserved capacity) across all DynamoDB usage types (including data transfer and storage costs). The unblended cost will be summed and in descending order.\nPricing Please refer to the DynamoDB pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, product_location, SPLIT_PART(line_item_resource_id, \u0026#39;table/\u0026#39;, 2) AS line_item_resource_id, CASE WHEN line_item_usage_type LIKE \u0026#39;%CapacityUnit%\u0026#39; THEN \u0026#39;DynamoDB Provisioned Capacity\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%HeavyUsage%\u0026#39; THEN \u0026#39;DynamoDB Provisioned Capacity\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%RequestUnit%\u0026#39; THEN \u0026#39;DynamoDB On-Demand Capacity\u0026#39; ELSE \u0026#39;DynamoDB Usage\u0026#39; END AS case_line_item_usage_type, CASE WHEN line_item_line_item_type LIKE \u0026#39;%Fee\u0026#39; THEN \u0026#39;DynamoDB Reserved Capacity\u0026#39; WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN \u0026#39;DynamoDB Reserved Capacity\u0026#39; ELSE \u0026#39;DynamoDB Usage\u0026#39; END AS case_purchase_option, CASE WHEN product_product_family = \u0026#39;Data Transfer\u0026#39; THEN \u0026#39;DynamoDB Data Transfer\u0026#39; WHEN product_product_family LIKE \u0026#39;%Storage\u0026#39; THEN \u0026#39;DynamoDB Storage\u0026#39; ELSE \u0026#39;DynamoDB Usage\u0026#39; END AS case_product_product_family, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_blended_cost AS DECIMAL(16,8))) AS sum_line_item_blended_cost, SUM(CAST(reservation_unused_quantity AS DOUBLE)) AS sum_reservation_unused_quantity, SUM(CAST(reservation_unused_recurring_fee AS DECIMAL(16,8))) AS sum_reservation_unused_recurring_fee, reservation_reservation_a_r_n FROM ${table_name} WHERE {$date_filter} AND line_item_product_code = \u0026#39;AmazonDynamoDB\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), product_location, SPLIT_PART(line_item_resource_id, \u0026#39;table/\u0026#39;, 2), 6, -- refers to case_line_item_usage_type 7, -- refers to case_purchase_option 8, -- refers to case_product_product_family reservation_reservation_a_r_n ORDER BY sum_line_item_blended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon Redshift Query Description This query will provide daily unblended and amortized cost as well as usage information per linked account for Amazon Redshift. The output will include detailed information about the resource id (cluster name), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order. This query includes RI and SP true up which will show any upfront fees to the account that purchased the pricing model.\nPricing Please refer to the Redshift pricing page . Please refer to the Redshift Cost Optimization Whitepaper for Cost Optimization techniques.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_instance_type, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7) AS split_line_item_resource_id, line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_usage_family, product_product_family, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN line_item_usage_amount WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN line_item_usage_amount WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN line_item_usage_amount ELSE 0 END) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN savings_plan_total_commitment_to_date - savings_plan_used_commitment WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 ELSE line_item_unblended_cost END) AS sum_amortized_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN -savings_plan_amortized_upfront_commitment_for_billing_period WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN -reservation_amortized_upfront_fee_for_billing_period ELSE 0 END) AS sum_ri_sp_trueup, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN line_item_unblended_cost WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Redshift\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_instance_type, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7), line_item_operation, line_item_usage_type, line_item_line_item_type, pricing_term, product_usage_family, product_product_family ORDER BY day_line_item_usage_start_date, product_product_family, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon ElastiCache Query Description This query will output the total monthly sum per resource for all Amazon ElastiCache purchase options (including reserved instances) across all ElastiCache instances types. The unblended and amortized cost will be summed and in descending order.\nPricing Please refer to the Amazon ElastiCache pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7) AS split_line_item_resource_id, SPLIT_PART(line_item_usage_type ,\u0026#39;:\u0026#39;,2) AS split_line_item_usage_type, CASE WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN \u0026#39;Reserved Instance\u0026#39; WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN \u0026#39;OnDemand\u0026#39; ELSE \u0026#39;Others\u0026#39; END AS case_purchase_option, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN line_item_usage_amount WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN line_item_usage_amount WHEN line_item_line_item_type = \u0026#39;Usage\u0026#39; THEN line_item_usage_amount ELSE 0 END) AS sum_line_item_usage_amount, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 ELSE line_item_unblended_cost END) AS sum_line_item_unblended_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39; THEN savings_plan_savings_plan_effective_cost WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN savings_plan_total_commitment_to_date - savings_plan_used_commitment WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN 0 WHEN line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39; THEN reservation_effective_cost WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN 0 ELSE line_item_unblended_cost END) AS sum_amortized_cost, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39; THEN -savings_plan_amortized_upfront_commitment_for_billing_period WHEN line_item_line_item_type = \u0026#39;RIFee\u0026#39; THEN -reservation_amortized_upfront_fee_for_billing_period WHEN line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39; THEN -line_item_unblended_cost ELSE 0 END) AS sum_ri_sp_trueup, SUM(CASE WHEN line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39; THEN line_item_unblended_cost WHEN line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39; THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon ElastiCache\u0026#39; AND product_product_family = \u0026#39;Cache Instance\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), bill_payer_account_id, line_item_usage_account_id, line_item_line_item_type, line_item_resource_id, line_item_usage_type ORDER BY month_line_item_usage_start_date, sum_line_item_usage_amount DESC, sum_line_item_unblended_cost; Help \u0026amp; Feedback Back to Table of Contents Amazon DocumentDB Query Description This query will output the total daily cost per DocumentDB cluster. The output will include detailed information about the resource id (cluster name) and usage type. The unblended cost will be summed and in descending order.\nPricing Please refer to the Amazon DocumentDB pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((\u0026#34;line_item_usage_start_date\u0026#34;),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7) AS line_item_resource_id, line_item_usage_type, product_region, line_item_product_code, sum(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, sum(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ${table_Name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonDocDB\u0026#39; AND line_item_line_item_type NOT IN (\u0026#39;Tax\u0026#39;,\u0026#39;Credit\u0026#39;,\u0026#39;Refund\u0026#39;,\u0026#39;Fee\u0026#39;,\u0026#39;RIFee\u0026#39;) GROUP BY 1, -- bill_payer_account_id 2, -- line_item_usage_account_id 3, -- day_line_item_usage_start_date 4, -- line_item_resource_id 5, -- line_item_usage_type 6, -- product_region 7 -- line_item_product_code ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/6_bonus_infrastructure_as_code/","title":"Bonus Infrastructure as Code","tags":[],"description":"","content":"Bonus Infrastructure as Code Optional: Advanced Setup using a CloudFormation Template This section is optional and automates the creation of the AWS organizations data collection using a CloudFormation template. The CloudFormation template allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates, create an IAM role, create an S3 Bucket, and create an Glue Grawler. If you do not have the required permissions skip over this section to continue using the standard setup.\nYou will still need to create your IAM role in your Managment account after you have deployed the below. This can be see in the create IAM Role and Policies in Management account step. Click here to continue with the CloudFormation Advanced Setup Create the Organization data collector using a CloudFormation Template Console Deploy through Console\nClick the Download CloudFormation by clicking here or if you wish to download stright into your managment account click here You can right-click then choose Save link as; or you can right click and copy the link to use with wget Login via SSO in your Cost Optimization account and search for Cloud Formation On the right side of the screen select Create stack and choose With new resources (standard) Choose Template is ready and Upload a template file and upload the main.yaml file you downloaded from above. Click Next. Input the stack name as Organization-data-collector-stack. Next filled in the Parameters. Click Next.\nDatabaseName - Athena Database name where you table will be created DestinationBucket - Unique bucket name that is created to hold org data, you will need to use a with cost at the start, (we have used cost-aws-lab-organisation-bucket) ManagementAccountId - Your Management Account Id where your Optimization is held RoleARN - ARN of the IAM role deployed in the management accounts which can retrieve AWS Org information e.g.arn:aws:iam::123456789:role/OrganizationLambdaAccessRole Tags - List of tags from your Organisation you would like to include separated by a comma. Scroll down and click Next Scroll down and tick the box acknowledgeing that this will create and IAM Role. Click Create stack Wait for the Cloudformation to deploy, this can be seen when it has CREATE_COMPLETE under the stack name. Select your stack and click on Resources and find the lambda function LambdaOrgData and click on the link to take you to the lambda. Repeat the above steps in your Managment Account using the Management.yaml template\nCreate the Organization data collector using a CloudFormation Template CLI Deploy through CLI download parameter.json update them with your parameter.\nRun the following in your terminal, esuring that you have access to the member account you wish to deploy in. aws cloudformation create-stack --stack-name Organization-data-collector-stack --template-body file://main.yaml --capabilities CAPABILITY_NAMED_IAM --parameters file://parameter.json\nTest Lamda Function Now you have deployed the cloudformation then you can test your lambda to get your first set of data in Amazon S3.\nTo test your lambda function click Test Enter an Event name of Test, click Create:\nClick Test\nThe function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in Organizations file size over time:\nGo to your S3 bucket and into the organisation-data folder and you should see a file of non-zero size is in it: Go to the Glue Service page: Now you have deployed your cloudfomation jump to step 11 on Create Glue Crawler on Utilize Organization Data Source page to run your crawler to create your athena table.\nIf you wish to add more tags at a later date you must repeat Step 4 from the lambda section, adding the tags to the list of Environment variables and replacing the Athena table with the tags appended.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/6_bonus_infrastructure_as_code/","title":"Bonus Infrastructure as Code","tags":[],"description":"","content":"Bonus Infrastructure as Code Optional: Advanced Setup using a CloudFormation Template This section is optional and automates the creation of the AWS organizations data collection using a CloudFormation template. The CloudFormation template allows you to complete the lab in less than half the time as the standard setup. You will require permissions to modify CloudFormation templates, create an IAM role, create an S3 Bucket, and create an Glue Grawler. If you do not have the required permissions skip over this section to continue using the standard setup.\nYou will still need to create your IAM role in your Managment account after you have deployed the below. This can be see in the create IAM Role and Policies in Management account step. Click here to continue with the CloudFormation Advanced Setup Create the Organization data collector using a CloudFormation Template Console Deploy through Console\nClick the Download CloudFormation by clicking here or if you wish to download stright into your managment account click here You can right-click then choose Save link as; or you can right click and copy the link to use with wget Login via SSO in your Cost Optimization account and search for Cloud Formation On the right side of the screen select Create stack and choose With new resources (standard) Choose Template is ready and Upload a template file and upload the main.yaml file you downloaded from above. Click Next. Input the stack name as Organization-data-collector-stack. Next filled in the Parameters. Click Next.\nDatabaseName - Athena Database name where you table will be created DestinationBucket - Unique bucket name that is created to hold org data, you will need to use a with cost at the start, (we have used cost-aws-lab-organisation-bucket) ManagementAccountId - Your Management Account Id where your Optimization is held RoleARN - ARN of the IAM role deployed in the management accounts which can retrieve AWS Org information e.g.arn:aws:iam::123456789:role/OrganizationLambdaAccessRole Tags - List of tags from your Organisation you would like to include separated by a comma. Scroll down and click Next Scroll down and tick the box acknowledgeing that this will create and IAM Role. Click Create stack Wait for the Cloudformation to deploy, this can be seen when it has CREATE_COMPLETE under the stack name. Select your stack and click on Resources and find the lambda function LambdaOrgData and click on the link to take you to the lambda. Repeat the above steps in your Managment Account using the Management.yaml template\nCreate the Organization data collector using a CloudFormation Template CLI Deploy through CLI download parameter.json update them with your parameter.\nRun the following in your terminal, esuring that you have access to the member account you wish to deploy in. aws cloudformation create-stack --stack-name Organization-data-collector-stack --template-body file://main.yaml --capabilities CAPABILITY_NAMED_IAM --parameters file://parameter.json\nTest Lamda Function Now you have deployed the cloudformation then you can test your lambda to get your first set of data in Amazon S3.\nTo test your lambda function click Test Enter an Event name of Test, click Create:\nClick Test\nThe function will run, it will take a minute or two given the size of the Organizations files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in Organizations file size over time:\nGo to your S3 bucket and into the organisation-data folder and you should see a file of non-zero size is in it: Go to the Glue Service page: Now you have deployed your cloudfomation jump to step 11 on Create Glue Crawler on Utilize Organization Data Source page to run your crawler to create your athena table.\nIf you wish to add more tags at a later date you must repeat Step 4 from the lambda section, adding the tags to the list of Environment variables and replacing the Athena table with the tags appended.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/6_custom_ec2/","title":"Create custom EC2 reports","tags":[],"description":"","content":"We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage.\nFrom the left menu click Cost Explorer, click Reports, and click and click Monthly costs by service: You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other, then click Apply filters: You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type: Change it to a Daily Line graph, then select More filters: click on Purchase Option, select On Demand and click Apply filters, which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as\u0026hellip;: Enter a report name and click Save Report \u0026gt;: Now click on the Service filter, and de-select EC2-Instances, so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as\u0026hellip; (do NOT click Save): Enter a report name and click Save Report \u0026gt;: You can access these by clicking on Saved Reports: Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/6_maintenance_windows/","title":"Creating Maintenance Windows and Scheduling Automated Operations Activities","tags":[],"description":"","content":"AWS Systems Manager: Maintenance Windows AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following:\nInstalling applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment NoteTo register Step Function tasks you must use the AWS CLI.\n6.1 Setting up Maintenance Windows Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles, and then choose Create role. In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2. This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions. Under Attached permissions policy: Search for AmazonSSMMaintenanceWindowRole. Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review. In the Review section: Enter a Role name, such as SSMMaintenanceWindowRole. Enter a Role description, such as Role for Amazon SSMMaintenanceWindow. Choose Create role. Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship. Delete the current policy, and then copy and paste the following policy into the Policy Document field: { \u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;:[ { \u0026#34;Sid\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;:{ \u0026#34;Service\u0026#34;:[ \u0026#34;ec2.amazonaws.com\u0026#34;, \u0026#34;ssm.amazonaws.com\u0026#34;, \u0026#34;sns.amazonaws.com\u0026#34; ] }, \u0026#34;Action\u0026#34;:\u0026#34;sts:AssumeRole\u0026#34; } ] } Choose Update Trust Policy. You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window.\nTo create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies, and then choose Create policy. On the Create policy page, in the Select a service area, next to Service choose Choose a service, and then choose IAM. In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \u0026ldquo;You choose actions that require the role resource type.\u0026rdquo;, and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field, delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy. On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy. You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups, and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy. On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy. You will be returned to the Summary page for the group. Creating Maintenance Windows To create a Maintenance Window , you must do the following:\nCreate the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution.\n6.2 Create a Patch Maintenance Window First, you must create the window and define its schedule and duration:\nOpen the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window. In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers. (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. NoteIf you choose Allow unregistered targets, then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don\u0026rsquo;t, then you must choose previously registered targets when you register a task with the Maintenance Window.\nSpecify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with, accept the default Cron schedule builder. Under Window starts, choose the third option, specify Every Day at, and select a time, such as 02:00. In the Duration field, type the number of hours the Maintenance Window should run, such as \u0026lsquo;3\u0026rsquo; hours. In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes. Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts, choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window. The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled. 6.3 Assigning Targets to Your Patch Maintenance Window After you create a Maintenance Window, you assign targets where the tasks will run.\nOn the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets. On the Register target page under Maintenance window target details: In the Target Name field, enter a name for the targets, such as TestWebServers. (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note: Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window.\nIn the Targets section, under Select Targets by: Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags, enter \u0026lsquo;Workload\u0026rsquo; as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register targetto register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags.\n6.4 Assigning Tasks to Your Patch Maintenance Window After you assign targets, you assign tasks to perform during the window:\nFrom the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task. On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers. (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by, select Selecting registered target groups. Select the group you created from the list. In the Rate control section: For Concurrency, leave the default targets selected and specify 1. For Error threshold, leave the default errors selected and specify 1. In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options, leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix NoteOnly the last 2500 characters of a command document\u0026rsquo;s output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs.\nIn SNS notifications, leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation, select Install. Choose Register Run command task to complete the task definition and return to the details page. 6.5 Review Maintenance Window Execution After allowing enough time for your maintenance window to complete: Navigate to the AWS Systems Manager console . Choose Maintenance Windows, and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History. Select a Windows execution ID and choose View details. On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID, and choose View output. Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/6_cloudwatch_event/","title":"Customize query strings and create scheduled CloudWatch event","tags":[],"description":"","content":" In you local path where AutoCURDelivery.zip is located. Unzip and re-open config.yml in a text editor.\nFind Body_Text, insert a description of new query MTD_Inter_AZ_DT.\nMTD_Inter_AZ_DT - Month to date inter-AZ data transfer split by resource ID Find the section Query_String_List, add following new query string at the bottom of file (note the indent should be same as other query strings), save config.yml.\n- MTD_Inter_AZ_DT: SELECT year ,month(line_item_usage_start_date) month ,line_item_product_code as Product_Name ,line_item_resource_id as Resource_Id ,line_item_usage_type as Usage_Type ,sum(line_item_usage_amount) as \u0026quot;Inter_AZ_Data_Transfer(GB)\u0026quot; ,sum(line_item_unblended_cost) as \u0026quot;Cost($)\u0026quot; FROM CUR_DB WHERE \u0026quot;line_item_usage_type\u0026quot; like '%Bytes%' AND \u0026quot;line_item_usage_type\u0026quot; like '%Regional%' AND year='CUR_YEAR' AND month='CUR_MONTH' GROUP BY 1,2,3,4,5 ORDER BY sum(\u0026quot;line_item_unblended_cost\u0026quot;) desc The paramemters CUR_DB, CUR_MONTH, CUR_YEAR are replaced when function is running\nAdd config.yml back into AutoCURDelivery.zip, and upload zip file to S3.\nGoto Lambda console, update function code path to above S3 path where new zip file is located, click Save.\nPerform another Test of the function.\nCheck the cost \u0026amp; utilization report in the mail your recipient receives, there should be one more tab added in the excel file for month to date inter-az data transfer cost.\nWe will now create a scheduled Cloudwatch event to trigger Lambda function periodically\nGo to the Cloudwatch dashboard, under Events click Rules\nClick Create rule.\nIn Event Source, choose Schedule, use default fixed rate of 5 minutes.\nIn Targets click Add Target and choose Lambda function in the drop-down box.\nChoose the function Auto_CUR_Delivery, click Configure details Configure a name 5_min_auto_cur_delivery, click Create rule. Wait for 5 minutes, your recipients should receive a cost \u0026amp; utilization report mail, and continually receive report mail every other 5 minutes.\nTo stop event triggering, choose the rule 5_min_auto_cur_delivery, click Actions and select Disable. Now you have completed this lab to query CUR with customized query strings from Athena and send it via SES periodically. To explore more, you can define your own query strings in config.yml and configure CloudWatch event rule to the rate as required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/6_multi_region_deploy/","title":"Multi-region Deployment with CloudFormation StackSets","tags":[],"description":"","content":"There might be situations where you want to deploy the same infrastructure in multiple AWS Regions and/or multiple AWS accounts to increase reliability of the workload or to improve performance by having the infrastructure geographically closer to your end users. You can use AWS CloudFormation StackSets to perform this as a single operation instead of switching regions or accounts to individually deploy each stack.\nFrom an administrator account, you can define a CloudFormation template and use it to provision stacks in multiple target accounts, across multiple AWS Regions.\nFor this exercise we will assume you now know how to edit your CloudFormation template and update your CloudFormation stack with the updated template.\n6.1 Set up permissions for CloudFormation StackSets AWS CloudFormation StackSets requires specific permissions to be able to deploy stacks in multiple AWS accounts across multiple AWS Regions. It needs an administrator role that is used to perform StackSets operations, and an execution role to deploy the actual stacks in target accounts. These roles require specific naming conventions - AWSCloudFormationStackSetAdministrationRole for the administrator role, and AWSCloudFormationStackSetExecutionRole for the execution role. StackSets execution will fail if either of these roles are missing. The AWSCloudFormationStackSetAdministrationRole should be created in the account where you are creating the StackSet (The Administrator account - see the diagram above). The AWSCloudFormationStackSetExecutionRole should be created in each target account where you wish to deploy the stack. Learn more about granting self-managed permissions for CloudFormation StackSets. If you accounts are managed using AWS Organizations, you can enable trusted access and CloudFormation will take care of provisioning all the necessary roles across the accounts.\nFor this lab, we will walk through the process of creating a StackSet to deploy stacks across multiple regions in a single account (the same account where the StackSet is being created). For simplicity and ease of use, we will use CloudFormation to create the administrator and execution roles.\nDownload the administrator role CloudFormation template - https://s3.amazonaws.com/cloudformation-stackset-sample-templates-us-east-1/AWSCloudFormationStackSetAdministrationRole.yml Go to the AWS CloudFormation console and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: AWSCloudFormationStackSetAdministrationRole.yml For Stack name use StackSetAdministratorRole For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options For Review Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack The stack will finish creating and the Status will be CREATE_COMPLETE in about 30 seconds.\nNow that a StackSet administrator role has been created, we need to create the StackSet execution role.\nDownload the execution role CloudFormation template - https://s3.amazonaws.com/cloudformation-stackset-sample-templates-us-east-1/AWSCloudFormationStackSetExecutionRole.yml Go to the AWS CloudFormation console and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is For Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: AWSCloudFormationStackSetExecutionRole For Stack name use StackSetExecutionRole For Parameters, enter the 12 digit account ID for the AWS account you are using for this lab. For Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options For Review Review the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack The stack will finish creating and the Status will be CREATE_COMPLETE in about 30 seconds.\nNow that the necessary permissions have been created, the next step is to launch CloudFormation stacks across different AWS Regions using StackSets.\n6.2 Deploy CloudFormation stacks using CloudFormation StackSets Go to the AWS CloudFormation StackSets console and click Create StackSet\nLeave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you used in the previous section (where you added an EC2 instance with user data). If you ran into any issues, or want a fresh template to use for this section you can right click and download this link - simple_stack_plus_s3_ec2_server.yaml Click Next\nFor Stack name use StackSetsLab\nEnsure that the values for the following Parameters are as follows. You can use default values for the rest.\nPublicEnabledParam - set to true EC2SecurityEnabledParam - set to true Click Next\nFor Configure StackSet options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value.\nFor Permissions select Self-service permissions.\nFor IAM admin role ARN - optional, select IAM role name and then select AWSCloudFormationStackSetAdministrationRole from the drop-down. For IAM execution role name enter AWSCloudFormationStackSetExecutionRole. Click Next\nUnder Accounts, select Deploy stacks in accounts under Deployment locations.\nUnder Account numbers enter the 12 digit AWS account ID for the account you are using for this lab. You can find this by clicking on the user/role drop down you have logged into the account with on the top right corner.\nUnder Specify regions select 2 regions you would like to deploy the stacks across. I have selected US East (N.Virginia) and US West (Oregon). You can select as many regions as you want to deploy stacks into, including the same region where the StackSet is being created.\nLeave values for Deployment options as-is and click Next.\nFor Review\nReview the contents of the page Click Submit The operation takes about 3-4 minutes to complete and the stacks to be deployed in the selected Regions.\n6.3 Review infrastructure created Go to the AWS CloudFormation StackSets console and click on the StackSet StackSetsLab.\nClick on the Stack instances tab to see the AWS account and region stacks were deployed in.\nChange the AWS Region you are on by clicking on the top right corner of the console and select one of the AWS Regions you specified for the StackSet. In my case, I will select US West (Oregon) us-west-2.\nAfter switching regions, go to the AWS CloudFormation console You should see a new CloudFormation stack that has been created with the prefix StackSet-StackSetsLab-.\nClick on the stack name and then click on the Outputs tab.\nClick on the Value for PublicServerDNS and observe the response.\nRepeat the previous steps for another AWS Region that you specified when creating the StackSet. You will see that the webpage has changed to reflect the region the instance was launched in. Using StackSets, you have deployed your infrastructure to various AWS Regions in a single operation. This will greatly increase the speed of multi-region and multi-account deployments of your infrastructure and is controlled from a central location.\nTroubleshooting\nIf the CloudFormation StackSet operation fails, then click on the Stack instances tab to find the source of the error Note that some AWS Service Quotas are regional. If you are seeing an error that says you have reached the limit for a particular resource type, try using a different region or submitting a ticket to AWS Support to increase the limit. If you see an error regarding missing execution role, make sure you have completed section 6.1 of this lab guide and created the necessary execution role. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/6_quicksight/","title":"Setup Amazon QuickSight","tags":[],"description":"","content":"This will setup Amazon QuickSight, so that users in the Cost Optimization Account can create analysis\u0026rsquo; and dashboards.\nSetup QuickSight for the first time Log into the console in the Cost Optimization account as an IAM user with the required permissions, go to the Amazon QuickSight console: If you havent used QuickSight before click on Sign up for QuickSight, otherwise skip this step and proceed to Setup QuickSight IAM Policy below: Select the Standard edition, and click Continue: Select the region which should be the same as your CUR file source, Enter your QuickSight account name, Notification email address, select Amazon Athena and click Choose S3 buckets: Select S3 Buckets You Can Access Across AWS, under Use a different bucket enter the CUR bucket name, click Add S3 bucket and click Finish: Click Finish: Setup QuickSight IAM Policy Go to the IAM Dashboard\nClick Policies and search for the AWSQuickSightS3Policy, click on the AWSQuickSightS3Policy policy: Click Edit policy, We will add the s3 resource arn:aws:s3:::cost* below the existing s3 bucket. This will allow QuickSight to access any S3 bucket starting with cost, so Cost Optimization users can easily create new datasets without requiring additional QuickSight privileges. Click Review policy: Click Save changes: Congratulations - QuickSight is now setup for your users. The Cost Optimization team can self manage QuickSight, and access to data sets in S3 with the correct bucket name.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/6_sub_acct/","title":"Sub Account Crawler Setup","tags":[],"description":"","content":"The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database.\n1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console.\n2 - Add a Crawler with the following details:\nInclude path: the S3 bucket in the account with the delivered CURs Exclude patterns: **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database\n6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables.\n8 - Go into Athena and execute a preview query to verify access and the data.\nYou have now given the sub account access to their specific CUR files as extracted from the Management/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/6_tear_down/","title":"Tear down","tags":[],"description":"","content":" Terminate the EC2 Instance Delete the IAM role CloudWatchAgentServerRole X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 6. \u0026ldquo;How do you meet cost targets when you select resource type, size and number?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/100_labs/100_walkthrough_of_the_well-architected_tool/6_tear_down/","title":"Tear down this lab","tags":[],"description":"","content":"In order to take down the lab environment, you simply delete the workload you created.\nSelect Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog: X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/6_tear_down/","title":"Teardown","tags":[],"description":"","content":"Log onto the console as your regular user with the required permissions.\nDelete the IAM policies We will delete the IAM policies created, as they are no longer applied to any groups.\nLog on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed: Select the policy you want to delete Region_Restrict: Click on Policy actions, and select Delete: Click on Delete: Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies.\nClick on Groups: Select the CostTest group, click Group Actions, click Delete Group: Click Yes, Delete: Click Users: Select TestUser1, and click Delete user: Click Yes, delete: Go to the EC2 dashboard: Click Security Groups on the left: Select the security groups you took note of, ensure you have the correct groups that were created. Click Actions, select Delete Security Groups: Triple check they are the groups you wrote down, and click Yes, Delete: Confirm there are no io1 unattached EBS volumes, go to the EC2 dashboard, click on Elastic Block Store, click Volumes. You can sort by the Created column to help identify volumes that were not terminated as part of this lab.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/6_tear_down/","title":"Teardown","tags":[],"description":"","content":"Savings Plan analysis is a critical requirement of cost optimization, so there is no specific tear down for this lab.\nThe following resources were created in this lab:\nS3 Bucket: (custom name) Lambda Functions: SPTool_ODPricing_Download and SPTool_SPPricing_Download IAM Role: SPTool_Lambda IAM Policy: s3_pricing_lambda CloudWatch Event, Rule: SPTool-Pricing Glue Crawlers: OD_Pricing and SP_Pricing IAM Role: AWSGlueServiceRole-SPToolPricing Glue Database: Pricing Athena Views: pricing.pricing and costmaster.SP_USage QuickSight Dataset: SP_Usage QuickSight Analysis: sp_usage analysis X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST7 - \u0026ldquo;How do you use pricing models to reduce cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_autonomous_monitoring_of_cryptographic_activity_with_kms/6_teardown/","title":"Teardown","tags":[],"description":"","content":"The following steps will remove the services which are deployed in the lab.\n6.1. Remove the CloudWatch Alarm 6.1.1. From the CloudWatch console, select Alarms from the left-hand dashboard, and select the alarms which you created in the lab using the radio box.\n6.1.2. From the Actions button, select Delete and confirm the alarm deletion.\n6.2. Remove the CloudWatch Metric Filter From the CloudWatch console, select Log group under Logs and locate the log group which you created in the lab.\nUnder the Metric filters section, click on 1 filter and check the radio button next to the metric filter that you created.\nSelect the Delete button and confirm the deletion of the metric filter.\n6.3. Remove the CloudWatch Log Group From the CloudWatch console, select Log group under Logs and select the log group which you created in the lab.\nSelect the Actions button and delete the log group confirming the deletion.\n6.4. Remove the Trail from CloudTrail From the CloudTrail console, select Trails from the left-hand menu.\nUse the radio button to select the trail that you created and select the Delete button to delete the Trail.\n6.5. Remove the SNS topic From the SNS console, select Topics and then select the name of the topic which you created, and select the Delete button, confirming the deletion in the next dialog box.\n6.6. Remove the ECS Cluster From the ECS console, select Clusters in the left-hand-menu and select the cluster which you created for the lab.\nSelect the Task tab and remove all active tasks for the cluster, confirming the deletion in the next window. Select the service tab and select the Delete button, confirming the delete in the next window. 6.7. Remove the ECR Repository From the ECR console, select Repositories in the left-hand menu, highlight the repository name in the main panel and select the Delete button, confirming the delete in the next window.\n6.8. Remove the Application Stack From the CloudFormation console, select the pattern1-app and select the Delete button, confirming the deletion in the next window.\n6.9. Remove the Base Stack Firstly, remove the data from the S3 bucket you created, or the deletion of the stack will fail. To do this, go to the S3 console and find the bucket which you created. Select the bucket and delete all the objects confirming the deletion in the next dialog box.\nNow, from the CloudFormation console, select the pattern1-base and select the Delete button, confirming the deletion in the next window.\n6.10. Finally Remove The Cloud9 Environment From the Cloud9 IDE highlight the environment which you created and select the delete button\n"},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/6_view_cw_logs/","title":"View your CloudWatch Logs","tags":[],"description":"","content":"Now that the CloudWatch Agent is up and running on your EC2 Instance, let’s go ahead and view those logs and metrics from the Console. CloudWatch is a useful place to view logs because it is centralized, meaning you can switch between examining logs from many sources.\nViewing Logs:\nOpen the CloudWatch console . On the left side menu, choose Log groups under Logs. On that screen, enter securitylablogs in the search bar. Click on the log group that appears in the results. You will see these log streams: cw-agent-logs, apache-access-logs, apache-error-logs, yum-logs, and ssh-logs. Click through all of them to view the logs from each of these services. You should see a record of log events. This is the data being collected on your EC2 instance, and then sent to CloudWatch by the CloudWatch Agent installed on the instance. Recap: In this section, you explored log files generated by your EC2 instance in the CloudWatch console. The CloudWatch console provides a unified location to view a variety of logs, enabling you to investigate or monitor security activity in a central location. Using the CloudWatch console illustrates the security best practice of “analyzing logs, findings, and metrics centrally”.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_automated_iam_user_cleanup/","title":"Level 200: Automated IAM User Cleanup","tags":[],"description":"","content":"Authors Pierre Liddle, Principal Security Architect Byron Pogson, Solutions Architect Introduction This hands-on lab will guide you through the steps to deploy a AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. You will use the AWS SAM CLI to package your deployment. Skills learned will help you secure your AWS account in alignment with the AWS Well-Architected Framework .\nThe AWS Lambda function is triggered by a regular scheduled event in Amazon CloudWatch Events. Once the Lambda function runs to check the status of the AWS IAM Users and associated IAM Access Keys the results are sent the designated email contact via Amazon SNS. A check is also performed for unused roles and unused permissions identifed by IAM Access Analyzer. The logs from the AWS Lambda function are captured in Amazon CloudWatch Logs for review and trouble shooting purposes.\nGoals Identify orphaned IAM Users and AWS Access Keys Take action to automatically remove IAM Users and AWS Access Keys no longer needed Reduce identity sprawl Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS Lambda from the list: AWS Regions and Endpoints . AWS Serverless Application Model (SAM) installed and configured. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster. Steps: Deploying IAM Lambda Cleanup with AWS SAM Tear down References \u0026amp; useful resources AWS Identity and Access Management User Guide What is IAM Access Analyzer? IAM Best Practices and Use Cases AWS SAM CLI AWS Serverless Application Model (SAM) "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_aws_resource_optimization/","title":"Level 200: Rightsizing with Compute Optimizer","tags":[],"description":"","content":"Last Updated October 2021\nAuthors Jeff Kassel, AWS Technical Account Manager Arthur Basbaum, AWS Cloud Economics Travis Ketcherside, AWS Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to install the CloudWatch agent to collect memory utilization (% GB consumption) and analyze how that new datapoint can help during rightsizing exercises with the AWS Compute Optimizer.\nGoals Install and collect memory statistics through a custom metric using Amazon CloudWatch Use AWS Compute Optimizer to gain insights into rightsizing recommendations on AWS Prerequisites Completion of Level 100: Rightsizing Recommendations Enable AWS Compute Optimizer at no additional cost. After AWS Compute Optimizer is enabled it may take up to 12 hours to fully analyze the AWS resources in your account. Costs Use of AWS Compute Optimizer is free, but costs associated with CloudWatch and EC2 will be accrued once free tier usage is consumed. Please visit the CloudWatch and EC2 pricing pages for additional details.\nPermissions required IAM permissions for read-only access for AWS Compute Optimizer NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Steps: Getting to know Amazon Cloudwatch Create an IAM Role to use with Amazon CloudWatch Agent Attach CloudWatch IAM role to selected EC2 Instances CloudWatch Agent Manual Install Rightsizing with AWS Compute Optimizer and Memory Utilization Enabled Tear down X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_lambda_cross_account_iam_role_assumption/","title":"Level 300: Lambda Cross Account IAM Role Assumption","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This lab demonstrates a Lambda function in AWS account 1 (the origin) using Python boto SDK to assume an IAM role in account 2 (the destination), then list the buckets. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id.\nIf in classroom and you do not have 2 AWS accounts, buddy up to use each other\u0026rsquo;s accounts, agree who will be account #1 and who will be account #2.\nThe skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Cross account role assumption Lambda assuming another role Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Steps: Create role for Lambda in account 2 Create role for Lambda in account 1 Create Lambda in account 1 Tear down References \u0026amp; useful resources https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_managing_credentials_and_authentication/","title":"Quest: Managing Credentials &amp; Authentication","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity \u0026amp; Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.\nAWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nBasic Identity and Access Management User, Group, Role Automated IAM User Cleanup Walkthrough This hands-on lab will guide you through the steps to deploy an AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account.\nIAM Tag Based Access Control for EC2 IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2 "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/","title":"300 Labs","tags":[],"description":"","content":"List of labs available Level 300: Automated Athena CUR Query and E-mail Delivery Level 300: Automated CUR Updates and Ingestion Level 300: AWS CUR Query Library Level 300: Splitting the CUR and Sharing Access Level 300: Optimization Data Collection Level 300: Organization Data CUR Connection "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/6_resource_type_size_number/","title":"Resource Type, Size &amp; Number","tags":[],"description":"","content":"Resource Type, Size, \u0026amp; Number Resource Size Over-provisioning Goal: Minimize waste due to over-provisioning Target: Waste due to compute over-provisioing in the 1st tier of production workloads not to exceed 10% of tier compute cost, as reported by tool X. Waste due to compute over-provisioining in the 2nd tier of production workloads not to exceed 5% of tier compute cost. Best Practice: Data-based selection Measures: % of over provisioning Good/Bad: Good Why? When does it work well or not?: Provides a $ amount, allows headroom for demand spikes \u0026amp; time to scale Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/7_failure_injection_az/","title":"Test Resiliency Using Availability Zone (AZ) Failure Injection","tags":[],"description":"","content":"6.1 AZ failure injection This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.\nIn Chaos Engineering we always start with a hypothesis. For this experiment the hypothesis is:\nHypothesis: If an entire Availability Zone dies, then availability will not be impacted\nGo to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in.\nNote: If you previously ran the RDS Failure Injection test, you must wait until the console shows the AZs for the primary and standby instances as swapped, before running this test A good way to run the AZ failure injection is first in an AZ other than this - we\u0026rsquo;ll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we\u0026rsquo;ll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios. To simulate failure of an AZ, select one of the Availability Zones used by your service (us-east-2a, us-east-2b, or us-east-2c) as \u0026lt;az\u0026gt;\nFor scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b use your VPC ID as \u0026lt;vpc-id\u0026gt;\nSelect one (and only one) of the scripts/programs below. (choose the language that you setup your environment for).\nLanguage Command Bash ./fail_az.sh \u0026lt;az\u0026gt; \u0026lt;vpc-id\u0026gt; Python python3 fail_az.py \u0026lt;vpc-id\u0026gt; \u0026lt;az\u0026gt; Java java -jar app-resiliency-1.0.jar AZ \u0026lt;vpc-id\u0026gt; \u0026lt;az\u0026gt; C# .\\AppResiliency AZ \u0026lt;vpc-id\u0026gt; \u0026lt;az\u0026gt; PowerShell .\\fail_az.ps1 \u0026lt;az\u0026gt; \u0026lt;vpc-id\u0026gt; The specific output will vary based on the command used.\nNote whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance 6.2 System response to AZ failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n6.2.1 System availability Refresh the service website several times\nScenario 1: If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2: If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur. Verify your observations by going through the canary run data:\nGo to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation click on the WebServersforResiliencyTesting stack click on the Outputs tab Open the URL for WorkloadAvailability in a new window You will see that canary runs are failing because the website is not available. 6.2.2 Scenario 1 - Load balancer and web server tiers This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens as you did before, for that test:\nEC2 Instances Load Balancer Target group Auto Scaling Groups One difference from the EC2 failure test that you will observe is that auto scaling will not replace the EC2 instance in the same AZ as the one that was terminated. Auto scaling attempts to balance the requested three EC2 instances across the remaining two AZs.\n6.2.3 Scenario 2 - Load balancer, web server, and data tiers This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console , navigate to your DB screen and observe the following tabs:\nConfiguration Monitoring Logs \u0026amp; Events 6.2.4 AZ failure injection - conclusion This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure.\nOur hypothesis is confirmed:\nHypothesis: If an entire Availability Zone dies, then availability will not be impacted\n6.2.5 AZ failure recovery This step is optional. To simulate the AZ returning to health do the following:\nGo to the Auto Scaling Group console Select the WebServersforResiliencyTesting auto scaling group Actions \u0026raquo; Edit In the Subnet field add any ResiliencyVPC-PrivateSubnets that are missing (there should be three total) and Save Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions \u0026raquo; Edit subnet associations Uncheck all boxes and click Edit Actions \u0026raquo; Delete network ACL Note how the auto scaling redistributes the EC2 servers across the availability zones X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Amazon S3 Cleanup 1.1 Navigate to S3 .\n1.2 Select the pilot-primary-uibucket-xxxx and click Empty.\n1.3 Enter permanently delete into the confirmation box and then click Empty.\n1.4 When you see the green banner at the top stating the bucket has is empty, click Exit.\nPlease repeat steps 1.1 through 1.4 for the following buckets:\npilot-secondary-uibucket-xxxx Database Clean up This step is required as we did manual promotion for the Aurora Database.\n2.1 Navigate to RDS in N. California (us-west-1) region.\n2.2 Select database under dr-immersionday-secondary-pilot cluster and delete the instance.\n2.3 De-select Create final snapshot option, Select \u0026ldquo;I acknowledge..\u0026rdquo; option. Click Delete button.\n2.4 Wait until Amazon Aurora Database Cluster is deleted.\nCloudFormation Secondary Region Cleanup 3.1 Navigate to CloudFormation in N. California (us-west-1) region.\n3.2 Select the Pilot-Secondary stack and click Delete.\n3.3 Click Delete stack to confirm the removal.\nWait for the stack deletion to complete.\n3.4 CloudFormation stack deletion fails due to the manual deletion of Aurora Database.\n3.5 Navigate to CloudFormation in N. California (us-west-1) region.\n3.6 Select the Pilot-Secondary stack and click Delete.\n3.7 Select all Resources to retain (this is OK because they were manually deleted in the prior section) and click Delete stack.\nAWS CloudFormation Primary Region Cleanup 4.1 Navigate to CloudFormation in N. Virginia (us-east-1) region.\n4.2 Select Pilot-Primary stack. Next click the Delete button to remove it.\n4.3 Click Delete stack to confirm the deletion.\nX Congratulations! This lab specifically helps you with the best practices covered in question REL 13 How do you plan for disaster recovery (DR)\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Amazon S3 Cleanup 1.1 Navigate to S3 .\n1.2 Select the warm-primary-uibucket-xxxx and click Empty.\n1.3 Enter permanently delete into the confirmation box and then click Empty.\n1.4 When you see the green banner at the top stating the bucket has is empty, click Exit.\nPlease repeat steps 1.1 through 1.4 for the following buckets:\nwarm-secondary-uibucket-xxxx Database Clean up This step is required as we did manual promotion for the Aurora Database.\n2.1 Navigate to RDS in N. California (us-west-1) region.\n2.2 Select database under dr-immersionday-secondary-warm cluster and delete the instance.\n2.3 De-select Create final snapshot option, Select \u0026ldquo;I acknowledge..\u0026rdquo; option. Click Delete button.\n2.4 Wait until Amazon Aurora Database Cluster is deleted.\nCloudFormation Secondary Region Cleanup 3.1 Navigate to CloudFormation in N. California (us-west-1) region.\n3.2 Select the Warm-Secondary stack and click Delete.\n3.3 Click Delete stack to confirm the removal.\nWait for the stack deletion to complete.\n3.4 CloudFormation stack deletion fails due to the manual deletion of Aurora Database.\n3.5 Navigate to CloudFormation in N. California (us-west-1) region.\n3.6 Select the Pilot-Secondary stack and click Delete.\n3.7 Select all Resources to retain (this is OK because they were manually deleted in the prior section) and click Delete stack.\nAWS CloudFormation Primary Region Cleanup 4.1 Navigate to CloudFormation in N. Virginia (us-east-1) region.\n4.2 Select Warm-Primary stack. Next click the Delete button to remove it.\n4.3 Click Delete stack to confirm the deletion.\nX Congratulations! This lab specifically helps you with the best practices covered in question REL 13 How do you plan for disaster recovery (DR)\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/disaster/","title":"Failover to Secondary","tags":[],"description":"","content":"When a regional service event affects the Unicorn application in the Primary, N. Virginia (us-east-1) region, you want to fail-over to Secondary, N. California (us-west-1) region.\nWe assume a regional service event has occurred. In this section, we will manually fail over the application to the Secondary region. The CloudFront distribution will detect the service interruption and automatically begin routing requests from the Primary-Active to the Passive-Secondary website seamlessly.\nBefore simulating the outage, we need to create test data through the web application. This step requires creating enrolling in the store, then adding items into the shopping cart. After the outage, the user’s session should remain active and uninterrupted.\nCreate and Populate the Shopping Cart 1.1 Navigate to the CloudFront Domain Name using your favorite browser.\nIf you don\u0026rsquo;t have your CloudFront Domain Name, you can retrieve it via Step 4.2 in Setup CloudFront.\n1.2 Click the Signup button.\n1.3 Register yourself into the application. You need to provide an e-mail address, which does not need to be valid.\n1.4 Log in to the application using your e-mail address from the previous step.\n1.5 Add/remove items to your shopping cart by clicking on a Unicorn, followed by clicking the Add to cart button.\nSimulating a Regional Service Event We will now simulate a regional service event affecting the S3 static website in N. Virginia (us-east-1) serving The Unicorn Shop website.\n2.1 Click S3 to navigate to the dashboard.\n2.2 Click the bucket link that begins with active-primary-uibucket- .\n2.3 Click the Permissions link. In the Block public access (bucket settings) section, click the Edit button.\n2.4 Enable the Block all public access checkbox, then click the Save button.\n2.5 Type confirm, then click the Confirm button.\nYour Amazon S3 bucket that hosts the Primary-Active website is now inaccessible. When CloudFront attempts to route the user’s request to this instance, it will receive an HTTP 403 status error (Forbidden). The Distribution will automatically handle this scenario by failing over to the Passive-Secondary instance.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/disaster/ec2-instance/","title":"Modify Application","tags":[],"description":"","content":"To connect the application to the newly promoted Aurora Database in the us-west-1 region, we need to modify the Passive-Secondary web application\u0026rsquo;s configuration.\nConnecting the Application 1.1 Click EC2 to navigate to the dashboard in the N. California (us-west-1) region.\n1.2 Click the Instances (running) link.\n1.3 Select UniShopAppV1EC2HotStandby, then click the Connect button.\n1.4 Click the Session Manager link, then click the Connect button.\n1.5 After a brief moment, a terminal prompt will display.\n1.6 Change the current directory to the ec2-user\u0026rsquo;s home folder.\nsudo su ec2-user cd /home/ec2-user/ 1.7 Open the unishoprun.sh file for editing with either nano or vi.\nsudo nano unishoprun.sh Tip: You can use the vi (Debian ManPage ) or nano command (Debian ManPage ) to edit the document.\n1.8 Delete the previous file contents. Then copy and paste this script into the unishoprun.sh script.\n#!/bin/bash java -jar /home/ec2-user/UniShopAppV1-0.0.1-SNAPSHOT.jar \u0026amp;\u0026gt; /home/ec2-user/app.log \u0026amp; 1.9 Save the modifications (CTRL+O) and close the editor (CTRL+X).\n1.10 Reboot the EC2 instance so our changes take effect.\nsudo reboot Congratulations! Your Application has been updated to use the Aurora Promoted Database! X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_pricing_model_analysis/","title":"Level 200: Pricing Model Analysis","tags":[],"description":"","content":"Last Updated March 2022\nYour browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Authors Stephanie Gooch, Commercial Architect (AWS) Nathan Besh, Cost Lead, Well-Architected (AWS) Nataliya Godunok, Technical Account Manager (AWS) Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through setting up pricing and usage data sources, then creating a visualization to view your costs over time for EC2 in Savings Plans rates, allowing you to make low risk, high return purchases for pricing models. The data sources will also allow you to do custom allocation of discounts for your organization (a separate lab).\nYou will create two pricing data sources, by using Lambda to download the AWS price list files (On Demand EC2 pricing, and Savings Plans rates from all regions) and extract the pricing components required. You can configure CloudWatch Events to periodically run these functions to ensure you have the most up to date pricing and the latest instances in your data.\nThe pricing files are then combined with your Cost and Usage Report (CUR), to provide an hourly usage report with multiple pricing dimensions, this allows you query and analyze your usage by on demand rates, savings plan rates, or the difference between the two (discount level).\nFinally you create a visualization with calculations in QuickSight which allows you to view your usage patterns, and also perform analysis to understand the commitment levels that are right for your business.\nNOTE: this lab demonstrates EC2 savings plans only, but can be extended to cover other services such as Fargate or Lambda.\nGoals Setup the pricing and usage data sources Create the visualization for recommendations and analysis Prerequisites An AWS Account An Amazon QuickSight Account A Cost and Usage Report (CUR) Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab Completed the Cost and Usage Visualization lab Basic knowledge of AWS Lambda, Amazon Athena and Amazon Quicksight Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Additional: Create a Lambda function with assiciated IAM roles, trigger it via CloudWatch Costs Small accounts approximately \u0026lt;$5 Time to complete The lab should take approximately 50-60 minutes to complete Steps: Create Pricing Data Sources Create the Usage Data Source Setup QuickSight Dashboard Create the Recommendation Dashboard Format the Recommendation Dashboard Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/200_labs/200_using_awscli_to_manage_wa_reviews/7_cleanup/","title":"Teardown","tags":[],"description":"","content":"Summary You have learned how to use the various AWS CLI commands to work with the AWS Well-Architected Tool.\nRemove all the resources Using the delete-workload API , you can remove the workload from the WA Tool. aws wellarchitected delete-workload --workload-id \u0026#34;\u0026lt;WorkloadId\u0026gt;\u0026#34; References \u0026amp; useful resources AWS CLI - wellarchitected AWS Well-Architected Tool Documentation AWS Well-Architected Boto3 Reference AWS Well-Architected API Reference X Congratulations! Click here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/7_cleanup/","title":"Teardown","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nYou should skip this step. Your AWS account will be cleaned automatically\nIf you are using your own AWS account:\nPlease use these steps when you are done with the lab\nCleaning up the CloudFormation Stack The following instructions will remove the resources that you have created in this lab.\nSign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack Shuffle-sharding-lab, and click on the Resources tab. Locate the resource with the logical ID CanaryBucket. Click on the URL next to it (in the Physical ID column). On the S3 console, empty the bucket of all objects. Return to the CloudFormation console and delete the Shuffle-sharding-lab stack. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/3_add_assumptions_iac/","title":"Add your own assumptions (using infrastructure as code)","tags":[],"description":"","content":"Introduction In lab 2 you learned how to extend your AWS Cost \u0026amp; Usage Report data by additional data \u0026amp; assumptions in Amazon Athena views. The manual approach from the previous lab is tedious and not feasible if you need to provide larger data sets. In this lab you learn to add more data in CSVs via an Infrastructure as Code (IaC) approach.\nYou will customize the library introduced in lab 1.4 for your own needs.\nLab Prerequisites This lab utilises the AWS CDK . If you do not already have the AWS CDK installed in your environment, please follow the prerequisites and installation guide from the AWS CDK getting started page .\nStage 1 - Deploy the CDK stack Head to the aws-usage-queries repository in github. Clone the repository to your development environment. Typically with git clone git@github.com:aws-samples/aws-usage-queries.git You will need to install the dependant libraries as the sample is built using the AWS CDK stack. Whilst in the root directory of the cloned repository where cdk.json exists, run npm install. The CDK template is written in TypeScript. TypeScript sources must be compiled to JavaScript initially and after each modification. Open a new terminal and keep this terminal open in the background if you like to change the source files. Cross compile the TypeScript code to JavaScript with npm run watch. Using run watch allows for continuous compilation as you make further changes to files later in the lab. Now the code is compiled for CDK, deploy the stack to your aws account with cdk deploy: cdk deploy \\ --parameters CurBucketName=\u0026lt;bucket name\u0026gt; \\ --parameters ReportPathPrefix=\u0026lt;path without leading or trailing slash\u0026gt; \\ --parameters ReportName=\u0026lt;report name\u0026gt; \\ --databaseName=\u0026lt;optional databasename override, default: aws_usage_queries_database\u0026gt; Note If you still have the serverless application repository app deployed from lab 1.4 , you will need to delete the stack named serverlessrepo-aws-usage-queries from AWS CloudFormation console before deploying via the CDK command to remove resource conflicts.\nNote If you have not previously used AWS CDK in your AWS account, you may need to bootstrap the account by running cdk bootstrap aws://ACCOUNT-NUMBER/REGION. Please review the AWS CDK getting started page for more information.\nStage 2 - Create data files Now you have the AWS CDK stack deployed in your account, the next step is to bring your own assumptions. You can use the example data to start, but you are encouraged to bring your own assumptions.\nWithin the cloned repository, navigate to referenceData directory. Within this directory, you will see the instanceTypes data used in earlier labs. Create a new directory named instanceFamilyPoints. You should end up with ../referenceData/instanceFamilyPoints. Create a new file named data.csv. You should end up with ../referenceData/instanceFamilyPoints/data.csv. Populate this file with your instance assumptions, you can use the below as an example. Ensure you save this file. instance_family,points t3,2 t2,2 t3a,1 m5,2 Perform the same steps to create a data file with path ../referenceData/regionPoints/data.csv and populate with the following example assumptions. region,points us-east-1,2 eu-central-1,1 Stage 3 - Create tables Next we will modify the ../lib/aws-usage-queries.ts file to include our new assumption data. The file already includes some constructs we can use to ingest our assumption data defined above.\nCreate a Amazon Athena table for instance family and region assumptions. This can be copied to line 279, above the assignment of referenceInstanceTypes. A complete file with the new code added can be found here. const regionPoints = new Table(this, \u0026#34;regionPoints\u0026#34;, { columns: [ { name: \u0026#34;region\u0026#34;, type: Schema.STRING }, { name: \u0026#34;points\u0026#34;, type: Schema.DOUBLE } ], database: database, tableName: \u0026#34;region_points\u0026#34;, s3Prefix \u0026#34;regionPoints\u0026#34; bucket: referenceDataBucket, dataFormat: { outputFormat: OutputFormat.HIVE_IGNORE_KEY_TEXT, inputFormat: InputFormat.TEXT, serializationLibrary: SerializationLibrary.OPEN_CSV, } }); setSerdeInfo(regionPoints, csvProperties); const instanceFamilyPoints = new Table(this, \u0026#34;instanceFamilyPoints\u0026#34;, { columns: [ { name: \u0026#34;instance_family\u0026#34;, type: Schema.STRING }, { name: \u0026#34;points\u0026#34;, type: Schema.DOUBLE } ], database: database, tableName: \u0026#34;instance_family_points\u0026#34;, s3Prefix \u0026#34;instanceFamilyPoints\u0026#34; bucket: referenceDataBucket, dataFormat: { outputFormat: OutputFormat.HIVE_IGNORE_KEY_TEXT, inputFormat: InputFormat.TEXT, serializationLibrary: SerializationLibrary.OPEN_CSV, } }); setSerdeInfo(instanceFamilyPoints, csvProperties); Let\u0026rsquo;s explore these objects further by diving into the regionPoints object. Two columns are defined as region and points as a string and double data types respectively. The table name is created as region_points and we provide the prefix of the S3 location where the data is stored. This should match the name of your local directory where the region points data.csv file is located. The dataFormat section is essentially telling Amazon Athena that the data type is text.\nSave aws-usage-queries.ts. Then, in your npm run watch terminal, you should see the file change detected and an incremental compilation. Go to the Amazon Athena console and check the existing tables and views. As the changes above have not yet been deployed, you should just see the two tables originally deployed via the AWS CDK deploy, and the four original views. Now deploy the new changes by running cdk deploy in the terminal. This will take a few moments whilst the data is copied to the S3 bucket and the new tables are created in Athena. Refresh your Amazon Athena console page. you should now see the two new tables, instance_family_points and region_points created. Preview the instance_family_points table, and check the results match your ../instanceFamilyPoints/data.csv data file. Stage 4 - Use the new tables in queries Now the new tables have been created, they can be used in the same query used in lab 2 Create a new query in the Amazon Athena console and use the same SQL from lab 2 SELECT instance_family, region, account_id, purchase_option, SUM(vcpu_hours) vcpu_hours, year, month, SUM(f.points * r.points * vcpu_hours) points FROM monthly_vcpu_hours_by_account JOIN region_points r USING (region) JOIN instance_family_points f USING (instance_family) GROUP BY instance_family, region, account_id, purchase_option, year, month ORDER BY 8 DESC If you don\u0026rsquo;t return any results, try troubleshooting your data. Do you have any instances of the defined types in the regions you have given points?\nCongratulations! You have now brought your own assumptions, defined them in infrastructure code, and deployed them with AWS CDK. Defining assumptions as IaC brings the benefit of change tracking and makes the reporting more maintainable. Make sure to follow the clean up instructions to remove unrequired resources after running the lab.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/6_consumer_validation/","title":"Amazon Redshift Consumer Cluster Validation","tags":[],"description":"","content":"Lab 6 Now, let\u0026rsquo;s validate consumer cluster access to the producer cluster datashare and objects. And then run queries from the consumer cluster against objects stored in producer cluster.\nConnect to the consumer cluster in us-west-1 to perform below steps:\nStep-1: Validate datashare is accessible Run below query to find the datashare type SELECT * FROM svv_datashares; You can see MarketingShare is an INBOUND type data share type.\nRun below query to list objects and type SELECT * FROM svv_datashare_objects; You can see a list of objects and types (schema, table etc.) shared, and all of them are as INBOUND share type.\nStep-2: Create database using datashare To consume the shared data, each consumer cluster administrator creates an Amazon Redshift database from the datashare:\nGo to the producer cluster in us-east-1 and note down cluster namespace from the Amazon Redshift cluster details page: Go to consumer cluster and create a database using the datashare and producer cluster namespace (noted from previous step): CREATE DATABASE consumer_marketing FROM DATASHARE MarketingShare of NAMESPACE \u0026#39;replace_with_your_producer_cluster_namespace\u0026#39;; Step-3: Validate access to database and objects Users and groups can list the shared objects as part of the standard metadata queries by viewing the following metadata system views and can start querying data immediately:\nRun below query to list all databases in consumer cluster: SELECT * FROM SVV_REDSHIFT_DATABASES; You can see the database type is “shared” for the “consumer_marketing” database.\nRun below query to find details of the datashare SELECT * FROM SVV_REDSHIFT_SCHEMAS WHERE database_name = \u0026#39;consumer_marketing\u0026#39;; You can see the schema type is “shared” for the schemas shared via the “consumer_marketing” database.\nRun below query to list all objects of the datashare SELECT * FROM SVV_REDSHIFT_TABLES WHERE database_name = \u0026#39;consumer_marketing\u0026#39;; Step-4: Run queries You can now run queries on the data in producer cluster, without having data stored locally on the consumer cluster. You can also run queries by joining tables from your local database, and tables shared from the producer cluster.\nSELECT COUNT(*) FROM consumer_marketing.public.lab_event; The above query fetches the total events from the lab_event table stored in the producer cluster in us-east-1 via the datashare created earlier. The table is not stored locally on the consumer cluster in us-west-1, so it reduced the data storage by half. You can also join consumer cluster locally stored tables with producer cluster shared tables in SQL queries.\nThis is in line with our Sustainability improvement goal for optimizing data patterns by removing unneeded or redundant data, and minimizing data movement across networks.\nOne key consideration is to note here is that, during the query execution, it did transfer the query processed result dataset over network, but limited to the result set (whereas the previous deployment transfers 10% of the total dataset every night part of the refresh cycle). Depending on the use case, trade-off analysis should be performed comparing daily refresh data transfer versus all queries execution data transfer over network.\nWe have now validated that the consumer cluster can access the data shared by the producer cluster, and ran queries against the producer database. Next, we will revisit metrics and KPIs to measure the sustainability optimization achieved by implementing the Amazon Redshift Data Sharing feature.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/6_consumer_validation/","title":"Amazon Redshift Consumer Cluster Validation","tags":[],"description":"","content":"Lab 6 Now, let\u0026rsquo;s validate consumer cluster access to the producer cluster datashare and objects. And then run queries from the consumer cluster against objects stored in producer cluster.\nConnect to the consumer cluster in us-west-1 to perform below steps:\nStep-1: Validate datashare is accessible Run below query to find the datashare type SELECT * FROM svv_datashares; You can see MarketingShare is an INBOUND type data share type.\nRun below query to list objects and type SELECT * FROM svv_datashare_objects; You can see a list of objects and types (schema, table etc.) shared, and all of them are as INBOUND share type.\nStep-2: Create database using datashare To consume the shared data, each consumer cluster administrator creates an Amazon Redshift database from the datashare:\nGo to the producer cluster in us-east-1 and note down cluster namespace from the Amazon Redshift cluster details page: Go to consumer cluster and create a database using the datashare and producer cluster namespace (noted from previous step): CREATE DATABASE consumer_marketing FROM DATASHARE MarketingShare of NAMESPACE \u0026#39;replace_with_your_producer_cluster_namespace\u0026#39;; Step-3: Validate access to database and objects Users and groups can list the shared objects as part of the standard metadata queries by viewing the following metadata system views and can start querying data immediately:\nRun below query to list all databases in consumer cluster: SELECT * FROM SVV_REDSHIFT_DATABASES; You can see the database type is “shared” for the “consumer_marketing” database.\nRun below query to find details of the datashare SELECT * FROM SVV_REDSHIFT_SCHEMAS WHERE database_name = \u0026#39;consumer_marketing\u0026#39;; You can see the schema type is “shared” for the schemas shared via the “consumer_marketing” database.\nRun below query to list all objects of the datashare SELECT * FROM SVV_REDSHIFT_TABLES WHERE database_name = \u0026#39;consumer_marketing\u0026#39;; Step-4: Run queries You can now run queries on the data in producer cluster, without having data stored locally on the consumer cluster. You can also run queries by joining tables from your local database, and tables shared from the producer cluster.\nSELECT COUNT(*) FROM consumer_marketing.public.lab_event; The above query fetches the total events from the lab_event table stored in the producer cluster in us-east-1 via the datashare created earlier. The table is not stored locally on the consumer cluster in us-west-1, so it reduced the data storage by half. You can also join consumer cluster locally stored tables with producer cluster shared tables in SQL queries.\nThis is in line with our Sustainability improvement goal for optimizing data patterns by removing unneeded or redundant data, and minimizing data movement across networks.\nOne key consideration is to note here is that, during the query execution, it did transfer the query processed result dataset over network, but limited to the result set (whereas the previous deployment transfers 10% of the total dataset every night part of the refresh cycle). Depending on the use case, trade-off analysis should be performed comparing daily refresh data transfer versus all queries execution data transfer over network.\nWe have now validated that the consumer cluster can access the data shared by the producer cluster, and ran queries against the producer database. Next, we will revisit metrics and KPIs to measure the sustainability optimization achieved by implementing the Amazon Redshift Data Sharing feature.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/_faq/","title":"FAQ","tags":[],"description":"","content":"Last Updated April 2022\nIf you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com My region is not covered in the \u0026lsquo;Code Bucket\u0026rsquo; Parameter Click here to expand step by step instructions We have set this up in the most common regions. If your region is not in the list please email costoptimization@amazon.com and we will create this resource for you\nCompute Optimizer Module Failing Click here to expand step by step instructions Please make sure you enable Compute Optimizer following this guide. I need to edit my StackSet Click here to expand step by step instructions If, when you deployed your StackSets, you chose to deploy to all accounts and you now wish to edit you will need to get your Root ID.\nThis can be found in your AWS Organizations part of the console.\nWhy should I have a separate cost account? Click here to expand for more information There are two main reasons for this:\nWe recommend customer avoid deploying workloads to the organization’s management account in general: \u0026ldquo;Since privileged operations can be performed within an organization’s management account and SCPs do not apply to the management account, we recommend that you limit access to an organization’s management account. You should also limit the cloud resources and data contained in the management account to only those that must be managed in the management account.\u0026rdquo; from here As there are lambda function deployed in the account these could benefit from Compute Savings plans. This means that there could be higher savings missed in other accounts because they are used on the lambdas first: \u0026ldquo;In a Consolidated Billing Family, Savings Plans are applied first to the owner account\u0026rsquo;s usage, and then to other accounts\u0026rsquo; usage. This occurs only if you have sharing enabled.\u0026rdquo; from Here This page will be updated regularly with new answers. If you have a FAQ you\u0026rsquo;d like answered here, please reach out to us here costoptimization@amazon.com .\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/end_user_computing/","title":"End User Computing","tags":[],"description":"","content":"These are queries for AWS Services under the End User Computing product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon WorkSpaces Amazon WorkSpaces - Auto Stop Amazon AppStream 2.0 Amazon WorkSpaces Query Description This query will provide unblended cost and usage information per linked account for Amazon WorkSpaces. The output will include detailed information about the resource id (WorkSpace ID), usage type, running mode, product bundle, and API operation. The cost will be summed and in descending order.\nPricing Please refer to the WorkSpaces pricing page . If you are interested in Cost Optimization, please refer to the AWS Solution, Amazon WorkSpaces Cost Optimizer .\nThis query will not run against CUR data that does not have any Amazon WorkSpaces usage.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id,\u0026#39;/\u0026#39;,2) AS split_line_item_resource_id, SPLIT_PART(product_bundle,\u0026#39;-\u0026#39;,1) AS split_product_bundle, product_operating_system, product_memory, product_storage, product_vcpu, product_running_mode, product_license, product_software_included, pricing_unit, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon WorkSpaces\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, product_bundle, product_operating_system, product_memory, product_storage, product_vcpu, product_running_mode, product_license, product_software_included, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon WorkSpaces - Auto Stop Query Description AutoStop Workspaces are cost effective when used for several hours per day. If AutoStop Workspaces run for more than 80 hrs per month it is more cost effective to switch to AlwaysOn mode. This query shows AutoStop Workspaces which ran more that 80 hrs in previous month. If the usage pattern for these Workspaces is the same month over month it\u0026rsquo;s possible to optimize cost by switching to AlwaysOn mode. For example, Windows PowerPro (8 vCPU, 32GB RAM) bundle in eu-west-1 runs for 400 hrs per month. In AutoStop mode it costs $612/month ($8.00/month + 400 * $1.53/hour) while if used in AlwaysOn mode it would cost $141/month.\nPricing Please refer to the WorkSpaces pricing page . If you are interested in Cost Optimization, please refer to the AWS Solution, Amazon WorkSpaces Cost Optimizer .\nThis query will not run against CUR data that does not have any Amazon WorkSpaces usage.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;/\u0026#39;, 2) AS split_line_item_resource_id, product_region, product_operating_system, product_bundle_description, product_software_included, product_license, product_rootvolume, product_uservolume, pricing_unit, sum_line_item_usage_amount, CAST(total_cost_per_resource AS DECIMAL(16, 8)) AS \u0026#34;sum_line_item_unblended_cost(incl monthly fee)\u0026#34; FROM ( SELECT bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, product_operating_system, pricing_unit, product_region, product_bundle_description, product_rootvolume, product_uservolume, product_software_included, product_license, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS total_cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_amount_per_resource_and_pricing_unit FROM $ {table_name} WHERE line_item_product_code = \u0026#39;AmazonWorkSpaces\u0026#39; -- get previous month AND CAST(month AS INT) = CAST(month(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS INT) -- get year for previous month AND CAST(year AS INT) = CAST(year(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS INT) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; AND line_item_usage_type LIKE \u0026#39;%AutoStop%\u0026#39; GROUP BY line_item_usage_account_id, line_item_resource_id, product_operating_system, pricing_unit, product_region, product_bundle_description, product_rootvolume, product_uservolume, bill_payer_account_id, product_software_included, product_license ) WHERE -- return only workspaces which ran more than 80 hrs usage_amount_per_resource_and_pricing_unit \u0026gt; 80 ORDER BY total_cost_per_resource DESC, line_item_resource_id, line_item_usage_account_id, product_operating_system, pricing_unit; Help \u0026amp; Feedback Back to Table of Contents Amazon AppStream 2.0 Query Description This query will provide unblended cost and usage information per linked account for Amazon AppStream 2.0. The output will include detailed information about the product family, product instance type, pricing unit, region along with usage amount. The cost will be summed and in descending order.\nPricing Please refer to the Amazon AppStream 2.0 pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, product_product_family, -- Stopped Instance, Streaming Instance, User Fees product_instance_type, -- stream.TYPE.SIZE pricing_unit, -- Hrs, Users product_region, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, CASE line_item_line_item_type WHEN \u0026#39;DiscountedUsage\u0026#39; THEN SUM(CAST(reservation_effective_cost AS DECIMAL(16,8))) WHEN \u0026#39;Usage\u0026#39; THEN SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) ELSE SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) END AS sum_line_item_unblended_cost_reservation_effective_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon AppStream\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), product_product_family, product_instance_type, pricing_unit, product_region, line_item_line_item_type ORDER BY month_line_item_usage_start_date, sum_line_item_usage_amount DESC, sum_line_item_unblended_cost_reservation_effective_cost, product_product_family; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/7_create_sns_topic/","title":"Creating a Simple Notification Service Topic","tags":[],"description":"","content":"Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.\n6.1 Create and Subscribe to an SNS Topic To create and subscribe to an SNS topic:\nNavigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic. In the Create new topic window: In the Type field, select Standard. In the Topic name field, enter AdminAlert. In the Display name field, enter AdminAlert. Choose Create topic. On the Topic details: AdminAlert page, choose Create subscription. In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription. You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN. You can now use this SNS topic to send notifications to your Administrator user.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/7_cost_explorer/","title":"Enable AWS Cost Explorer","tags":[],"description":"","content":"AWS Cost Explorer has an easy-to-use interface that lets you visualize, analyze, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts, once enabled it is enabled for ALL accounts and controlled through IAM policies. The default configuration of Cost Explorer is free, however we will enable hourly granularity, which incurrs an additional cost - AWS Cost Management Pricing .\nLog in to your Management account with administrative privileges, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer: You will receive notification that Cost Explorer has been enabled, and data will be populated: After 24hrs, go into Cost Explorer: Click Settings in the top right: Select Hourly and Resource Level Data, and click Save: This will incur costs depending on the number of resources you are running.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/7_export_to_s3/","title":"Export Logs to S3","tags":[],"description":"","content":"After collecting logs, you may want to export logs from CloudWatch to an S3 Bucket. This is useful as storing data in S3 is more cost effective and reliable than storing it in CloudWatch, making S3 a good option for long-term storage and archival of log files.\nOpen up the CloudWatch console . On the left side menu, choose Log groups under Logs. On that screen, enter securitylablogs in the search bar. Click on the log group that appears in the results. Click Actions and Export data to Amazon S3 in the top menu. You will have to fill out information about what data to export.\nIn the From field, set the YYYY/MM/DD field to today’s date (the date you are doing this lab). This is the earliest creation date of logs you want to export. In the To field, set set the YYYY/MM/DD field to tomorrow’s date (the date after the day you are doing this lab). This is the latest creation date of logs you want to export. Leave the Stream prefix field blank, as we want to export all logs. This field allows you to select which logs you want to export. Set S3BucketName to the bucket name you entered in your CloudFormation stack, likely wa-lab-\u0026lt;your-account-id\u0026gt;-\u0026lt;date\u0026gt;. This is the bucket your logs will be exported to. Set S3 bucket prefix to lablogs. This is the subdirectory your exported logs will be stored in. Click Export\nClick on the View export tasks in the pop up box that appears. This will bring you to a list of Export tasks performed from CloudWatch\nClick the radio bubble next to the most recent export. Click View in Amazon S3 to open these logs in the S3 bucket you created. You should now see folders corresponding to all of the log streams you viewed earlier. You can explore these logs and download the .gz files if you’d like to see their contents. Recap: In this portion of the lab, you exported logs from CloudWatch to S3, a good way to archive logs for long term storage. This demonstrates an important component of the security best practice of “configuring logging centrally” - the ability to extract meaningful insights from large volumes of log data. Compared with CloudWatch, storing log files in S3 is more cost-effective and allows you to use lifecycle policies on your stored logs. As the volume of logs generated by your workloads increase, so does the value of storing these data in S3. It also enables you to analyze logs from Athena, as you will see in the next section.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_5_cost_visualization/7_tear_down/","title":"Tear down","tags":[],"description":"","content":"We will delete both custom reports that were created.\nClick on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete: Verify the names of the reports you are going to delete, click Delete: The reports are no longer listed in the reports available: X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST1 - \u0026ldquo;How do you implement cloud financial management?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/7_tear_down/","title":"Tear Down","tags":[],"description":"","content":"We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab.\n7.1 Sub Account 1 - Log into the sub account as an IAM user with the required privileges\n2 - Go to the Glue service dashboard\n3 - Delete the created database and tables\n4 - Delete the recurring Glue crawler\n7.2 Management/Payer Account 1 - Log into the management/payer account as an IAM user with the required privileges\n2 - Go to the Cloudformation service dashboard, and select the CUR update stack\n3 - Update the stack and use the original Template yml file\n4 - Go to the Lambda service dashboard\n5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions\n6 - Go to the IAM service dashboard\n7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles\n8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies\n9 - Go to the Athena service dashboard\n10 - Delete the create_linked_ and delete_linked_ Athena saved queries\n11 - Delete any temp tables\n12 - Go into the S3 service dashboard\n13 - Delete the S3 output folder\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_deploy_and_update_cloudformation/7_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"When a CloudFormation stack is deleted, CloudFormation will automatically delete the resources that it created.\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor:\nThere is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account:\nYou may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Remove AWS CloudFormation provisioned resources You will now delete the CloudFormationLab CloudFormation stack.\nHow to delete an AWS CloudFormation stack Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events How to delete an AWS CloudFormation StackSet Go to the AWS CloudFormation StackSets console Select the StackSet you wish to delete Click on Actions and then Delete stacks from StackSet. Under Accounts, select Deploy stacks in accounts under Deployment locations. Under Account numbers enter the 12 digit AWS account ID for the account you are using for this lab. You can find this by clicking on the user/role drop down you have logged into the account with on the top right corner. For Specify regions click on Add all regions. This will automatically select the AWS Regions that the StackSet deployed stacks into. Click Next. Click Submit on the Review page. StackSets will now delete the individual stacks that were created in the different accounts/regions. The operation takes about 3 minutes to complete and the Status to change to SUCCEEDED. After the stacks have been deleted, click on Actions on the top right corner and then click on Delete StackSet. References \u0026amp; useful resources What is AWS CloudFormation? CloudFormation Resource and property reference Working with AWS CloudFormation StackSets X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 8 How do you implement change?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_automated_cur_query_and_email_delivery/7_tear_down/","title":"Teardown","tags":[],"description":"","content":" Delete IAM role Lambda_Auto_CUR_Delivery_Role and policy Lambda_Auto_CUR_Delivery_Access Delete Lambda function Auto_CUR_Delivery Delete CloudWatch event 5_min_auto_cur_delivery Delete SES configuration Delete S3 bucket for CUR query results storing X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with COST 3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_basic_ec2_with_waf_protection/","title":"Level 200: Basic EC2 Web Application Firewall Protection","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes.\nNOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints . Steps: Launch Instance Create AWS WAF Rules Create Application Load Balancer with WAF integration Tear down References \u0026amp; useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_control_human_access/","title":"Quest: Control Human Access","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity \u0026amp; Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.\nBasic Identity and Access Management User, Group, Role IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.\nIAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.\nIAM Tag Based Access Control for EC2 "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_cost_estimation/","title":"Level 100: Cost Estimation","tags":[],"description":"","content":"Last Updated June 2021\nAuthors Shankar Ramachandran, Principal SA, Cost Optimization Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to estimate AWS costs for a sample workload - 3 tier LAMP (Linux Apache MySQL PHP) stack based Web Application. The skills you learn will help you use the AWS Pricing Calculator to estimate costs for your workloads.\nGoals Learn how to estimate AWS Costs using AWS Pricing Calculator Prerequisites None Permissions required None Costs There are no costs for this lab Time to complete N/A Steps: Launch AWS Pricing Calculator Add \u0026amp; Configure Services Review Estimate Export Estimate Save and Share X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/","title":"Level 100: Goals and Targets","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Last Updated February 2021\nAuthors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This lab will be used as a collaboration space for people to learn, contribute and discuss Cost Optimization goals. The goals are categorized according to the W-A questions and best practices. Each goal has a target and supporting information to help explain the goal/target and how you can adapt and use it best for your organization, along with any potential caveats.\nThe goals and targets listed should be treated as suggestions (some stronger than others!), to help promote thought \u0026amp; discussion - not necessarily direct implementation in your organization.\nThere is also an optional space to add contacts for the contributor, if you wish to be contacted and open for a discussion you can use this field.\nGoals Create goals and targets for your organization Prerequisites None Permissions required None Costs There are no costs for this lab Time to complete N/A Steps: Cloud Financial Management Govern Usage Monitor Cost and Usage Decommission Resources Service Selection Resource Type, Size \u0026amp; Number Pricing Models Data Transfer Manage Demand \u0026amp; Supply Resources Evaluate New Services X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/7_pricing_models/","title":"Pricing Models","tags":[],"description":"","content":"Pricing Models Savings Plans Purchasing Savings Plans Goal: Minimize resource cost by using Savings Plans Target: Unpurchased Savings Plans offering more than 10% discount not to exceed $100 Best Practice: Commitment Discounts - Savings Plans Measures: $ value of unpurchased plans above 10% discount Good/Bad: Good Why? When does it work well or not?: Maximizes high quality, low risk SP consumption, \u0026amp; ensures SP\u0026rsquo;s that dont make business sense are not purchased Contact/Contributor: natbesh@amazon.com Purchasing Savings Plans Goal: Minimize resource cost by using Savings Plans Target: Ensure 80% of my on-demand usage is covered with Savings Plans Best Practice: Commitment Discounts - Savings Plans Measures: Coverage % Good/Bad: Bad Why? When does it work well or not?: It ignores the return on investment in SP \u0026amp; the risk. If you run \u0026lt;100% uptime or small resources with licensed operating systems and/or software, there can be low returns \u0026amp; also high risks. It can force a purchase which would be a bad business decision Contact/Contributor: natbesh@amazon.com Reserved Instances Purchase Reserved Instances Goal: Minimize resource cost by using Reserved Instances Target: Unpurchased Reserved Instances offering more than 10% discount, with a pay-off period of less than 9 months must not exceed $300 Best Practice: Commitment Discounts - Reserved Instances Measures: $ value of unpurchased RI\u0026rsquo;s above 10% discount that pay off in less than 9 months Good/Bad: Good Why? When does it work well or not?: Maximizes high quality, low risk RI consumption, \u0026amp; ensures RI\u0026rsquo;s that dont make business sense are not purchased Contact/Contributor: natbesh@amazon.com Purchase Reserved Instances Goal: Maximize discounts by maximizing use of purchased Reserved Instances Target: Have cost of low RI utilization below $100 for any purchased RI Best Practice: Commitment Discounts - Reserved Instances Measures: Wasted $ per RI Good/Bad: Good Why? When does it work well or not?: Minimizes waste from unused RI\u0026rsquo;s, it ensures you only act when it makes business sense \u0026amp; effort can be recovered by making a change. Contact/Contributor: natbesh@amazon.com Purchase Reserved Instances Goal: Maximize discounts by maximizing use of purchased Reserved Instances Target: Have RI utilization above 80% Best Practice: Commitment Discounts - Reserved Instances Measures: RI utilization % Good/Bad: Bad Why? When does it work well or not?: High amounts of effort required to know whether you\u0026rsquo;re actually losing money, and how much money. It can work in environments that have only a few different resource types running. This target can lead to large effort resulting in minimal gains or even a loss. Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/8_failure_injection_optional/","title":"Test Resiliency Using Failure Injection - Optional steps","tags":[],"description":"","content":"The following are optional lab steps you may run to further explore failure injection\n8.1 S3 failure injection Failure of S3 means that the image will not be available You may ONLY do this testing if you supplied your own websiteimage reference to an S3 bucket you control 8.1.1 Bucket name You will need to know the bucket name where your image is. For example if the websiteimage value you supplied was \u0026quot;https://s3.us-east-2.amazonaws.com/my-awesome-bucketname/my_image.jpg\u0026quot;, then the bucket name is my-awesome-bucketname\nFor this failure simulation it is most straightforward to use the AWS Console as follows. (If you are interested in doing this using the AWS CLI then see here - choose either AWS Console or AWS CLI)\n8.1.2 AWS Console Navigate to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the object, then select the \u0026ldquo;Permissions\u0026rdquo; tab Select the \u0026ldquo;Public Access\u0026rdquo; radio button, and deselect the \u0026ldquo;Read object\u0026rdquo; checkbox and Save To re-enable access (after testing), do the same steps, tick the \u0026ldquo;Read object\u0026rdquo; checkbox and Save 8.1.3 System response to S3 failure What is the expected effect? How long does it take to take effect?\nNote that due to browser caching you may still see the image on refreshing the site. On most systems Shift-F5 does a clean refresh with no cache How would you diagnose if this is a larger problem than permissions?\n8.2 More testing you can do You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal the failure modes.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_8_tag_policies/","title":"Level 100: Tag Policies","tags":[],"description":"","content":"Last Updated May 2021\nAuthors Tony Weldon, Commercial Architect Stephanie Gooch, Commercial Architect (AWS) Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This lab will help you to develop Tagging Policies utilizing your AWS Organization. Tags allow you to label resources using key-value pairs, which can be used to segment or identify resources by dimensions such as owner, cost center, or environment. Tag Policies help to ensure that the AWS users within your AWS Organization are tagging resources consistently and aligned to the strategy your Organization has defined. Tags can serve many purposes, but this lab will be focused specifically on using tags to distribute the cost of AWS services.\nEffective tagging requires standardization. Each technology team in your organization deploying AWS resources needs to apply tags consistent with how resources are expected to be categorized. When this doesn’t happen, it often leads to missing tags, misnamed tags, and inconsistent tags that ultimately make them an unreliable source of resource categorization.\nStandardization can be challenging when you have a large organization with multiple technology teams and AWS accounts. Tools that automate the enforcement of applying the proper tags, like Tag Policies, help prevent and remediate tags that don’t comply with the standard.\nAWS Cost Explorer and Cost and Usage Report provide the ability to break down AWS costs by tag. Typically, customers use business tags such as cost center, business unit, or project to associate AWS costs with traditional financial reporting dimensions within their organization. Tags are also a great way to enforce a governance model within an Organization, determining who in the Organization can access which resources.\nGoals Create a Tag policy Visualize resources not in compliance with the policy Prerequisites AWS Account Setup has been completed AWS organization must have all features enabled - If you aren’t sure if all features is enabled, please reference this documentation. Must be signed into organization\u0026rsquo;s Management account Permissions required Access to the Cost Optimization team created in AWS Account Setup Costs https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed Time to complete The lab should take approximately 15 minutes to complete Steps: Create a Tag Policy Attach the Tag Policy to an account Check for non-compliant resources Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/verify-failover/","title":"Verify Failover","tags":[],"description":"","content":"If you go back and refresh your browser (using the CloudFront Distribution’s Domain Name from before), you should now see The Unicorn Shop - us-west-1 website. You might need to do a hard-page refresh (using CTRL+F5). The user’s session should still be active, and their cart still contains the products previously added.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_fault_isolation_with_shuffle_sharding/8_resources/","title":"References &amp; useful resources","tags":[],"description":"","content":" AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328) Amazon Builders\u0026rsquo; Library: Workload isolation using shuffle sharding AWS re:Invent 2018: Architecture Patterns for Multi-Region Active-Active Applications (ARC209-R2) AWS re:Invent 2018: How AWS Minimizes the Blast Radius of Failures (ARC338) AWS re:Invent 2019: Innovation and operation of the AWS global network infrastructure (NET339) AWS Well-Architected Best Practices Use bulkhead architectures : Like the bulkheads on a ship, this pattern ensures that a failure is contained to a small subset of requests/users so that the number of impaired requests is limited, and most can continue without error. Bulkheads for data are usually called partitions or shards, while bulkheads for services are known as cells.\nThank you for using this lab. X Congratulations! With completion of this lab you have learned several best practices. Consider how you can implement these, and update the Well-Architected Review for your workloads: REL 10 How do you use fault isolation to protect your workload? - Use bulkhead architectures\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/cleanup/","title":"Cleanup","tags":[],"description":"","content":"To cleanup the resources created by this lab, please follow these steps:\nDelete the AWS Cost \u0026amp; Usage Report in the billing console if you have created one for Lab 1.1 . Empty and delete the Amazon S3 bucket if you have created one to store the sample CUR data for Lab 1.1 . Go to the AWS CloudFormation console in the region in which you deployed the AWS Serverless Application Repository application in Lab 1.4 . Delete the stack called serverlessrepo-aws-usage-queries. For Lab 3 , run cdk destroy in the directory where cdk.json is. X Congratulations! You should now have a firm understanding of using proxy metrics to inform environmental sustainability improvements.\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/7_review_sustainability_kpi_optimization/","title":"Review Sustainability KPI Optimization","tags":[],"description":"","content":"Lab 7 With optimization completed in the previous lab, let’s follow below steps on the consumer cluster to measure the improvement on our sustainability KPI:\nConnect to the consumer cluster in us-west-1 region, and click/expand on “consumer_marketing”. You can see in the tool tip that it is a datashare which is connected to a producer cluster. All the tables listed below are from producer cluster. Now, let check if the consumer cluster is storing any tables locally in the database by running below query: SELECT SUM(size) FROM SVV_TABLE_INFO WHERE \u0026#34;table\u0026#34; NOT LIKE \u0026#39;%auto%\u0026#39;; The above query did not return any table size, which means that the consumer database does not have any tables locally, hence not consuming any data storage to store the tables.\nWith, that below are revised (improved) metrics and KPI:\nData storage Total data storage consumed (provisioned) by two clusters (producer and consumer) = 640+0 = 640MB Per event data storage = 640MB / 8798 events = 0.07MB Data transfer Total data transfer over network = per use case, and amount of data processed by queries executed in Consumer cluster Per event daily data transfer = total data processed by query / 8798 events For per event data storage sustainability KPI, we see there is 50% reduction (improvement) by using the Amazon Redshift Data Sharing feature.\nFor per event data transfer KPI, trade-off analysis should be performed comparing daily refresh data transfer vs. all queries execution dataset transfer over network. One option is to analyze data transfer between regions is using AWS Cost Explorer. Refer to this AWS blog post explaining how to use AWS Cost Explorer to analyze data transfer volume and cost.\nWith the above comparison of the baseline \u0026amp; revised metrics and KPI, we tested how the Amazon Redshift Data Sharing feature helped to improve our sustainability KPI for AnyCompany\u0026rsquo;s Marketing data warehouse environment.\nNext, we will cleanup the AWS resources created to make sure no further costs are incurred.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/7_review_sustainability_kpi_optimization/","title":"Review Sustainability KPI Optimization","tags":[],"description":"","content":"Lab 7 With optimization completed in the previous lab, let’s follow below steps on the consumer cluster to measure the improvement on our sustainability KPI:\nConnect to the consumer cluster in us-west-1 region, and click/expand on “consumer_marketing”. You can see in the tool tip that it is a datashare which is connected to a producer cluster. All the tables listed below are from producer cluster. Now, let check if the consumer cluster is storing any tables locally in the database by running below query: SELECT SUM(size) FROM SVV_TABLE_INFO WHERE \u0026#34;table\u0026#34; NOT LIKE \u0026#39;%auto%\u0026#39;; The above query did not return any table size, which means that the consumer database does not have any tables locally, hence not consuming any data storage to store the tables.\nWith, that below are revised (improved) metrics and KPI:\nData storage Total data storage consumed (provisioned) by two clusters (producer and consumer) = 640+0 = 640MB Per event data storage = 640MB / 8798 events = 0.07MB Data transfer Total data transfer over network = per use case, and amount of data processed by queries executed in Consumer cluster Per event daily data transfer = total data processed by query / 8798 events For per event data storage sustainability KPI, we see there is 50% reduction (improvement) by using the Amazon Redshift Data Sharing feature.\nFor per event data transfer KPI, trade-off analysis should be performed comparing daily refresh data transfer vs. all queries execution dataset transfer over network. One option is to analyze data transfer between regions is using AWS Cost Explorer. Refer to this AWS blog post explaining how to use AWS Cost Explorer to analyze data transfer volume and cost.\nWith the above comparison of the baseline \u0026amp; revised metrics and KPI, we tested how the Amazon Redshift Data Sharing feature helped to improve our sustainability KPI for AnyCompany\u0026rsquo;s Marketing data warehouse environment.\nNext, we will cleanup the AWS resources created to make sure no further costs are incurred.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/","title":"Level 300: Optimization Data Collection","tags":[],"description":"","content":"Last Updated May 2022\nAuthors Stephanie Gooch, Commercial Architect (AWS) Yuriy Prykhodko, Sr. Technical Account Manager (AWS) Iakov Gan, Sr. Technical Account Manager (AWS) Andy Brown, OPTICS Manager - Commercial Architects IBUs Feedback If you wish to provide feedback on this lab, report an error, or you have a suggestion, please email: costoptimization@amazon.com Introduction Amazon Web Services offers a broad set of global cloud-based products including compute, storage, databases, analytics, networking, mobile, developer tools, management tools, security and enterprise applications. These services help organizations move faster, lower IT costs and scale.\nThis lab is designed to enable you to collect utilization data from different AWS services to help you identify optimization opportunities. This lab provides set of optional modules to automate data collection and explains how to create custom modules to pull additional data sets.\nModules The main sources of the data used in optional modules:\nTrusted Advisor Module collects AWS Trusted Advisor results. Rightsizing Recommendations Module collects Cost Explorer Rightsizing Recommendations (for EC2 only). Compute Optimizer Module collects AWS Compute Optimizer right sizing recommendations (EC2, ASG, EBS and Lambda) Inventory Module collects Amazon EC2 service inventories like Amazon EBS volumes, snapshots and AMIs ECS Chargeback Module collects Amazon Elastic Container Service chargeback data RDS Utilization Module collects Amazon Relational Database Service utilization data AWS Organization Module exports data about AWS Organizations AWS Budgets Module uses AWS Budgets Export Modules can be installed in any combination and can be added post install using update of the CloudFormation stack. Detailed description of each module can be found here .\nArchitecture Resources for this lab deployed with AWS CloudFormation:\nOptimization Data Collection Stack deploys core resources for the lab and allows to choose which data collection modules to deploy. Each data collection module is optional. We recommend to deploy this stack in separate optimization data collection AWS account in order to limit number of assets in the Management account. Optimization Management Data Role Stack deploys AWS IAM Role for AWS Lambda which allows read-only access to retrieve linked accounts information from AWS Organizations. This stack should be deployed in Management AWS account (some time also referenced as Governance, Master or Payer account). Optimization Data Collection StackSet deploys IAM role required for AWS Lambda to get optimization data for each module. StackSet should be deployed from either organization\u0026rsquo;s management account or a delegated administrator account to all linked accounts in organization. Resources deployed with Optimization Data Collection Stack launch following workflow:\nCollecting linked account information:\nAmazon EventBridge rule invokes account collector AWS Lambda based on schedule in optimization data collection account. By default schedule triggers Lambda function every 14 days and can be adjusted if needed. The Lambda function assumes AWS Identity and Access Management (IAM) role in management account, retrieves linked accounts ids and names via AWS Organizations SDK and sends them to Amazon Simple Queue Service (SQS) queues for every deployed data collection module. Collecting optimization data from linked accounts:\nMessages in SQS queue trigger Lambda functions for each data collection module Each data collection module Lambda function assumes IAM role in linked accounts listed in SQS messages and retrieves respective optimization data via AWS SDK for Python . Collected data stored in data collection Amazon S3 bucket Analyzing and visualizing optimization data:\nOnce data stored in S3 bucket, Lambda function triggers AWS Glue crawler which creates or updates table in Glue Data Catalog Optimization data can be queried and analyzed with Amazon Athena or visualized with Amazon QuickSight to get optimization recommendations It is possible to deploy Optimization Data Collection Stack to organization\u0026rsquo;s management account. Deployment in such case will look like on architecture diagram below Architecture for management account deployment Goals Deploy core resources and data collection modules Collect optimization data in S3 bucket Query and analyze optimization data with Amazon Athena or visualize it with Amazon QuickSight Prerequisites Access to the Management AWS Account of the AWS Organization to deploy Cloudformation Access to a sub account within the Organization - referred to as Cost Optimization Account Completed the Account Setup Lab Inventory, ECS Chargeback and Trusted Advisor Modules of this Lab require lambda code that only available in a limited number of regions (eu-west-1,us-west-2,us-east-2,us-east-1,us-west-1,us,ap-southeast-1,eu-central-1,eu-west-2,eu-north-1,ap-southeast-2). Please make sure you choose to use these regions to install the Data Collection stack. Trusted Advisor Modules will require a Business, Enterprise On-Ramp, or Enterprise Support plan. Deployment Options We suggest you do not deploy the main resources and collectors into your management account and instead use the cost account created in Account Setup Lab . However, Some IAM resources will be needed to read data from the management account.\nPermissions required Be able to create the below in the management account:\nIAM role and policy Deploy CloudFormation Stacks and StackSets Be able to create the below in a sub account where your CUR data is accessible:\nDeploy CloudFormation Stacks and StackSets Amazon S3 Bucket AWS Lambda function IAM role and policy Amazon CloudWatch trigger Amazon Athena Table AWS Glue Crawler Enroll into Compute Optimization Costs Estimated costs should be \u0026lt;$5 a month for small Organization Time to complete 30 minutes Steps: Grant permissions to your accounts in your AWS Organization Deploy core infrastructure for data retrieval Data Collection Modules and Testing Deployment Utilize Data Create Custom Data Collection Module (Optional) Teardown FAQ X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/","title":"Level 300: Organization Data CUR Connection","tags":[],"description":"","content":"Last Updated December 2020\nAuthors Stephanie Gooch, Commercial Architect (AWS) Feedback If you wish to provide feedback on this lab, there is an error, or you have a suggestion, please email: costoptimization@amazon.com Introduction This lab will show you how to combine your organizations information with your AWS Cost \u0026amp; Usage Report, this will enable you to view cost \u0026amp; usage in a way that is more relevant to your organization. It will guide you through the process of setting up an AWS Lambda function to extract the data from AWS Organizations, such as account ID, account name, organization parent and specified tags. This will then be place into Amazon S3. From there, Amazon Athena will be able to read this data to produce a table that can be connected to your AWS Cost \u0026amp; Usage Report to enrich it. This can be deployed manually or through AWS CloudFormation. We also now offer a terraform module to deploy this code.\nNOTE: If you are thinking about combining your AWS Organizations data with other sources of data such as your Cost \u0026amp; Usage Report, Trusted Advisor, Compute Optimizer, etc. (creating a datalake) and/or using the Cloud Intelligence Dashboards dashboards then we recommend using the 300 Optimization Data Collection Lab. This lab contains the setup of this module, plus many more data collectors.\nArchitecture Goals Combine your AWS Organizations information with your CUR Allows you to view costs against accounts with names you provide enriching the data Prerequisites Access to the management AWS Account of the AWS Organization to deploy a cross account role A sub account within the Organization Completed the Account Setup Lab here Completed the Cost and Usage Analysis lab here Completed the Cost Visualization Lab here Deployment Options We suggest you do not deploy resources into your management account and instead use the cost account created here . However, there are options to deploy all resources into your management account if you wish. To do this complete all steps in you management account and do not create the role in the \u0026lsquo;Create IAM Role and Policies in Management account\u0026rsquo; step.\nPermissions required Be able to create the below in the management account:\nIAM role and policy Be able to create the below in a sub account where your CUR data is accessible:\nAmazon S3 Bucket AWS Lambda function IAM role and policy Amazon CloudWatch trigger Amazon Athena Table Optional Completed the Enterprise Dashboards lab here . Costs Estimated costs should be \u0026lt;$5 a month for small Organization Amazon QuickSight pricing Time to complete 30 minutes Steps: Create Static Resources Create Automation Resources Utilize Organization Data Source Visualize Organization Data in QuickSight Join with the Enterprise Cost Intelligence Dashboard Teardown If you wish to deploy this in your AWS Management Account instead of a Member Account then there is a separate code version which can do this. You do not have to make the IAM Role and Policy made in the Management account in the next step.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/global/","title":"Global Queries","tags":[],"description":"","content":"These are queries which return information about global usage.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Account Region Service Bill Details by Service Premium Support Chargeback by Accounts Unblended Cost by Charge Type Serverless Product Spend Amortized Cost By Charge Type Account Query Description This query will provide monthly unblended and amortized costs per linked account for all services. The query includes ri_sp_trueup and ri_sp_upfront_fees columns to allow you to visualize the difference between unblended and amortized costs. Unblended cost equals usage plus upfront fees. Amortized cost equals usage plus the portion of upfront fees applicable to the period (both used and unused). True-up therefore represents the difference between total upfront fees incurred in the period using an unblended/cash-based accounting model, and the smaller portion of upfront fees applicable to the period using an amortized/accrual-based accounting model. This query excludes discounts, credits, refunds and taxes, as well as Route 53 Domains usage type due to differences in how usage date is recorded between Cost Explorer and CUR.\nNotes:\nCharges for the current billing month may differ slightly when comparing between Cost Explorer and CUR due to differences in how current month charges are estimated . Charges will match between Cost Explorer and CUR once billing has been finalized at the close of the month.\nThis query expects that you have reserved instances purchased within at least one of the accounts. Running this query as is without reserved instances in the CUR data set will result in an error. To use this query without reserved instances, remove or comment out lines containing reservation_reservation_a_r_n.\nPricing Please refer to the AWS pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Amortized Cost Link Sample Output: Download SQL File: Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-01\u0026#39;) AS month_line_item_usage_start_date, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) AS amortized_cost, (SUM(line_item_unblended_cost) - SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) ) AS ri_sp_trueup, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN line_item_unblended_cost WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND line_item_usage_type != \u0026#39;Route53-Domains\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;,\u0026#39;SavingsPlanNegation\u0026#39;,\u0026#39;SavingsPlanRecurringFee\u0026#39;,\u0026#39;SavingsPlanUpfrontFee\u0026#39;,\u0026#39;RIFee\u0026#39;,\u0026#39;Fee\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, 3 ORDER BY month_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Region Query Description This query will provide monthly unblended and amortized costs per linked account for all services by region where the service is operating. The query includes ri_sp_trueup and ri_sp_upfront_fees columns to allow you to visualize the difference between unblended and amortized costs. Unblended cost equals usage plus upfront fees. Amortized cost equals usage plus the portion of upfront fees applicable to the period (both used and unused). True-up therefore represents the difference between total upfront fees incurred in the period using an unblended/cash-based accounting model, and the smaller portion of upfront fees applicable to the period using an amortized/accrual-based accounting model. This query excludes discounts, credits, refunds and taxes, as well as Route 53 Domains usage type due to differences in how usage date is recorded between Cost Explorer and CUR.\nNotes:\nCharges for the current billing month may differ slightly when comparing between Cost Explorer and CUR due to differences in how current month charges are estimated . Charges will match between Cost Explorer and CUR once billing has been finalized at the close of the month.\nThis query expects that you have reserved instances purchased within at least one of the accounts. Running this query as is without reserved instances in the CUR data set will result in an error. To use this query without reserved instances, remove or comment out lines containing reservation_reservation_a_r_n.\nPricing Please refer to the AWS pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Amortized Cost Link Sample Output: Download SQL File: Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-01\u0026#39;) AS month_line_item_usage_start_date, CASE product_region WHEN NULL THEN \u0026#39;Global\u0026#39; WHEN \u0026#39;\u0026#39; THEN \u0026#39;Global\u0026#39; WHEN \u0026#39;global\u0026#39; THEN \u0026#39;Global\u0026#39; ELSE product_region END AS product_region, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) AS amortized_cost, (SUM(line_item_unblended_cost) - SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) ) AS ri_sp_trueup, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN line_item_unblended_cost WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND line_item_usage_type != \u0026#39;Route53-Domains\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;,\u0026#39;SavingsPlanNegation\u0026#39;,\u0026#39;SavingsPlanRecurringFee\u0026#39;,\u0026#39;SavingsPlanUpfrontFee\u0026#39;,\u0026#39;RIFee\u0026#39;,\u0026#39;Fee\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, 4; Help \u0026amp; Feedback Back to Table of Contents Service Query Description This query will provide monthly unblended and amortized costs per linked account for all services by service. Data Transfer is also broken out for each service. The query includes ri_sp_trueup and ri_sp_upfront_fees columns to allow you to visualize the difference between unblended and amortized costs. Unblended cost equals usage plus upfront fees. Amortized cost equals usage plus the portion of upfront fees applicable to the period (both used and unused). True-up therefore represents the difference between total upfront fees incurred in the period using an unblended/cash-based accounting model, and the smaller portion of upfront fees applicable to the period using an amortized/accrual-based accounting model. This query excludes discounts, credits, refunds and taxes, as well as Route 53 Domains usage type due to differences in how usage date is recorded between Cost Explorer and CUR.\nNotes:\nCharges for the current billing month may differ slightly when comparing between Cost Explorer and CUR due to differences in how current month charges are estimated . Charges will match between Cost Explorer and CUR once billing has been finalized at the close of the month.\nThis query expects that you have reserved instances purchased within at least one of the accounts. Running this query as is without reserved instances in the CUR data set will result in an error. To use this query without reserved instances, remove or comment out lines containing reservation_reservation_a_r_n.\nPricing Please refer to the AWS pricing page .\nCost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nUnblended Cost Link Amortized Cost Link Sample Output: Download SQL File: Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-01\u0026#39;) AS month_line_item_usage_start_date, CASE WHEN (line_item_line_item_type = \u0026#39;Usage\u0026#39; AND product_product_family = \u0026#39;Data Transfer\u0026#39;) THEN CONCAT(\u0026#39;DataTransfer-\u0026#39;,line_item_product_code) ELSE line_item_product_code END AS service_line_item_product_code, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) AS amortized_cost, (SUM(line_item_unblended_cost) - SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) ) AS ri_sp_trueup, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN line_item_unblended_cost WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees FROM ${table_name} WHERE ${date_filter} AND line_item_usage_type != \u0026#39;Route53-Domains\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;,\u0026#39;SavingsPlanNegation\u0026#39;,\u0026#39;SavingsPlanRecurringFee\u0026#39;,\u0026#39;SavingsPlanUpfrontFee\u0026#39;,\u0026#39;RIFee\u0026#39;,\u0026#39;Fee\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, 3, 4 ORDER BY month_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Bill Details by Service Query Description This query will provide a monthly cost summary by AWS Service Charge which is an approximation to the monthly bill in the billing console.\nPricing Please refer to the AWS pricing page .\nSample Output: Download SQL File: Link to file Copy Query SELECT DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-01\u0026#39;) AS month_line_item_usage_start_date, bill_bill_type, CASE WHEN (product_product_family = \u0026#39;Data Transfer\u0026#39;) THEN \u0026#39;Data Transfer\u0026#39; ELSE replace(replace(replace(product_product_name, \u0026#39;Amazon \u0026#39;),\u0026#39;Amazon\u0026#39;),\u0026#39;AWS \u0026#39;) END AS product_product_name, product_location, line_item_line_item_description, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(line_item_usage_amount) AS sum_line_item_usage_amount FROM ${table_name} WHERE ${date_filter} GROUP BY 1, bill_bill_type, 3, product_location, line_item_line_item_description HAVING SUM(line_item_usage_amount) \u0026gt; 0 ORDER BY month_line_item_usage_start_date, bill_bill_type, product_product_name, product_location, line_item_line_item_description; Help \u0026amp; Feedback Back to Table of Contents Premium Support Chargeback by Accounts Query Description This query will provide a monthly individual account chargeback for the premium support cost based on its contribution to overall AWS bill. This query computes the total monthly aws bill (without tax and support charges) and then calculates just the support charges. Based on the Individual accounts usage/spend percentage, its equivalent support fee is computed.\nPricing Please refer to the AWS pricing page .\nSample Output: Download SQL File: Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, ROUND(total_support_cost *((SUM(line_item_unblended_cost)/total_cost)),2) AS support_cost, ROUND(SUM(line_item_unblended_cost)/total_cost*100, 2) AS percentage_of_total_cost, ${table_name}.year, ${table_name}.month FROM ${table_name} RIGHT JOIN -- Total AWS bill without support (SELECT SUM(line_item_unblended_cost) AS total_cost, year, month FROM ${table_name} WHERE line_item_line_item_type \u0026lt;\u0026gt; \u0026#39;Tax\u0026#39; AND line_item_product_code \u0026lt;\u0026gt; \u0026#39;OCBPremiumSupport\u0026#39; GROUP BY year, month) AS aws_total_without_support ON (${table_name}.year = aws_total_without_support.year AND ${table_name}.month = aws_total_without_support.month) RIGHT JOIN -- Total support (SELECT SUM(line_item_unblended_cost) AS total_support_cost, year, month FROM ${table_name} WHERE line_item_product_code = \u0026#39;OCBPremiumSupport\u0026#39; AND line_item_line_item_type \u0026lt;\u0026gt; \u0026#39;Tax\u0026#39; GROUP BY year, month ) AS aws_support ON (${table_name}.year=aws_support.year AND ${table_name}.month = aws_support.month) WHERE line_item_line_item_type \u0026lt;\u0026gt; \u0026#39;Tax\u0026#39; AND line_item_product_code \u0026lt;\u0026gt; \u0026#39;OCBPremiumSupport\u0026#39; AND ${table_name}.year = \u0026#39;2020\u0026#39; AND (${table_name}.month BETWEEN \u0026#39;7\u0026#39; AND \u0026#39;9\u0026#39; OR ${table_name}.month BETWEEN \u0026#39;07\u0026#39; AND \u0026#39;09\u0026#39;) GROUP BY bill_payer_account_id, total_support_cost, total_cost, ${table_name}.year, ${table_name}.month, line_item_usage_account_id ORDER BY support_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Cost by Charge type Query Description This query will aggregate charge types for one or more payers. For more information on various charge types please reference our Cost Explorer documentation . This query will replicate Cost Explorer results when filtering by charge type in the cost explore filters as shown below.\nIn order to obtain more granular data, try adding the column \u0026rsquo;line_item_line_item_description\u0026rsquo; into the SELECT and Group By Sections (see example #2).\nNote: This query expects that you have reserved instances purchased within at least one of the accounts. This query will not run correctly without reserved instances within the CUR data set.\nPricing N/A\nSample Output: Download SQL File: Link to file Query Preview: Example 1:\nSELECT bill_payer_account_id, CASE WHEN (line_item_line_item_type = \u0026#39;Fee\u0026#39; AND product_product_name = \u0026#39;AWS Premium Support\u0026#39;) THEN \u0026#39;Support fee\u0026#39; WHEN (line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;) THEN \u0026#39;Upfront reservation fee\u0026#39; ELSE line_item_line_item_type END charge_type, round(sum(line_item_unblended_cost),2) sum_unblended_cost FROM ${table_name} WHERE ${date_filter} GROUP BY bill_payer_account_id, 2 -- reference to charge_type case statement ORDER BY sum_unblended_cost DESC ; Example 2:\nSELECT bill_payer_account_id, CASE WHEN (line_item_line_item_type = \u0026#39;Fee\u0026#39; AND product_product_name = \u0026#39;AWS Premium Support\u0026#39;) THEN \u0026#39;Support fee\u0026#39; WHEN (line_item_line_item_type = \u0026#39;Fee\u0026#39; AND reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;) THEN \u0026#39;Upfront reservation fee\u0026#39; ELSE line_item_line_item_type END charge_type, line_item_line_item_description, round(sum(line_item_unblended_cost),2) sum_unblended_cost FROM ${table_name} WHERE ${date_filter} GROUP BY bill_payer_account_id, 2, -- reference to charge_type case statement line_item_line_item_description ORDER BY sum_unblended_cost DESC ; Help \u0026amp; Feedback Serverless Product Spend Query Description This query will provide monthly unblended cost for all Serverless products in use across all regions. This query is helpful in tracking Serverless product adoption as application teams modernize their applications. You can expand the query to include line_item_usage_account_id to show individual service charges per linked account. This query helps provide a view that is difficult to achieve within Cost Explorer.\nPricing AWS Lambda pricing page AWS Fargate pricing page Amazon EventBridge pricing page AWS Step Functions pricing page Amazon SQS pricing page Amazon SNS pricing page Amazon API Gateway pricing page AWS AppSync pricing page Amazon S3 pricing page Amazon DynamoDB pricing page Amazon RDS Proxy pricing page Amazon Aurora pricing page Sample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, -- if uncommenting, also uncomment three other occurrences of line_item_usage_account_id: -- two in SELECTs that are UNIONed and one in GROUP BY or ^F. -- line_item_usage_account_id, month_line_item_usage_start_date, line_item_product_code, split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS double)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS decimal(16,8))) AS sum_line_item_unblended_cost FROM ( SELECT bill_payer_account_id, -- line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, line_item_product_code, CASE REGEXP_REPLACE(SPLIT_PART(line_item_usage_type, \u0026#39;:\u0026#39;, 1), \u0026#39;^[^-]*-\u0026#39;) WHEN \u0026#39;Fargate-GB-Hours\u0026#39; THEN \u0026#39;Fargate\u0026#39; WHEN \u0026#39;Fargate-vCPU-Hours\u0026#39; THEN \u0026#39;Fargate\u0026#39; WHEN \u0026#39;SpotUsage-Fargate-GB-Hours\u0026#39; THEN \u0026#39;Fargate\u0026#39; WHEN \u0026#39;SpotUsage-Fargate-vCPU-Hours\u0026#39; THEN \u0026#39;Fargate\u0026#39; ELSE \u0026#39;--\u0026#39; -- should not be reached! END AS split_line_item_usage_type, line_item_usage_amount, line_item_unblended_cost, year, month FROM ${table_name} WHERE ( line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;Credit\u0026#39;, \u0026#39;RIFee\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;, \u0026#39;SavingsPlanNegation\u0026#39;) ) AND ( line_item_usage_type LIKE \u0026#39;%Fargate%\u0026#39; AND line_item_product_code IN (\u0026#39;AmazonECS\u0026#39;, \u0026#39;AmazonEKS\u0026#39;) ) UNION ALL SELECT bill_payer_account_id, -- line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, line_item_product_code, CASE SPLIT_PART(line_item_usage_type, \u0026#39;:\u0026#39;, 2) WHEN \u0026#39;ProxyUsage\u0026#39; THEN \u0026#39;RDS Proxy Usage\u0026#39; WHEN \u0026#39;ServerlessUsage\u0026#39; THEN \u0026#39;Aurora Serverless\u0026#39; ELSE \u0026#39;--\u0026#39; END AS split_line_item_usage_type, line_item_usage_amount, line_item_unblended_cost, year, month FROM ${table_name} WHERE ( line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;Credit\u0026#39;, \u0026#39;RIFee\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;, \u0026#39;SavingsPlanNegation\u0026#39;) ) AND ( ( line_item_product_code = \u0026#39;AmazonRDS\u0026#39; AND SPLIT_PART(line_item_usage_type, \u0026#39;:\u0026#39;, 2) IN (\u0026#39;ServerlessUsage\u0026#39;, \u0026#39;ProxyUsage\u0026#39;) ) OR ( line_item_product_code IN (\u0026#39;AmazonDynamoDB\u0026#39;, \u0026#39;AmazonDAX\u0026#39;, \u0026#39;AmazonS3\u0026#39;, \u0026#39;AWSAppSync\u0026#39;, \u0026#39;AmazonApiGateway\u0026#39;, \u0026#39;Amazon Simple Notification Service\u0026#39;, \u0026#39;AWSQueueService\u0026#39;, \u0026#39;AWSLambda\u0026#39;, \u0026#39;AWSEvents\u0026#39; ) ) ) ) WHERE ${date_filter} GROUP BY bill_payer_account_id, -- line_item_usage_account_id, month_line_item_usage_start_date, line_item_product_code, split_line_item_usage_type ORDER BY month_line_item_usage_start_date, line_item_product_code, split_line_item_usage_type Help \u0026amp; Feedback Back to Table of Contents Amortized Cost By Charge Type Query Description This query provides amortized cost by charge type for a given month. The output includes payer account ID, the month, charge types and the amortized cost for the charge type. It closely matches Cost Explorer result when \u0026ldquo;show costs as\u0026rdquo; amortized cost is selected under the advanced options and grouped by charge type.\nPricing Please refer to the AWS pricing page .\nChoosing advanced options Cost Explorer documentation - Link Cost Explorer Links These links are provided as an example to compare CUR report output to Cost Explorer output.\nAmortized Cost Link Sample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, CASE WHEN (line_item_line_item_type = \u0026#39;Fee\u0026#39; AND product_product_name = \u0026#39;AWS Premium Support\u0026#39;) THEN \u0026#39;Support fee\u0026#39; WHEN (line_item_line_item_type = \u0026#39;Fee\u0026#39; AND bill_billing_entity \u0026lt;\u0026gt; \u0026#39;AWS\u0026#39;) THEN \u0026#39;Marketplace fee\u0026#39; WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN \u0026#39;Reservation applied usage\u0026#39; ELSE line_item_line_item_type END charge_type, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date , round(sum(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN round((savings_plan_total_commitment_to_date - savings_plan_used_commitment),8) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END),2) sum_amortized_cost FROM ${table_name} WHERE ${date_filter} GROUP BY bill_payer_account_id, 2, -- month_line_item_usage_start_date 3 -- sum_amortized_cost ORDER BY sum_amortized_cost DESC ; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/customizations/","title":"Customizations","tags":[],"description":"","content":"Last Updated March 2022\nIf you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: cloud-intelligence-dashboards@amazon.com Cloud Intelligence Dashboards Customizations Now that you have the Cloud Intelligence Dashboards deployed, you can follow some of these steps to see how you can customize your deployment to suit your needs. The amount of customization you can do is unlimited, so here we will attempt to show you how to get started with some common customization requests. Have an idea for another one - please send it to us at the e-mail address above.\nCustomization Prerequisites The first step to customizing your dashboards is to make sure you save your QuickSight dashboard as an Analysis.\nClick here to expand step by step instructions on creating an editable Analysis Visit your dashboard and in the top right, click share dashboard, then share dashboard. Give yourself save as permissions and click back to dashboard. Refresh your browser. A new button on the top right should appear labeled save as. Click it and give your new Analysis a name. You have now created an editable fork of the dashboard. Your template will remain unchanged. Customization Guides Adding Tags Filtering Visuals by Cost Allocation Tags SaaS Unit Metrics X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/faq/","title":"FAQ","tags":[],"description":"","content":"Last Updated June 2022\nIf you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: cloud-intelligence-dashboards@amazon.com How do I setup the dashboards on top of multiple payer accounts? This scenario allows customers with multiple payer (management) accounts to deploy all the CUR dashboards on top of the aggregated data from multiple payers. To fulfill prerequisites customers should set up or have setup a new Governance Account. The payer account CUR S3 buckets will have S3 replication enabled, and will replicate to a new S3 bucket in your separate Governance account. Click here to expand step by step instructions NOTE: These steps assume you\u0026rsquo;ve already setup the CUR to be delivered in each payer (management) account.\nSetup S3 CUR Bucket Replication Create or go to the console of your Governance account. This is where the Cloud Intelligence Dashboards will be deployed. Your payer account CURs will be replicated to this account. Note the region, and make sure everything you create is in the same region. To see available regions for QuickSight, visit this website . Create an S3 bucket with enabled versioning. Open S3 bucket and apply following S3 bucket policy with replacing respective placeholders {PayerAccountA}, {PayerAccountB} (one for each payer account) and {GovernanceAccountBucketName}. You can add more payer accounts to the policy later if needed. { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCombinedBucket\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Set permissions for objects\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [\u0026#34;{PayerAccountA}\u0026#34;,\u0026#34;{PayerAccountB}\u0026#34;] }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ReplicateObject\u0026#34;, \u0026#34;s3:ReplicateDelete\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::{GovernanceAccountBucketName}/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Set permissions on bucket\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [\u0026#34;{PayerAccountA}\u0026#34;,\u0026#34;{PayerAccountB}\u0026#34;] }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:List*\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:PutBucketVersioning\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::{GovernanceAccountBucketName}\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Set permissions to pass object ownership\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [\u0026#34;{PayerAccountA}\u0026#34;,\u0026#34;{PayerAccountB}\u0026#34;] }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ReplicateObject\u0026#34;, \u0026#34;s3:ReplicateDelete\u0026#34;, \u0026#34;s3:ObjectOwnerOverrideToBucketOwner\u0026#34;, \u0026#34;s3:ReplicateTags\u0026#34;, \u0026#34;s3:GetObjectVersionTagging\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::{GovernanceAccountBucketName}/*\u0026#34; } ] } This policy supports objects encrypted with either SSE-S3 or not encrypted objects. For SSE-KMS encrypted objects additional policy statements and replication configuration will be needed: see https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html Set up S3 bucket replication from each Payer (Management) account to S3 bucket in Governance account This step should be done in each payer (management) account.\nOpen S3 bucket in Payer account with CUR. On Properties tab under Bucket Versioning section click Edit and set bucket versioning to Enabled. On Management tab under Replication rules click on Create replication rule. Specify rule name. Select Specify a bucket in another account and provide Governance account id and bucket name in Governance account. Select Change object ownership to destination bucket owner checkbox. Select Create new role under IAM Role section. Leave rest of the settings by default and click Save. Copy existing objects from CUR S3 bucket to S3 bucket in Governance account This step should be done in each payer (management) account.\nSync existing objects from CUR S3 bucket to S3 bucket in Governance account.\naws s3 sync s3://{curBucketName} s3://{GovernanceAccountBucketName} --acl bucket-owner-full-control After performing this step in each payer (management) account S3 bucket in Governance account will contain CUR data from all payer accounts under respective prefixes.\nPrepare Glue Crawler These actions should be done in Governance account\nOpen AWS Glue Service in AWS Console in the same region where S3 bucket with aggregated CUR data is located and go to Crawlers section Click Add Crawler Specify Crawler name and click Next In Specify crawler source type leave settings by default. Click Next In Add a data store select S3 bucket name with aggregated CUR data and add following exclusions **.zip, **.json, **.gz, **.yml, **.sql, **.csv, **/cost_and_usage_data_status/*, aws-programmatic-access-test-object. Click Next In Add another data store leave No by default. Click Next\nIn Choose an IAM role select Create an IAM role and provide role name. Click Next\nIn Create a schedule for this crawler select Daily and specify Hour and Minute for crawler to run\nIn Configure the crawler’s output choose Glue Database in which you’d like crawler to create a table or add new one. Select Create a single schema for each S3 path checkbox. Select Add new columns only and Ignore the change and don’t update the table in the data catalog in Configuration options. Click Next\nPlease make sure Database name doesn’t include ‘-’ character\nCrawler configuration should look as on the screenshot below. Click Finish\nResume deployment methodoly of choice from previous page.\nHow do I limit access to the data in the Dashboards using row level security? Do you want to give access to the dashboards to someone within your organization, but you only want them to see data from accounts or business units associated with their role or position? You can use row level seucirty in QuickSight to accomplish limiting access to data by user. In these steps below, we will define specific Linked Account IDs against individual users. Once the Row-Level Security is enabled, users will continue to load the same Dashboards and Analyses, but will have custom views that restrict the data to only the Linked Account IDs defined.\nVideo Tutorial\nClick here to expand step by step instructions Considerations:\nThe permissions dataset can\u0026rsquo;t contain duplicate values. Duplicates are ignored when evaluating how to apply the rules.\nEach user or group specified can see only the rows that match the field values in the dataset rules.\nIf you add a rule for a user or group and leave all other columns with no value (NULL), you grant them access to all the data.\nIf you don\u0026rsquo;t add a rule for a user or group, that user or group can\u0026rsquo;t see any of the data.\nIf the userbase of QuickSight will be changing frequently, consider storing your csv in S3 rather than a local file. The full set of rule records that are applied per user must not exceed 999. This applies to the total number of rules that are directly assigned to a user name plus any rules that are assigned to the user through group names.\nIf a field includes a comma (,) Amazon QuickSight treats each word separated from another by a comma as an individual value in the filter. For example, in (\u0026lsquo;AWS\u0026rsquo;, \u0026lsquo;INC\u0026rsquo;), AWS,INC is considered as two strings: AWS and INC. To filter with AWS,INC, wrap the string with double quotation marks in the permissions dataset.\nCreate a CSV file defining users and Linked Account IDs Create a CSV file that looks something like this:\nusername,account_id user1@amazon.co.uk,\u0026quot;123456123456\u0026quot; user1@amazon.co.uk,\u0026quot;987654987654\u0026quot; user2@amazon.fr,\u0026quot;123456123456\u0026quot; user3@amazon.com,\u0026quot;789123456123\u0026quot; Any Account IDs that you wish the given user to see should be defined in the account_id field of the CSV. Create a separate row for a single username having access to multiple account IDs. Ensure there are no spaces after your final quote character. Name this file something similar to CUDOS_Dataset_rules.csv\nIf you want to use QuickSight groups the CSV input file is slightly different. Instead of UserName as the initial field you have to use GroupName. Also, you can only use Users or Groups in the input file, not both. This page provides more details. QuickSight groups can only be created and managed via the QuickSight CLI. There is no UI for this in the QuickSight console.\nYou now have 2 options on how to proceed:\nKeep your CSV file local and create a new Dataset by Uploading a File as the Data Source Upload your CSV to S3 and create a new Dataset with an S3 Manifest file as the Data Source Using a local CSV file as the data source Create a new Dataset using the CSV file above as the Data Source: Click New Dataset and select Upload a file. Locate your CUDOS_Dataset_rules.csv and a Preview will appear.\nClick Edit settings and prepare data. Verify that the account_id field is a String data type. If it appears as an Integer, change the data type for account_id to String.\nThe reason we need to do this is that we will lose any leading or trailing zeroes on an Account ID if it remains as an Integer value. This is also the same Data Type used for the account_id field in all the CUDOS Datasets.\nSave the CUDOS_Dataset_rules.csv dataset.\nSelect the dataset name Click Row-level security Select CUDOS_Dataset_rules.csv Click Apply dataset Click Apply dataset again Wait until you see Selected dataset rules populate with CUDOS_Dataset_rules Click x to exit Click x to exit Repeat steps for all CUDOS-related datasets (See below for a list of current Dataset Names) Once you have applied the CUDOS_Dataset_rules S3 Dataset to all your CUDOS datasets, visit the CUDOS Dashboard as a user who is defined in the csv file, and confirm the Account IDs shown are only the ones specified in that file.\nUse S3 as the data source Upload your csv file to the Athena query location bucket. eg. aws-athena-query-results-123456123456-us-east-1\nCreate an S3 manifest file that looks something like this:\n{ \u0026quot;entries\u0026quot;: [ {\u0026quot;url\u0026quot;:\u0026quot;s3://aws-athena-query-results-123456123456-us-east-1/CUDOS_Dataset_rules.csv\u0026quot;, \u0026quot;mandatory\u0026quot;:true}, ] } This manifest file can be saved locally, or uploaded to the same S3 bucket where the csv file is stored. Save this file as something similar to CUDOS_manifest.json.\nBack in the QuickSight Admin Console, click New Dataset and select S3. Name the Dataset CUDOS_Dataset_rules and locate your CUDOS_manifest.json file either by entering the S3 URL where it is stored, or choosing to upload from your local machine.\nClick Edit/preview data.\nVerify that the account_id field is a String data type. If it appears as an Integer, change the data type for account_id to String.\nSave the CUDOS_Dataset_rules dataset.\nOn each of the datasets that the CUDOS dashboard is using, define Row Level Security by following these steps:\nSelect the dataset name Click Row-level security Select CUDOS_Dataset_rules Click Apply dataset Click Apply dataset again Wait until you see Selected dataset rules populate with CUDOS_Dataset_rules Click x to exit Click x to exit Repeat steps for all CUDOS-related datasets (See below for a list of current Dataset Names) Once you have applied the CUDOS_Dataset_rules S3 Dataset to all your CUDOS datasets, visit the CUDOS Dashboard as a user who is defined in the csv file, and confirm the Account IDs shown are only the ones specified in that file.\nHow do I fix the COLUMN_GEOGRAPHIC_ROLE_MISMATCH error? When attempting to deploy the dashboard manually, some users get an error that states COLUMN_GEOGRAPHIC_ROLE_MISMATCH. Click here to expand answer This error is caused by there being too many data source connectors in QuickSight with the same name. To check how many data source connectors you have, visit QuickSight datasets and click on new datasets. Scroll to the bottom and note how many Athena data connectors there are with the same name.\nUnless you know which datasets are tied to which data sources, it is faster to simply delete all the Cloud Intelligence Dashboards data sources and data sets from QuickSight, and start adding them again, this time only using a single data source. This is described in detail in this lab under the manual deployment option as step 22. You should only have one data source for all your Cloud Intelligence Dashboard datasets, including customer_all. If you wish to use separate data sources, they must not have the same name.\nHow do I fix the ‘product_cache_engine’ cannot be resolved error? When attempting to deploy the dashboard, some users get an error that states product_cache_engine cannot be resolved. Click here to expand answer This view is dependent on having or historically having an RDS database instance and an ElastiCache cache instance run in your organization. If you get the error that the column product_database_engine or product_deployment_option does not exist, then you do not have any RDS database instances running. There are two options to resolve this.\nOption 1 To make this column show up in the CUR spin up a database in the RDS service, let it run for a couple of minutes and in the next integration of the crawler the column will appear. If you get the error that the column product_cache_engine does not exist, then you do not have any ElastiCache cache instances running. To make this column show up in the CUR spin up an ElastiCache cache instance in the ElastiCache service, let it run for a couple of minutes and in the next integration of the crawler the column will appear. You can verify this by running the Athena query: SHOW COLUMNS FROM tablename - and replace the tablename accordingly after selecting the correct CUR database in the dropdown on the left side in the Athena view.\nOption 2 Follow the below steps to remove the colum. Be aware this will mean if you do add ElastiCache instances to your accounts you should put this back.\nIn Amazon Athena click \u0026lsquo;Show Edit Query\u0026rsquo; for kpi_instance_all Remove product_cache_engine and remove the last Group by number Run query If you are running from CloudShell re-run the deploy command Go to Amazon Quicksight Find the kpi_instance_all dataset Click on \u0026lsquo;Edit Dataset\u0026rsquo; On the top right of the screen click \u0026lsquo;save and publish\u0026rsquo; This page will be updated regularly with new answers. If you have a FAQ you\u0026rsquo;d like answered here, please reach out to us here cloud-intelligence-dashboards@amazon.com .\n"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/","title":"Level 200: Cloud Intelligence Dashboards","tags":[],"description":"","content":"Last Updated May 2022\nThis Well Architected lab is a consolidation of labs and AWS workshops formerly called Enterprise Dashboards, CUDOS Workshop, and the TAO Workshop.\nFeedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: cloud-intelligence-dashboards@amazon.com Get Help Ask your questions on re:Post and get answers from our team, other AWS experts, and other customers using the dashboards.\nSubscribe to our YouTube channel to see guides, tutorials, and walkthroughs on all things Cloud Intelligence Dashboards.\nIntroduction Do you know how much you’re spending per hour on AWS Lambda? How about per S3 bucket? How do you know buying Savings Plan or using Spot Instances is saving you money? Does your team know how much their application costs to run on AWS? Visualizing and understanding your cost and usage data is critical to good cloud financial management and accountability.\nCloud Financial Management (CFM) is the practice of bringing financial accountability to the variable spend model of cloud. CFM practitioners are pursuing business efficiency across all of their accounts by first visualizing their cost and usage, then setting goals, and finally driving accountability from their IT teams to meet or exceed these goals.\nCloud infrastructure provides more agility and responsiveness than traditional IT environments. This requires organizations to think differently about how they design, build, and manage applications.\nCloud resources are disposable, and with a pay-per-use model it requires a strong integration between IT governance and organizational governance. Builders need to be able to operate in a cloud environment that’s agile and safe at the same time.\nThis Well Architected lab will walk you through implementing a series of dashboards for all of your AWS accounts that will help you drive financial accountability, optimize cost, track usage goals, implement best-practices for governance, and achieve operational excellence.\nCloud Intelligence Dashboards Overview In this lab, you will find step-by-step guides on how to implement some or all of the foundational Cloud Intelligence Dashboards as well as additional dashboards.\nCost Intelligence Dashboard (CID) - Overview CUDOS Dashboard - Overview Trusted Advisor Organizational (TAO) Dashboard - Overview KPI \u0026amp; Modernization Dashboard (KPI) - Overview Compute Optimizer Dashboard (COD) - Overview Additional Dashboards - Overview The Cloud Intelligence Dashboards include (but are not limited to) the following benefits:\nEasy to Use All insights in plain language Organized by services Includes high level overviews Secure Supports IAM No agents running anywhere Data stays in your organizations Uses all AWS native services In Depth 100s of pre-built visuals Resource level granularity Fully customizable Machine Learning driven Insights Cost Efficient Amazing QuickSight Enterprise licenses start at $18 per month Serverless so only pay as you go Cost Intelligence Dashboard (CID) The Cost Intelligence Dashboard is a customizable and accessible dashboard to help create the foundation of your own cost management and optimization tool. Executives, directors, and other individuals within the CFO\u0026rsquo;s line of business or who manage cloud financials for an organization will find the Cloud Intelligence Dashboard easy to use and relevant to their use cases. Little to no technical knowledge or understanding of AWS Services is required. Out-of-the-box benefits of the CID include (but are not limited to):\nCreate chargeback or showback reports for internal business units, accounts, or cost centers. Track how Savings Plans (SP), Reserved Instances (RI), and Spot Instance usage has impacted your unit metrics such as your average hourly cost of Amazon EC2. Keep track of which accounts or internal business units receive savings and when RIs and SPs expire. Services used: AWS Cost and Usage Report (CUR), Amazon Athena, AWS Glue, Amazon S3, and Amazon QuickSight.\nExplore a sample Cost Intelligence Dashboard CUDOS Dashboard The CUDOS Dashboard is an in-depth, granular, and recommendation-driven dashboard to help customers dive deep into cost and usage and to fine-tune efficiency. Executives, directors, and other individuals within the CIO or CTO line of business or who manage DevOps and IT organizations will find the CUDOS Dashboard highly detailed and tailored to solve their use cases. Out-of-the-box benefits of the CUDOS dashboard include (but are not limited to):\nUse the built-in tag explorer to group and filter cost and usage by your tags. View resource-level detail such as your hourly AWS Lambda or individual Amazon S3 bucket costs. Get alerted to service-level areas of focus such as top 3 On-Demand database instances by cost. Services used: AWS Cost and Usage Report (CUR), Amazon Athena, AWS Glue, Amazon S3, and Amazon QuickSight.\nExplore a sample CUDOS Dashboard Trusted Advisor Organizational (TAO) Dashboard Amazon Trusted Advisor helps you optimize your AWS infrastructure, improve security and performance, reduce overall costs, and monitors service limits. Organizational view lets you view Trusted Advisor checks for all accounts in your AWS Organizations. The only way to visualize the organizational view is to use the TAO dashboard. The TAO dashboard is a set of visualizations that provide comprehensive details and trends across your entire AWS Organization. Out-of-the-box benefits of the CUDOS dashboard include (but are not limited to):\nQuickly locate accounts that haven\u0026rsquo;t rotated their AWS IAM keys. Find then sort unutilized and underutilized resources by cost or account. See a list of accounts that have reached 80% of individual service limits. Services used: AWS Trusted Advisor, AWS Trusted Advisor Organizational report, Amazon Athena, AWS Glue, Amazon S3, and Amazon QuickSight.\nTrusted Advisor Organizational (TAO) requires the management account in your organization to have Business or Enterprise support plan and enabled organizational view. Click to learn more Explore a sample TAO Dashboard KPI Dashboard The KPI and Modernization Dashboard helps your organization combine DevOps and IT infrastructure with Finance and the C-Suite to grow more efficiently and effectively on AWS. This dashboard lets you set and track modernization and optimization goals such as percent OnDemand, Spot adoption, and Graviton usage. By enabling every line of business to create and track usage goals, and your cloud center of excellence to make recommendations organization-wide, you can grow more efficiently and innovate more quickly on AWS. Out-of-the-box benefits of the KPI dashboard include (but are not limited to):\nTrack percent on-demand across all your teams. See potential cost savings by meeting certain KPIs and goals for your organization. Quickly locate cost-optimization opportunities such as infrequently used S3 buckets, old EBS snapshots, and Graviton eligible instance usage. Services used: AWS Cost and Usage Report (CUR), Amazon Athena, AWS Glue, Amazon S3, and Amazon QuickSight.\nExplore a sample KPI Dashboard Compute Optimizer Dashboard (COD) This dashboard helps your organization to visualize and trace right sizing recommendations from AWS Compute Optimizer. These recommendations will help you indentify Cost savings opportunities for over provisioned resources and also see the Operational risk from under provisioned ones.\nServices used: AWS Compute Optimizer, Amazon Athena, AWS Glue, Amazon S3, and Amazon QuickSight.\nExplore a sample Compute Optimizer Dashboard Additional Dashboards In addition to the 3 foundational dashboards, there are additional dashboards you can leverage to gain deeper insights into your cost and usage.\nData Transfer Dashboard Trends Dashboard Services used: AWS Cost and Usage Report (CUR), Amazon Athena, AWS Glue, Amazon S3, and Amazon QuickSight.\nSteps: Get started with the Cost and Usage Dashboards Get started with the TAO Dashboard Get started with the COD Authors Aaron Edell, Global Head of Business and GTM (Cloud Intelligence Dashboards) Alee Whitman, Sr. Commercial Architect (AWS OPTICS) Timur Tulyaganov, AWS Principal Technical Account Manager Yuriy Prykhodko, AWS Sr. Technical Account Manager Iakov Gan, Senior Technical Account Manager Contributors Arun Santhosh, Specialist SA (Amazon QuickSight) Barry Miller, Technical Account Manager Chaitanya Shah, Sr. Technical Account Manager Cristian Popa, Sr. Technical Account Manager Kareem Syed-Mohammed, Senior Product Manager - Technical (Amazon QuickSight) Lakhvinder Singh Gill, Enterprise Support Lead Manik Chopra, Senior Technical Account Manager Meredith Holborn, Technical Account Manager Nisha Notani, Technical Account Manager Oleksandr Moskalenko, Technical Account Manager William Hughes, Technical Account Manager X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Next "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/","title":"Level 200: Enterprise Dashboards","tags":[],"description":"","content":" This Lab has moved under the Level 200 Cloud Intelligence Dashboards.Click this link to navigate to the updated Lab Last Updated March 2021\nFeedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: quicksightcostdashboards@amazon.com Introduction The goal of the Enterprise Dashboards is to remove the complexities of cost \u0026amp; usage analysis, and provide enterprises with a clear understanding of something, to enable them to make the right business decisions quickly. The Enterprise Dashboard are made up of multiple templates known as modules to help you gain insight into different aspects of your cost and usage as well as enable your teams to better understand the cost of their applications and opportunities to optimize. Every dashboard complements the other modules so you can grow your reporting analytics and gain additional insight. Using separate modules provides greater flexibility, allowing you to customize existing modules and take advantage of the new templates without overwriting your existing customizations. If the dashboards were in a single report it would overwrite all customizations each time you create the latest template. This hands-on lab will guide you through the steps to copy and customize the QuickSight dashboard to better leverage your cost and usage report.\nThe Cost Intelligence Dashboard is an interactive, customizable and business accessible QuickSight dashboard to help customers create the foundation for their own Cost Management and Optimization reporting tool. The Data Transfer Dashboard allows your organization to understand their data transfer cost and usage across all AWS products so you can take action on optimization opportunities. Interested in a detailed description of the dashboards options to get the dashboards in a single view? Download read the FAQ Note: This QuickSight dashboard is not an official AWS dashboard and should be used as a self-service tool. We recommend validating your data by comparing the aggregate un-grouped Payer and Linked Account spend for a prior month.\nGoals Create the Cost Intelligence dashboard Distribute your dashboards in your organization Prerequisites An AWS Account with Cost Optimization team permissions An Amazon Enterprise Edition QuickSight Account A Cost and Usage Report (CUR) Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab Completed the Cost and Usage Visualization lab Requested template access here Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Access to AWS CLI Costs Small accounts approximately \u0026lt;$5 when using your free QuickSight trial Time to complete Cost Intelligence Dashboard: Should take approximately 45-60 minutes to complete Optional Advanced Setup: Should take approximately 15-30 minutes to complete Data Transfer Dashboard: Should take approximately 15-20 minutes to complete Steps: Start Lab\n"},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/8_cost_tags/","title":"Enable AWS-Generated Cost Allocation Tags","tags":[],"description":"","content":"Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information. This is automatically applied to resources that are created, and allows you to view and allocate costs based on who created a resource.\nLog in to your Management Account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on AWS-generated cost allocaiton tags: Select aws:createdBy tag key and click on Activate to enable the tag: Click on Activate to confirm that you want to activate the tag: You will see that it is activated: X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/8_query_from_athena/","title":"Query logs from S3 using Athena","tags":[],"description":"","content":"With your log data now stored in S3, you will utilize Amazon Athena - a serverless interactive query service. You will run SQL queries on your log files to extract information from them. In this section, we will focus on the Apache access logs, although Athena can be used to query any of your log files. It is possible to query your log data from CloudWatch Insights, however, Athena querying allows you to pull data from files stored in S3, as well as other sources, where Insights only allows to query data in CloudWatch. Athena supports SQL querying - an industry standard language.\nOpen up the Athena console .\nIf this is the first time you are using Athena:\nClick Get Started to go to the Query Editor. Set up a query result location by clicking the link that appears at the top of the page. If this is not the first time you are using Athena:\nSet up a query result location by clicking Settings in the top right corner of the page. Enter the following into the Query result location field, replacing REPLACE_ME_BUCKETNAME with the name of the S3 bucket you created, likely wa-lab-\u0026lt;your-account-id\u0026gt;-\u0026lt;date\u0026gt;.\ns3://REPLACE_ME_BUCKETNAME/athenaqueries/\nClick Save. You should now see the blank query editor, as seen in the image below. This is where you will enter SQL queries to manipulate and extract information from your log files. Enter the following command in “New query 1” box to create a new Database , which will hold the Table containing our log data. This command creates a database called security_lab_logs. CREATE database security_lab_logs Press Run query to execute this command. Once complete, you should see Query successful. displayed in the results box. On the left side menu, click the dropdown under Database and select the newly created database called security_lab_logs. You will create a table within this database to hold our logs. Click the plus icon next to New query 1 to open a new query editor tab. Copy the SQL code below into the editor to create a table from our log data. Replace REPLACE_ME_BUCKET with the name of the bucket you created to your logs in S3, likely wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;. You will need to identify the folder your logs are in for REPLACE_ME_STRING. Follow these steps to identify the path. Open the S3 console . Open the bucket you created, likely wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;. Open the lablogs folder. You should see a folder with a long, random looking string (e.g. c848ff11-df30-481c-8d9f-5805741606d3). This string is what you should use for REPLACE_ME_STRING. CREATE EXTERNAL TABLE IF NOT EXISTS `security_lab_apache_access_logs` ( request_date string, request_timestamp string, request_ip string, request_method string, request string, response_code int, response_size int, user_client_data string, garbage string ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.RegexSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#34;input.regex\u0026#34; = \u0026#34;([^ ]*)T([^ ]*)Z ([^ ]*) (?:[^ ]* [^ ]*) (?:[^\\\u0026#34;]*) \\\u0026#34;([^ ]*) ([^\\\u0026#34;]*)\\\u0026#34; ([^ ]*) ([^ ]*) (?:\\\u0026#34;[^\\\u0026#34;]*\\\u0026#34;) (\\\u0026#34;[^\\\u0026#34;]*\\\u0026#34;)([^\\n]*)\u0026#34; ) LOCATION \u0026#39;s3://REPLACE_ME_BUCKET/lablogs/REPLACE_ME_STRING/apache-access-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Let’s break this down a little.\nThe CREATE EXTERNAL TABLE... statement creates your new table and defines its columns, such as request_date, request_timestamp, and so on. The ROW FORMAT SERDE statement specifies that the table rows are formatted using the RegEx SerDe (serializer/deserializer). The WITH SERDEPROPERTIES statement specifies the RegEx input format of your log files. This is how your raw log data is converted into columns. The LOCATION statement specifies the source of your table data, which is the S3 bucket containing your log files. The TBLPROPERTIES statement specifies that your log files are initially compressed using the GZIP format. Click Run query. One it is successfully finished, you should see your new 'security_lab_apache_access_logs' table in the left side menu under Tables. Next, you will query data from this table. Click the plus icon next to New query 2 to open a new query editor tab. First, view your whole table. Copy the following commands into the query editor and press Run query. You should see the table in the Results box. SELECT * FROM security_lab_logs.security_lab_apache_access_logs limit 15 Let’s say you want to view only a certain column from this table, such as the response code frequency and size of the response. Replace the query you just made with the code below to do so. SELECT response_code, count(response_code) AS count FROM security_lab_logs.security_lab_apache_access_logs WHERE response_code IS NOT NULL GROUP BY response_code ORDER BY count desc This isolates the response_code and response_size columns from your table and creates a new column called count, which is the frequency of each response type.\nIf you needed to track different metrics for your workload, you can always use different Athena queries to do so, but for the purposes of this lab, we will just be focusing on response code frequency.\nClick Run query. In the Results box, you should see a table similar to the one below. Recap: In this section, you analyzed information from your workloads’s log files using Amazon Athena. Although you only focused on the response codes and sizes in this lab, Athena can be used to query any data from S3, making it a powerful tool to analyze log files without directly accessing them. This demonstrates the best practices of “enabling people to perform actions at a distance” and “keeping people away from data. You’ve been able to minimize direct interaction with your data and instances - first by using Systems Manager for configuration, and now through S3 and Athena for log analysis.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/operational-excellence/100_labs/100_inventory_patch_management/8_cleanup/","title":"Removing Lab Resources","tags":[],"description":"","content":" Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier.\n7.1 Remove resources created with CloudFormation Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack. Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager. Select the association you created. Choose Delete. If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic, delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics. If you created a Maintenance Window, delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows. Select the maintenance window you created. Choose Delete. In the Delete maintenance window window, choose Delete. If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users. Select your user from the list. Choose Delete user. Select the check box next to \u0026ldquo;One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\u0026rdquo;. Choose Yes, delete. When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment or workload, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with OPS5 - \u0026ldquo;How do you reduce defects, ease remediation, and improve flow into production?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_certificate_manager_request_public_certificate/","title":"Level 200: AWS Certificate Manager Request Public Certificate","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.\nGoals Request AWS Certificate Manager public certificate Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . See pricing for further information on AWS Certificate Manager. Steps: Requesting a public certificate using the console Tear down "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_workload_efficiency/","title":"Level 200: Workload Efficiency","tags":[],"description":"","content":"Last Updated May 2020\nYour browser doesnt support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through the steps to measure the efficiency of a workload. It shows you how to get the overall efficiency, then look deeper for patterns in usage to be able to allocate different weights to different outputs of a system.\nThe lab uses a simple web application to demonstrate the efficiency, but will teach you the techniques so that it can be applied to ANY workload you have, whether its born in the cloud or legacy.\nThe first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files for each workload you have.\nGoals Setup the application data source Combine the application and cost data sources Create the visualization for efficiency Prerequisites An AWS Account An Amazon QuickSight Account Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab Completed the Cost and Usage Visualization lab Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Costs \u0026lt;$5 depending on the size of your data sources, and existing QuickSight subscription Time to complete The lab should take approximately 50-60 minutes to complete Steps: Create the Data Sources Create the efficiency data source Create the Visualizations Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_control_programmatic_access/","title":"Quest: Control Programmatic Access","tags":[],"description":" ","content":" Labs coming soon "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/8_data_transfer/","title":"Data Transfer","tags":[],"description":"","content":"Data Transfer Data transfer modelling Goal: Reduce unnecessary data transfer Target: Data transfer pattern between 1st \u0026amp; 2nd tier, must be within 10% of the pattern between the internet and 1st tier Best Practice: Data transfer modelling Measures: Number of reports delivered, number of teams delivering Good/Bad: Bad Why? When does it work well or not?: Hard to convert to $, but it could help to indicate there is an issue - but again hard to quantify the cost of the issue Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/indexfile/","title":"Index File","tags":[],"description":"","content":" This file is generated automatically\nfile Commit Date File Modified Date Commit Hash connect.sql 2021-04-22 2021-04-25 5faff1d ecs_eks.sql 2021-04-21 2021-04-25 b7d7b1c ecs_hours_day.sql 2021-04-21 2021-04-25 b7d7b1c account_spend_of_shared_sp.sql 2021-04-21 2021-04-25 44e118f compute_sp.sql 2021-04-21 2021-04-25 44e118f ec2_sp_inventory.sql 2021-04-21 2021-04-25 44e118f ec2_total_spend.sql 2021-04-21 2021-04-25 44e118f ec2ricoverage.sql 2021-04-21 2021-04-25 44e118f ec2runninghours.sql 2021-04-21 2021-04-25 44e118f ec2speffective.sql 2021-04-21 2021-04-25 44e118f elb_unused_wrid.sql 2021-04-21 2021-04-25 44e118f lambda_sp.sql 2021-04-21 2021-04-25 44e118f marketplacespend.sql 2021-04-20 2021-04-25 34b201e ses.sql 2021-04-20 2021-04-25 0fe35fe sqs.sql 2021-04-20 2021-04-25 0fe35fe athena.sql 2021-04-20 2021-04-25 05d83cf elasticsearch.sql 2021-04-20 2021-04-25 05d83cf gluewrid.sql 2021-04-20 2021-04-25 05d83cf quicksight.sql 2021-04-20 2021-04-25 05d83cf elb_unused_wrid.sql 2021-04-20 2021-04-25 92668ec lambda_sp.sql 2021-04-20 2021-04-25 92668ec ecs_eks.sql 2021-04-20 2021-04-25 92668ec ecs_hours_day.sql 2021-04-20 2021-04-25 92668ec elasticachewrid.sql 2021-04-20 2021-04-25 92668ec rds-w-id.sql 2021-04-20 2021-04-25 92668ec redshiftwrid.sql 2021-04-20 2021-04-25 92668ec workspaces_autostop_wrid.sql 2021-04-20 2021-04-25 92668ec billservice.sql 2021-04-20 2021-04-25 92668ec spendaccount.sql 2021-04-20 2021-04-25 92668ec spendregion.sql 2021-04-20 2021-04-25 92668ec spendservice.sql 2021-04-20 2021-04-25 92668ec rsum.sql 2021-04-20 2021-04-25 92668ec natgateway_idle_wrid.sql 2021-04-20 2021-04-25 92668ec ec2_sp_inventory.sql 2021-04-13 2021-04-25 466e28d ec2speffective.sql 2021-04-13 2021-04-25 466e28d elasticsearch.sql 2021-04-13 2021-04-25 8b04b1a marketplacespend.sql 2021-04-09 2021-04-25 ce274e8 athena.sql 2021-04-09 2021-04-25 ce274e8 elasticsearch.sql 2021-04-09 2021-04-25 ce274e8 emr.sql 2021-04-09 2021-04-25 ce274e8 gluewrid.sql 2021-04-09 2021-04-25 ce274e8 kinesis.sql 2021-04-09 2021-04-25 ce274e8 quicksight.sql 2021-04-09 2021-04-25 ce274e8 mqwrid.sql 2021-04-09 2021-04-25 ce274e8 ses.sql 2021-04-09 2021-04-25 ce274e8 sns.sql 2021-04-09 2021-04-25 ce274e8 sqs.sql 2021-04-09 2021-04-25 ce274e8 account_spend_of_shared_sp.sql 2021-04-09 2021-04-25 ce274e8 compute_sp.sql 2021-04-09 2021-04-25 ce274e8 ec2_sp_inventory.sql 2021-04-09 2021-04-25 ce274e8 ec2_total_spend.sql 2021-04-09 2021-04-25 ce274e8 ec2ricoverage.sql 2021-04-09 2021-04-25 ce274e8 ec2riutilization.sql 2021-04-09 2021-04-25 ce274e8 ec2runninghours.sql 2021-04-09 2021-04-25 ce274e8 ec2speffective.sql 2021-04-09 2021-04-25 ce274e8 elb_unused_wrid.sql 2021-04-09 2021-04-25 ce274e8 lambda_sp.sql 2021-04-09 2021-04-25 ce274e8 ecs_eks.sql 2021-04-09 2021-04-25 ce274e8 ecs_hours_day.sql 2021-04-09 2021-04-25 ce274e8 connect.sql 2021-04-09 2021-04-25 ce274e8 dynamodb.sql 2021-04-09 2021-04-25 ce274e8 elasticachewrid.sql 2021-04-09 2021-04-25 ce274e8 monthly_rds_usage_type_by_tag.sql 2021-04-09 2021-04-25 ce274e8 rds-w-id.sql 2021-04-09 2021-04-25 ce274e8 redshiftwrid.sql 2021-04-09 2021-04-25 ce274e8 appstream.sql 2021-04-09 2021-04-25 ce274e8 workspaces_autostop_wrid.sql 2021-04-09 2021-04-25 ce274e8 workspaceswrid.sql 2021-04-09 2021-04-25 ce274e8 billservice.sql 2021-04-09 2021-04-25 ce274e8 premiumsupport.sql 2021-04-09 2021-04-25 ce274e8 spendaccount.sql 2021-04-09 2021-04-25 ce274e8 spendregion.sql 2021-04-09 2021-04-25 ce274e8 spendservice.sql 2021-04-09 2021-04-25 ce274e8 rekognition.sql 2021-04-09 2021-04-25 ce274e8 sagemakerwrid.sql 2021-04-09 2021-04-25 ce274e8 textract.sql 2021-04-09 2021-04-25 ce274e8 cloudtrail.sql 2021-04-09 2021-04-25 ce274e8 cloudwatch-spend.sql 2021-04-09 2021-04-25 ce274e8 config.sql 2021-04-09 2021-04-25 ce274e8 rsum.sql 2021-04-09 2021-04-25 ce274e8 apigateway.sql 2021-04-09 2021-04-25 ce274e8 cloudfront.sql 2021-04-09 2021-04-25 ce274e8 data-transfer-msk.sql 2021-04-09 2021-04-25 ce274e8 data-transfer.sql 2021-04-09 2021-04-25 ce274e8 direct-connect.sql 2021-04-09 2021-04-25 ce274e8 natgateway_idle_wrid.sql 2021-04-09 2021-04-25 ce274e8 natgatewaywrid.sql 2021-04-09 2021-04-25 ce274e8 networkusagewrid.sql 2021-04-09 2021-04-25 ce274e8 tgwwrid.sql 2021-04-09 2021-04-25 ce274e8 cognito.sql 2021-04-09 2021-04-25 ce274e8 guardduty.sql 2021-04-09 2021-04-25 ce274e8 waf.sql 2021-04-09 2021-04-25 ce274e8 backup_spend.sql 2021-04-09 2021-04-25 ce274e8 ebs_gp2_to_gp3.sql 2021-04-09 2021-04-25 ce274e8 ebssnapshot-spend.sql 2021-04-09 2021-04-25 ce274e8 ebssnapshotratio.sql 2021-04-09 2021-04-25 ce274e8 ebsusagespend.sql 2021-04-09 2021-04-25 ce274e8 efswrid.sql 2021-04-09 2021-04-25 ce274e8 fsxwrid.sql 2021-04-09 2021-04-25 ce274e8 s3costusagetypewrid.sql 2021-04-09 2021-04-25 ce274e8 storage.sql 2021-04-09 2021-04-25 ce274e8 ec2_total_spend.sql 2021-04-09 2021-04-25 6b20ddd marketplacespend.sql 2021-03-30 2021-04-25 0303c5e athena.sql 2021-03-30 2021-04-25 0303c5e elasticsearch.sql 2021-03-30 2021-04-25 0303c5e emr.sql 2021-03-30 2021-04-25 0303c5e gluewrid.sql 2021-03-30 2021-04-25 0303c5e kinesis.sql 2021-03-30 2021-04-25 0303c5e quicksight.sql 2021-03-30 2021-04-25 0303c5e mqwrid.sql 2021-03-30 2021-04-25 0303c5e ses.sql 2021-03-30 2021-04-25 0303c5e sns.sql 2021-03-30 2021-04-25 0303c5e sqs.sql 2021-03-30 2021-04-25 0303c5e compute_sp.sql 2021-03-30 2021-04-25 0303c5e ec2_sp_inventory.sql 2021-03-30 2021-04-25 0303c5e ec2_total_spend.sql 2021-03-30 2021-04-25 0303c5e ec2ricoverage.sql 2021-03-30 2021-04-25 0303c5e ec2riutilization.sql 2021-03-30 2021-04-25 0303c5e ec2runninghours.sql 2021-03-30 2021-04-25 0303c5e ec2speffective.sql 2021-03-30 2021-04-25 0303c5e lambda_sp.sql 2021-03-30 2021-04-25 0303c5e ecs_eks.sql 2021-03-30 2021-04-25 0303c5e ecs_hours_day.sql 2021-03-30 2021-04-25 0303c5e connect.sql 2021-03-30 2021-04-25 0303c5e elasticachewrid.sql 2021-03-30 2021-04-25 0303c5e rds-w-id.sql 2021-03-30 2021-04-25 0303c5e redshiftwrid.sql 2021-03-30 2021-04-25 0303c5e appstream.sql 2021-03-30 2021-04-25 0303c5e workspaceswrid.sql 2021-03-30 2021-04-25 0303c5e billservice.sql 2021-03-30 2021-04-25 0303c5e spendaccount.sql 2021-03-30 2021-04-25 0303c5e spendregion.sql 2021-03-30 2021-04-25 0303c5e spendservice.sql 2021-03-30 2021-04-25 0303c5e rekognition.sql 2021-03-30 2021-04-25 0303c5e sagemakerwrid.sql 2021-03-30 2021-04-25 0303c5e textract.sql 2021-03-30 2021-04-25 0303c5e cloudtrail.sql 2021-03-30 2021-04-25 0303c5e cloudwatch-spend.sql 2021-03-30 2021-04-25 0303c5e config.sql 2021-03-30 2021-04-25 0303c5e apigateway.sql 2021-03-30 2021-04-25 0303c5e cloudfront.sql 2021-03-30 2021-04-25 0303c5e data-transfer-msk.sql 2021-03-30 2021-04-25 0303c5e data-transfer.sql 2021-03-30 2021-04-25 0303c5e natgatewaywrid.sql 2021-03-30 2021-04-25 0303c5e networkusagewrid.sql 2021-03-30 2021-04-25 0303c5e tgwwrid.sql 2021-03-30 2021-04-25 0303c5e backup_spend.sql 2021-03-30 2021-04-25 0303c5e ebs_gp2_to_gp3.sql 2021-03-30 2021-04-25 0303c5e ebssnapshot-spend.sql 2021-03-30 2021-04-25 0303c5e ebssnapshotratio.sql 2021-03-30 2021-04-25 0303c5e ebsusagespend.sql 2021-03-30 2021-04-25 0303c5e efswrid.sql 2021-03-30 2021-04-25 0303c5e fsxwrid.sql 2021-03-30 2021-04-25 0303c5e storage.sql 2021-03-30 2021-04-25 0303c5e premiumsupport.sql 2021-03-29 2021-04-25 f11e653 ec2_sp_inventory.sql 2021-03-22 2021-04-25 1d3496b gluewrid.sql 2021-03-22 2021-04-25 f298125 ec2_sp_inventory.sql 2021-03-21 2021-04-25 4ed53b4 sqs.sql 2021-03-21 2021-04-25 7e2ca10 athena.sql 2021-03-21 2021-04-25 93df43e elasticsearch.sql 2021-03-21 2021-04-25 93df43e emr.sql 2021-03-21 2021-04-25 93df43e gluewrid.sql 2021-03-21 2021-04-25 93df43e kinesis.sql 2021-03-21 2021-04-25 93df43e quicksight.sql 2021-03-21 2021-04-25 93df43e mqwrid.sql 2021-03-21 2021-04-25 93df43e ses.sql 2021-03-21 2021-04-25 93df43e sns.sql 2021-03-21 2021-04-25 93df43e sqs.sql 2021-03-21 2021-04-25 93df43e lambda_sp.sql 2021-03-21 2021-04-25 93df43e ecs_eks.sql 2021-03-21 2021-04-25 93df43e ecs_hours_day.sql 2021-03-21 2021-04-25 93df43e dynamodb.sql 2021-03-21 2021-04-25 93df43e elasticachewrid.sql 2021-03-21 2021-04-25 93df43e rds-w-id.sql 2021-03-21 2021-04-25 93df43e redshiftwrid.sql 2021-03-21 2021-04-25 93df43e appstream.sql 2021-03-21 2021-04-25 93df43e workspaceswrid.sql 2021-03-21 2021-04-25 93df43e spendaccount.sql 2021-03-21 2021-04-25 93df43e spendregion.sql 2021-03-21 2021-04-25 93df43e spendservice.sql 2021-03-21 2021-04-25 93df43e rekognition.sql 2021-03-21 2021-04-25 93df43e sagemakerwrid.sql 2021-03-21 2021-04-25 93df43e textract.sql 2021-03-21 2021-04-25 93df43e cloudtrail.sql 2021-03-21 2021-04-25 93df43e cloudwatch-spend.sql 2021-03-21 2021-04-25 93df43e config.sql 2021-03-21 2021-04-25 93df43e rsum.sql 2021-03-21 2021-04-25 93df43e apigateway.sql 2021-03-21 2021-04-25 93df43e cloudfront.sql 2021-03-21 2021-04-25 93df43e data-transfer-msk.sql 2021-03-21 2021-04-25 93df43e data-transfer.sql 2021-03-21 2021-04-25 93df43e direct-connect.sql 2021-03-21 2021-04-25 93df43e natgatewaywrid.sql 2021-03-21 2021-04-25 93df43e networkusagewrid.sql 2021-03-21 2021-04-25 93df43e tgwwrid.sql 2021-03-21 2021-04-25 93df43e cognito.sql 2021-03-21 2021-04-25 93df43e guardduty.sql 2021-03-21 2021-04-25 93df43e waf.sql 2021-03-21 2021-04-25 93df43e backup_spend.sql 2021-03-21 2021-04-25 93df43e ebssnapshot-spend.sql 2021-03-21 2021-04-25 93df43e ebssnapshotratio.sql 2021-03-21 2021-04-25 93df43e ebsusagespend.sql 2021-03-21 2021-04-25 93df43e efswrid.sql 2021-03-21 2021-04-25 93df43e fsxwrid.sql 2021-03-21 2021-04-25 93df43e s3costusagetypewrid.sql 2021-03-21 2021-04-25 93df43e storage.sql 2021-03-21 2021-04-25 93df43e athena.sql 2021-03-20 2021-04-25 2b72bfe creditsdetail.sql 2021-03-20 2021-04-25 2b72bfe ebs_gp2_to_gp3.sql 2021-03-19 2021-04-25 5872efa config.sql 2021-03-11 2021-04-25 4288f87 apigateway.sql 2021-03-11 2021-04-25 4288f87 cloudfront.sql 2021-03-11 2021-04-25 4288f87 data-transfer-msk.sql 2021-03-11 2021-04-25 4288f87 data-transfer.sql 2021-03-11 2021-04-25 4288f87 direct-connect.sql 2021-03-11 2021-04-25 4288f87 natgatewaywrid.sql 2021-03-11 2021-04-25 4288f87 networkusagewrid.sql 2021-03-11 2021-04-25 4288f87 tgwwrid.sql 2021-03-11 2021-04-25 4288f87 cognito.sql 2021-03-11 2021-04-25 4288f87 guardduty.sql 2021-03-11 2021-04-25 4288f87 waf.sql 2021-03-11 2021-04-25 4288f87 backup_spend.sql 2021-03-11 2021-04-25 4288f87 ebssnapshot-spend.sql 2021-03-11 2021-04-25 4288f87 ebssnapshotratio.sql 2021-03-11 2021-04-25 4288f87 ebsusagespend.sql 2021-03-11 2021-04-25 4288f87 efswrid.sql 2021-03-11 2021-04-25 4288f87 fsxwrid.sql 2021-03-11 2021-04-25 4288f87 s3costusagetypewrid.sql 2021-03-11 2021-04-25 4288f87 ecs_eks.sql 2021-03-11 2021-04-25 b7da69a ecs_hours_day.sql 2021-03-11 2021-04-25 b7da69a dynamodb.sql 2021-03-11 2021-04-25 b7da69a elasticachewrid.sql 2021-03-11 2021-04-25 b7da69a rds-w-id.sql 2021-03-11 2021-04-25 b7da69a redshiftwrid.sql 2021-03-11 2021-04-25 b7da69a appstream.sql 2021-03-11 2021-04-25 b7da69a workspaceswrid.sql 2021-03-11 2021-04-25 b7da69a billservice.sql 2021-03-11 2021-04-25 b7da69a creditsdetail.sql 2021-03-11 2021-04-25 b7da69a spendaccount.sql 2021-03-11 2021-04-25 b7da69a spendregion.sql 2021-03-11 2021-04-25 b7da69a spendservice.sql 2021-03-11 2021-04-25 b7da69a rekognition.sql 2021-03-11 2021-04-25 b7da69a sagemakerwrid.sql 2021-03-11 2021-04-25 b7da69a textract.sql 2021-03-11 2021-04-25 b7da69a cloudtrail.sql 2021-03-11 2021-04-25 b7da69a cloudwatch-spend.sql 2021-03-11 2021-04-25 b7da69a s3costusagetypegroupwrid.sql 2021-03-09 2021-04-25 e44ab40 s3costusagetypewrid.sql 2021-03-09 2021-04-25 e44ab40 storage.sql 2021-03-09 2021-04-25 621b0dc ebssnapshotratio.sql 2021-03-08 2021-04-25 4195344 gluewrid.sql 2021-03-08 2021-04-25 500daeb ec2riutilization.sql 2021-03-08 2021-04-25 500daeb marketplacespend.sql 2021-03-08 2021-04-25 27962eb athena.sql 2021-03-08 2021-04-25 27962eb emr.sql 2021-03-08 2021-04-25 27962eb gluewrid.sql 2021-03-08 2021-04-25 27962eb kinesis.sql 2021-03-08 2021-04-25 27962eb quicksight.sql 2021-03-08 2021-04-25 27962eb mqwrid.sql 2021-03-08 2021-04-25 27962eb ses.sql 2021-03-08 2021-04-25 27962eb sqs.sql 2021-03-08 2021-04-25 27962eb account_spend_of_shared_sp.sql 2021-03-08 2021-04-25 27962eb ec2_total_spend.sql 2021-03-08 2021-04-25 27962eb ec2ricoverage.sql 2021-03-08 2021-04-25 27962eb ec2riutilization.sql 2021-03-08 2021-04-25 27962eb ec2runninghours.sql 2021-03-08 2021-04-25 27962eb ec2speffective.sql 2021-03-08 2021-04-25 27962eb ebsusagespend.sql 2021-03-08 2021-04-25 9537db2 ebssnapshotratio.sql 2021-03-08 2021-04-25 584426e storage.sql 2021-03-08 2021-04-25 ff2ca94 marketplacespend.sql 2021-03-06 2021-04-25 82ae943 athena.sql 2021-03-06 2021-04-25 82ae943 mqwrid.sql 2021-03-06 2021-04-25 82ae943 ses.sql 2021-03-06 2021-04-25 82ae943 sns.sql 2021-03-06 2021-04-25 82ae943 sqs.sql 2021-03-06 2021-04-25 82ae943 account_spend_of_shared_sp.sql 2021-03-06 2021-04-25 8015dfa compute_sp.sql 2021-03-06 2021-04-25 8015dfa gluewrid.sql 2021-03-05 2021-04-25 35d86b0 ec2ricoverage.sql 2021-03-05 2021-04-25 35d86b0 ec2riutilization.sql 2021-03-05 2021-04-25 35d86b0 creditsdetail.sql 2021-03-05 2021-04-25 35d86b0 rsum.sql 2021-03-05 2021-04-25 35d86b0 athena.sql 2021-03-05 2021-04-25 794cd2a emr.sql 2021-03-05 2021-04-25 794cd2a gluewrid.sql 2021-03-05 2021-04-25 794cd2a kinesis.sql 2021-03-05 2021-04-25 794cd2a quicksight.sql 2021-03-05 2021-04-25 794cd2a athena.sql 2021-03-04 2021-04-25 7f82fb7 ebs_gp2_to_gp3.sql 2021-03-04 2021-04-25 78c5ca8 sqs.sql 2021-03-02 2021-04-25 301808f ec2runninghours.sql 2021-02-24 2021-04-25 9e1743a ec2_sp_inventory.sql 2021-02-14 2021-04-25 045379b ebs_gp2_to_gp3.sql 2021-02-11 2021-04-25 7e113aa ec2_total_spend.sql 2021-02-03 2021-04-25 ddad141 spendfamily.sql 2021-01-22 2021-04-25 8c1f8ab spendxmonth.sql 2021-01-22 2021-04-25 8c1f8ab dynamodb.sql 2021-01-22 2021-04-25 57e4b29 ecs_eks.sql 2021-01-22 2021-04-25 11b7883 ecs_hours_day.sql 2021-01-22 2021-04-25 11b7883 dynamodb.sql 2021-01-22 2021-04-25 11b7883 spendfamily.sql 2021-01-22 2021-04-25 11b7883 spendxmonth.sql 2021-01-22 2021-04-25 11b7883 quicksight.sql 2021-01-22 2021-04-25 7cad13c elasticachewrid.sql 2021-01-22 2021-04-25 7cad13c appstream.sql 2021-01-22 2021-04-25 7cad13c backup_spend.sql 2021-01-22 2021-04-25 22bceec fsxwrid.sql 2021-01-22 2021-04-25 22bceec account_spend_of_shared_sp.sql 2021-01-21 2021-04-25 a7a360f elb_unused_wrid.sql 2021-01-14 2021-04-25 d20075e natgateway_idle_wrid.sql 2021-01-14 2021-04-25 d20075e workspaces_autostop_wrid.sql 2021-01-13 2021-04-25 74503ed monthly_rds_usage_type_by_tag.sql 2021-01-12 2021-04-25 17710e7 ec2_total_spend.sql 2020-12-28 2021-04-25 fddf7f2 billservice.sql 2020-12-17 2021-04-25 f2e4da0 ebssnapshotratio.sql 2020-12-17 2021-04-25 8c6dcfd ebssnapshotspend.sql 2020-12-17 2021-04-25 8c6dcfd ecs.sql 2020-12-17 2021-04-25 8c6dcfd ecswrid.sql 2020-12-17 2021-04-25 8c6dcfd billservice.sql 2020-12-17 2021-04-25 8c6dcfd creditsdetail.sql 2020-12-17 2021-04-25 8c6dcfd ec2ricoverage.sql 2020-12-17 2021-04-25 8c6dcfd ec2riutilization.sql 2020-12-17 2021-04-25 8c6dcfd reservationssavings.sql 2020-12-17 2021-04-25 8c6dcfd reservationssavings_sp.sql 2020-12-17 2021-04-25 8c6dcfd spendaccountsp.sql 2020-12-17 2021-04-25 8c6dcfd spendfamily.sql 2020-12-17 2021-04-25 8c6dcfd spendregionri.sql 2020-12-17 2021-04-25 8c6dcfd spendregionsp.sql 2020-12-17 2021-04-25 8c6dcfd spendservicesp.sql 2020-12-17 2021-04-25 8c6dcfd spendxmonth.sql 2020-12-17 2021-04-25 8c6dcfd cloudwatch-costs.sql 2020-12-17 2021-04-25 8c6dcfd cloudwatch-logusage.sql 2020-12-17 2021-04-25 8c6dcfd cloudwatch-usage.sql 2020-12-17 2021-04-25 8c6dcfd dxwrid.sql 2020-12-17 2021-04-25 8c6dcfd billservice.sql 2020-12-15 2021-04-25 cba2184 sagemakerusagewrid.sql 2020-12-14 2021-04-25 86ad47a sagemakerwrid.sql 2020-12-14 2021-04-25 86ad47a textract.sql 2020-12-14 2021-04-25 c63661b ec2speffective.sql 2020-12-14 2021-04-25 6693364 rekognition.sql 2020-12-14 2021-04-25 01eefda ebsusagespend.sql 2020-12-14 2021-04-25 aa0e360 ebsusagespend.sql 2020-12-14 2021-04-25 aa0e360 ebsusagespend.sql 2020-12-14 2021-04-25 0e36440 elasticsearch.sql 2020-12-12 2021-04-25 cfe977b emr.sql 2020-12-12 2021-04-25 cfe977b kinesis.sql 2020-12-12 2021-04-25 cfe977b mqwrid.sql 2020-12-12 2021-04-25 cfe977b ses.sql 2020-12-12 2021-04-25 cfe977b sns.sql 2020-12-12 2021-04-25 cfe977b redshiftwrid.sql 2020-12-12 2021-04-25 cfe977b appstreamwrid.sql 2020-12-12 2021-04-25 cfe977b s3costusagetypegroup.sql 2020-12-12 2021-04-25 cfe977b s3costusagetypegroupwrid.sql 2020-12-12 2021-04-25 cfe977b s3transitioncost.sql 2020-12-12 2021-04-25 cfe977b s3usage.sql 2020-12-12 2021-04-25 cfe977b s3usagerequests.sql 2020-12-12 2021-04-25 cfe977b s3usagerequestswrid.sql 2020-12-12 2021-04-25 cfe977b s3usagewrid.sql 2020-12-12 2021-04-25 cfe977b spendaccount.sql 2020-12-12 2021-04-25 ff3e55e spendregion.sql 2020-12-12 2021-04-25 ff3e55e spendservice.sql 2020-12-12 2021-04-25 ff3e55e ec2runninghours.sql 2020-12-11 2021-04-25 6a376f5 ec2savingsplans.sql 2020-12-11 2021-04-25 6a376f5 ec2hoursday_sp.sql 2020-12-11 2021-04-25 1bbed13 ec2hourstype.sql 2020-12-11 2021-04-25 1bbed13 ec2hourstype_sp.sql 2020-12-11 2021-04-25 1bbed13 ec2savingplan.sql 2020-12-11 2021-04-25 1bbed13 ec2spendaccount.sql 2020-12-11 2021-04-25 1bbed13 ec2spendtype.sql 2020-12-11 2021-04-25 1bbed13 ec2spendtype_sp.sql 2020-12-11 2021-04-25 1bbed13 ec2spinventory.sql 2020-12-11 2021-04-25 1bbed13 ecshoursday.sql 2020-12-11 2021-04-25 1bbed13 ecshoursday_sp.sql 2020-12-11 2021-04-25 1bbed13 lambda-w-rid.sql 2020-12-11 2021-04-25 1bbed13 lambda_sp.sql 2020-12-11 2021-04-25 7b161bd lambda_sp.sql 2020-12-11 2021-04-25 6b4857e rds-w-id.sql 2020-12-11 2021-04-25 6b4857e lambda_sp.sql 2020-12-11 2021-04-25 00f1322 marketplacespend.sql 2020-12-10 2021-04-25 f82aed1 subscription.sql 2020-12-10 2021-04-25 f82aed1 elasticsearch.sql 2020-12-10 2021-04-25 f82aed1 emr.sql 2020-12-10 2021-04-25 f82aed1 redshiftwrid.sql 2020-12-10 2021-04-25 f82aed1 kinesis.sql 2020-11-30 2021-04-25 23d6eaf direct-connect.sql 2020-11-27 2021-04-25 a50e95e elasticity.sql 2020-11-27 2021-04-25 daa19c6 dynamodb.sql 2020-11-27 2021-04-25 b44b5cb ec2runninghours.sql 2020-11-21 2021-04-25 79f34e0 compute_sp.sql 2020-11-17 2021-04-25 d47e1d1 marketplacespend.sql 2020-11-17 2021-04-25 0ecb1fe cloudfront.sql 2020-11-17 2021-04-25 913ddbc ec2runninghours.sql 2020-11-10 2021-04-25 aa01943 compute_sp.sql 2020-11-10 2021-04-25 6a743dc ec2hoursday.sql 2020-11-10 2021-04-25 6a743dc ec2runninghours.sql 2020-11-10 2021-04-25 6a743dc lambda_sp.sql 2020-11-10 2021-04-25 1c4db43 subscription.sql 2020-11-09 2021-04-25 e49bbdd kinesis.sql 2020-11-09 2021-04-25 e49bbdd mqwrid.sql 2020-11-09 2021-04-25 e49bbdd ses.sql 2020-11-09 2021-04-25 e49bbdd sns.sql 2020-11-09 2021-04-25 e49bbdd redshiftwrid.sql 2020-11-09 2021-04-25 b9b9b98 workspaceswrid.sql 2020-11-09 2021-04-25 b9b9b98 spendservicesp.sql 2020-11-09 2021-04-25 04cb8df cloudfront.sql 2020-11-08 2021-04-25 5711353 apigateway.sql 2020-11-08 2021-04-25 fbb98e3 spendaccount.sql 2020-11-07 2021-04-25 7a227ee spendaccountsp.sql 2020-11-07 2021-04-25 7a227ee spendfamily.sql 2020-11-07 2021-04-25 7a227ee spendregion.sql 2020-11-07 2021-04-25 7a227ee spendregion_sp.sql 2020-11-07 2021-04-25 7a227ee spendregionri.sql 2020-11-07 2021-04-25 7a227ee spendregionsp.sql 2020-11-07 2021-04-25 7a227ee spendservice.sql 2020-11-07 2021-04-25 7a227ee spendservice_sp.sql 2020-11-07 2021-04-25 7a227ee spendxmonth.sql 2020-11-07 2021-04-25 7a227ee waf.sql 2020-11-05 2021-04-25 5c77382 ebsusagespend.sql 2020-11-05 2021-04-25 32ec726 cognito.sql 2020-11-04 2021-04-25 e07c7b8 config.sql 2020-11-04 2021-04-25 8eef243 configspend.sql 2020-11-04 2021-04-25 8eef243 workspaceswrid.sql 2020-11-04 2021-04-25 5cf42e2 redshiftwrid.sql 2020-11-04 2021-04-25 7a97b17 connect.sql 2020-11-04 2021-04-25 93de01a ec2hoursday_sp.sql 2020-11-04 2021-04-25 63add7b ec2hourstype_sp.sql 2020-11-04 2021-04-25 63add7b networkusagewrid.sql 2020-11-03 2021-04-25 c319089 cloudwatch-spend.sql 2020-11-02 2021-04-25 d664472 natgatewaywrid.sql 2020-11-02 2021-04-25 d664472 tgwwrid.sql 2020-11-02 2021-04-25 e2f065f data-transfer.sql 2020-11-02 2021-04-25 9a6014d compute_sp.sql 2020-10-29 2021-04-25 bd9bfc8 ec2hoursday.sql 2020-10-29 2021-04-25 bd9bfc8 ec2hoursday_sp.sql 2020-10-29 2021-04-25 bd9bfc8 ec2hourstype.sql 2020-10-29 2021-04-25 bd9bfc8 ec2hourstype_sp.sql 2020-10-29 2021-04-25 bd9bfc8 ec2savingsplans.sql 2020-10-29 2021-04-25 bd9bfc8 ec2speffective.sql 2020-10-29 2021-04-25 bd9bfc8 lambda-w-rid.sql 2020-10-29 2021-04-25 bd9bfc8 lambda_sp.sql 2020-10-29 2021-04-25 bd9bfc8 rds-w-id.sql 2020-10-29 2021-04-25 bd9bfc8 subscription.sql 2020-10-29 2021-04-25 54a63ce elasticsearch.sql 2020-10-29 2021-04-25 54a63ce kinesis.sql 2020-10-29 2021-04-25 54a63ce mq-w-rid.sql 2020-10-29 2021-04-25 54a63ce mqwrid.sql 2020-10-29 2021-04-25 54a63ce ses.sql 2020-10-29 2021-04-25 54a63ce productdescriptions.sql 2020-10-29 2021-04-25 54a63ce data-transfer-msk.sql 2020-10-29 2021-04-25 3834d1a data-transfer.sql 2020-10-29 2021-04-25 3834d1a direct-connect.sql 2020-10-29 2021-04-25 3834d1a ebssnapshot-spend.sql 2020-10-29 2021-04-25 8b12988 elasticsearch.sql 2020-10-28 2021-04-25 c1216ab efswrid.sql 2020-10-23 2021-04-25 3709855 emr.sql 2020-10-23 2021-04-25 3243f2d cloudtrail.sql 2020-10-23 2021-04-25 3e1cdaf emr.sql 2020-10-23 2021-04-25 22f8363 elasticsearch.sql 2020-10-23 2021-04-25 f6f06a0 gluewrid.sql 2020-10-22 2021-04-25 63a9802 glue-w-rid.sql 2020-10-22 2021-04-25 6fd2e99 gluewrid.sql 2020-10-22 2021-04-25 6fd2e99 spendregion_ri.sql 2020-10-21 2021-04-25 50cd152 spendregion.sql 2020-10-21 2021-04-25 a205837 spendfamily.sql 2020-10-21 2021-04-25 94bc776 spendaccount.sql 2020-10-21 2021-04-25 3a40a07 spendaccount_sp.sql 2020-10-21 2021-04-25 4e2128d s3costusagetypegroup.sql 2020-10-21 2021-04-25 1ea5b5f s3costusagetypegroupwrid.sql 2020-10-21 2021-04-25 1ea5b5f spendaccount.sql 2020-10-21 2021-04-25 fe2f362 waf.sql 2020-10-21 2021-04-25 cef9a5b guardduty.sql 2020-10-21 2021-04-25 223e98d cognito.sql 2020-10-21 2021-04-25 3ced3e2 cloudfront.sql 2020-10-20 2021-04-25 f1f191f config.sql 2020-10-20 2021-04-25 f238f95 apigateway.sql 2020-10-20 2021-04-25 b9de167 apigateway.sql 2020-10-20 2021-04-25 f4a9a7d apigateway.sql 2020-10-20 2021-04-25 dc2a1a9 config.sql 2020-10-20 2021-04-25 d829657 dynamodb.sql 2020-10-20 2021-04-25 07fae54 tgwwrid.sql 2020-10-20 2021-04-25 233c09c natgatewaywrid.sql 2020-10-19 2021-04-25 d4f6d15 cloudtrail.sql 2020-10-19 2021-04-25 f2aa6a9 ec2savingsplans.sql 2020-10-19 2021-04-25 e31d883 lambda_sp.sql 2020-10-19 2021-04-25 217a23f ec2savingsplans.sql 2020-10-19 2021-04-25 f8ba04e ec2speffective.sql 2020-10-19 2021-04-25 f8ba04e ec2hoursday_sp.sql 2020-10-19 2021-04-25 eea67b3 ec2hourstype.sql 2020-10-19 2021-04-25 eea67b3 ec2hourstype_sp.sql 2020-10-19 2021-04-25 eea67b3 direct-connect.sql 2020-10-17 2021-04-25 9015406 mq-w-rid.sql 2020-10-17 2021-04-25 6368e8e sns.sql 2020-10-17 2021-04-25 6368e8e cloudwatch-costs.sql 2020-10-17 2021-04-25 6368e8e ebssnapshot-spend.sql 2020-10-17 2021-04-25 ef4e7d4 cloudwatch-spend.sql 2020-10-17 2021-04-25 c9991f9 elasticity.sql 2020-10-17 2021-04-25 2edf11e elasticity.sql 2020-10-17 2021-04-25 3692b11 data-transfer-msk.sql 2020-10-17 2021-04-25 b94e4a6 data-transfer.sql 2020-10-17 2021-04-25 9f3a498 ec2hoursday.sql 2020-10-16 2021-04-25 1ce743a lambda-w-rid.sql 2020-10-16 2021-04-25 53b3f15 compute_sp.sql 2020-10-16 2021-04-25 9643731 lambda-w-rid.sql 2020-10-16 2021-04-25 9643731 rds-w-id.sql 2020-10-16 2021-04-25 7ffb399 rds.sql 2020-10-16 2021-04-25 7ffb399 lambda-w-rid.sql 2020-10-16 2021-04-25 19b02bd subscription.sql 2020-10-15 2021-04-25 a75247e glue-w-rid.sql 2020-10-15 2021-04-25 a75247e ses.sql 2020-10-15 2021-04-25 a75247e sns.sql 2020-10-15 2021-04-25 a75247e productdescriptions.sql 2020-10-15 2021-04-25 c63c52d subscription.sql 2020-10-15 2021-04-25 60d9243 glue-w-rid.sql 2020-10-15 2021-04-25 60d9243 kinesis.sql 2020-10-15 2021-04-25 60d9243 mq-w-rid.sql 2020-10-15 2021-04-25 60d9243 ses.sql 2020-10-15 2021-04-25 60d9243 sesspend.sql 2020-10-15 2021-04-25 60d9243 sns.sql 2020-10-15 2021-04-25 60d9243 snsspend.sql 2020-10-15 2021-04-25 60d9243 lambda-w-rid.sql 2020-10-14 2021-04-25 0ad1abc kinesis.sql 2020-10-13 2021-04-25 b37ea45 lambda-w-rid.sql 2020-10-09 2021-04-25 5794ccb lambda-w-rid.sql 2020-10-09 2021-04-25 4388b73 data-transfer-msk.sql 2020-10-08 2021-04-25 e2ba7b8 rds.sql 2020-10-05 2021-04-25 21e5dff lambda-w-rid.sql 2020-10-05 2021-04-25 ffa8979 lambda-w-rid.sql 2020-10-05 2021-04-25 8cbc8e7 lambda-w-rid.sql 2020-10-02 2021-04-25 56da85c mq-w-rid.sql 2020-10-01 2021-04-25 f1132b4 cloudtrail.sql 2020-09-27 2021-04-25 3fab925 elasticity.sql 2020-09-27 2021-04-25 f874b4e elasticity.sql 2020-09-27 2021-04-25 cab9634 glue-w-rid.sql 2020-09-26 2021-04-25 fa78ee8 kinesis.sql 2020-09-26 2021-04-25 fa78ee8 glue-w-rid.sql 2020-09-26 2021-04-25 96bad6a cloudwatch-logusage.sql 2020-09-25 2021-04-25 f40eab1 cloudwatch-usage.sql 2020-09-25 2021-04-25 f40eab1 data-transfer.sql 2020-09-24 2021-04-25 d39c73c marketplacespend.sql 2020-09-18 2021-04-25 1b2b27f subscription.sql 2020-09-18 2021-04-25 1b2b27f sesspend.sql 2020-09-18 2021-04-25 1b2b27f snsspend.sql 2020-09-18 2021-04-25 1b2b27f ebssnapshotratio.sql 2020-09-18 2021-04-25 1b2b27f ebssnapshotspend.sql 2020-09-18 2021-04-25 1b2b27f ebsusagespend.sql 2020-09-18 2021-04-25 1b2b27f ec2hoursday.sql 2020-09-18 2021-04-25 1b2b27f ec2hoursday_sp.sql 2020-09-18 2021-04-25 1b2b27f ec2hourstype.sql 2020-09-18 2021-04-25 1b2b27f ec2hourstype_sp.sql 2020-09-18 2021-04-25 1b2b27f ec2savingplan.sql 2020-09-18 2021-04-25 1b2b27f ec2savingsplans.sql 2020-09-18 2021-04-25 1b2b27f ec2speffective.sql 2020-09-18 2021-04-25 1b2b27f ec2spendaccount.sql 2020-09-18 2021-04-25 1b2b27f ec2spendtype.sql 2020-09-18 2021-04-25 1b2b27f ec2spendtype_sp.sql 2020-09-18 2021-04-25 1b2b27f ec2spinventory.sql 2020-09-18 2021-04-25 1b2b27f ecs.sql 2020-09-18 2021-04-25 1b2b27f ecshoursday.sql 2020-09-18 2021-04-25 1b2b27f ecshoursday_sp.sql 2020-09-18 2021-04-25 1b2b27f ecswrid.sql 2020-09-18 2021-04-25 1b2b27f connect.sql 2020-09-18 2021-04-25 1b2b27f dynamodb.sql 2020-09-18 2021-04-25 1b2b27f appstreamwrid.sql 2020-09-18 2021-04-25 1b2b27f creditsdetail.sql 2020-09-18 2021-04-25 1b2b27f ec2ricoverage.sql 2020-09-18 2021-04-25 1b2b27f ec2riutilization.sql 2020-09-18 2021-04-25 1b2b27f reservationssavings.sql 2020-09-18 2021-04-25 1b2b27f reservationssavings_sp.sql 2020-09-18 2021-04-25 1b2b27f spendaccount.sql 2020-09-18 2021-04-25 1b2b27f spendaccount_sp.sql 2020-09-18 2021-04-25 1b2b27f spendfamily.sql 2020-09-18 2021-04-25 1b2b27f spendregion.sql 2020-09-18 2021-04-25 1b2b27f spendregion_ri.sql 2020-09-18 2021-04-25 1b2b27f spendregion_sp.sql 2020-09-18 2021-04-25 1b2b27f spendservice.sql 2020-09-18 2021-04-25 1b2b27f spendservice_sp.sql 2020-09-18 2021-04-25 1b2b27f spendxmonth.sql 2020-09-18 2021-04-25 1b2b27f productsdescriptions.sql 2020-09-18 2021-04-25 1b2b27f sagemakerusagewrid.sql 2020-09-18 2021-04-25 1b2b27f configspend.sql 2020-09-18 2021-04-25 1b2b27f dxwrid.sql 2020-09-18 2021-04-25 1b2b27f guardduty.sql 2020-09-18 2021-04-25 1b2b27f s3costusagetypegroup.sql 2020-09-18 2021-04-25 1b2b27f s3costusagetypegroupwrid.sql 2020-09-18 2021-04-25 1b2b27f s3transitioncost.sql 2020-09-18 2021-04-25 1b2b27f s3usage.sql 2020-09-18 2021-04-25 1b2b27f s3usagerequests.sql 2020-09-18 2021-04-25 1b2b27f s3usagerequestswrid.sql 2020-09-18 2021-04-25 1b2b27f s3usagewrid.sql 2020-09-18 2021-04-25 1b2b27f glue-w-rid.sql 2020-09-18 2021-04-25 a49236b glue-w-rid.sql 2020-09-11 2021-04-25 cea5280 glue-w-rid.sql 2020-09-11 2021-04-25 8884427 "},{"uri":"https://wellarchitectedlabs.com/security/300_labs/300_vpc_flow_logs_analysis_dashboard/","title":"Level 300: VPC Flow Logs Analysis Dashboard","tags":[],"description":"","content":"Author Chaitanya Shah, Sr. Technical Account Manager, AWS Introduction VPC Flow Logs enables you to capture information about the IP traffic going to and from network interfaces in your VPC. The VPC Flow Logs Analysis Dashboard is an interactive, customizable and accessible QuickSight dashboard to help customers gain insights into traffic details of VPC in a graphical way.\nThe dashboard depends on all the fields below. Therefore all of these fields are required in the VPC Flow Logs that are stored in S3:\nversion, account-id, interface-id, srcaddr, dstaddr, srcport, dstport, protocol, packets, bytes, start, end, action, log-status, vpc-id, az-id, instance-id, pkt-srcaddr, pkt-dstaddr, region, subnet-id, sublocation-id, sublocation-type, tcp-flags, type, flow-direction, pkt-dst-aws-service, pkt-src-aws-service, traffic-path This dashboard contains breakdowns with the following visuals. Available views are Summary, Details by daily, Minutes level granularity, and Enhanced view:\nBy VPC, InterfaceIds Between Source and Destination IPs By Region, AZ and Instances Source and destination AWS services paths (Enhanced view) Supported flow log record formats:\nCSV Parquet Supported Glue Partitions:\nNon Hive-compatible S3 prefix Hive-compatible S3 prefix Note: We recommend creating Parquet file format with Hive-compatible S3 prefix for better performance and reducing cost for querying data from S3\nArchitecture Goals This dashboard allows you to analyze and visualize vpc flow log data more flexibly, instead of focusing on the underlaying infrastructure, you can focus on investigating the logs. Prerequisites An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Amazon Enterprise Edition QuickSight Account For supported QuickSight regions please visit link Amazon QuickSight user has been already created Costs Note: Please refer to pricing page for current prices for below services\nA QuickSight Enterprise license starts at $18 per month and Readers $0.30/session up to $5 max/month AWS Athena $5.00 per TB of data scanned AWS Glue Storage: Free for the first million objects stored and $1.00 per 100,000 objects stored above 1M, per . Requests: Free for the first million requests per month. $1.00 per million requests above 1M in a month VPC Flow logs pricing (Example 5) to ingest data in S3 Data Transfer costs to store data coming from different accounts to central account bucket Time to complete The lab should take approximately 15-20 minutes to complete X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps: Enable VPC Flow Logs Create Athena resources, Lambda function and CloudWatch rule Create VPC Flow Logs QuickSight Analysis Dashboard Teardown "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/9_cleanup/","title":"Tear down this lab","tags":[],"description":"","content":"If you are attending an in-person workshop and were provided with an AWS account by the instructor:\nThere is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account:\nYou may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Delete S3 Bucket used by canary The canary (synthetic monitor) puts its run data into an S3 bucket. You need to empty and delete the bucket\nGo to the S3 Console Select the checkbox next to the bucket whose name starts with webserversforresiliencytesting-canarybucket Click Empty Follow the directions to empty the bucket Click Exit Ensuring the checkbox next to the canary bucket is still selected, click Delete Follow the directions to delete the bucket Remove manually provisioned resources Some resources were created by the failure simulation scripts. If you ran the AZ failure injection You need to remove these.\nGo to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions \u0026raquo; Edit subnet associations Uncheck all boxes and click Edit Actions \u0026raquo; Delete network ACL Remove AWS CloudFormation provisioned resources As part of lab setup you have deployed several AWS CloudFormation stacks. These directions will show you:\nHow to delete an AWS CloudFormation stack In what specific order the stacks must be deleted How to delete an AWS CloudFormation stack Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete\nIn the confirmation dialog, click Delete stack\nThe Status changes to DELETE_IN_PROGRESS\nClick the refresh button to update and status will ultimately progress to DELETE_COMPLETE\nWhen complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box.\nTo see progress during stack deletion\nClick the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal Single region If you deployed the single region option, then delete your stacks in the following order\nOrder CloudFormation stack 1 WebServersforResiliencyTesting 1 MySQLforResiliencyTesting 2 ResiliencyVPC 2 DeployResiliencyWorkshop Multi region If you deployed the multi region option, then see these instructions for the order in which to delete the CloudFormation stacks Delete remaining resources Delete Lambda execution role used to create custom resource This role was purposely not deleted by the CloudFormation stack, because CloudFormation needs it to delete the custom resource it was used to create. Choose ONE: AWS CLI or AWS Console.\nDo this step only after ALL CloudFormation stacks are DELETE_COMPLETE Using AWS CLI:\naws iam delete-role-policy --role-name LambdaCustomResourceRole-SecureSsmForRds --policy-name LambdaCustomResourcePolicy aws iam delete-role --role-name LambdaCustomResourceRole-SecureSsmForRds Using AWS Console:\nGo to the IAM Roles Console: https://console.aws.amazon.com/iam/home#/roles Search for SecureSsmForRds Check the box next to LambdaCustomResourceRole-SecureSsmForRds Click Delete role button Click Yes, delete button Delete Systems Manager parameter The password(s) for your Amazon RDS instances were stored in AWS Systems Manager secure parameter store. These steps will verify the parameter(s) were deleted, and if not then guide you to deleting them. Choose ONE: AWS CLI or AWS Console.\nsingle region You only need to do the following steps in us-east-2 multi region Do the following steps for both us-east-2 and us-west-2 Using AWS CLI:\nThe following command will delete the parameter:\naws ssm delete-parameter --name 300-ResiliencyofEC2RDSandS3 If you get ParameterNotFound then the password was already deleted by the CloudFormation stack (as expected).\nUsing AWS Console:\nSelect the region Wait until ResiliencyVPC CloudFormation stack is DELETE_COMPLETE in the region Go to the AWS Console for AWS Systems Manager parameter store Look for the parameter created for your infrastructure. If you used our default values, this will be named 300-ResiliencyofEC2RDSandS3 If it is not present (check all regions you deployed to) then you are finished If it is present then Click on the parameter name Click the Delete button Click Delete again Delete FIS experiment templates Navigate to the FIS console at http://console.aws.amazon.com/fis For each experiment template created as part of this lab: Select the template Click Actions Select Delete experiment template Delete FIS service role Navigate to the AWS Identity and Access Management (IAM) console . Delete the WALab-FIS-policy IAM policy. Delete the WALab-FIS-role IAM role. References \u0026amp; useful resources EC2 Auto Scaling Groups What Is an Application Load Balancer? High Availability (Multi-AZ) for Amazon RDS Amazon RDS Under the Hood: Multi-AZ Regions and Availability Zones Injecting Chaos to Amazon EC2 using AWS System Manager Build a serverless multi-region, active-active backend solution in an hour X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 12 How do you test reliability?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Amazon S3 1.1 Click S3 to navigate to the dashboard.\n1.2 Select active-primary-uibucket-xxxx and then click the Empty button.\n1.3 Enter permanently delete into the confirmation box and then click Empty.\n1.4 Wait until you see the green banner across the top of the page, indicating the bucket is empty. Then click the Exit button.\nPlease repeat steps 1.1 through 1.4 for the following buckets: passive-secondary-uibucket-xxxx active-primary-assetbucket-xxxx passive-secondary-assetbucket-xxxx\nAmazon DynamoDB 2.1 Click DynamoDB to navigate to the dashboard in the N. Virginia (us-east-1) region.\n2.2 Click the Tables link.\n2.3 Click unishophotstandy.\n2.4 Click the Global Tables link. Select N. California (us-west-1), then click the Delete region button.\n2.5 Enter delete then click the Delete button.\n2.6 Click RDS to navigate to the dashboard in the N. California (us-west-1) region.\n2.7 Click the DB Instances link.\n2.8 Select unishopappv1db, then click Delete under the Actions dropdown.\n2.9 Disable the Create final snapshot checkbox. Enable the I acknowledgement \u0026hellip; checkbox. Enter delete me and click the Delete button.\nAmazon CloudFormation 3.1 Click CloudFormation to navigate to the dashboard in the N. California (us-west-1) region.\n3.2 Select Passive-Secondary, then click the Delete button.\n3.3 Click the Delete stack button.\n3.4 Change your console ’s region to N. Virginia (us-east-1) using the Region Selector in the upper right corner.\n3.5 Select Active-Primary, then click the Delete button.\n3.6 Click the Delete stack button.\nAmazon CloudFormation 4.1 Click CloudFront to navigate to the dashboard.\n4.2\nX Congratulations! This lab specifically helps you with the best practices covered in question REL 13 How do you plan for disaster recovery (DR)\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/sustainability/200_labs/200_optimize_ec2_using_cloudwatch_compute_optimizer/cleanup/","title":"Cleanup","tags":[],"description":"","content":"To avoid incurring further costs for AWS resources, let’s delete the Amazon Redshift clusters in both regions. Login to AWS console, and go to the Amazon Redshift service . Then follow the below steps in each region to delete producer (us-east-1 region) and consumer (us-west-1 region) clusters:\nSelect the cluster in respective region, and select Delete from Actions menu: To confirm deletion, type in “delete” in the field at the bottom (make sure, you do not check Create final snapshot button as it will incur cost for the storage of the snapshot), and click Delete cluster. Follow above steps for the cluster in the other region as well. Ensure you have deleted clusters in both us-east-1 and us-west-1 or whichever two regions you chose to complete this lab. X Congratulations! You should now have a firm understanding of how to use proxy metric, business metric, and sustainability KPI with AWS Services like Amazon Redshift Data Sharing for optimizing workload data patterns for environmental sustainability improvements.\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/sustainability/300_labs/300_optimize_data_pattern_using_redshift_data_sharing/cleanup/","title":"Cleanup","tags":[],"description":"","content":"To avoid incurring further costs for AWS resources, let’s delete the Amazon Redshift clusters in both regions. Login to AWS console, and go to the Amazon Redshift service . Then follow the below steps in each region to delete producer (us-east-1 region) and consumer (us-west-1 region) clusters:\nSelect the cluster in respective region, and select Delete from Actions menu: To confirm deletion, type in “delete” in the field at the bottom (make sure, you do not check Create final snapshot button as it will incur cost for the storage of the snapshot), and click Delete cluster. Follow above steps for the cluster in the other region as well. Ensure you have deleted clusters in both us-east-1 and us-west-1 or whichever two regions you chose to complete this lab. X Congratulations! You should now have a firm understanding of how to use proxy metric, business metric, and sustainability KPI with AWS Services like Amazon Redshift Data Sharing for optimizing workload data patterns for environmental sustainability improvements.\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_licensing/","title":"Level 200: Licensing","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Authors Nathan Besh, Cost Lead, Well-Architected (AWS) Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction This hands-on lab will guide you through analyzing your cost and usage for licensing costs, and the cost of licensing in your workloads. You will then be shown how to analyze the data and decide if it is beneficial to change to non-licensed software. In this lab we show the techniques using Operating System licences, but these techniques can be applied to any licensed software.\nYou will first setup a data source (if required) using the sample provided. This is a Cost and Usage report that contains licensed usage. You may use your own cost and usage report, however you need to ensure there is licensed usage and modify the examples in the lab.\nYou analyze the CUR to discover the costs of operating system licenses, and also the cost of running licensed operating systems. With this data you will make an analysis of the cost savings by switching to an unlicensed operating system. We have also provided an additional data set with the changes applied, to simulate this change and verify the savings.\nGoals Discover the cost of licensed software in your cost and usage Analyze and understand the benefit of moving to unlicensed software Prerequisites An AWS Account (Optional) Your own Cost and Usage Report with licensed software (RHEL - RedHat linux) usage Completed the AWS Account Setup lab Completed the Cost and Usage Analysis lab Permissions required Log in as the Cost Optimization team, created in AWS Account Setup The following additional permissions: ec2:DescribeImages, ec2:DescribeVpcs, ec2:DescribeSubnets are optional, as you can complete the lab without them - however you will not be able to access pages in the EC2 console Costs Approximately \u0026lt;$5 Time to complete The lab should take approximately 15-20 minutes to complete Steps: Create Pricing Data Source Analyze and Understand Licensing Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/machine_learning/","title":"Machine Learning","tags":[],"description":"","content":"These are queries for AWS Services under the Machine Learning product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon Rekognition Amazon SageMaker Amazon Textract Amazon Rekognition Query Description This query will provide daily unblended and usage information per linked account for Amazon Rekognition. The output will include detailed information about the usage type and usage region. The cost will be summed by day, account, and usage type, and displayed in descending order.\nPricing Please refer to the Rekognition pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_usage_type, product_region, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM {$table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonRekognition\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, product_region ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon SageMaker Query Description This query will provide daily unblended cost and usage information per resource ID for Amazon SageMaker. The output will include detailed information about associated usage types. The cost and usage will be summed by day, account, resource ID, and usage type, and displayed in descending order.\nPricing Please refer to the SageMaker pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_resource_id, line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM {$table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonSageMaker\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon Textract Query Description This query will provide daily unblended and usage information per linked account for Amazon Textract. The output will include detailed information about the usage type and usage region. The cost and usage will be summed by day, account, and usage type, and displayed in descending order.\nPricing Please refer to the Textract pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_usage_type, product_region, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM {$table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonTextract\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, product_region ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/9_monthly_report/","title":"Configure Monthly Reports (Optional)","tags":[],"description":"","content":"It is possible to enable a CSV version of your summary cost and usage information called \u0026ldquo;Monthly Report. This is typically used by customers looking to analyze their costs in a spreadsheet format with ease of use. This is part of a legacy feature called \u0026ldquo;Detailed Billing Reports,\u0026rdquo; but is used across many organizations for bill validations. Once setup, it will take up to 24 hours to create a file for your most recent bill.\nConfigure Monthly Reports Go to the billing dashboard: Click on Billing Preferences from the left menu: Click Detailed Billing Reports [Legacy] and check the box to turn on this feature. Click Configure S3 Bucket to configure a bucket to store these reports. Enter a unique bucket name, ensure the region is correct, and click Next. Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct, then click Save: Check off both Monthly Report and Cost Allocation Report and click Save Preferences to complete turning on Monthly Reporting. For more details on the monthly report please take a look here: https://docs.aws.amazon.com/cur/latest/userguide/monthly-report.html .\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/9_create_quicksight_dashboard/","title":"Create a QuickSight Visualization","tags":[],"description":"","content":"Amazon QuickSight is a Business Intelligence service that allows you to streamline delivery of data insights. Prior to building out a dashboard, you need to prepare QuickSight by creating an account and linking your table from Athena. Using QuickSight, you will build out a dashboard to display responses to site visits on your website.\nOpen the QuickSight console . If this is your first time accessing QuickSight, you will need to create your QuickSight account. You may be prompted to sign up for QuickSight - click Sign up for QuickSight. Ensure that the AWS Account number shown matches your AWS Account. If not, click the log in again link to sign in with the correct account. Choose the edition you would like to use. For this lab, the Standard edition is sufficient. On the next screen, you will need to configure some options. First, select the QuickSight region. This should be the same region you have been operating in for the rest of the lab. Enter a QuickSight account name and Notification email address. Ensure that the Amazon Athena box is checked. This allows QuickSight to access Athena databases. Check the Amazon S3 box, on the screen that follows. Click the tick box next to the bucket you created for this lab, likely called wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;, also select the Write permission for Athena Workgroup tick box for that bucket. This gives permissions for QuickSight to access data in the S3 bucket and write to the Athena Workgroup used for querying. Click Finish. Click Finish. On the next screen, click Go to QuickSight. If this is not your first time using QuickSight, you will need to give QuickSight permission to access log files from your S3 bucket. Click on your user icon in the top right corner of the screen. Click Manage QuickSight. Click Security \u0026amp; Permissions. QuickSight may prompt you to change your region to N. Virginia to do so. Click on the user icon and change your region if needed. Under QuickSight access to AWS Services, click Add or remove. Tick the tick box next to Amazon Athena. This allows QuickSight to access Athena databases. Tick the tick box next to Amazon S3. If S3 is already ticked, click **Details **and then Select S3 buckets. Click the tick box next to the bucket you created for this lab, likely called wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;, also select the Write permission for Athena Workgroup tick box for that bucket. This gives permissions for QuickSight to access data in the S3 bucket and write to the Athena Workgroup used for querying. Click Finish. Click Update. Click on the QuickSight logo in the top right corner of the page to return to the QuickSight homepage. If you changed your region to N. Virginia to change permissions, switch it back to the region you have been operating in for this lab. Now you can create your analysis dashboard. Click the New analysis button in the top left corner of the screen. Click the New dataset button in the top left corner of the screen to link your Athena database to QuickSight. On the screen that appears, click Athena Enter a Data source name, such as security-lab-log-data. Leave the Athena workgroup as [ primary ]. Click Create data source. In the Choose your table screen, click the dropdown menu and select security_lab_logs, the database you created in Athena. In the table selection menu that appears, select security_lab_apache_access_logs. Click Select. In the Finish data set creation screen, select the Directly query your data radio bubble. Click Visualize. You should now see a screen similar to the one below. Now, you will create a bar graph showing frequency of response types by day. To do so, click on the Vertical bar chart in the Visual types section in the bottom left. You will see three Field wells at the top of the page. You will set these fields by dragging and dropping fields from the left side menu into the field wells. Set X axis to request_date. Set the Value and Group/Color to response_code. Your graph should be created similar to the one above. You have now created a graph that displays the count of each response type your page received each day. Since this architecture was deployed for this lab, your graph will be very simple - with only one date on the X Axis. Now, you can publish this analysis into a QuickSight dashboard. Click the Share button in the top right corner of the screen. In the dropdown menu, click Publish dashboard. Select Publish new dashboard as in the pop-up menu. Enter a name, such as Security-Lab-CW-Apache-Responses. Click Publish dashboard. This will take you to your new dashboard with a pop-up window to share the dashboard with users. This allows you to send the dashboard to other people. For the purposes of this lab, you can close out of this window by clicking the X in the top right corner. You should now see your QuickSight dashboard. This dashboard will be very simple, only containing the one graph created in your analysis. However, this should elucidate the purpose of QuickSight. You are able to communicate important information about your website to a broad audience, all without directly touching the data needed. Recap: In this section, you built out a QuickSight visualization to display the responses generated by hits on your website. In doing this, you create a digestible source of data and information that anyone can understand. This also demonstrates the security best practices of “analyzing logs centrally” and “keeping people away from data”, as your log files are centrally graphed in QuickSight and the source log data is abstracted from viewers on many levels.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_for_web_application/","title":"Level 200: CloudFront for Web Application","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront. Steps: Configure CloudFront - EC2 or Load Balancer Tear down "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_detect_and_investigate_events/","title":"Quest: Detect &amp; Investigate Events","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Detective Controls . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.\nStart the Lab! Enable Security Hub Introduction AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.\nStart the Lab! Further Learning: AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/9_manage_demand_supply/","title":"Manage Demand &amp; Supply Resources","tags":[],"description":"","content":"Manage Demand, and Supply Resources Supply resources dynamically Demand vs Supply Goal: Minimize unused resources Target: Workload resourcing should only deviate by a maximum of $x from workload demand Best Practice: Dynamic-based supply Measures: Deviation % and $ Good/Bad: Good Why? When does it work well or not?: Ensure there is not an excess of resources compared to the workload demand, and if there is variable demand that the resourcing follows this closely. Contact/Contributor: natbesh@amazon.com Supply resources Supply vs business/demand Goal: Minimize unused resources Target: Non-production or development/test workloads must not have more than 5% (by cost) of resources available outside of business hours Best Practice: Time-based supply Measures: Deviation % Good/Bad: Good Why? When does it work well or not?: Can be hard to track when development/test activities occur outside of defined hours. Contact/Contributor: natbesh@amazon.com Manage demand Supply vs business/demand Goal: Minimize unused resources Target: Workload demand should not vary by more than 10% on any single day. Best Practice: Manage Demand Measures: Deviation % Good/Bad: Good Why? When does it work well or not?: Will be unachievable when demand cannot be altered. Works well when demand can be throttled or buffered to smooth out the peaks. Contact/Contributor: natbesh@amazon.com X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cost_journey/","title":"Level 200: Cost Journey","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Last Updated March 2021\nAuthors Nathan Besh, Cost Lead Well-Architected (AWS) Tom McMeekin, Solutions Architect (AWS) Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Introduction In this lab you will create your organizations cost optimization journey. It will show you where your workload and organization is currently at, in terms of capability, and what lies ahead - so you can plan and resource accordingly.\nThe lab will create a lambda function, which reads all the AWS Well-Architected reviews in the account, and produce a webpage with an image showing your journey, as below.\nGoals Create a cost optimization journey for each workload with a Well-Architected review Prerequisites Performed at least one AWS Well-Architected review on a workload Permissions required Create and manage an S3 bucket Create an IAM role to run a lambda function Create and run a lambda function Access to the Well-Architected service Costs Estimated costs are \u0026lt;$5 for an average customer (lambda execution, s3 storage) Time to complete 15minutes Steps: Configure Services Create Journey Teardown X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/management__governance/","title":"Management &amp; Governance","tags":[],"description":"","content":"These are queries for AWS Services under the Management \u0026amp; Governance product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents AWS Config AWS CloudTrail AWS CloudWatch Regional Service Usage Mapping Tag Coverage AWS Config Query Description This query will provide daily unblended and usage information per linked account for AWS Config. The output will include detailed information about the usage type and usage region. The cost will be summed by day, account, and usage type.\nPricing Please refer to the AWS Config pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_region, CASE WHEN line_item_usage_type LIKE \u0026#39;%%ConfigurationItemRecorded%%\u0026#39; THEN \u0026#39;ConfigurationItemRecorded\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%ActiveConfigRules%%\u0026#39; THEN \u0026#39;ActiveConfigRules\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%SecurityHubConfigRules%%\u0026#39; THEN \u0026#39;SecurityHubConfigRules\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%ConfigRuleEvaluations%%\u0026#39; THEN \u0026#39;ConfigRuleEvaluations\u0026#39; ELSE \u0026#39;Others\u0026#39; END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE (year = \u0026#39;2020\u0026#39; AND month IN (\u0026#39;1\u0026#39;,\u0026#39;01\u0026#39;) OR year = \u0026#39;2020\u0026#39; AND month IN (\u0026#39;2\u0026#39;,\u0026#39;02\u0026#39;)) AND product_product_name = \u0026#39;AWS Config\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_region, line_item_usage_type ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, case_line_item_usage_type; Help \u0026amp; Feedback Back to Table of Contents AWS CloudTrail Query Description This query will provide monthly unblended and usage information per linked account for AWS CloudTrail. The output will include detailed information about the usage type and usage region. The cost will be summed by month, account, and usage type, and displayed in descending order.\nPricing Please refer to the CloudTrail pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, product_product_name, SPLIT_PART(line_item_usage_type,\u0026#39;-\u0026#39;,2) AS split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;AWS CloudTrail\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), SPLIT_PART(line_item_usage_type,\u0026#39;-\u0026#39;,2), product_product_name ORDER BY sum_line_item_unblended_cost DESC, month_line_item_usage_start_date, sum_line_item_usage_amount; Help \u0026amp; Feedback Back to Table of Contents AWS CloudWatch Query Description This query will provide monthly unblended and usage information per linked account for AWS CloudWatch. The output will include detailed information about the usage type. The cost will be summed by month, account, and usage type, and displayed in descending order.\nResource ID can also be included by uncommenting the appropriate lines in the query.\nPricing Please refer to the CloudWatch pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) month_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE \u0026#39;%%Requests%%\u0026#39; THEN \u0026#39;Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%DataProcessing-Bytes%%\u0026#39; THEN \u0026#39;DataProcessing\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%TimedStorage-ByteHrs%%\u0026#39; THEN \u0026#39;Storage\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%DataScanned-Bytes%%\u0026#39; THEN \u0026#39;DataScanned\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%AlarmMonitorUsage%%\u0026#39; THEN \u0026#39;AlarmMonitors\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%DashboardsUsageHour%%\u0026#39; THEN \u0026#39;Dashboards\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%MetricMonitorUsage%%\u0026#39; THEN \u0026#39;MetricMonitor\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%VendedLog-Bytes%%\u0026#39; THEN \u0026#39;VendedLogs\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%GMD-Metrics%%\u0026#39; THEN \u0026#39;GetMetricData\u0026#39; ELSE \u0026#39;Others\u0026#39; END AS line_item_usage_type, -- if uncommenting, also uncomment one other occurrence of line_item_resource_id in GROUP BY -- SPLIT_PART(line_item_resource_id,\u0026#39;:\u0026#39;,7) as ResourceID, line_item_operation, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;AmazonCloudWatch\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_usage_type, -- line_item_resource_id, line_item_operation ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Regional Service Usage Mapping Query Description Amazon Web Services publishes our most up-to-the-minute information on our Service Health Dashboard . This dashboard is based on region and service. While we try to notify you of ongoing problems that may be impactful to your workloads via your Personal Health Dashboard you may want to proactively check where you currently have service usage and cost that may be impacted by our event or another regional issue.\nThis Regional Service Usage Mapping query transforms your billing data into a summarized view of your usage of AWS services by region and availability zone, providing your operations teams with the ability to respond quickly and accurately during impacting service events.\nSample Output Download SQL File Link to Code Copy Query SELECT CASE WHEN (bill_billing_entity = \u0026#39;AWS Marketplace\u0026#39; AND line_item_line_item_type NOT LIKE \u0026#39;%Discount%\u0026#39;) THEN product_product_name WHEN (product_product_name = \u0026#39;\u0026#39;) THEN line_item_product_code ELSE product_product_name END AS product_name, CASE product_region WHEN NULL THEN \u0026#39;Global\u0026#39; WHEN \u0026#39;\u0026#39; THEN \u0026#39;Global\u0026#39; WHEN \u0026#39;global\u0026#39; THEN \u0026#39;Global\u0026#39; ELSE product_region END AS product_region, line_item_availability_zone, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY 1, line_item_product_code, line_item_availability_zone, product_region HAVING SUM(line_item_unblended_cost) \u0026gt; 0 ORDER BY product_region, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Tag Coverage Query Description This query will return a the percentage of resources which are tagged by linked account and product for a specific tag. This will help you determine how good your tagging quality is. For example, if they wish to use to use this field for chargeback or automation. Currently this using the resource_tags_user_name tag but this can be swapped out for any other tags in your CUR. To see these tags use this query:\nSELECT column_name FROM information_schema.columns WHERE table_schema = \u0026#39;${database_name}\u0026#39; AND table_name = \u0026#39;${table_name}\u0026#39; AND column_name LIKE \u0026#39;resource_tags%\u0026#39; Sample Output Download SQL File Link to Code Copy Query WITH allresources AS ( SELECT line_item_usage_account_id , line_item_product_code , count(DISTINCT line_item_resource_id) AS count_resource , CASE WHEN resource_tags_user_name = \u0026#39;\u0026#39; THEN \u0026#39;NoTag\u0026#39; ELSE \u0026#39;Tag\u0026#39; END AS Tag_Status FROM ${table_name} WHERE ${date_filter} GROUP BY line_item_usage_account_id, line_item_product_code, 4 -- 4 represents the Tag_Status column ) SELECT line_item_usage_account_id , line_item_product_code , sum(count_resource) AS \u0026#34;resources\u0026#34; , sum(CASE WHEN Tag_Status = \u0026#39;Tag\u0026#39; THEN count_resource ELSE 0 END) AS \u0026#34;tagged_resources\u0026#34; , round(sum(CASE WHEN Tag_Status = \u0026#39;Tag\u0026#39; THEN CAST(count_resource AS double) ELSE 0. END)/ sum(count_resource) * 100., 1) AS \u0026#34;percentage_tagged\u0026#34; FROM allresources GROUP BY 1,2 Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/settingup/backupregion/","title":"Backup Region","tags":[],"description":"","content":"Backup Region In this section we\u0026rsquo;ll deploy necessary infrastructure to the backup region to support data backup and workload failover.\nS3 bucket for templates First, create an S3 bucket to hold the CloudFormation templates. We\u0026rsquo;ll call this the TEMPLATEBUCKET. Choose a globally unique bucket name that adheres to the bucket naming rules .\naws s3 mb s3://\u0026lt;TEMPLATEBUCKET\u0026gt; --region \u0026lt;REGION\u0026gt; aws s3api put-bucket-versioning --bucket \u0026lt;TEMPLATEBUCKET\u0026gt; --versioning-configuration Status=Enabled Download and review scripts and templates Download the following files:\ncreate-dr.sh dr-data.yaml Let\u0026rsquo;s review what\u0026rsquo;s in these files. You can open them in your favorite text editor to review in detail. The CloudFormation script, dr-data.yaml, creates the S3 buckets used for data replication and inventory, the AWS Glue metadata catalog and table definitions, and a Lambda function used to update metadata catalog partitions. The script accepts eight input parameters. Five of these are for the AWS Glue metadata catalog, and the default settings should be fine, although you are free to change them. Another parameter defines a tag value that helps us identify resources used in this stack, and again the default value should be fine. The last two parameters define the names of the S3 buckets that we use for data replication and for capturing S3 inventory. We will pass in these parameters through the create-dr.sh script.\nThe create-dr.sh script is purely for convenience. It uploads dr-data.yaml to the S3 template bucket, and then creates the CloudFormation stack, passing in the required input arguments.\nIn your working directory, place create-dr.sh in a directory called scripts and place the file dr-data.yaml in a directory called cfn.\nDeploy stack Now create the stack in the backup region:\nexport AWS_PROFILE=BACKUP chmod +x ./scripts/create-dr.sh ./scripts/create-dr.sh \u0026lt;template bucket\u0026gt; \u0026lt;template prefix\u0026gt; \u0026lt;stack name\u0026gt; \u0026lt;REGION\u0026gt; \u0026lt;backup bucket name\u0026gt; \u0026lt;inventory bucket name\u0026gt; The input arguments are:\ntemplate bucket - the name of the S3 bucket we created earlier in this section. We use it to store the CloudFormation templates and other data. template prefix - An S3 prefix we append to the CloudFormation template file names. We use cfn as a convention. stack name - The name of the CloudFormation stack. You can pick any suitable name. REGION - The primary region. This argument is the region that the template bucket is in, and we created the template bucket in the primary region. backup bucket name - The name for the S3 bucket used for data replication. You can pick any globally unique name that satisfies the S3 bucket naming rules. inventory bucket name - The name for the S3 bucket used for S3 inventory. You can pick any globally unique name that satisfies the S3 bucket naming rules. For example:\n./scripts/create-dr.sh backuprestore cfn BackupRestore us-west-2 MyBackupBucket MyInventoryBucket To update the stack, add the --update flag as the last argument.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failover/backupinfra/","title":"Backup Region Infrastructure","tags":[],"description":"","content":"Workload in the Backup Region If we\u0026rsquo;ve decided to fail over to the backup region, the first thing we need to do is deploy the rest of the workload in the backup region.\nDownload and review scripts and templates Download the following files:\ncreate-dr-infra.sh dr-infra.yaml Review these files and adjust any of the input parameters to suit your needs.\nIn your working directory, place create-dr-infra.sh in a directory called scripts and place the file dr-infra.yaml in a directory called cfn.\nDeploy stack Now create the stack in the backup region:\nexport AWS_PROFILE=BACKUP chmod +x ./scripts/create-dr-infra.sh ./scripts/create-dr-infra.sh \u0026lt;template bucket\u0026gt; \u0026lt;template prefix\u0026gt; \u0026lt;stack name\u0026gt; \u0026lt;REGION\u0026gt; \u0026lt;backup bucket name\u0026gt; \u0026lt;ingress prefix\u0026gt; \u0026lt;ingress CIDR\u0026gt; \u0026lt;backup ARN\u0026gt; \u0026lt;source table ARN\u0026gt; \u0026lt;target table name\u0026gt; Note that we pass in the primary region as the fourth argument. The ingress CIDR argument is the static IP for our example producer, which you can find in the output of the CFN stack used in the primary region. The source table ARN argument can be found in the DynamoDB console, and the target table name argument is processed_tweets. The backup ARN argument is optional in case you want to restore from a specific backup rather than using PITR; you can set it to a placeholder value like arn otherwise.\nFor example:\n./scripts/create-dr-infra.sh backuprestore cfn BackupRestore us-west-2 MyBackupBucket MyPrefix 1.2.3.4/32 arn:aws:dynamodb:us-west-2:XXX:table/processed_tweets/backup/01628628265604-97d01acc arn:aws:dynamodb:us-west-2:XXX:table/processed_tweets processed_tweets To update the stack, add the --update flag as the last argument.\nStart Analtyics Application Now, navigate to Kinesis Analytics in the AWS Console in the backup region. Click on the radio button for the application called \u0026lt;stack name\u0026gt;-KinesisAnalyticsApplication and select Run.\nEnable DynamoDB point-in-time recovery Navigate to DynamoDB in the AWS Console. Select the processed_tweets table and go to Backups. Enable PITR and save the changes.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/account/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites To run this workshop, you need an AWS account, and a user identity with access to the following services:\nGlue S3 Athena DynamoDB Global Accelerator EC2 and ELB Lambda Kinesis You can use your own account, or an account provided through Event Engine as part of an AWS organized workshop. Using an account provided by Event Engine is the easier path, as you will have full access to all AWS services, and the account will terminate automatically when the event is over.\nYou should also have familiarity with using the AWS CLI, including configuring the CLI for a specific account and region profile. If not, please follow the CLI setup instructions . Make sure you have a default profile set up; you may need to run aws configure if you have never set up the CLI before.\nAccount setup Using an account provided through Event Engine If you are running this workshop as part of an Event Engine lab, please log into the console using this link and enter the hash provided to you as part of the workshop.\nUsing your own AWS account If you are using your own AWS account, be sure you have access to create and manage resources in the services noted above.\nAfter completing the workshop, remember to complete the cleanup section to remove any unnecessary AWS resources.\nNote your account and region After you have your account identified, pick a primary AWS region to work in, such as us-west-2. We\u0026rsquo;ll refer to this as REGION going forward. Then pick a backup region, such as us-east-2. We\u0026rsquo;ll refer to this as BACKUPREGION going forward.\nCLI Profiles Set up two CLI profiles, one for the primary region and one for the backup region. We\u0026rsquo;ll name these PRIMARY and BACKUP.\nIn this example PRIMARY uses us-west-2 and BACKUP uses us-east-2. Choose whichever regions you prefer.\n$ aws configure --profile BACKUP AWS Access Key ID [None]: \u0026lt;\u0026lt;provide access key id\u0026gt;\u0026gt; AWS Secret Access Key [None]: \u0026lt;\u0026lt;provide access key \u0026gt;\u0026gt; Default region name [None]: us-east-2 Default output format [None]: $ aws configure --profile PRIMARY AWS Access Key ID [None]: \u0026lt;\u0026lt;provide access key id\u0026gt;\u0026gt; AWS Secret Access Key [None]: \u0026lt;\u0026lt;provide access key \u0026gt;\u0026gt; Default region name [None]: us-west-2 Default output format [None]: For more details refer to the CLI documentation .\nRegions This workshop should work in us-east-1, us-east-2, or us-west-2. You can likely use it in other regions but may have to make some minor adjustments to the CloudFormation templates.\nAccount Number Also note your AWS account number. You find this in the console or by running aws sts get-caller-identity on the CLI. We\u0026rsquo;ll refer to this as ACCOUNT going forward. You can store this in an environment variable for convenience:\nexport AWS_PROFILE=PRIMARY export ACCOUNT=$(aws sts get-caller-identity --query Account --output text) Managed Prefix List You will have to create two prefix lists, one in the backup region and another one in the primary region. These prefix lists should include the network range (CIDR) that you want to use for ingress traffic, such as your corporate network. If you do not know the CIDR to use and you are working in an AWS-provided account, you can set this to 0.0.0.0/0 to allow inbound traffic from anywhere, but be aware that this is an insecure configuration and should not be used in your own accounts without a security review.\nFor instructions on creating prefix list, refer to the documentation . From the command line, you could run:\nexport AWS_PROFILE=PRIMARY aws ec2 create-managed-prefix-list \\ --prefix-list-name \u0026lt;choose a name\u0026gt; \\ --entries Cidr=\u0026lt;enter your CIDR\u0026gt;,Description=CorpNetworkPrimary \\ --max-entries 10 \\ --address-family IPv4 export AWS_PROFILE=BACKUP aws ec2 create-managed-prefix-list \\ --prefix-list-name \u0026lt;choose a name\u0026gt; \\ --entries Cidr=\u0026lt;enter your CIDR\u0026gt;,Description=CorpNetworkBackup \\ --max-entries 10 \\ --address-family IPv4 X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failback/recreatedynamodb/","title":"Recreate DynamoDB","tags":[],"description":"","content":"Recreate DynamoDB When failing back to the primary region, we need to get the latest data from the DynamoDB table in the backup region into the table in the primary region. We\u0026rsquo;ll do this by restoring the table from the backup region. Note that using DynamoDB Global Tables is a more convenient way to handle multi-region scenarios with DynamoDB, but in this example we\u0026rsquo;ll use the less convenient but more cost effective approach of relying on backups.\nFirst, navigate the DynamoDB console in the primary region and delete the processed_tweets table.\nNow, in the backup region, go to the processed_tweets table. Select the Backups tab and, under the PITR section, choose Restore.\nProvide the table name (processed_tweets), select cross region restore, and pick the primary region.\nOnce the table is active, you can perform a scan query and check that the item counts are in sync with the backup region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failback/traffic/","title":"Redirect Traffic","tags":[],"description":"","content":"Redirecting traffic to primary region Execute the SSM automation document failback_runbook. You\u0026rsquo;ll need to provide this input:\nBackupGroupArn: Set this to the ARN of the Global Accelerator endpoint group that points to the backup region. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/10_recap/","title":"Lab Recap","tags":[],"description":"","content":"In this lab, you explored Well-Architected security best practices in the context of monitoring application and system logs.\nFirst, you deployed a CloudFormation template containing an EC2 web server instance, an S3 bucket, and networking infrastructure for the lab. Then, you installed, configured, and started up the CloudWatch agent remotely. Using Systems Manager Run Command demonstrates the best practices of “enabling people to perform actions at a distance” and “reducing attack surface” by enabling you to close ports on your instance and avoid having to SSH directly into it. Storing the agent configuration file in Parameter Store highlighted the best practice of “configuring services and resources centrally” by maintaining reusable configuration data in AWS.\nThen, you generated some logs and viewed them in CloudWatch, illustrating the principle of “analyzing logs centrally” as you were able to view all your raw log data in a single location in CloudWatch. From there, you exported these logs to S3. This provides a method to store logs long term more cost-effectively in CloudWatch. Doing this facilitates the best practice of “configuring logging centrally” by enabling you to extract meaningful insights from large volumes of log data.\nIn Athena, you were able to query this S3 data using serverless SQL commands. You created a table within a database to track responses to site visits. In QuickSight, you directly used the results of your work in Athena to create a visualization. This keeps with the theme of “enabling people to perform actions from a distance” by abstracting the raw log data from users in both QuickSight and Athena. Additionally, you “analyzed logs centrally” in both services, generating useful insights from your application.\nOverall, you should now have a better understanding of how to collect log data in a secure manner. You kept people away from directly accessing data and instances, which reduced the exposure of your workload. You also centrally implemented service and application logging, a valuable practice to monitor and investigate any security threats.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_1_aws_account_setup/10_tear_down/","title":"Tear down","tags":[],"description":"","content":"This exercise covered fundamental steps that are required for all AWS accounts to enable Cost Optimization. There is no specific tear down for this lab.\nCancel your QuickSight subscription To cancel your QuickSight subscription follow the steps below.\nClick on your profile icon in the top right, select Manage QuickSight: Click on Account settings: Click on Unsubscribe: Review the notifications, click Unsubscribe: X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment or workload, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026rdquo; and COST3 - \u0026ldquo;How do you monitor usage and cost?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_cloudfront_with_waf_protection/","title":"Level 200: CloudFront with WAF Protection","tags":[],"description":"","content":"Authors Ben Potter, Security Lead, Well-Architected Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nGoals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Steps: Launch Instance Configure AWS WAF Configure Amazon CloudFront Tear down this lab "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_defend_against_new_threats/","title":"Quest: Defend Against New Threats","tags":[],"description":" ","content":" Labs coming soon Check out:\nAWS Security Blog AWS Security Bulletins "},{"uri":"https://wellarchitectedlabs.com/cost/100_labs/100_goals_and_targets/10_evaluate_new_services/","title":"Evaluate New Services","tags":[],"description":"","content":"Evaluate new services Develop a workload review process Goal: Perform Well-Architected reviews throughout a workloads lifecycle Target: For Tier1 workloads each stage has a full review, for Tier2 and below workloads the minimum requirement is: In the development stage review Security, Reliability and Operational Excellence. In Performance test review Performance Efficiency and Cost Optimization. Best Practice: Optimize Over Time Measures: % of workloads reviewed in each stage Good/Bad: Good Why? When does it work well or not?: Ensures the correct level review is performed inline with organization requirements. It also allows for tradeoffs if the organization is prioritizing speed to market for some workloads. Contact/Contributor: natbesh@amazon.com Review and analyze this workload regularly Goal: Perform Well-Architected reviews regularly on active workloads Target: Tier1 workloads will have milestones with all pillars completed at least every 3 months, for Tier2 workloads milestones with all pillars are completed at least every 6 months, all other workloads are to have milestones with all pillars completed yearly. Best Practice: Optimize Over Time Measures: Number of milestones completed, number of teams delivering Good/Bad: Good Why? When does it work well or not?: Ensures workloads are actively maintained and inline with best practices. Will result in higher operational costs if there are minimal changes in the workload, and no additional services/features available. Contact/Contributor: natbesh@amazon.com X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your organization, you should complete a milestone in the Well-Architected tool. This lab specifically helps you with COST2 - \u0026ldquo;How do you govern usage?\u0026rdquo;\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/networking__content_delivery/","title":"Networking &amp; Content Delivery","tags":[],"description":"","content":"These are queries for AWS Services under the Networking \u0026amp; Content Delivery product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon API Gateway Amazon CloudFront Data Transfer Data Transfer Regional Data Transfer - MSK AWS Direct Connect NAT Gateway NAT Gateway - Idle NATGW AWS Transit Gateway Network Usage Imbalance Inter-Az Data Transfer Low Activity VPC Interface Endpoints Low Activity AWS Network Firewall Amazon API Gateway Query Description This query provides daily unblended cost and usage information about Amazon API Gateway usage including the resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon API Gateway pricing page for more details.\nSample Output Download SQL File Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;apis/\u0026#39;, 2) AS split_line_item_resource_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE \u0026#39;%%ApiGatewayRequest%%\u0026#39; OR line_item_usage_type LIKE \u0026#39;%%ApiGatewayHttpRequest%%\u0026#39; THEN \u0026#39;Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%DataTransfer%%\u0026#39; THEN \u0026#39;Data Transfer\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%Message%%\u0026#39; THEN \u0026#39;Messages\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%Minute%%\u0026#39; THEN \u0026#39;Minutes\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%%CacheUsage%%\u0026#39; THEN \u0026#39;Cache Usage\u0026#39; ELSE \u0026#39;Other\u0026#39; END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon API Gateway\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon CloudFront Query Description This query provides daily unblended cost and usage information about Amazon CloudFront usage including the distribution name, region, and operation. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon CloudFront pricing page for more details.\nSample Output Download SQL File Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_region, product_product_family, -- NOTE: product_product_family used in place of large line_item_usage_type CASE line_item_operation, SPLIT_PART(line_item_resource_id, \u0026#39;distribution/\u0026#39;, 2) AS split_line_item_resource_id, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonCloudFront\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;), product_region, product_product_family, line_item_operation, line_item_resource_id ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Data Transfer Query Description This query provides daily unblended cost and usage information about Data Transfer usage including resource id that sourced the traffic, the product code corresponding to the source traffic, and the to/from locations of the usage. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to each individual service pricing page for more details on how data transfer charges are handled for that service.\nSample Output Download SQL File Link to file Copy Query SELECT line_item_product_code, line_item_usage_account_id , DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS date_line_item_usage_start_date, line_item_usage_type, product_from_location, product_to_location, product_product_family, line_item_resource_id, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_family = \u0026#39;Data Transfer\u0026#39; AND line_item_line_item_type = \u0026#39;Usage\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY line_item_product_code, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date, \u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, line_item_usage_type, product_from_location, product_to_location, product_product_family ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Data Transfer Regional Query Description This query provides monthly unblended cost and usage information about Data Transfer Regional (Inter AZ) usage including resource id that sourced the traffic and the product code corresponding to the source traffic. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to each individual service pricing page for more details on how data transfer charges are handled for that service.\nSample Output Download SQL File Link to file Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m\u0026#39;) AS day_line_item_usage_start_date, line_item_product_code, product_product_family, product_region, line_item_line_item_description, line_item_resource_id, sum(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_line_item_description LIKE \u0026#39;%regional data transfer%\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m\u0026#39;), line_item_product_code, product_product_family, product_region, line_item_line_item_description, line_item_resource_id ORDER BY sum_line_item_unblended_cost DESC ; Help \u0026amp; Feedback Back to Table of Contents Data Transfer - MSK Query Description This query provides monthly unblended cost and usage information about Data Transfer related to Amazon MSK including resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon MSK pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT line_item_product_code, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS date_line_item_usage_start_date, line_item_resource_id, line_item_usage_type, line_item_line_item_description, product_product_family, SUM(line_item_usage_amount)/1024 AS sum_line_item_usage_amount, ROUND(SUM(line_item_unblended_cost),2) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_product_code = \u0026#39;AmazonMSK\u0026#39; AND line_item_usage_type LIKE \u0026#39;%DataTransfer%\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY line_item_product_code, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date, \u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, line_item_usage_type, product_product_family, line_item_line_item_description ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents AWS Direct Connect Query Description: The query will output AWS Direct Connect charges split by Direct Connect port charges and Data Transfer charges for a specific resource using Direct Connect. They query will output port speed metrics and transfer source and destination locations.\nPricing Please refer to the AWS Direct Connect pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, product_port_speed, product_product_family, product_transfer_type, product_from_location, product_to_location, product_direct_connect_location, pricing_unit, line_item_operation, line_item_resource_id, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;AWS Direct Connect\u0026#39; AND product_transfer_type NOT IN (\u0026#39;IntraRegion Inbound\u0026#39;,\u0026#39;InterRegion Inbound\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_resource_id, line_item_operation, product_port_speed, product_product_family, product_transfer_type, product_from_location, product_to_location, product_direct_connect_location, pricing_unit ORDER BY sum_line_item_unblended_cost Desc; Help \u0026amp; Feedback Back to Table of Contents NAT Gateway Query Description This query provides monthly unblended cost and usage information about NAT Gateway Usage including resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the VPC pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, line_item_resource_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE \u0026#39;%%NatGateway-Bytes\u0026#39; THEN \u0026#39;NAT Gateway Data Processing Charge\u0026#39; -- Charge for per GB data processed by NatGateways WHEN line_item_usage_type LIKE \u0026#39;%%NatGateway-Hours\u0026#39; THEN \u0026#39;NAT Gateway Hourly Charge\u0026#39; -- Hourly charge for NAT Gateways ELSE line_item_usage_type END AS line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_family = \u0026#39;NAT Gateway\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_resource_id, line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC, sum_line_item_usage_amount; Help \u0026amp; Feedback Back to Table of Contents NAT Gateway - Idle NATGW Query Description This query shows cost and usage of NAT Gateways which didn\u0026rsquo;t receive any traffic last month and ran for more than 336 hrs. Resources returned by this query could be considered for deletion.\nPricing Please refer to the VPC pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 6) split_line_item_resource_id, product_region, pricing_unit, sum_line_item_usage_amount, CAST(cost_per_resource AS DECIMAL(16, 8)) AS sum_line_item_unblended_cost FROM ( SELECT line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(SUM(line_item_unblended_cost)) OVER (PARTITION BY line_item_resource_id) AS cost_per_resource, SUM(SUM(line_item_usage_amount)) OVER (PARTITION BY line_item_resource_id, pricing_unit) AS usage_per_resource_and_pricing_unit, COUNT(pricing_unit) OVER (PARTITION BY line_item_resource_id) AS pricing_unit_per_resource FROM ${table_name} WHERE line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND line_item_usage_type LIKE \u0026#39;%Nat%\u0026#39; -- get previous month AND CAST(month AS INT) = CAST(month(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS INT) -- get year for previous month AND CAST(year AS INT) = CAST(year(current_timestamp + -1 * INTERVAL \u0026#39;1\u0026#39; MONTH) AS INT) AND line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY line_item_resource_id, product_region, pricing_unit, line_item_usage_account_id, bill_payer_account_id ) WHERE -- filter only resources which ran more than half month (336 hrs) usage_per_resource_and_pricing_unit \u0026gt; 336 AND pricing_unit_per_resource = 1 ORDER BY cost_per_resource DESC; Help \u0026amp; Feedback Back to Table of Contents AWS Transit Gateway Query Description This query provides monthly unblended cost and usage information about AWS Transit Gateway Usage including attachment type, and resource id. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the TGW pricing page for more details.\nSample Output: Download SQL File: Link to file Query Preview: SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, CASE WHEN line_item_resource_id LIKE \u0026#39;arn%\u0026#39; THEN CONCAT(SPLIT_PART(line_item_resource_id,\u0026#39;/\u0026#39;,2),\u0026#39; - \u0026#39;,product_location) ELSE CONCAT(line_item_resource_id,\u0026#39; - \u0026#39;,product_location) END AS line_item_resource_id, product_location, product_attachment_type, pricing_unit, CASE WHEN pricing_unit = \u0026#39;hour\u0026#39; THEN \u0026#39;Hourly charges\u0026#39; WHEN pricing_unit = \u0026#39;GigaBytes\u0026#39; THEN \u0026#39;Data processing charges\u0026#39; END AS pricing_unit, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_group = \u0026#39;AWSTransitGateway\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_resource_id, product_location, product_attachment_type, pricing_unit ORDER BY sum_line_item_unblended_cost DESC, month_line_item_usage_start_date, sum_line_item_usage_amount, product_attachment_type; Help \u0026amp; Feedback Back to Table of Contents Network Usage Query Description This query provides daily unblended cost and usage information about AWS Network Usage including VPCPeering, PublicIP, InterZone, LoadBalancing, and resource id. Usage will be in ascending order and cost will be in descending order.\nPricing The Pricing Calculator is a useful tool for assisting with cost estimates for data transfer costs. To aid in Cost Analysis we highly recommend implementing the Data Transfer Cost Analysis Dashboard .\nSample Output Download SQL File Link to file Query Preview SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_operation, line_item_resource_id, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_operation IN ( \u0026#39;LoadBalancing-PublicIP-In\u0026#39;, \u0026#39;LoadBalancing-PublicIP-Out\u0026#39;, \u0026#39;InterZone-In\u0026#39;, \u0026#39;InterZone-Out\u0026#39;, \u0026#39;PublicIP-In\u0026#39;, \u0026#39;PublicIP-Out\u0026#39;, \u0026#39;VPCPeering-In\u0026#39;, \u0026#39;VPCPeering-Out\u0026#39; ) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_operation, line_item_resource_id ORDER BY day_line_item_usage_start_date ASC, sum_line_item_usage_amount DESC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Imbalance Inter-Az Data Transfer Query Description This query shows cost and usage of inbalance inter-az data transfer last month. These resources are commonly collectors or aggregators mostly receiving data from other resources in different Availability Zones. Distributing the resource functionality to each of the Availability Zones and redirect the data transfer from sending sources to be within the same Availability Zone will eliminate the $0.01/GB data transfer charges between Availability Zones.\nPricing The Pricing Calculator is a useful tool for assisting with cost estimates for data transfer costs. To aid in Cost Analysis we highly recommend implementing the Data Transfer Cost Analysis Dashboard .\nSample Output Download SQL File Link to file Query Preview SELECT line_item_usage_account_id as \u0026#34;account\u0026#34;, product_region as \u0026#34;region\u0026#34;, line_item_resource_id as \u0026#34;resource\u0026#34;, round (sum (IF ((line_item_operation = \u0026#39;InterZone-In\u0026#39;), line_item_unblended_cost, 0)), 2) as \u0026#34;interaz_in_cost\u0026#34;, round (sum (IF ((line_item_operation = \u0026#39;InterZone-Out\u0026#39;), line_item_unblended_cost, 0)), 2) as \u0026#34;interaz_out_cost\u0026#34; FROM $(table_name) WHERE (line_item_usage_type LIKE \u0026#39;%DataTransfer-Regional-Bytes%\u0026#39;) and (line_item_line_item_type = \u0026#39;Usage\u0026#39;) and ((line_item_operation = \u0026#39;InterZone-In\u0026#39;) or (line_item_operation = \u0026#39;InterZone-Out\u0026#39;)) and (month(bill_billing_period_start_date) = month((current_date) - INTERVAL \u0026#39;1\u0026#39; month)) and (year(bill_billing_period_start_date) = year((current_date) - INTERVAL \u0026#39;1\u0026#39; month)) GROUP BY line_item_usage_account_id, product_region, line_item_resource_id HAVING ((round (sum (IF ((line_item_operation LIKE \u0026#39;InterZone-In\u0026#39;), line_item_unblended_cost, 0)), 2) / (round (sum (IF ((line_item_operation LIKE \u0026#39;InterZone-Out\u0026#39;), line_item_unblended_cost, 0)), 2))) \u0026gt; 20) and ((round (sum (IF ((line_item_operation LIKE \u0026#39;InterZone-In\u0026#39;), line_item_unblended_cost, 0)), 2)) \u0026gt; 1000) ORDER BY \u0026#34;interaz_in_cost\u0026#34; DESC, \u0026#34;interaz_out_cost\u0026#34; DESC, account ASC, region ASC, resource ASC Help \u0026amp; Feedback Back to Table of Contents Low Activity VPC Interface Endpoints Query Description This query shows cost and usage of Interface Endpoints which did not receive significant traffic last month. Resources returned by this query could be considered for deletion or rearchitecture for centralized deployment.\nPricing The Pricing Calculator is a useful tool for assisting with cost estimates for data transfer costs. To aid in Cost Analysis we highly recommend implementing the Data Transfer Cost Analysis Dashboard .\nSample Output Download SQL File Link to file Query Preview SELECT line_item_usage_account_id as \u0026#34;account\u0026#34;, product_region as \u0026#34;region\u0026#34;, split_part(line_item_resource_id, \u0026#39;:\u0026#39;, 6) as \u0026#34;resource\u0026#34;, round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Endpoint-Hour%\u0026#39;), line_item_unblended_cost, 0)), 2) as \u0026#34;hourly_cost\u0026#34;, round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Endpoint-Bytes%\u0026#39;), line_item_blended_cost, 0)), 2) as \u0026#34;traffic_cost\u0026#34; FROM ${table_name} WHERE (line_item_product_code = \u0026#39;AmazonVPC\u0026#39;) and (line_item_line_item_type = \u0026#39;Usage\u0026#39;) and ((line_item_usage_type LIKE \u0026#39;%Endpoint-Hour%\u0026#39;) or (line_item_usage_type LIKE \u0026#39;%Endpoint-Byte%\u0026#39;)) and (month(bill_billing_period_start_date) = month((current_date) - INTERVAL \u0026#39;1\u0026#39; month)) and (year(bill_billing_period_start_date) = year((current_date) - INTERVAL \u0026#39;1\u0026#39; month)) GROUP BY line_item_usage_account_id, product_region, line_item_resource_id HAVING ((round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Endpoint-Hour%\u0026#39;), line_item_unblended_cost, 0)), 2) / (round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Endpoint-Bytes%\u0026#39;), line_item_blended_cost, 0)), 2))) \u0026gt; 20) ORDER BY \u0026#34;hourly_cost\u0026#34; DESC, \u0026#34;traffic_cost\u0026#34; DESC, account ASC, region ASC, resource ASC Help \u0026amp; Feedback Back to Table of Contents Low Activity AWS Network Firewall Query Description This query shows cost and usage of Network Firewall Endpoints which did not receive significant traffic last month. Resources returned by this query could be considered for deletion or rearchitecture for centralized deployment.\nPricing The Pricing Calculator is a useful tool for assisting with cost estimates for data transfer costs. To aid in Cost Analysis we highly recommend implementing the Data Transfer Cost Analysis Dashboard .\nSample Output Download SQL File Link to file Query Preview SELECT line_item_usage_account_id as \u0026#34;account\u0026#34;, product_region as \u0026#34;region\u0026#34;, split_part(line_item_resource_id, \u0026#39;:\u0026#39;, 6) as \u0026#34;resource\u0026#34;, round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Endpoint-Hour%\u0026#39;), line_item_unblended_cost, 0)), 2) as \u0026#34;hourly_cost\u0026#34;, round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Traffic-GB%\u0026#39;), line_item_unblended_cost, 0)), 2) as \u0026#34;traffic_cost\u0026#34; FROM ${table_name} WHERE (line_item_product_code = \u0026#39;AWSNetworkFirewall\u0026#39;) and (line_item_line_item_type = (\u0026#39;Usage\u0026#39;)) and ((line_item_usage_type LIKE \u0026#39;%Endpoint-Hour%\u0026#39;) or (line_item_usage_type LIKE \u0026#39;%Traffic-GB%\u0026#39;)) and (month(bill_billing_period_start_date) = month((current_date) - INTERVAL \u0026#39;1\u0026#39; month)) and (year(bill_billing_period_start_date) = year((current_date) - INTERVAL \u0026#39;1\u0026#39; month)) GROUP BY line_item_usage_account_id, product_region, line_item_resource_id HAVING (round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Endpoint-Hour%\u0026#39;), line_item_unblended_cost, 0)), 2)/round (sum (IF ((line_item_usage_type LIKE \u0026#39;%Traffic-GB%\u0026#39;), line_item_unblended_cost, 0)), 2) \u0026gt; 20) ORDER BY \u0026#34;hourly_cost\u0026#34; DESC, \u0026#34;traffic_cost\u0026#34; DESC, account ASC, region ASC, resource ASC Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/11_teardown/","title":"Lab Teardown","tags":[],"description":"","content":"Deleting the QuickSight visualization\nVisit the QuickSight console . On the home screen, click the three dots under the analysis called security_lab_apache_access_log.... Click Delete, and click Delete again on the screen that pops up. Next, click on the Manage data button in the top right corner of the page. Under Your Data Sets, click on security_lab_apache_acc.... Click Delete data set and click Delete on the pop up window. Deleting Athena database/table.\nVisit the Athena console . In the left side menu, under Tables, click the three dots next to security_lab_apache_access_logs. Click Delete table in the menu that appears. Click Yesin the pop up box Create a new query. Type DROP database security_lab_logs in the query editor and press Run query. Deleting the Systems Manager stored parameter\nVisit the Systems Manager console . In the left side menu, click on Parameter store. Click the box next to your created parameter, likely called AmazonCloudWatch-securitylab-cw-config. Click Delete. Click Delete parameters on the pop up that appears. Deleting the S3 Bucket\nVisit the S3 console . Click the button next to your created bucket, likely called wa-lab-\u0026lt;your-last-name\u0026gt;-\u0026lt;date\u0026gt;. Click Delete, follow the instructions in the pop-up to delete your bucket. Tearing down the CloudFormation stack.\nVisit the CloudFormation console . Click on the stack you created for this lab, likely called security-cw-lab. Click Delete and then Delete stack on the window that pops up. X Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with SEC4 - How do you detect and investigate security events? and SEC6 - How do you protect your compute resources?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_networks/","title":"Quest: Protect Networks","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes.\nNOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration.\nStart the Lab! "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/security_identity__compliance/","title":"Security, Identity, &amp; Compliance","tags":[],"description":"","content":"These are queries for AWS Services under the Security, Identity, \u0026amp; Compliance product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon GuardDuty Amazon Cognito AWS WAF Amazon GuardDuty Query Description This query provides daily unblended cost and usage information about Amazon GuardDuty Usage. The usage amount and cost will be summed.\nPricing Please refer to the Amazon GuardDuty pricing page for more details.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_usage_type, TRIM(REPLACE(product_group, \u0026#39;Security Services - Amazon GuardDuty \u0026#39;, \u0026#39;\u0026#39;)) AS trim_product_group, pricing_unit, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon GuardDuty\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, product_group, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, trim_product_group; Help \u0026amp; Feedback Back to Table of Contents Amazon Cognito Query Description This query provides daily unblended cost and usage information about Amazon Cognito Usage. The usage amount and cost will be summed.\nPricing Please refer to the Amazon Cognito pricing page for more details.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, product_product_name, line_item_operation, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Cognito\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), product_product_name, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, line_item_operation; Help \u0026amp; Feedback Back to Table of Contents AWS WAF Query Description This query provides daily unblended cost and usage information about AWS WAF Usage including web acl, rule id, and region. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the WAF pricing page for more details.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(SPLIT_PART(line_item_resource_id,\u0026#39;/\u0026#39;,2),\u0026#39;+\u0026#39;,1) AS split_webaclid_line_item_resource_id, SPLIT_PART(SPLIT_PART(line_item_resource_id,\u0026#39;/\u0026#39;,2),\u0026#39;+\u0026#39;,2) AS split_ruleid_line_item_resource_id, line_item_usage_type, product_group, product_group_description, product_location, product_location_type, line_item_line_item_description, pricing_unit, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;AWS WAF\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, line_item_usage_type, product_group, product_group_description, product_location, product_location_type, line_item_line_item_description, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, product_group; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_compute/","title":"Quest: Protect Compute","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab.\nStart the Lab! Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. You must have first deployed the Automated Deployment of VPC lab.\nStart the Lab! "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/queries/storage/","title":"Storage","tags":[],"description":"","content":"These are queries for AWS Services under the Storage product family .\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Amazon S3 Amazon EBS Amazon EBS Snapshots Amazon EBS Snapshots Copy Amazon EBS Volumes vs Snapshots ratio Amazon EFS Amazon FSx AWS Backup Amazon EBS Volumes Upgrade gp2 to gp3 Amazon S3 Query Description This query provides daily unblended cost and usage information for Amazon S3. The output will include detailed information about the resource id (bucket name), operation, and usage type. The usage amount and cost will be summed, and rows will be sorted by day (ascending), then cost (descending).\nPricing Please refer to the S3 pricing page . Please refer to understanding your AWS billing and usage reports for Amazon S3 to understand of of the usage types populated for S3 use.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, line_item_resource_id, line_item_operation, CASE --S3 Early Delete WHEN line_item_usage_type LIKE \u0026#39;%EarlyDelete-ByteHrs\u0026#39; THEN \u0026#39;Early Delete Glacier\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%EarlyDelete%\u0026#39; THEN \u0026#39;Early Delete \u0026#39; || SPLIT_PART(line_item_usage_type,\u0026#39;EarlyDelete-\u0026#39;,2) --S3 Requests WHEN line_item_usage_type LIKE \u0026#39;%Requests-INT%\u0026#39; THEN \u0026#39;Requests INT\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%Requests-Tier1\u0026#39; OR line_item_usage_type LIKE \u0026#39;%Requests-Tier2\u0026#39;) THEN \u0026#39;Requests Standard\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%Requests-GLACIER%\u0026#39; OR line_item_usage_type LIKE \u0026#39;%Requests-Tier3\u0026#39; OR line_item_usage_type LIKE \u0026#39;%Requests-Tier5\u0026#39; OR line_item_usage_type LIKE \u0026#39;%Requests-Tier6\u0026#39;) THEN \u0026#39;Requests Glacier\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Requests-GDA%\u0026#39; THEN \u0026#39;Requests GDA\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%Requests-Tier4\u0026#39; OR line_item_usage_type LIKE \u0026#39;%Requests-SIA%\u0026#39;) THEN \u0026#39;Requests SIA\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Requests-ZIA%\u0026#39; THEN \u0026#39;Requests ZIA\u0026#39; --S3 Retrieval WHEN (line_item_usage_type LIKE \u0026#39;%Retrieval-Bytes\u0026#39; AND line_item_operation = \u0026#39;RestoreObject\u0026#39;) THEN \u0026#39;Retrieval Glacier\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%Retrieval-Bytes\u0026#39; AND line_item_operation = \u0026#39;DeepArchiveRestoreObject\u0026#39;) THEN \u0026#39;Retrieval GDA\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Retrieval%\u0026#39; THEN \u0026#39;Retrieval \u0026#39; || SPLIT_PART(line_item_usage_type,\u0026#39;Retrieval-\u0026#39;,2) --S3 Storage WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;StandardStorage\u0026#39;) THEN \u0026#39;Storage Standard\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;StandardIAStorage\u0026#39;) THEN \u0026#39;Storage SIA\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;StandardIASizeOverhead\u0026#39;) THEN \u0026#39;Storage SIA-Overhead\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;OneZoneIAStorage\u0026#39;) THEN \u0026#39;Storage ZIA\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;OneZoneIASizeOverhead\u0026#39;) THEN \u0026#39;Storage ZIA-Overhead\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;GlacierStorage\u0026#39;) THEN \u0026#39;Storage Glacier\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;GlacierStagingStorage\u0026#39;) THEN \u0026#39;Storage Glacier-Staging\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND (line_item_operation = \u0026#39;GlacierObjectOverhead\u0026#39; or line_item_operation = \u0026#39;GlacierS3ObjectOverhead\u0026#39;)) THEN \u0026#39;Storage Glacier-Overhead\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;RestoreObject\u0026#39;) THEN \u0026#39;Storage Glacier-Restored\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;DeepArchiveStorage\u0026#39;) THEN \u0026#39;Storage GDA\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;DeepArchiveStagingStorage\u0026#39;) THEN \u0026#39;Storage GDA-Staging\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND (line_item_operation = \u0026#39;DeepArchiveObjectOverhead\u0026#39; or line_item_operation = \u0026#39;DeepArchiveS3ObjectOverhead\u0026#39;)) THEN \u0026#39;Storage GDA-Overhead\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;DeepArchiveRestoreObject\u0026#39;) THEN \u0026#39;Storage GDA-Restored\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation = \u0026#39;ReducedRedundancyStorage\u0026#39;) THEN \u0026#39;Storage RRS\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation LIKE \u0026#39;IntelligentTieringFA%\u0026#39;) THEN \u0026#39;Storage INT-FA\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%TimedStorage%\u0026#39; AND line_item_operation LIKE \u0026#39;IntelligentTieringIA%\u0026#39;) THEN \u0026#39;Storage INT-IA\u0026#39; --Data Transfer WHEN line_item_usage_type LIKE \u0026#39;%AWS-In-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Region to Region (In)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%AWS-In-ABytes%\u0026#39;THEN \u0026#39;Data Transfer Accelerated Region to Region (In)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%AWS-Out-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Region to Region (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%AWS-Out-ABytes%\u0026#39; THEN \u0026#39;Data Transfer Accelerated Region to Region (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%CloudFront-In-Bytes%\u0026#39; THEN \u0026#39;Data Transfer CloudFront (In)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%CloudFront-Out-Bytes%\u0026#39; THEN \u0026#39;Data Transfer CloudFront (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%DataTransfer-Regional-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Inter AZ\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%DataTransfer-In-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Internet (In)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%DataTransfer-Out-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Internet (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%DataTransfer-In-ABytes%\u0026#39; THEN \u0026#39;Data Transfer Accelerated Internet (In)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%DataTransfer-Out-ABytes%\u0026#39; THEN \u0026#39;Data Transfer Accelerated Internet (Out)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%S3RTC-In-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Replication Time Control (In)\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%S3RTC-Out-Bytes%\u0026#39; THEN \u0026#39;Data Transfer Replication Time Control (Out)\u0026#39; --S3 Fees \u0026amp; Misc WHEN line_item_usage_type LIKE \u0026#39;%Monitoring-Automation-INT\u0026#39; THEN \u0026#39;S3 INT Monitoring Fee\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%StorageAnalytics%\u0026#39; THEN \u0026#39;S3 Storage Analytics\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%BatchOperations-Jobs%\u0026#39; THEN \u0026#39;S3 Batch Operations-Jobs\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%BatchOperations-Objects%\u0026#39; THEN \u0026#39;S3 Batch Operations-Objects\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%TagStorage%\u0026#39; THEN \u0026#39;S3 Tag Storage\u0026#39; WHEN (line_item_usage_type LIKE \u0026#39;%Select-Returned%\u0026#39; OR line_item_usage_type LIKE \u0026#39;%Select-Scanned%\u0026#39;) THEN \u0026#39;S3 Select\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%Inventory%\u0026#39; THEN \u0026#39;S3 Inventory\u0026#39; WHEN line_item_operation LIKE \u0026#39;%StorageLens%\u0026#39; THEN \u0026#39;Storage Lens\u0026#39; ELSE \u0026#39;Other \u0026#39; || line_item_usage_type END AS case_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE (year = \u0026#39;2020\u0026#39; AND month IN (\u0026#39;7\u0026#39;,\u0026#39;9\u0026#39;) OR year = \u0026#39;2020\u0026#39; AND month IN (\u0026#39;07\u0026#39;,\u0026#39;09\u0026#39;)) AND line_item_product_code = \u0026#39;AmazonS3\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;,\u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_resource_id, line_item_operation, 6 --refers to case_line_item_usage_type ORDER BY day_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Query Description This query provides an overview of cost and usage for Amazon EC2 EBS. Output includes daily unblended cost and usage quantity by payer account, linked account, usage type, and resource ID. The usage amount and cost will be summed, and rows will be sorted by day (ascending), then cost (descending).\nPricing Please refer to the Amazon EBS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, CASE WHEN line_item_usage_type LIKE \u0026#39;%SnapshotUsage%\u0026#39; THEN \u0026#39;Snapshots\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeIOUsage%\u0026#39; THEN \u0026#39;Magnetic IO\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage\u0026#39; THEN \u0026#39;Magnetic\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage.gp2\u0026#39; THEN \u0026#39;GP2\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage.gp3\u0026#39; THEN \u0026#39;GP3\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeP-IOPS.gp3\u0026#39; THEN \u0026#39;GP3 IOPS\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeP-Throughput.gp3\u0026#39; THEN \u0026#39;GP3 Throughput\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage.piops\u0026#39; THEN \u0026#39;io1\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeP-IOPS.piops\u0026#39; THEN \u0026#39;io1 IOPS\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage.io2%\u0026#39; THEN \u0026#39;io2\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeP-IOPS.io2%\u0026#39; THEN \u0026#39;io2 IOPS\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage.st1\u0026#39; THEN \u0026#39;st1\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%VolumeUsage.sc1\u0026#39; THEN \u0026#39;sc1\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%directAPI%\u0026#39; THEN \u0026#39;Direct API Requests\u0026#39; WHEN line_item_usage_type LIKE \u0026#39;%FastSnapshotRestore\u0026#39; THEN \u0026#39;Fast Snapshot Restore\u0026#39; ELSE SPLIT_PART(line_item_usage_type,\u0026#39;:\u0026#39;,2) END AS case_line_item_usage_type, CASE WHEN line_item_resource_id LIKE \u0026#39;%snap%\u0026#39; THEN SPLIT_PART(line_item_resource_id,\u0026#39;/\u0026#39;,2) ELSE line_item_resource_id END AS case_line_item_resource_id, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM {$tableName} WHERE {$date_filter} AND line_item_product_code = \u0026#39;AmazonEC2\u0026#39; AND line_item_usage_type LIKE \u0026#39;%EBS:%\u0026#39; AND line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), 4, --refers to case_line_item_usage_type 5 --refers to case_line_item_resource_id ORDER BY day_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Snapshots Query Description This query provides daily unblended cost and usage information about Amazon EBS Snapshot Usage per account including region. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EBS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;) AS date_line_item_usage_start_date, product_region, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic Compute Cloud\u0026#39; AND line_item_usage_type LIKE \u0026#39;%%EBS%%Snapshot%%\u0026#39; AND product_product_family LIKE \u0026#39;Storage Snapshot\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m-%d\u0026#39;), product_region ORDER BY sum_line_item_unblended_cost DESC, sum_line_item_usage_amount DESC, date_line_item_usage_start_date ASC; Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Snapshots Copy Query Description This query helps correlate cross-region data transfer cost and usage with EBS Snapshot Copy operations for specific EBS snapshots. Understanding the amount of data transfer associated with specific snapshots, and tangentially how much data change is happening on the associated volume, can be used to inform changes to snapshotting strategy. The query provides total unblended cost and usage information grouped by snapshot, usage type, and line item description. Output is sorted by cost in descending order.\nPricing EBS Snapshot Copy is charged according to data transferred across regions. Please refer to the data transfer section of the Amazon EC2 pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT line_item_resource_id, line_item_usage_type, line_item_line_item_description, SUM(line_item_usage_amount) as sum_line_item_usage_amount, SUM(line_item_unblended_cost) as sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND line_item_operation = \u0026#39;EBS Snapshot Copy\u0026#39; GROUP BY line_item_resource_id, line_item_usage_type, line_item_line_item_description ORDER BY SUM(line_item_unblended_cost) DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Volumes Query Description This query provides daily unblended cost and usage information about Amazon EBS Volume Usage per account. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EBS pricing page . Please refer to the Amazon EBS Volume Charges page for more info on the calculations used on your bill.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, CASE SPLIT_PART(line_item_usage_type,\u0026#39;:\u0026#39;,2) WHEN \u0026#39;VolumeUsage\u0026#39; THEN \u0026#39;EBS - Magnetic\u0026#39; WHEN \u0026#39;VolumeIOUsage\u0026#39; THEN \u0026#39;EBS Magnetic IO\u0026#39; WHEN \u0026#39;VolumeUsage.gp2\u0026#39; THEN \u0026#39;EBS GP2\u0026#39; WHEN \u0026#39;VolumeUsage.gp3\u0026#39; THEN \u0026#39;EBS GP3\u0026#39; WHEN \u0026#39;VolumeP-IOPS.gp3\u0026#39; THEN \u0026#39;EBS GP3 IOPS\u0026#39; WHEN \u0026#39;VolumeP-Throughput.gp3\u0026#39; THEN \u0026#39;EBS GP3 Throughput\u0026#39; WHEN \u0026#39;VolumeUsage.piops\u0026#39; THEN \u0026#39;EBS io1\u0026#39; WHEN \u0026#39;VolumeP-IOPS.piops\u0026#39; THEN \u0026#39;EBS io1 IOPS\u0026#39; WHEN \u0026#39;VolumeUsage.io2\u0026#39; THEN \u0026#39;EBS io2\u0026#39; WHEN \u0026#39;VolumeP-IOPS.io2\u0026#39; THEN \u0026#39;EBS io2 IOPS\u0026#39; WHEN \u0026#39;VolumeUsage.st1\u0026#39; THEN \u0026#39;EBS st1\u0026#39; WHEN \u0026#39;VolumeUsage.sc1\u0026#39; THEN \u0026#39;EBS sc1\u0026#39; WHEN \u0026#39;directAPI\u0026#39; THEN \u0026#39;EBS Direct API Requests\u0026#39; WHEN \u0026#39;FastSnapshotRestore\u0026#39; THEN \u0026#39;EBS Fast Snapshot Restore\u0026#39; ELSE SPLIT_PART(line_item_usage_type,\u0026#39;:\u0026#39;,2) END AS line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic Compute Cloud\u0026#39; AND line_item_usage_type LIKE \u0026#39;%%EBS%%Volume%%\u0026#39; AND product_product_family IN (\u0026#39;Storage\u0026#39;,\u0026#39;System Operation\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;), line_item_usage_type ORDER BY sum_line_item_unblended_cost DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Volumes vs Snapshots ratio Query Description This query provides monthly ratio of unblended cost and usage information between Amazon EBS Volume vs Amazon EBS Snapshots Usage per account and region. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EBS pricing page . Please refer to the Amazon EBS Volume Charges page for more info on the calculations used on your bill.\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id , line_item_usage_account_id, DATE_FORMAT(line_item_usage_start_date,\u0026#39;%Y-%m\u0026#39;) AS month_line_item_usage_start_date, CASE WHEN product_product_family = \u0026#39;Storage\u0026#39; THEN \u0026#39;EBS Volume\u0026#39; WHEN product_product_family = \u0026#39;Storage Snapshot\u0026#39; THEN \u0026#39;EBS Snapshot\u0026#39; END AS usage_type_product_product_family, product_region, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${tableName} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic Compute Cloud\u0026#39; AND (product_product_family = \u0026#39;Storage Snapshot\u0026#39; OR product_product_family = \u0026#39;Storage\u0026#39;) AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m\u0026#39;), CASE WHEN product_product_family = \u0026#39;Storage\u0026#39; THEN \u0026#39;EBS Volume\u0026#39; WHEN product_product_family = \u0026#39;Storage Snapshot\u0026#39; THEN \u0026#39;EBS Snapshot\u0026#39; END, product_region ORDER BY month_line_item_usage_start_date ASC, sum_line_item_unblended_cost DESC, sum_line_item_usage_amount DESC; Help \u0026amp; Feedback Back to Table of Contents Amazon EFS Query Description This query will provide daily unblended cost and usage information per linked account for Amazon EFS. The output will include detailed information about the resource id (File System), usage type, and API operation. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon EFS pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id, \u0026#39;file-system/\u0026#39;, 2) AS split_line_item_resource_id, line_item_usage_type, product_product_family, product_storage_class, pricing_unit, line_item_operation, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic File System\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, pricing_unit, product_product_family, product_storage_class, line_item_resource_id, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost DESC, line_item_usage_type; Help \u0026amp; Feedback Back to Table of Contents Amazon FSx Query Description This query will provide daily unblended cost and usage information per linked account for Amazon FSx. The output will include detailed information about the resource id (FSx file system), usage type, and Storage type. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the Amazon FSx pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 6) AS split_line_item_resource_id, product_deployment_option, line_item_usage_type, product_product_family, pricing_unit, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon FSx\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), SPLIT_PART(line_item_resource_id, \u0026#39;:\u0026#39;, 6), product_deployment_option, line_item_usage_type, product_product_family, pricing_unit ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost; Help \u0026amp; Feedback Back to Table of Contents AWS Backup Query Description This query will provide daily unblended cost and usage information per linked account for AWS Backup. The output will include detailed information about the usage type, product family, pricing unit and others. The usage amount and cost will be summed and the cost will be in descending order.\nPricing Please refer to the AWS Backup pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date), \u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, pricing_unit, product_product_family, line_item_usage_type, line_item_operation, SPLIT_PART(line_item_usage_type, \u0026#39;-\u0026#39;, 4) AS split_line_item_usage_type, SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost FROM ${table_name} WHERE ${date_filter} AND product_product_name LIKE \u0026#39;%Backup%\u0026#39; AND line_item_line_item_type IN (\u0026#39;DiscountedUsage\u0026#39;, \u0026#39;Usage\u0026#39;, \u0026#39;SavingsPlanCoveredUsage\u0026#39;) GROUP BY bill_payer_account_id, line_item_usage_account_id, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), line_item_usage_type, product_product_family, pricing_unit, line_item_operation ORDER BY day_line_item_usage_start_date, sum_line_item_usage_amount, sum_line_item_unblended_cost, line_item_usage_type; Help \u0026amp; Feedback Back to Table of Contents Amazon EBS Volumes Upgrade gp2 to gp3 Query Description This query will display cost and usage of Elastic Block Storage Volumes that are type gp3. These resources returned by this query could be considered for upgrade to gp3 as with up to 20% cost savings, gp3 volumes help you achieve more control over your provisioned IOPS, giving the ability to provision storage with your unique applications in mind. This query uses 0.088 gp3 pricing please check the pricing page to confirm you are using the correct pricing for your applicable region. For Additional information checkout this AWS Blog Post. Pricing Please refer to the Elastic Block Storage pricing page .\nSample Output Download SQL File Link to Code Copy Query SELECT * FROM (SELECT bill_payer_account_id, line_item_usage_account_id, product_location, product_region, month, pricing_public_on_demand_rate, line_item_resource_id, line_item_usage_type, SPLIT_PART(SPLIT_PART(line_item_usage_type , \u0026#39;:\u0026#39;,2),\u0026#39;.\u0026#39;,2) AS ebs_type, SUM(line_item_usage_amount) AS gb_charged, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost, SUM(line_item_usage_amount)*.088 AS gp3_price -- 0.088 eu-west-1 pricing , (SUM(line_item_unblended_cost)-(SUM(line_item_usage_amount)*.088)) AS gp3_savings -- 0.088 eu-west-1 pricing FROM ${table} WHERE ${date_filter} AND product_product_name = \u0026#39;Amazon Elastic Compute Cloud\u0026#39; AND line_item_usage_type LIKE \u0026#39;%%EBS%%Volume%%\u0026#39; AND product_product_family IN (\u0026#39;Storage\u0026#39;,\u0026#39;System Operation\u0026#39;) AND line_item_line_item_type = (\u0026#39;Usage\u0026#39;) AND product_region = \u0026#39;eu-west-1\u0026#39; AND SPLIT_PART(SPLIT_PART(line_item_usage_type,\u0026#39;:\u0026#39;,2),\u0026#39;.\u0026#39;,2) = \u0026#39;gp2\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, month, line_item_usage_type, product_location, product_region, line_item_resource_id, pricing_public_on_demand_rate, month ORDER BY sum_line_item_unblended_cost DESC) WHERE gb_charged \u0026lt; 1000; Help \u0026amp; Feedback Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_classify_data/","title":"Quest: Classify Data","tags":[],"description":" ","content":"Labs coming soon Check out:\nAmazon Macie User Guide "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_data_at_rest/","title":"Quest: Protect Data at Rest","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Data Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Create a Data Bunker Account Introduction In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.\nStart the Lab! Further Learning S3: Protecting Data Using Server-Side Encryption with AWS KMS–Managed Keys Opt-in to Default Encryption for New EBS Volumes "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/","title":"Getting Started","tags":[],"description":"","content":"Getting Started In this chapter, we\u0026rsquo;ll cover the prerequisites you\u0026rsquo;ll need for the rest of the workshop.\nPrerequisites Architecture X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/next/cleanup/","title":"Cleanup","tags":[],"description":"","content":"If you used an account provided by Event Engine, you do not need to do any cleanup. The account terminates when the event is over.\nIf you used your own account, please remove the three CloudFormation stacks you deployed.\nX Congratulations! Now that you have completed the lab, if you have implemented this knowledge in your environment, you should re-evaluate the questions in the Well-Architected tool. This lab specifically helps you with REL 13 How do you plan for disaster recovery (DR)?\nClick here to access the Well-Architected Tool Previous Step Complete this lab "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_protect_data_in_transit/","title":"Quest: Protect Data in Transit","tags":[],"description":" ","content":"Labs coming soon Check out:\nAWS Certificate Manager - Getting Started AWS Encryption SDK AWS Site-to-Site VPN User Guide AWS Client VPN User Guide "},{"uri":"https://wellarchitectedlabs.com/security/quests/quest_incident_response/","title":"Quest: Incident Response","tags":[],"description":" ","content":"Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .\nPrerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.\nStart the Lab! Further Learning AWS Security Incident Response Guide "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/query-building/","title":"CUR Query Building","tags":[],"description":"","content":"Last Updated May 2021\nFeedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: curquery@amazon.com Introduction This lab will demonstrate the basic principles for constructing a basic CUR query. The skills you learn will help you construct your own CUR queries or modify queries already in use. While this lab cannot cover every use case for cost and usage analysis, many AWS services follow similar CUR logic and can be easily adapted.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nTable of Contents Query Framework Query Description Finding Product Names and Codes Interesting CUR Fields CUR Data Dictionary Query Framework This example shows how to query the usage and cost of an individual product in a specific date range. The results are summed for each individual account that incurred charges. The following sections will break the query down by section and explain the individual components.\nSELECT bill_payer_account_id, line_item_usage_account_id, line_item_line_item_description, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SUM(line_item_usage_amount) AS sum_line_item_usage_amount, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost FROM ${table_name} WHERE year = \u0026#39;2020\u0026#39; AND (month BETWEEN \u0026#39;7\u0026#39; AND \u0026#39;9\u0026#39; OR month BETWEEN \u0026#39;07\u0026#39; AND \u0026#39;09\u0026#39;) AND product_product_name LIKE \u0026#39;%${product_name}%\u0026#39; GROUP BY bill_payer_account_id, line_item_usage_account_id, line_item_line_item_description, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), ORDER BY sum_line_item_unblended_cost DESC; Back to Table of Contents Query Description Let\u0026rsquo;s take a look at each section of the query framework:\nSELECT bill_payer_account_id, line_item_usage_account_id, line_item_line_item_description, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;) AS day_line_item_usage_start_date, SUM(line_item_unblended_cost) AS sum_line_item_unblended_cost In this section, you select a number of fields from the CUR.\nbill_payer_account_id: this is the payer account responsible for all charge of it and associated member accounts. line_item_usage_account_id: this is the account where the usage charge was incurred. line_item_line_item_description: this is a description of the usage you are being charged for. line_item_usage_start_date: this is the date the charge was incurred. The DATE_FORMAT function is formatting this field in YYYY/MM/DD format. This field can be omitted if you are not looking for daily costs. line_item_unblended_cost: this is the actual cost of the usage being charged for. The SUM function totals all records returned and is optional. FROM ${table_name} WHERE year = \u0026#39;2020\u0026#39; AND (month BETWEEN \u0026#39;7\u0026#39; AND \u0026#39;9\u0026#39; OR month BETWEEN \u0026#39;07\u0026#39; AND \u0026#39;09\u0026#39;) AND product_product_name LIKE \u0026#39;%${product_name}%\u0026#39; In this section, you set parameters to filter a subset of records in CUR. The first line of the WHERE clause sets a date range. See Filtering by Date in the CUR Query Library Help section for additional details.\n${table_name}: this is the name of your table in Athena. ${product_product_name}: this is the name of the product reflected in CUR. Using the LIKE operator allows you to match with wildcards if you do not know the specific product name in CUR. GROUP BY bill_payer_account_id, line_item_usage_account_id, line_item_line_item_description, DATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-%d\u0026#39;), ORDER BY sum_line_item_unblended_cost DESC; In this section, you set grouping and ordering.\nGROUP BY groups like records together. In this example, all records are grouped as follows:\nFirst, group all records by payer account. Next, group all records by usage account. Next, group all records by description. Finally, group records by date. This wil result in a sum of all charges per product per day in each usage account. Note: you cannot group by fields that use a SUM function.\nORDER BY sorts the list of returned records by cost in descending order (largest to smallest).\nBack to Table of Contents Finding Product Names and Codes SELECT DISTINCT product_product_name, line_item_product_code FROM ${table_name} The above query will list each distinct product names and codes in CUR. You can use this list as a reference for filtering CUR queries by product name or product code. The product columns provide metadata about the product that incurred the expense, and the line item. The product columns are dynamic and their visibility in Cost and Usage Reports depends on the usage of product in the billing period.\nWhen working with AWS Marketplace products it is best to use the column product_product_name as it contains a friendly name for the third party product. If you were to use line_item_product_code for AWS Marketplace products it will contain a unique ID.\nproduct_product_name = CloudBeaver line_item_product_code = 581uljmnj07lfrc1uqfd9skb2p When working with Native AWS products is is best to use the column line_item_product_code . You could also add into your query a CASE statement to handle this:\nCASE WHEN (\u0026#34;bill_billing_entity\u0026#34; = \u0026#39;AWS Marketplace\u0026#39; AND \u0026#34;line_item_line_item_type\u0026#34; NOT LIKE \u0026#39;%Discount%\u0026#39;) THEN \u0026#34;product_product_name\u0026#34; WHEN (\u0026#34;bill_billing_entity\u0026#34; = \u0026#39;AWS\u0026#39;) THEN \u0026#34;line_item_product_code\u0026#34; END \u0026#34;Service\u0026#34;, It is always best to confirm your query output against Cost Explorer before using it in a production setting.\nBack to Table of Contents Interesting CUR Fields Detailed descriptions of the all CUR fields can be found in the CUR Data Dictionary: https://docs.aws.amazon.com/cur/latest/userguide/data-dictionary.html line_item_blended_cost: this field shows the average cost of usage billed at different rates. An example is EC2 on-demand vs. Savings Plan discounted rate line_item_line_item_type: this field is used to differentiate between credits, fees, tax, refunds, and discounted usage. line_item_resource_id: if enabled in CUR, this field will show resource IDs for services that support this field. reservation_reservation_a_r_n: the Amazon Resource Name (ARN) of an RI that has been applied to usage. resource_tags_user_TAGNAME: user-defined resource tags and cost allocation tags will appear in CUR. cost_category_TAGNAME: Cost Category tags will appear in CUR. More information:\nResource Tags: https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html Cost Categories: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html Back to Table of Contents CUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/","title":"CUR Query Library Help","tags":[],"description":"","content":"The CUR Query Library Help section is intended to provide tips and information about navigating the CUR dataset. We will cover beginner topics like getting started with querying the CUR, filtering query results, common query format, links to public documentation, and getting product information. We will also cover advanced topics like understanding your AWS Cost Datasets while working with the CUR data.\nUse the clipboard in the top right of the text boxes below to copy all of the text to be pasted.\nCUR Query Library uses placeholder variables, indicated by a dollar sign and curly braces (${ }). ${table_name} and ${date_filter} are common placeholder variables used throughout CUR Query Library, which must be replaced before a query will run. For example, if your CUR table is called cur_table and is in a database called cur_db, you would replace ${table_name} with cur_db.cur_table. For ${date_filter}, you have multiple options. See Filtering by Date in the CUR Query Library Help section for additional details.\nTable of Contents Documentation Links Filtering Query Results Filtering by Date: ${date_filter} Query Format Overview Additional Functions Retrieving Data from CUR Understanding Cost Data AWS Product Descriptions and Pricing Units CUR Table Data with Reserved Instances or Savings Plans Unblended, Blended, Amortized, and Net Costs CUR Query Building Documentation Links Cost and Usage Report Data Dictionary Cost and Usage Report User Guide Understanding your AWS Cost Datasets Cost and Usage Report Analysis Tutorial SQL Tutorials Introduction to Amazon Athena Filtering Query Results Filtering query results allows for precise data retrieval. The WHERE clause is used to extract only those records that fulfill a specified condition. Let\u0026rsquo;s review an example where we match a filtered Cost Explorer view with a CUR query to achieve the same results.\nWHERE year = \u0026#39;2020\u0026#39; AND (month BETWEEN \u0026#39;8\u0026#39; AND \u0026#39;10\u0026#39;) AND product_product_name = (\u0026#39;AWS Glue\u0026#39;) AND line_item_usage_type LIKE \u0026#39;%Crawler%\u0026#39; AND product_region = \u0026#39;us-east-1\u0026#39; AND line_item_line_item_type NOT IN (\u0026#39;Tax\u0026#39;,\u0026#39;Credit\u0026#39;,\u0026#39;Refund\u0026#39;) We will explain what the query filter is doing as well as a screenshot of the equivalent filter in Cost Explorer:\nTime frame to be August to October 2020 The Service to be AWS Glue The region to be us-east-1 The usage type to include usage like %Crawler% To exclude Charge Types like credit, refund, and tax You can mimic this example using the following query:\nLink to Code You can configure Cost Explorer like the below screenshot:\nIn Usage Type, type Crawler and select all region-Crawler-DPU-Hour entries.\nYou should now be able to align the unblended cost for AWS Glue using both CUR and Cost Explorer. This is a common approach used to validate your CUR query is working as expected.\nFiltering by Date Using SQL to filter Cost and Usage data by date can be done in a variety of ways. Throughout CUR Query Library, the placeholder variable ${date_filter} is used to indicate where you should insert the date filter of your choice. A few date filtering examples are provided below. These examples are not exhaustive, and there are other approaches you may consider.\nNotes:\nAthena tables created using the CUR Athena integration have, and are partioned by, month and year columns. Queries with date filters that use these columns will only scan table data from the relevant partitions, resulting in faster queries as well as lower cost, since Athena usage is billed based on quantity of data scanned. When using the month column, be aware a single-digit month is used in some cases, for example \u0026lsquo;1\u0026rsquo; for January, \u0026lsquo;2\u0026rsquo; for February, etc. In other cases, a two-digit month with leading \u0026lsquo;0\u0026rsquo; is used. For example, \u0026lsquo;01\u0026rsquo; for January, \u0026lsquo;02\u0026rsquo; for February, etc. To determine which format you have stored in your table, you can use the simple query: SELECT DISTINCT(month) FROM ${table_name} Two additional columns with date data useful for filtering are bill_billing_period_start_date and line_item_usage_start_date. bill_billing_period_start_date can be a good choice when you want monthly granularity in your reports. line_item_usage_start_date can be a good choice when you want daily or hourly granularity in your reports. Throughout CUR Query Library, the DATE_FORMAT() function is often used to format the line_item_usage_start_date column. The DATE_FORMAT() function should not be confused with the ${date_filter} placeholder variable discussed here. Filtering by Date with Columns month and year Example: Full Year (January 1, 2020 12:00:00AM - December 31, 2020 11:59:59PM)\nWHERE year = \u0026#39;2020\u0026#39; Example: Single Month (July 1, 2020 12:00:00AM - July 31, 2020 11:59:59PM)\nWHERE year = \u0026#39;2020\u0026#39; AND month = \u0026#39;7\u0026#39; Example: 3 Months Using BETWEEN (July 1, 2020 12:00:00AM - September 30, 2020 11:59:59PM)\nWHERE year = \u0026#39;2020\u0026#39; AND month BETWEEN \u0026#39;7\u0026#39; AND \u0026#39;9\u0026#39; Example: 3 Months Using IN (July 1, 2020 12:00:00AM - September 30, 2020 11:59:59PM)\nWHERE year = \u0026#39;2020\u0026#39; AND month IN (\u0026#39;7\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;9\u0026#39;) Example: 3 Months Using Greater Than/Less Than (July 1, 2020 12:00:00AM - September 30, 2020 11:59:59PM)\nWHERE year = \u0026#39;2020\u0026#39; AND (month \u0026gt;= \u0026#39;7\u0026#39; AND \u0026lt; \u0026#39;10\u0026#39;) Example: Month Range Crossing End of Calendar Year (November 1, 2020 12:00:00AM - February 28, 2021 11:59:59PM)\nNote: due to year and month being stored in separate columns, OR is required to deal with date ranges crossing the end of a calendar year.\nWHERE (year = \u0026#39;2020\u0026#39; AND month BETWEEN \u0026#39;11\u0026#39; AND \u0026#39;12\u0026#39;) OR (year = \u0026#39;2021\u0026#39; AND month BETWEEN \u0026#39;1\u0026#39; AND \u0026#39;2\u0026#39;) Example: Two Non-Contiguous Months (January 1, 2020 12:00:00AM - January 31, 2020 11:59:59PM \u0026amp; January 1, 2021 12:00:00AM - January 31, 2021 11:59:59PM)\nWHERE (year = \u0026#39;2020\u0026#39; AND month = \u0026#39;1\u0026#39;) AND (year = \u0026#39;2021\u0026#39; AND month = \u0026#39;1\u0026#39;) Example: Two Non-Contiguous Month Ranges (January 1, 2020 12:00:00AM - March 31, 2020 11:59:59PM \u0026amp; January 1, 2021 12:00:00AM - March 31, 2021 11:59:59PM)\nWHERE (year = \u0026#39;2020\u0026#39; AND month IN (\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;)) AND (year = \u0026#39;2021\u0026#39; AND month IN (\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;)) Date Filtering with Columns line_item_usage_start_date and bill_billing_period_start_date bill_billing_period_start_date and line_item_usage_start_date are typically stored as timestamps. In order to use these columns with comparison functions and operators, for example \u0026ldquo;all dates after (greater than) January 1, 2021\u0026rdquo; or \u0026ldquo;all dates within the past two weeks,\u0026rdquo; strings must be converted to timestamps. This can be accomplished with functions such as FROM_ISO8601_TIMESTAMP(), which parses an ISO 8601 formatted string into a timestamp with time zone, or CAST(), which explicitly casts a value as another type. Additional date and time operators can be found in Presto SQL documentation. Example: Arbitrary Date/Time Range Using BETWEEN and FROM_ISO8601_TIMESTAMP() (July 1, 2020 12:00:00AM - July 8, 2020 01:23:45AM)\nWHERE line_item_usage_start_date BETWEEN FROM_ISO8601_TIMESTAMP(\u0026#39;2020-07-01T00:00:00\u0026#39;) AND FROM_ISO8601_TIMESTAMP(\u0026#39;2020-07-08T01:23:45\u0026#39;) Example: Arbitrary Date/Time Range Using Greater Than/Less Than and CAST() (July 1, 2020 12:00:00AM - July 8, 2020 01:23:45AM)\nWHERE line_item_usage_start_date \u0026gt;= CAST(\u0026#39;2020-07-01 00:00:00\u0026#39; AS TIMESTAMP) AND line_item_usage_start_date \u0026lt; CAST(\u0026#39;2020-07-08 01:23:45\u0026#39; AS TIMESTAMP) Example: Arbitrary Date/Time until Present (July 8, 2020 01:23:45AM - Present)\nWHERE line_item_usage_start_date \u0026gt;= FROM_ISO8601_TIMESTAMP(\u0026#39;2020-07-01 01:23:45\u0026#39;) Example: Previous 3 Months Before Now\nNote: \u0026lsquo;month\u0026rsquo; in this example is part of the INTERVAL function and should not be confused with the month column.\nWHERE line_item_usage_start_date \u0026gt;= now() - INTERVAL \u0026#39;3\u0026#39; month Example: 6 Months After an Arbitrary Date/Time (October 5, 2020 01:23:45AM - March 5, 2021 01:23:45AM)\nNote: \u0026lsquo;month\u0026rsquo; in this example is part of the INTERVAL function and should not be confused with the month column.\nWHERE line_item_usage_start_date \u0026gt;= CAST(\u0026#39;2020-10-05 01:23:45\u0026#39; AS TIMESTAMP) AND line_item_usage_start_date \u0026lt; CAST(\u0026#39;2020-10-05 01:23:45\u0026#39; AS TIMESTAMP) + INTERVAL \u0026#39;6\u0026#39; month Query Format Overview Below is an overview of the key elements of an Athena Query.\nThe SELECT statement is used to select data you require and the FROM specifies which table from which database.\nEach column from the data you would like returned is placed between the SELECT and the FROM separated by a comma.\nThe AS command is used to rename a column or table with an alias.\nThe WHERE clause is used to filter records. The WHERE clause is used to extract only those records that fulfil a specified condition. For example WHERE year = \u0026lsquo;2020\u0026rsquo;\nThe GROUP BY clause divides the output of a SELECT statement into groups of rows containing matching values. A simple GROUP BY clause may contain any expression composed of input columns or it may be an ordinal number selecting an output column by position (starting at one).\nThe ORDER BY clause is used to sort a result set by one or more output expressions. These can be ascending ASC or descending DESC.\nAdditional Functions The CASE statement goes through conditions and returns a value when the first condition is met (like an IF-THEN-ELSE statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the ELSE clause.\nWe may use this to add information to the Athena query based on what results we get back. The example below shows how we have added the Spot Instance item into the data if there is nothing in the pricing term column.\nCASE pricing_term WHEN \u0026#39;Reserved\u0026#39; THEN \u0026#39;Reserved Instance\u0026#39; WHEN \u0026#39;OnDemand\u0026#39; THEN \u0026#39;OnDemand\u0026#39; WHEN \u0026#39;\u0026#39; THEN \u0026#39;Spot Instance\u0026#39; ELSE \u0026#39;Other\u0026#39; END AS Reservation The CAST() function converts a value (of any type) into a specified datatype. This is often use to change a column that\u0026rsquo;s is a Double to a Decimal. This is because a Double is used for binary and DECIMAL floating-point arithmetic whereas a Decimal is more useful for financial reporting. You can see this in the example below.\nCAST(line_item_unblended_cost AS DECIMAL(16,8)) The SUM() function returns the total sum of a numeric column. This is often used to calculate the total spend on a service to give you a complete total.\nSUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) The ROUND() function rounds a number to a specified number of DECIMAL places.\nIf you would like to learn more about SQL Queries we recommend using https://www.w3schools.com/sql/ and for Athena specific https://prestodb.io/docs/current/ Retrieving Data from CUR We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are.Let\u0026rsquo;s discuss a couple common examples when exploring the CUR dataset.\nExample 1 - Answers the question \u0026ldquo;What are all the columns and data in the CUR table?\u0026rdquo; SELECT * FROM ${table_name} LIMIT 10; This is helpful when trying to understand what data is available in the CUR. Use the CSV Export functionality in Athena to obtain a copy of the query results making it is easier to see all of the columns and data.\nIf you are interested in just the column name, you can use the DDL statement: SHOW TABLES IN $your table name .\nExample 2 - Answers the question of \u0026ldquo;What are all the columns from the CUR, where a specific value is in the column?\u0026rdquo; SELECT * FROM ${table_name} WHERE line_item_line_item_type LIKE \u0026#39;%Usage%\u0026#39; LIMIT 10; This is a helpful technique when searching for a particular Charge Type. Take note that the column values are case sensitive.\nExample 3 - Answers the question \u0026ldquo;What services are available in my CUR?\u0026rdquo; SELECT DISTINCT(line_item_product_code) FROM ${table_name} LIMIT 10; This query shows the available services within your CUR. The SELECT DISTINCT statement is helpful when looking for unique values in a specific column.\nThe column product_product_name contains similar information and would be a better choice for querying if you are looking for third party marketplace products since line_item_product_code provides a unique id. Be advised that when using the column product_product_name certain line_item_line_item_type\u0026rsquo;s (such as discounts) do not populate this column.\nExample 4 - Answers the question \u0026ldquo;What billing periods are available?\u0026rdquo; SELECT DISTINCT(bill_billing_period_start_date) FROM ${table_name} LIMIT 10; Additional helpful getting started queries can be found in the in 200 level Cost and Usage Analysis Lab .\nUnderstanding Cost Data AWS Product Descriptions and Pricing Units Query Description This query will provide AWS Product Descriptions and Pricing Units including Product Name, Product Family, and Operations. This query is intended to be used to help you understand the data published in the CUR and assist in query development.\nPricing Please refer to the AWS pricing page for any service you are interested in.\nSample Output Download SQL File Link to Code Copy Query SELECT product_product_name, product_product_family, line_item_operation, pricing_unit, product_description, product_usage_family FROM ${tableName} WHERE line_item_line_item_type = \u0026#39;Usage\u0026#39; GROUP BY product_product_name, product_product_family, line_item_operation, pricing_unit, product_description, product_usage_family ORDER BY product_product_name, product_product_family, line_item_operation, pricing_unit, product_description, product_usage_family; CUR Table Data with Reserved Instances or Savings Plans In CUR many columns are dynamic, and their visibility in Cost and Usage Reports depends on the usage of product in the billing period. [6] You will need to adjust your queries for RIs and SPs depending on your purchase of these products during the billing period that you are querying.\nReserved Instances - Unblended You can use the following columns to understand the unblended costs of your RIs for the billing period. The values for these columns appear for RI line items with reservation_reservation_a_r_n filled in. Note: When trying to calculate amortized total costs, lineItem/UnblendedCost should be removed for \u0026ldquo;Fee\u0026rdquo; line items.\nlineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged - Operation: Sum/Add - Filter: Not needed for Reserved Instances Reserved Instances - Amortized You can use the following columns to understand the amortized costs of your RIs for the billing period. The values for these columns appear only for RI subscription line items (also known as \u0026ldquo;RI Fee\u0026rdquo; line items) and not for the actual instances using the RIs. [7] Typically you would add these two columns (reservation_unused_recurring_fee and reservation_unused_amortized_upfront_fee_for_billing_period) together, along with reservation_effective_cost documented below, to get the total amortized total costs. Additionally you will also need to ignore lineItem/UnblendedCost for \u0026ldquo;Fee\u0026rdquo; line items. See the examples at the bottom of this section for additional clarity.\nreservation/unusedRecurringFee - CUR Column: reservation_unused_recurring_fee - Description: Initial upfront RI fee amortized for Partial Upfront RIs and No Upfront RIs - Operation: Sum/Add - Filter: line_item_line_item_type = 'RIFee' reservation/unusedAmortizedUpfrontFeeForBillingPeriod - CUR Column: reservation_unused_amortized_upfront_fee_for_billing_period - Description: Unused portion of Initial upfront RI fee amortized for Partial Upfront RIs and No - Operation: Sum/Add - Filter: line_item_line_item_type = 'RIFee' lineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged [as filtered, this is the upfront RI fee] - Operation: Ignore (set to 0 in calculation) - Filter: Where reservation_reservation_a_r_n \u0026lt;\u0026gt; '' and line_item_line_item_type = 'Fee' If you wish to compare unblended costs to amortized costs you can subtract the following reservation_amortized_upfront_fee_for_billing_period and add in the upfront RI fee from the line_item_unblended_cost, ignored right above in the amortized cost calculation.\nreservation/amortizedUpfrontFeeForBillingPeriod - CUR Column: reservation_amortized_upfront_fee_for_billing_period - Description: Initial upfront RI fee amortized for All Upfront RIs and Partial Upfront RIs - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'RIFee' The values for these columns appear for actual instances using RIs and represent the Discounted Usage (also known as \u0026ldquo;DiscountedUsage\u0026rdquo; line items). [8]\nreservation/EffectiveCost - CUR Column: reservation_effective_cost - Description: The sum of both the upfront and hourly rate of your RI, averaged into an effective hourly rate. - Additional Breakdown of this column: reservation_amortized_upfront_cost_for_usage + reservation_recurring_fee_for_usage - Operation: Sum/Add - Filter: Where line_item_line_item_type = 'DiscountedUsage' Savings Plans - Unblended You can use the following columns to understand the unblended costs of your SPs for the billing period. The values for these columns appear for SP line items with savings_plan_savings_plan_a_r_n filled in. When trying to calculate unblended costs, line_item_unblended_cost should be removed for \u0026ldquo;SavingsPlanNegation\u0026rdquo; line items. [9]\nlineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged - Operation: Sum/Add - Filter: line_item_line_item_type != 'SavingsPlanNegation' Savings Plans - Amortized You can use the following columns to understand the amortized costs of your SPs for the billing period. The values for these columns appear only for SP subscription line items (also known as \u0026ldquo;SavingsPlanCoveredUsage\u0026rdquo; and \u0026ldquo;SavingsPlanRecurringFee\u0026rdquo; line items) and not for the actual instances using the SPs. For the amortized costs you must also ignore \u0026ldquo;SavingsPlanNegation\u0026rdquo; and \u0026ldquo;SavingsPlanUpfrontFee\u0026rdquo; line items, these can be used to show you the difference between unblended and amortized costs.\nTypically you would add these two columns (savings_plan_savings_plan_effective_cost and savings_plan_total_commitment_to_date) together, and subtract the third (savings_plan_used_commitment). See the examples at the bottom of this section for additional clarity.\nsavingsPlan/SavingsPlanEffectiveCost - CUR Column: savings_plan_savings_plan_effective_cost - Description: Proportion of the SP monthly commitment amount for Upfront and Recurring - Operation: Sum/Add - Filter: line_item_line_item_type = 'SavingsPlanCoveredUsage' savingsPlan/TotalCommitmentToDate - CUR Column: savings_plan_total_commitment_to_date - Description: The total amortized upfront commitment and recurring commitment - Operation: Sum/Add - Filter: line_item_line_item_type = 'SavingsPlanRecurringFee' savingsPlan/UsedCommitment - CUR Column: savings_plan_used_commitment - Description: The total dollar amount of the Savings Plan commitment used - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'SavingsPlanRecurringFee' If you wish to compare unblended costs to amortized costs you can subtract the following two columns for AmortizedUpfrontCommitmentForBillingPeriod and the lineItem/UnblendedCost fropm the SavingsPlanNegation line items, and finally add the SavingsPlanUpfrontFee fee from the lineItem/UnblendedCost, ignored above in the amortized cost calculation. [10]\nsavingsPlan/AmortizedUpfrontCommitmentForBillingPeriod - CUR Column: savings_plan_amortized_upfront_commitment_for_billing_period - Description: The amount of upfront fee a Savings Plan subscription is costing you. Applies to all upfront and partial upfront. - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'SavingsPlanRecurringFee' lineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged [as filtered, this is the upfront SP fee] - Operation: Sum/Subtract - Filter: line_item_line_item_type = 'SavingsPlanNegation' lineItem/UnblendedCost - CUR Column: line_item_unblended_cost - Description: Cost on the day that you're charged [as filtered, this is the Savings Plans discount applied] - Operation: Sum/Add - Filter: line_item_line_item_type = 'SavingsPlanUpfrontFee' Reserved Instance and Savings Plan Example The following snippits from the SELECT portion of the SQL query shows you how to put the above documented logic into practical usage. We will use this same logic across queries where RIs and SPs are used in the CUR query library. When comparing Cost Explorer unblended costs to your CUR queries, use sum_line_item_unblended_cost. When comparing Cost Explorer amortized cost to your CUR queries, use amortized_cost. The ri_sp_trueup and ri_sp_upfront_fees are included in these examples to help you reconcile the difference between unblended and amortized costs. You will find that sum_line_item_unblended_cost = amortized_cost + ri_sp_trueup + ri_sp_upfront_fees. We have already taken care of the additions and subtractions in the values, so you can just add them together.\nAccounts have both RI and SP usage during the billing period\nDATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-01\u0026#39;) AS month_line_item_usage_start_date, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 ELSE line_item_unblended_cost END) AS sum_line_item_unblended_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) AS amortized_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (-savings_plan_amortized_upfront_commitment_for_billing_period) WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (-reservation_amortized_upfront_fee_for_billing_period) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN (-line_item_unblended_cost) ELSE 0 END) AS ri_sp_trueup, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN line_item_unblended_cost WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees Accounts have SP usage but no RI usage during the billing period\nDATE_FORMAT((line_item_usage_start_date),\u0026#39;%Y-%m-01\u0026#39;) AS month_line_item_usage_start_date, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 ELSE line_item_unblended_cost END) AS sum_line_item_unblended_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanCoveredUsage\u0026#39;) THEN savings_plan_savings_plan_effective_cost WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (savings_plan_total_commitment_to_date - savings_plan_used_commitment) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN 0 WHEN (line_item_line_item_type = \u0026#39;DiscountedUsage\u0026#39;) THEN reservation_effective_cost WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (reservation_unused_amortized_upfront_fee_for_billing_period + reservation_unused_recurring_fee) WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN 0 ELSE line_item_unblended_cost END) AS amortized_cost, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanRecurringFee\u0026#39;) THEN (-savings_plan_amortized_upfront_commitment_for_billing_period) WHEN (line_item_line_item_type = \u0026#39;RIFee\u0026#39;) THEN (-reservation_amortized_upfront_fee_for_billing_period) WHEN (line_item_line_item_type = \u0026#39;SavingsPlanNegation\u0026#39;) THEN (-line_item_unblended_cost ) ELSE 0 END) AS ri_sp_trueup, SUM(CASE WHEN (line_item_line_item_type = \u0026#39;SavingsPlanUpfrontFee\u0026#39;) THEN line_item_unblended_cost WHEN ((line_item_line_item_type = \u0026#39;Fee\u0026#39;) AND (reservation_reservation_a_r_n \u0026lt;\u0026gt; \u0026#39;\u0026#39;)) THEN line_item_unblended_cost ELSE 0 END) AS ri_sp_upfront_fees Unblended, Blended, Amortized, and Net Costs Each customer is unique and so are your needs when it comes to viewing and reporting on cost data.\nUnblended and blended costs allow customers with consolidated billing to understand their cost and usage in management payer and linked member accounts. Unblended costs refers to the cost incurred for the usage by an individual account while blended costs refers to costs within a consolidated billing family looking at the total usage for all accounts compared to the linked member account\u0026rsquo;s usage. The ability to view linked account\u0026rsquo;s unblended cost data allows you to more easily reconcile your charges across your payer and linked accounts. [1]\nThe amortized cost metric reflects the effective cost of the upfront and monthly reservation fees spread across the billing period. By default, Cost Explorer shows the fees for Reserved Instances and Savings Plans as a spike on the day that you\u0026rsquo;re charged, but if you choose to show costs as amortized costs, the costs are amortized over the billing period.\nAmortization, simply stated is breaking down cost into an effective daily rate and enables you to see your costs in accrual-based accounting as opposed to cash-based accounting. For example within the context of a Reserved Instance, if you pay $365 for an All Upfront RI for one year and you have a matching instance that uses that RI, that instance costs you $1 a day, amortized. Unblended and blended costs would be instead report on the full initial $365 cost on the day of purchase. [2]\nIn the Well-Architected Labs CUR Query Library we are focused on examples showing unblended costs, amortized costs where applicable, and if applicable we will show true-up and upfront fee columns to allow you to calculate and visualize the difference between unblended and amortized costs.\nUnblended/Blended – Consolidated Billing / Management Accounts with Linked Member Accounts Applicable to Reserved Instances, Savings Plans, and On-Demand Instance, as well as pricing tiers (e.g. AWS Free Tier and S3 volume tiers). Unblended costs are associated with the current cost of the product, while blended rates are the averaged costs of the Reserved Instances, On-Demand Instances, and pricing tiered products that are used by member accounts in an organization in AWS Organizations. AWS calculates blended costs by multiplying the blended rate for each service with an account\u0026rsquo;s usage of that service. [4]\nAmortized Applicable to Reserved Instances and Savings Plans, amortizing is when you distribute one-time reservation costs across the billing period that is affected by that cost. For example, if you pay $365 for an All Upfront RI for one year and you have a matching instance that uses that RI, that instance costs you $1 a day, amortized.\nIn Cost Explorer amortized costs are estimated by combining unblended costs with the amortized portion of your upfront and recurring fees. The unused portion of your upfront fees and recurring charges are shown on the first of the month. [5]\nNet In some cases, customers operating at scale on AWS may be able to take advantage of specialized discounts. The net unblended costs reflect usage costs after these discounts are applied while the net amortized costs adds additional logic to amortize discount-related information, in addition to your Savings Plans or Reservation-related charges. [3]\n[1] https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/con-bill-blended-rates.html [2] https://docs.aws.amazon.com/cur/latest/userguide/amortized-reservation.html [3] https://aws.amazon.com/blogs/aws-cost-management/understanding-your-aws-cost-datasets-a-cheat-sheet/ [4] https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/con-bill-blended-rates.html#Blended_CB [5] https://console.aws.amazon.com/cost-management/home?#/custom [6] https://docs.aws.amazon.com/cur/latest/userguide/product-columns.html [7] https://docs.aws.amazon.com/cur/latest/userguide/amortized-reservation.html [8] https://docs.aws.amazon.com/cur/latest/userguide/reservation-columns.html [9] https://docs.aws.amazon.com/cur/latest/userguide/cur-sp.html [10] https://docs.aws.amazon.com/cur/latest/userguide/Lineitem-columns.html Contributing Community contributions are encouraged and welcome. Please follow the Contribution Guide . The goal is to pull together useful CUR queries in to a single library that is open, standardized, and maintained.\nContributors Please refer to the CUR Query Library Contributors section .\nCUR queries are provided as is. We recommend validating your data by comparing it against your monthly bill and Cost Explorer prior to making any financial decisions. If you wish to provide feedback on these queries, there is an error, or you want to make a suggestion, please email: curquery@amazon.com "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/contribution-guide/","title":"Library Contribution Guide","tags":[],"description":"","content":"Contribution Process For ease of use and education, it is crucial we maintain uniformity and consistency. To contribute your query please follow the steps below:\nDevelop your query following the CUR Library Style Guide rules defined below Execute and validate the query using your own Cost and Usage Report Validate your query results against your Cost Explorer Save your query as a .sql file without underscores as seperators (i.e. rds_mysql_engine_query.sql NOT rdsmysqlenginequery.sql). Clone the github repo and perform a pull request to contribute to the labs using this guide Place your query in the folder static/Cost/300_CUR_Queries/Code/\u0026lt; AWS Product Category Folder \u0026gt;/your_query.sql (NOTE: please check below for more information about the folder structure) Contributions will take between 2-3 weeks to review, validate and publish. Not all queries will be published if they overlap with an existing query.\nCUR Library Style Guide For a reference of a properly constructed query please reference the Examples below:\nSELECT SECTION RULES:\nIt is recommended to sanitize the query of your account and resource related data. Please reference the Fictitious names and numbers documentation. Be advised, during the review process, queries submitted with customer account data will be sanitized.\nTo fill fields with dummy data, use the following select statement (example shown fills bill_payer_account_id with dummy data):\nEXAMPLE: '111122223333' AS bill_payer_account_id EXAMPLE: '444455556666' AS line_item_usage_account_id For line_item_resource_id there is not a reference to all resources in the safenames document. Where possible use:\nEXAMPLE: ‘\u0026lt;resource id\u0026gt;' AS line_item_resource_id COLUMN NAMING:\nIf a column requires a specific name to be defined ( i.e. after running a sum function ), use the name of the outer most function followed by the column name:\nEXAMPLE: SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, EXAMPLE: SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount, EXAMPLE: SPLIT_PART(line_item_resource_id, 'crawler/', 2) AS split_line_item_resource_id, EXAMPLE: SPLIT_PART(line_item_usage_type ,':',2) AS split_line_item_usage_type For DATE_FORMAT use the value defined by the format:\nEXAMPLE: DATE_FORMAT(line_item_usage_start_date,'%Y-%m') AS month_line_item_usage_start_date, EXAMPLE: DATE_FORMAT(line_item_usage_start_date,'%Y-%m-%d') AS day_line_item_usage_start_date, If multiple nested functions are used, use the name of the left-most function only followed by the column name:\nEXAMPLE: TRIM(REPLACE(product_group, 'Security Services - Amazon GuardDuty ', '')) AS trim_product_group, FROM SECTION RULES:\nUse ${table_name} as the variable for the customer table name.\nWHERE SECTION RULES:\nTo aggregate data, always try to use the default CUR partitions as defined by the CUR CFN template. The data is partitioned on year and month. Below is an example on how we are formatting this WHERE statement:\nEXAMPLE: ${date_filter} For month we use both ‘mm\u0026rsquo; vs. ‘m\u0026rsquo; as per the example above as previous CFN templates have included both formats.\nORDER BY SECTION RULES:\nOrder is currently at authors discretion. You can use the selected data including your functions used in your select, a column alias, or you can substitute with a column number. It is most readable if you use column aliases in the SELECT and ORDER BY clauses.\nEXAMPLE: ${table_name} EXAMPLE: ${payer_id} OTHER:\nVariables use a dollar sign ($) curly brackets {} and a name with fields separated by underscore _. Rule: For fields without spaces, do not use quotes \u0026quot; \u0026quot; around field name. Rule: Queries should end with a semi-colon. Rule: Review Domain Markdown Process defined below. Rule: Must be run on a CUR Athena Database before loaded. Compare data against the testing accounts Cost Explorer data. Query Folders We have split the CUR queries up into folders matching the AWS Cloud Product groups . This is to enable users to easily find the query they are looking for based on the service they are interested in.\nWhen adding a query please consider these points:\nCheck if the service your query focuses on has a AWS Product Category folder and add it in there if it does If not create a folder using the AWS Product Category name If it is a multi product query, choose the AWS Product Category with the most influence on the query If it crosses 3+ services or is a global query, use the Global folder If it is a unique CUR query not related to cost, use the Global folder and we will sort the location as needed "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/gettingstarted/architecture/","title":"Architecture","tags":[],"description":"","content":"Architecture In this workshop we\u0026rsquo;ll work with a typical Big Data workload with streaming ingest and batch processing.\nIngest Our data source is streaming data coming from external applications. We present a Global Accelerator endpoint for the producers to send data to. The endpoint is serviced in a particular region by a Lambda function behind an Application Load Balancer. The Lambda function relays the data to a Kinesis stream.\nBatch Processing Once the data lands in a Kinesis stream, the batch processing flow picks up with a Firehose landing the data in S3. A nightly Glue job performs batch processing. A Lambda function registers new partitions in the Glue catalog.\nDisaster Recovery Looking at our architecture, we have three primary data stores, S3, DynamoDB, and the Glue catalog. For S3, we use cross-region replication to backup data to a DR region.\nFor DynamodB, we use the point-in-time-recovery (PITR) feature of DynamoDB, and AWS Backup for taking an hourly backup. During a failover, a Lambda function automatically restores the most recent PITR snapshot into a DynamoDB table in the backup region. Note that we could choose to restore from the PITR snapshot on a regular schedule instead of during failover.\nFor the Glue catalog, we use a Lambda function to register new partitions in the Glue catalog. The function triggers when objects are created in S3. It looks at the object prefix to understand the partition structure, and creates a new partition in the Glue catalog if necessary. Because the function is running in the primary region and the backup region, we do not need to take extra steps to replicate the Glue catalog. The function in the backup region will create new partitions as S3 cross-region replication copies objects into the bucket in the backup region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failover/endpoint/","title":"Endpoint Switch","tags":[],"description":"","content":"Switching the endpoint Now we\u0026rsquo;re ready to reconfigure the Global Accelerator endpoint.\nGo to the Systems Manager console in the primary region and execute the automation document called failover_runbook. You\u0026rsquo;ll need to pass in two inputs:\nBackupRegion: Set this to your backup region, e.g. us-east-2 BackupEndpoint: Set this to the ARN of the backup ALB. X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/settingup/primaryregion/","title":"Primary Region","tags":[],"description":"","content":"Primary Region In this section we\u0026rsquo;ll deploy the workload to the primary region.\nCreate a prefix list for ingress Follow the instructions to create a VPC prefix list that specifes the CIDR ranges for ingress. We use this prefix list to control access to lab resources from an approved network block. If you are using an Event Engine account, you can create a prefix list that just contains 0.0.0.0/0.\nDownload and review scripts and templates Download the following files:\ncreate.sh workload.yaml tweetmaker.py compaction.py Let\u0026rsquo;s review what\u0026rsquo;s in these files. You can open them in your favorite text editor to review in detail. The CloudFormation script, workload.yaml, creates the analytics workload, including the S3 CRR policy. The script accepts six groups of inputs, and we have provided sane default values for most of them.\nVPC networking parameters. The defaults will work for most of these arguments, and the driver script create.sh will prompt for the others. The Global Accelerator endpoint name. The default value should work well. AWS Glue database and table names. The default values should be fine. Settings for the EC2 instance that we use as a tweet producer. The default values should be fine. The shard count for the Kinesis streams we use. The default values should be fine for this workshop. A tag value we use to identify resources used in this stack. The default value should be fine. The bucket name we use to store data and a bucket name used for cross-region replication in the backup region. Both of these arguments come from the previous section where we deployed some DR infrastructure in the backup region. The tweetmaker.py script is the Python code that simulates sending incoming tweets to our endpoint. The compaction.py script is code for a Glue ETL job that processes raw data into a more efficient storage format.\nThe create.sh script is purely for convenience. It uploads workload.yaml and the Python scripts to the S3 template bucket, and then creates the CloudFormation stack, passing in the required input arguments.\nIn your working directory, place create.sh in a directory called scripts and place the file workload.yaml in a directory called cfn. Place tweetmaker.py in a directory called src and compaction.py in a directory called glue.\nDeploy stack Now create the stack in the primary region:\nexport AWS_PROFILE=PRIMARY chmod +x ./scripts/create.sh ./scripts/create.sh \u0026lt;template bucket\u0026gt; \u0026lt;template prefix\u0026gt; \u0026lt;stack name\u0026gt; \u0026lt;REGION\u0026gt; \u0026lt;backup bucket name\u0026gt; \u0026lt;ingress prefix list\u0026gt; The input arguments are:\ntemplate bucket - the name of the S3 bucket we created in the previous section. We use it to store the CloudFormation templates and other data. template prefix - An S3 prefix we append to the CloudFormation template file names. We use cfn as a convention. stack name - The name of the CloudFormation stack. You can pick any suitable name. REGION - The primary region. This argument is the region that the template bucket is in, and we created the template bucket in the primary region. backup bucket name - The name for the S3 bucket used for data replication. Use the name of the backup bucket you created in the previous section. ingress prefix list - Use the identifier of the VPC prefix list you created for the primary region in the Getting Started section. For example:\n./scripts/create.sh backuprestore cfn BackupRestore us-west-2 MyBackupBucket MyPrefixList To update the stack, add the --update flag as the last argument.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failback/removeinfra/","title":"Remove Redundant Infrastructure","tags":[],"description":"","content":"Removing redundant infrastructure Delete the infrastructure stack (BackupRestoreInfra) from the backup region. In our simple example, leaving this infrastructure in place wouldn\u0026rsquo;t do any harm, other than resulting in some unnecessary expense. But in a more complex scenario you might have scheduled jobs running that would conflict with replicated data coming from the primary region.\nYou can leave this stack in place for a time if you want to ensure that the workload is running properly in the primary region, but at some point you\u0026rsquo;ll want to remove it to eliminate unnecessary cost.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_cur_queries/query_help/contributors/","title":"CUR Query Library Contributors","tags":[],"description":"","content":"Introduction The CUR Library is intended to be an open source library of Cost \u0026amp; Usage Reports queries that can be shared. We want to thank all contributors both from Amazon and External parties for helping make this a robust tool for navigating the CUR.\nLibrary Authors Chris Strzelczyk, Sr. TAM, AWS Enterprise Support Bill Pfeiffer, Sr. TAM, AWS Enterprise Support Stephanie Gooch, Cost Optimization Lead, AWS OPTICS Matthew Brend, Enterprise Support Lead, AWS Enterprise Support Jonathan Banas, Sr. TAM, AWS Enterprise Support Alee Whitman, Commercial Architect, AWS Optics Justin Marks, TAM, AWS Enterprise Support Query Contributors Alon Jupiter, Sr. TAM, AWS Enterprise Support Paul Abruzzo, Enterprise Support Lead, AWS Enterprise Support Benjamin Lecoq, Sr. TAM, AWS Enterprise Support Oleksandr Moskalenko, TAM, AWS Enterprise Support Rony Blum, Sr. TAM, AWS Enterprise Support Jonathan Rudge, Sr.TAM, AWS Enterprise Support Luis Osses, Sr. Edge Specialist Solutions Architect Alee Whitman, Commercial Architect, Optics Arabinda Pani, Sr. Partner Solutions Architect, Database Yuriy Prykhodko, Sr. TAM, AWS Enterprise Support Mariano Iumiento, TAM, AWS Enterprise Support Roger Vandervort, Sr. TAM, AWS Enterprise Support Mark Rekai, Sr. TAM, AWS Enterprise Support Rob Yang, Sr. TAM, AWS Enterprise Support Geno Erickson, Sr. TAM, AWS Enterprise Support "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/settingup/","title":"Setting Up","tags":[],"description":"","content":"Setting Up In this chapter, we\u0026rsquo;ll deploy our Big Data workload. We\u0026rsquo;ll deploy necessary infrastructure into the backup region, deploy the workload in the primary region, and then test that the workload is functioning correctly.\nFor the sake of convenience, we create the data replication targets in the backup region first.\nBackup Region Primary Region Test Workload X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failback/resync/","title":"Resync Data","tags":[],"description":"","content":"Data synchronization We run an inventory in the backup bucket on a daily basis. We can use that to find files that were written directly into the backup bucket rather than through replication. Then we can sync these files back to the primary region.\nLet\u0026rsquo;s start with the raw files. In Athena, run this query in the backup region:\nMSCK REPAIR TABLE inventory; Note that you must wait for an inventory schedule to complete before you see data in this table.\nThen run this query to find any files that were created outside of replication:\nselect * from inventory where version_id \u0026lt;\u0026gt; 'REPLICA'; Now repeat for the nightly files:\nMSCK REPAIR TABLE inventory_compacted; Then run this query to find any files that were created outside of replication:\nselect * from inventory_compacted where version_id \u0026lt;\u0026gt; 'REPLICA'; If you find any files, download the query results as a CSV file. Then download and run this script :\npython resync_s3.py --input \u0026lt;CSV file\u0026gt; --primary \u0026lt;bucket in primary region\u0026gt; X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/settingup/testworkload/","title":"Test Workload","tags":[],"description":"","content":"Test the Workload In this section we\u0026rsquo;ll make sure that the workload is running properly, and that data is replicating to the backup region. We use a python script to simulate a stream of incoming tweets.\nOpen a Session Manager connection to the EC2 instance sample producer in the primary region. You can find the instance ID and the Global Accelerator DNS endpoint in the CloudFormation output.\nOpen the EC2 console and navigate to Instances. Select the producer instance and click Connect. Choose Session Manager. Once connected, run:\nsudo su - ec2-user python3 tweetmaker.py --endpoint \u0026lt;Global Accelerator endpoint\u0026gt; Now, navigate to Kinesis Analytics in the AWS Console. Click on the radio button for the application called \u0026lt;stack name\u0026gt;-KinesisAnalyticsApplication and select Run.\nAfter a few minutes you should be able to preview the table in Athena and see some output. The Glue database is called backuprestoredb and the raw tweets are in a table called rawdata.\nTo produce the nightly compacted files, run the Glue job called CompactNightly. Then you can preview the table compacteddata.\nTo verify data replication, you can look for files in the bucket in the backup region.\nYou can also verify that data is getting populated in the DynamoDB table - processed_tweets. We have set up an AWS backup plan where we would be backing up this table every hour.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failover/verify/","title":"Verify Failover","tags":[],"description":"","content":"Verifying failover If you run the sample producer, you\u0026rsquo;ll now see data going directly into the backup bucket in the backup region, rather than the primary region.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failover/","title":"Failover","tags":[],"description":"","content":"Failover In this chapter, we\u0026rsquo;ll test failing over to the backup region, assuming that our entire primary region is now unavailable. Note that in reality you may need to failover if the primary region has experienced only a partial degradation of service.\nBackup Region Infrastructure Endpoint Switch Verify Failover X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/failback/","title":"Failback","tags":[],"description":"","content":"Failback In this chapter, we\u0026rsquo;ll see how to move the workload back to the primary region when the primary region is functional again.\nWe have four things to consider:\nWe have data in a DynamoDB table in the backup region that is not in the primary region. Our incoming traffic is still going to the backup region. We have data in the S3 bucket in the backup region that is not in the primary region. We have infrastructure deployed in the backup region that we don\u0026rsquo;t want to live permanently. We\u0026rsquo;ll tackle these one at a time. We\u0026rsquo;ll start by restoring the DynamoDB table from the backup region to the primary region, as that requires deleting and recreating the table. Then we\u0026rsquo;ll switch the endpoint to redirect traffic to the primary region, and take care of any data in S3.\nRecreate DynamoDB Redirect Traffic Remove Redundant Infrastructure Resync Data X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_backup_restore_failback_analytics/next/","title":"Next Steps","tags":[],"description":"","content":"Cleaning up Congratulations for finishing this workshop! Continue on to learn how to clean up after using this workshop.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/builders_guide/","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3","tags":[],"description":"","content":"Introduction This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates\u0026rsquo; parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment.\nThis guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in.\nPrerequisites An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON License Documentation License Licensed under the Creative Commons Share Alike 4.0 license.\nBuilding and uploading the AWS Lambda Functions Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory\u0026rsquo;s contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows:\n% cd \u0026lt;LambdaDirectory\u0026gt; % make % cd .. % aws s3 cp \u0026lt;lambda\u0026gt;.zip s3://\u0026lt;S3 bucket\u0026gt;/\u0026lt;directory prefix\u0026gt;/\u0026lt;lambda\u0026gt;.zip Debugging the AWS Lambda Functions The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows:\n% python -m pdb \u0026lt;lambda_function\u0026gt;.py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine:\nlog_level: This is the python logger logging level. To make it verbose in the logs, use the value \u0026ldquo;DEBUG\u0026rdquo; region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \u0026ldquo;folder\u0026rdquo; (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \u0026ldquo;folder\u0026rdquo; (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following:\n{ 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg' } There is considerable \u0026ldquo;shared knowledge\u0026rdquo; between the state machine functions that is all hard-coded, like stack names.\nThe state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object:\n{ 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within.\nThe Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda.\nBuilding and Uploading the Web Application The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows:\n% cd go % make % aws s3 cp FragileWebApp s3://\u0026lt;S3 bucket\u0026gt;/\u0026lt;diretory prefix\u0026gt;/FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy.\nThe Bootstrapping Script The bootstrapping script assumes 4 things:\nThe name of the SQL to run to create the table used is hardcoded to \u0026ldquo;createIPTable.sql\u0026rdquo; The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \u0026ldquo;FragileWebApp\u0026rdquo; The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs.\nThe SQL in the Bootstrapping Script The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses.\nDeploying the State Machine The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it.\nCloudFormation templates The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/troubleshooting_guide/","title":"Troubleshooting Guide for 300 - Testing for Resiliency of EC2, RDS, and S3","tags":[],"description":"","content":"Introduction The purpose of this guide is to prepare for the expected questions and problems.\nCommon AWS Account Problems If running these labs on your own, you will need to use an AWS account that meets the following qualifications. If you are at a live workshop, you may have been supplied with an AWS account for the lab. If not, and you cannot remedy your account issues, please see a proctor who can help pair you with another student who does have these permissions and you can “pair lab”.\nYou will need to be able to log into the console as a user with permissions to run CloudFormation. If you do not have permission to run CloudFormation, please create a new IAM User with these permissions or use a different AWS account.\nThe next most common problem in deploying the test application is exceeding the default limit of 5 Elastic IPs in an account. The VPC is created with 3 NAT Gateways, which each require an EIP. You will either will have to release some that you are using, or use a different AWS account.\nThe service linked roles may exist already in an account. If they do, you will see a failure to deploy the first CFN stack for lambda_functions_for_deploy.json. You should delete the stack and redeploy it, but please make sure you are appropriately setting the Boolean parameters of the deployment machine stack. If at a live workshop, please see a proctor if you need more help with this.\nProblems with Service Linked Roles If you don’t see the existing service linked IAM Role and try to create it in the deployment machine, it will not deploy. It will fail back with an error that the Role already exists under another name. Simply set the parameter to false and redeploy.\nProblems with the Step Functions State Machine and/or Lambda Functions The state machine is idempotent and can be re-run if something times out.\nIf a function fails, you can debug it by creating a test for the Lambda Function. For example, to the test the DeployVPC Lambda function, navigate to the StepFunctions console, and select the DeployVPC function in the Visual Workflow, and click on the Input in the Step details on the right:\nOnce you’ve clicked the Input, you can select the input and copy it into the copy buffer:\nThen navigate to the Lambda console and click on the DeployVPC Lambda Function:\nYou can then click the down arrow to the left of the “Test” button with the grayed text “Select a test event..” and click on “Configure test events:”\nName the event TestDeployVPC and insert the copied input from the step function, then click “Create:”\nNow you can click the “Test” button to execute the test:\nAfter execution, you can click on the “Details” and see the log of the function to determine what went wrong: You can also go to the CloudWatch logs to see details of the execution.\nDeployRDS step fails If you get the following error for DeployRDS then there is an obsolete DB password stored in SSM Parameter store. This can happen if you had a problem with deployment, stopped it, and then restarted it.\nThe ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access.\nSolution:\nIn the AWS console go to SSM Parameter store Delete the parameters stored there\nGo to the CloudFormation console and delete (roll back) the ResiliencyVPC stack\nResume by re-starting deployment of the infrastructure\nGo to the AWS Step Function console at https://console.aws.amazon.com/states On the Step Functions dashboard, you will see “State Machines” and you will have a new one named “DeploymentMachine-random characters.” Click on that state machine. This will bring up an execution console. Click on the “Start execution” button. On the \u0026ldquo;New execution\u0026rdquo; dialog, for \u0026ldquo;Enter an execution name\u0026rdquo; delete the auto-generated name and replace it with: BuildResiliency\nThen for \u0026ldquo;Input\u0026rdquo; enter JSON that will be used to supply parameter values to the Lambdas in the workflow.\nsingle region uses the following values:\n{ \u0026quot;log_level\u0026quot;: \u0026quot;DEBUG\u0026quot;, \u0026quot;region_name\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_region\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;folder\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;workshop\u0026quot;: \u0026quot;300-ResiliencyofEC2RDSandS3\u0026quot;, \u0026quot;boot_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;boot_prefix\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;websiteimage\u0026quot; : \u0026quot;https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg\u0026quot; } multi region uses the values here Note: for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab\nThen click the “Start Execution” button.\nRDSStackCompleteChoice -\u0026gt; DeployFailedStatus If your deployment machine fails and looks like this\nAnd if the following is true:\nclick on the RDSStackCompleteChoice stage of your workflow\nselect Output\nRDS stack shows status as CREATE_IN_PROGRESS\n\u0026quot;rds\u0026quot;: { \u0026quot;stackname\u0026quot;: \u0026quot;MySQLforResiliencyTesting\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;CREATE_IN_PROGRESS\u0026quot; } Then it is likley that your RDS deployment timed out before the workflow could complete. Do the following to continue:\nGo to the CloudFormation console Verify the status for MySQLforResiliencyTesting is CREATE_COMPLETE\nGo back to your state machine\nClick New Execution\nGive your execution a new name, unique from previous ones (such as \u0026ldquo;BuildResiliency3\u0026rdquo;)\nThe workflow will quickly determine which stacks have already been deployed, and start immediately on the final (web server) stack. Problems Executing the Scripts If you are not using Amazon Linux, you will need to install the AWS CLI.\nInstalling jq is pretty easy, just download the executable and install it where the PATH will see it.\nOlder versions of bash and the windows bash implementation complain about the { and } characters in the sed commands. They can be deleted in older version of bash (but note that they are required in newer versions of bash).\nAssisting with the Failure Tests Failure modes individual The shell script will delete the first instance it finds running the VPC. There shouldn’t be a problem with the jq parser of the JSON returned from the ec2 describe-instances but it is theoretically possible. If you get a jq error, it is more likely the web layer has not actually been created correctly.\nIf you see nulls in output messages, it is possible that you are not specifying the correct VPC ID.\nInstalling boto3 via pip on an Amazon Linux instance will cause you to have errors on the command line:\n$ aws help Traceback (most recent call last): File \u0026quot;/usr/local/bin/aws\u0026quot;, line 19, in \\\u0026lt;module\\\u0026gt; import awscli.clidriver File \u0026quot;/usr/local/lib/python2.7/dist-packages/awscli/clidriver.py\u0026quot;, line 19, in \\\u0026lt;module\\\u0026gt; from botocore.hooks import AliasedEventEmitter ImportError: cannot import name AliasedEventEmitter To fix this, you need to remove the aws-cli, downgrade boto, and install an older version of the aws-cli:\n$ sudo yum remove aws-cli $ sudo yum downgrade python27-botocore 1.8 $ sudo yum install aws-cli-1.14.9 EC2 Instance Failure Some additional questions to ask yourself:\nOpen fail_instance.sh/fail_instance.py/InstanceFailover.java/InstanceFailover.cs in an editor. How could you make this randomly select an instance?\nWhat are the concerns if you have hundreds or thousands of instances?\nWhat if you don’t have an Auto Scaling group? How could you recover?\nHow do they test EC2 AutoRecovery? The answer is to open a support ticket. This requires extra effort on our side.\nHow would you “undo” this failure mode?\nRDS Failover The console does not update the active AZ of the RDS instance until an unknown amount of time passes (~5 min). This can make it appear it didn’t fail over, but it did. You can see the failover in the events log of RDS.\nIf you see nulls in output messages, it is possible that you are not specifying the correct VPC ID.\nSome additional questions to ask yourself:\nWhy didn’t the Auto Scaling Group terminate them and replace the instances? Or why did it?\nHow could you make the application resilient to the transient failure?\nWhat if this was a single AZ RDS?\nHow would you fail the S3 portion of the application?\nAZ Failure The Java and C# implementations have some improved error checking over the bash implementation, but essentially perform the same logic.\nThis is what the failure simulation does:\nLoop through all the Auto Scaling Groups by calling Auto Scaling’s DescribeAutoScalingGroups; for each group, look at the AZs it is configured for. If the desired AZ in in the list, we can reconfigure the group by calling UpdateAutoScalingGroup to update the AZs to the list without this AZ in it.\nCall EC2’s DescribeSubnets to identify the subnets in the AZ desired within the VPC. It then creates a NACL, adds entries to block ingress and egress of all ports and protocols, then calls EC2’s ReplaceNetworkAclAssociation to associate the subnets with the NACL. This will cause the ELB to route traffic to the other AZs since it has Cross AZ enabled.\nLoop through all the RDS Instances by calling RDS DescribeInstances. If this instance’s AvailabilityZone is this AZ, then if it is an RDS Multi-AZ, call RDS RebootDBInstance with ForceFailover set to True.\nSome additional questions to ask yourself:\nWhat is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs.\nWhat would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances\nHow would you undo all these changes?\nRegion Failure Unfortunately, you need a DNS domain registered to effect a Region failover, so you won’t be able to perform this failure. However, you can think about how you would simulate it.\nRoute53 uses HealthChecks to see if the destination is available. You could use the network ACL modification above that relates to the AZ to specify that change on the health check endpoint. (A SecurityGroup modification would likely work as well). This would cause the HeathCheck to fail and the record set to use the second region. You would want to also have an alert fire on this and send a call to DMS StopReplicationTask to stop the replication if you are using DMS to replicate data to an instance in a second AWS Region.\nSome additional questions to ask yourself:\nHow would you fail back? You could set up a DMS instance that is configured for the source and target going the other way, then recover by call StartReplicationTask, using a load and cdc configuration.\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/6_failure_injection_az/","title":"Availability Zone (AZ) Failure Injection Has Been Moved","tags":[],"description":"","content":" Go here for AZ failure injection X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Previous Step Next Step "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/intro/","title":"Introduction","tags":[],"description":"","content":" Background Unishop is THE one-stop-shop for all your Unicorn needs. You can find the best Unicorn selection online at the Unishop and get your Unicorn delivered in less than 24 hours!\nAs a young startup Unishop built a great service which was focused on customers and business outcomes but less on technology and architecture. After a few years establishing a business model and securing the next round of venture capital funding, the business is looking to expand to other markets, such as Unicorn-Insurance, Unicorn-Banking and Unicorn-Ride-Sharing.\nThe CTO has asked you to ensure the system architecture is sufficiently resilient to recover from a disaster. A number of Disaster Recovery (DR) solutions are available from Amazon Web Services (AWS) representing various Recovery Point Objectives (RPO) and Recovery Time Objectives (RTO).\n"},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_1/","title":"Module 1: Backup and Restore","tags":[],"description":"","content":"In this module, you will go through the Backup and Restore DR strategy. To learn more about this DR strategy, you can review this Disaster Recovery blog .\nOur test application is Unishop. It is a Spring Boot Java application connected to a MySQL database with a frontend written using bootstrap.\nThe app is deployed on a single EC2 instance (t3.small) within a dedicated VPC using a single public subnet. Note that this is not the ideal infrastructure architecture for running highly available production applications but suffices for this workshop.\nTo configure the infrastructure and deploy the application, we will use CloudFormation. CloudFormation is an easy way to speed up cloud provisioning with infrastructure as code.\nWe will initially deploy Unishop to the us-east-1 AWS region and verify functionality. Then we will use an AWS EC2 AMI and AWS Backup to create copies of the application server and database in the us-west-1 region. Finally, we will use the copies to create and test a fully functional application in the us-west-1 region.\nPrior experience with the AWS Console and Linux command line are helpful but not required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_2/","title":"Module 2: Pilot Light","tags":[],"description":"","content":"In this module, you will go through the Pilot-Light Disaster Recovery (DR) strategy. To learn more about this DR strategy, you can review this Disaster Recovery blog .\nOur test application is Unishop. It is a Spring Boot Java application with a frontend written using bootstrap.\nThe app uses an Amazon S3 bucket to host a static web interface. A single EC2 instance serves as a proxy for API calls to an Amazon Aurora MySQL database. The database contains mock user and product information.\nWe will initially deploy the primary Unishop instance into the N. Virginia region. Next, the N. California region will host the Pilot-Light DR instance. To configure and deploy this infrastructure, we will use AWS CloudFormation. CloudFormation enables Infrastructure as Code (IaC) automation to quickly provision cloud resources.\nAfterward, we will verify the DR scenario. Meeting our RPO / RTO within 10s of minutes requires Amazon Aurora MySQL Cluster with the Amazon Aurora MySQL Global database feature enabled. This feature adds cross-region database replication capabilities.\nPrior experience with the AWS Console and Linux command line are helpful but not required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_3/","title":"Module 3: Warm Standby","tags":[],"description":"","content":"In this module, you will go through the Warm Standby Disaster Recovery (DR) strategy. To learn more about this DR strategy, you can review this Disaster Recovery blog .\nOur test application is Unishop. It is a Spring Boot Java application with a frontend written using bootstrap. The app uses an Amazon S3 bucket to host a static web interface. A single EC2 instance serves as a proxy for API calls to an Amazon Aurora MySQL database. The database contains mock user and product information.\nWe will initially deploy the primary Unishop instance into the N. Virginia region. Next, the N. California region will host the Warm-Standby DR instance. To configure and deploy this infrastructure, we will use AWS CloudFormation. CloudFormation enables Infrastructure as Code (IaC) automation to quickly provision cloud resources.\nAfterward, we will verify the DR scenario. Meeting our RPO / RTO within minutes requires Amazon Aurora MySQL Clusters with the 1/ Read-Replica Write Forwarding and 2/ Amazon Aurora MySQL Global tables enabled. These features support replicating database changes from either region.\nPrior experience with the AWS Console and Linux command line are helpful but not required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/workshop_4/","title":"Module 4: Hot Standby","tags":[],"description":"","content":"Our test application is Unishop. It is a Spring Boot Java application with a frontend written using bootstrap. The app uses an Amazon S3 bucket to host a static web interface. A single EC2 instance serves as a proxy for API calls to an Amazon Aurora MySQL database. The database contains mock user and product information. Amazon API Gateway is used to connect via AWS Lambda to a DynamoDB database storing shopping cart and session information.\nTo configure the infrastructure and deploy the application we will use CloudFormation. CloudFormation enables Infrastructure as Code (IaC) automation to quickly provision cloud resources.\nWe will initially deploy the primary Unishop instance into the us-east-1 (N. Virginia) region. Next, the us-west-1 (N. California) region will host the Hot-Standby Disaster Recovery (DR) instance. To configure and deploy this infrastructure, we will use AWS CloudFormation.\nAfterward, we will verify the DR scenario. Meeting our RPO / RTO virtually instantaneously, requires an Amazon CloudFront distribution with Orgin Failover policy. Additionally, the workshop deploys an Amazon RDS Aurora MySQL Cluster in each region, and enables 1/ Read-Replica Write Forwarding and 2/ Amazon Aurora MySQL Global tables. These features support replicating database changes from either region. Finally we will configure DynamoDB Global Tables which will replicate data between regions.\nPrior experience with the AWS Console and Linux command line are helpful but not required.\nX Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab "},{"uri":"https://wellarchitectedlabs.com/reliability/disaster-recovery/elasticdisasterrecovery/","title":"AWS Elastic Disaster Recovery","tags":[],"description":"","content":"Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication. You can perform non-disruptive tests to confirm that implementation is complete. During normal operation, maintain readiness by monitoring replication and periodically performing non-disruptive recovery and failback drills. If you need to recover applications, you can launch recovery instances on AWS within minutes, using the most up-to-date server state or a previous point in time. After your applications are running on AWS, you can choose to keep them there, or you can initiate data replication back to your primary site when the issue is resolved. You can fail back to your primary site whenever you’re ready. Learn more about AWS Elastic Disaster Recovery. "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/kpi_ebs_snap_view/","title":"","tags":[],"description":"","content":"KPI EBS Snap View This view will be used to create the KPI EBS Snap view that is used to analyze EBS snap metrics and potential savings opportunities. There is only one version of this view and it is not dependent on if you have or do not have Reserved Instances or Savings Plans.\nCreate View Click here to expand the view Modify the following SQL query for the KPI EBS Snap view:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_ebs_snap AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Filter CUR to return all ebs ec2 snapshot usage data snapshot_usage_all_time AS ( SELECT year , month , bill_billing_period_start_date billing_period , line_item_usage_start_date usage_start_date , bill_payer_account_id payer_account_id , line_item_usage_account_id linked_account_id , line_item_resource_id resource_id , (CASE WHEN (line_item_usage_type LIKE '%EBS:SnapshotArchive%') THEN 'Snapshot_Archive' WHEN (line_item_usage_type LIKE '%EBS:Snapshot%') THEN 'Snapshot' ELSE \u0026quot;line_item_operation\u0026quot; END) snapshot_type , line_item_usage_amount , line_item_unblended_cost , pricing_public_on_demand_cost FROM (database).(tablename) WHERE (((((bill_payer_account_id \u0026lt;\u0026gt; '') AND (line_item_resource_id \u0026lt;\u0026gt; '')) AND (line_item_line_item_type LIKE '%Usage%')) AND (line_item_product_code = 'AmazonEC2')) AND (line_item_usage_type LIKE '%EBS:Snapshot%')) ),\t-- Step 3: Return most recent billing_period and the first billing_period request_dates AS ( SELECT DISTINCT resource_id request_dates_resource_id , \u0026quot;min\u0026quot;(usage_start_date) start_date FROM snapshot_usage_all_time WHERE (snapshot_type = 'Snapshot') GROUP BY 1 ), snapshot_usage_all_time_before_map AS ( -- Step 4: Pivot table so looking at previous month filtered for only snapshots still available in the current month SELECT DISTINCT billing_period , request_dates.start_date , payer_account_id , linked_account_id , snapshot_type , resource_id , \u0026quot;sum\u0026quot;(line_item_usage_amount) usage_quantity , \u0026quot;sum\u0026quot;(line_item_unblended_cost) ebs_snapshot_cost , \u0026quot;sum\u0026quot;(pricing_public_on_demand_cost) public_cost , \u0026quot;sum\u0026quot;((CASE WHEN ((request_dates.start_date \u0026gt; (billing_period - INTERVAL '12' MONTH)) AND (snapshot_type = 'Snapshot')) THEN line_item_unblended_cost ELSE 0 END)) \u0026quot;ebs_snapshots_under_1yr_cost\u0026quot; /*No savings estimate since it uses uses 100% of snapshot cost for snapshots over 6mos as savings estimate*/ , \u0026quot;sum\u0026quot;((CASE WHEN ((request_dates.start_date \u0026lt;= (billing_period - INTERVAL '12' MONTH)) AND (snapshot_type = 'Snapshot')) THEN line_item_unblended_cost ELSE 0 END)) \u0026quot;ebs_snapshots_over_1yr_cost\u0026quot; FROM (snapshot_usage_all_time snapshot LEFT JOIN request_dates ON (request_dates.request_dates_resource_id = snapshot.resource_id)) WHERE (CAST(\u0026quot;concat\u0026quot;(snapshot.year, '-', snapshot.month, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) GROUP BY 1, 2, 3, 4, 5, 6 ) ( -- Step 5: Add in map data SELECT billing_period , start_date , payer_account_id , linked_account_id , map.* , resource_id , snapshot_type , usage_quantity , ebs_snapshot_cost , public_cost , \u0026quot;ebs_snapshots_under_1yr_cost\u0026quot; , \u0026quot;ebs_snapshots_over_1yr_cost\u0026quot; FROM (snapshot_usage_all_time_before_map LEFT JOIN map ON (map.account_id = linked_account_id)) ) Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - for an example with a cost allocation tags Example uses the tag resource_tags_user_project\nCREATE OR REPLACE VIEW kpi_ebs_snap AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Filter CUR to return all ebs ec2 snapshot usage data snapshot_usage_all_time AS ( SELECT year , month , bill_billing_period_start_date billing_period , line_item_usage_start_date usage_start_date , bill_payer_account_id payer_account_id , line_item_usage_account_id linked_account_id , resource_tags_user_project , line_item_resource_id resource_id , (CASE WHEN (line_item_usage_type LIKE '%EBS:SnapshotArchive%') THEN 'Snapshot_Archive' WHEN (line_item_usage_type LIKE '%EBS:Snapshot%') THEN 'Snapshot' ELSE \u0026quot;line_item_operation\u0026quot; END) snapshot_type , line_item_usage_amount , line_item_unblended_cost , pricing_public_on_demand_cost FROM (database).(tablename) WHERE (((((bill_payer_account_id \u0026lt;\u0026gt; '') AND (line_item_resource_id \u0026lt;\u0026gt; '')) AND (line_item_line_item_type LIKE '%Usage%')) AND (line_item_product_code = 'AmazonEC2')) AND (line_item_usage_type LIKE '%EBS:Snapshot%')) ),\t-- Step 3: Return most recent billing_period and the first billing_period request_dates AS ( SELECT DISTINCT resource_id request_dates_resource_id , \u0026quot;min\u0026quot;(usage_start_date) start_date FROM snapshot_usage_all_time WHERE (snapshot_type = 'Snapshot') GROUP BY 1 ), snapshot_usage_all_time_before_map AS ( -- Step 4: Pivot table so looking at previous month filtered for only snapshots still available in the current month SELECT DISTINCT billing_period , request_dates.start_date , payer_account_id , linked_account_id , resource_tags_user_project , snapshot_type , resource_id , \u0026quot;sum\u0026quot;(line_item_usage_amount) usage_quantity , \u0026quot;sum\u0026quot;(line_item_unblended_cost) ebs_snapshot_cost , \u0026quot;sum\u0026quot;(pricing_public_on_demand_cost) public_cost , \u0026quot;sum\u0026quot;((CASE WHEN ((request_dates.start_date \u0026gt; (billing_period - INTERVAL '12' MONTH)) AND (snapshot_type = 'Snapshot')) THEN line_item_unblended_cost ELSE 0 END)) \u0026quot;ebs_snapshots_under_1yr_cost\u0026quot; /*No savings estimate since it uses uses 100% of snapshot cost for snapshots over 6mos as savings estimate*/ , \u0026quot;sum\u0026quot;((CASE WHEN ((request_dates.start_date \u0026lt;= (billing_period - INTERVAL '12' MONTH)) AND (snapshot_type = 'Snapshot')) THEN line_item_unblended_cost ELSE 0 END)) \u0026quot;ebs_snapshots_over_1yr_cost\u0026quot; FROM (snapshot_usage_all_time snapshot LEFT JOIN request_dates ON (request_dates.request_dates_resource_id = snapshot.resource_id)) WHERE (CAST(\u0026quot;concat\u0026quot;(snapshot.year, '-', snapshot.month, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) GROUP BY 1, 2, 3, 4, 5, 6,7 ) ( -- Step 5: Add in map data SELECT billing_period , start_date , payer_account_id , linked_account_id , resource_tags_user_project , map.* , resource_id , snapshot_type , usage_quantity , ebs_snapshot_cost , public_cost , \u0026quot;ebs_snapshots_under_1yr_cost\u0026quot; , \u0026quot;ebs_snapshots_over_1yr_cost\u0026quot; FROM (snapshot_usage_all_time_before_map LEFT JOIN map ON (map.account_id = linked_account_id)) ) Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from kpi_ebs_snap limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/kpi_ebs_storage_all_view/","title":"","tags":[],"description":"","content":"KPI EBS Storage All View This view will be used to create the KPI EBS Storage All view that is used to analyze EBS storage metrics and potential savings opportunities. There is only one version of this view and it is not dependent on if you have or do not have Reserved Instances or Savings Plans.\nCreate View Click here to expand the view Modify the following SQL query for the KPI EBS Storage All view:\nUpdate line 21, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_ebs_storage_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Filter CUR to return all EC2 EBS storage usage data ebs_all AS ( SELECT bill_billing_period_start_date , line_item_usage_start_date , bill_payer_account_id , line_item_usage_account_id , line_item_resource_id , product_volume_api_name , line_item_usage_type , pricing_unit , line_item_unblended_cost , line_item_usage_amount FROM (database).(tablename) WHERE (line_item_product_code = 'AmazonEC2') AND (line_item_line_item_type = 'Usage') AND bill_payer_account_id \u0026lt;\u0026gt; '' AND line_item_usage_account_id \u0026lt;\u0026gt; ''\tAND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) AND product_volume_api_name \u0026lt;\u0026gt; '' AND line_item_usage_type NOT LIKE '%Snap%' AND line_item_usage_type LIKE '%EBS%' ), -- Step 3: Pivot table so storage types cost and usage into separate columns ebs_spend AS ( SELECT DISTINCT bill_billing_period_start_date AS billing_period , date_trunc('month',line_item_usage_start_date) AS usage_date , bill_payer_account_id AS payer_account_id , line_item_usage_account_id AS linked_account_id , line_item_resource_id AS resource_id , product_volume_api_name AS volume_api_name , SUM (CASE WHEN (((pricing_unit = 'GB-Mo' or pricing_unit = 'GB-month') or pricing_unit = 'GB-month') AND line_item_usage_type LIKE '%EBS:VolumeUsage%') THEN line_item_usage_amount ELSE 0 END) \u0026quot;usage_storage_gb_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'IOPS-Mo' AND line_item_usage_type LIKE '%IOPS%') THEN line_item_usage_amount ELSE 0 END) \u0026quot;usage_iops_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'GiBps-mo' AND line_item_usage_type LIKE '%Throughput%') THEN line_item_usage_amount ELSE 0 END) \u0026quot;usage_throughput_gibps_mo\u0026quot; , SUM (CASE WHEN ((pricing_unit = 'GB-Mo' or pricing_unit = 'GB-month') AND line_item_usage_type LIKE '%EBS:VolumeUsage%') THEN (line_item_unblended_cost) ELSE 0 END) \u0026quot;cost_storage_gb_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'IOPS-Mo' AND line_item_usage_type LIKE '%IOPS%') THEN (line_item_unblended_cost) ELSE 0 END) \u0026quot;cost_iops_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'GiBps-mo' AND line_item_usage_type LIKE '%Throughput%') THEN (line_item_unblended_cost) ELSE 0 END) \u0026quot;cost_throughput_gibps_mo\u0026quot; FROM ebs_all GROUP BY 1, 2, 3, 4, 5,6 ), ebs_spend_with_unit_cost AS ( SELECT * , cost_storage_gb_mo/usage_storage_gb_mo AS \u0026quot;current_unit_cost\u0026quot; , CASE WHEN usage_storage_gb_mo \u0026lt;= 150 THEN 'under 150GB-Mo' WHEN usage_storage_gb_mo \u0026gt; 150 AND usage_storage_gb_mo \u0026lt;= 1000 THEN 'between 150-1000GB-Mo' ELSE 'over 1000GB-Mo' END AS storage_summary , CASE WHEN volume_api_name \u0026lt;\u0026gt; 'gp2' THEN 0 WHEN usage_storage_gb_mo*3 \u0026lt; 3000 THEN 3000 - 3000 WHEN usage_storage_gb_mo*3 \u0026gt; 16000 THEN 16000 - 3000 ELSE usage_storage_gb_mo*3 - 3000 END AS gp2_usage_added_iops_mo , CASE WHEN volume_api_name \u0026lt;\u0026gt; 'gp2' THEN 0 WHEN usage_storage_gb_mo \u0026lt;= 150 THEN 0 ELSE 125 END AS gp2_usage_added_throughput_gibps_mo , cost_storage_gb_mo + cost_iops_mo + cost_throughput_gibps_mo AS ebs_all_cost , CASE WHEN volume_api_name = 'sc1' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_sc1_cost\u0026quot;\t, CASE WHEN volume_api_name = 'st1' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_st1_cost\u0026quot;\t, CASE WHEN volume_api_name = 'standard' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_standard_cost\u0026quot;\t, CASE WHEN volume_api_name = 'io1' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_io1_cost\u0026quot; , CASE WHEN volume_api_name = 'io2' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_io2_cost\u0026quot;\t, CASE WHEN volume_api_name = 'gp2' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_gp2_cost\u0026quot; , CASE WHEN volume_api_name = 'gp3' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_gp3_cost\u0026quot; , CASE WHEN volume_api_name = 'gp2' THEN cost_storage_gb_mo*0.8/usage_storage_gb_mo ELSE 0 END AS \u0026quot;estimated_gp3_unit_cost\u0026quot; FROM ebs_spend ), ebs_before_map AS\t( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_id , volume_api_name , storage_summary , sum(usage_storage_gb_mo) AS usage_storage_gb_mo , sum(usage_iops_mo) AS usage_iops_mo , sum(usage_throughput_gibps_mo) AS usage_throughput_gibps_mo , sum(gp2_usage_added_iops_mo) gp2_usage_added_iops_mo , sum(gp2_usage_added_throughput_gibps_mo) AS gp2_usage_added_throughput_gibps_mo , sum(ebs_all_cost) AS ebs_all_cost , sum(ebs_sc1_cost) AS ebs_sc1_cost , sum(ebs_st1_cost) AS ebs_st1_cost , sum(ebs_standard_cost) AS ebs_standard_cost , sum(ebs_io1_cost) AS ebs_io1_cost , sum(ebs_io2_cost) AS ebs_io2_cost , sum(ebs_gp2_cost) AS ebs_gp2_cost , sum(ebs_gp3_cost) AS ebs_gp3_cost /* Calculate cost for gp2 gp3 estimate using the following - Storage always 20% cheaper - Additional iops per iops-mo is 6% of the cost of 1 gp3 GB-mo - Additional throughput per gibps-mo is 50% of the cost of 1 gp3 GB-mo */ , sum(CASE /*ignore non gp2' */ WHEN volume_api_name = 'gp2' THEN ebs_gp2_cost - (cost_storage_gb_mo*0.8 + estimated_gp3_unit_cost * 0.5 * gp2_usage_added_throughput_gibps_mo + estimated_gp3_unit_cost * 0.06 * gp2_usage_added_iops_mo) ELSE 0 END) AS ebs_gp3_potential_savings FROM ebs_spend_with_unit_cost GROUP BY 1, 2, 3, 4, 5, 6) SELECT DISTINCT billing_period , payer_account_id , linked_account_id , map.* , resource_id , volume_api_name , usage_storage_gb_mo , usage_iops_mo , usage_throughput_gibps_mo , storage_summary , gp2_usage_added_iops_mo , gp2_usage_added_throughput_gibps_mo , ebs_all_cost , ebs_sc1_cost , ebs_st1_cost , ebs_standard_cost , ebs_io1_cost , ebs_io2_cost , ebs_gp2_cost , ebs_gp3_cost , ebs_gp3_potential_savings FROM ebs_before_map LEFT JOIN map ON map.account_id = linked_account_id Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - for an example with a cost allocation tags Example uses the tag resource_tags_user_project\nCREATE OR REPLACE VIEW kpi_ebs_storage_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Filter CUR to return all EC2 EBS storage usage data ebs_all AS ( SELECT bill_billing_period_start_date , line_item_usage_start_date , bill_payer_account_id , line_item_usage_account_id , resource_tags_user_project , line_item_resource_id , product_volume_api_name , line_item_usage_type , pricing_unit , line_item_unblended_cost , line_item_usage_amount FROM (database).(tablename) WHERE (line_item_product_code = 'AmazonEC2') AND (line_item_line_item_type = 'Usage') AND bill_payer_account_id \u0026lt;\u0026gt; '' AND line_item_usage_account_id \u0026lt;\u0026gt; ''\tAND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) AND product_volume_api_name \u0026lt;\u0026gt; '' AND line_item_usage_type NOT LIKE '%Snap%' AND line_item_usage_type LIKE '%EBS%' ), -- Step 3: Pivot table so storage types cost and usage into separate columns ebs_spend AS ( SELECT DISTINCT bill_billing_period_start_date AS billing_period , date_trunc('month',line_item_usage_start_date) AS usage_date , bill_payer_account_id AS payer_account_id , line_item_usage_account_id AS linked_account_id , resource_tags_user_project , line_item_resource_id AS resource_id , product_volume_api_name AS volume_api_name , SUM (CASE WHEN (((pricing_unit = 'GB-Mo' or pricing_unit = 'GB-month') or pricing_unit = 'GB-month') AND line_item_usage_type LIKE '%EBS:VolumeUsage%') THEN line_item_usage_amount ELSE 0 END) \u0026quot;usage_storage_gb_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'IOPS-Mo' AND line_item_usage_type LIKE '%IOPS%') THEN line_item_usage_amount ELSE 0 END) \u0026quot;usage_iops_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'GiBps-mo' AND line_item_usage_type LIKE '%Throughput%') THEN line_item_usage_amount ELSE 0 END) \u0026quot;usage_throughput_gibps_mo\u0026quot; , SUM (CASE WHEN ((pricing_unit = 'GB-Mo' or pricing_unit = 'GB-month') AND line_item_usage_type LIKE '%EBS:VolumeUsage%') THEN (line_item_unblended_cost) ELSE 0 END) \u0026quot;cost_storage_gb_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'IOPS-Mo' AND line_item_usage_type LIKE '%IOPS%') THEN (line_item_unblended_cost) ELSE 0 END) \u0026quot;cost_iops_mo\u0026quot; , SUM (CASE WHEN (pricing_unit = 'GiBps-mo' AND line_item_usage_type LIKE '%Throughput%') THEN (line_item_unblended_cost) ELSE 0 END) \u0026quot;cost_throughput_gibps_mo\u0026quot; FROM ebs_all GROUP BY 1, 2, 3, 4, 5,6,7 ), ebs_spend_with_unit_cost AS ( SELECT * , cost_storage_gb_mo/usage_storage_gb_mo AS \u0026quot;current_unit_cost\u0026quot; , CASE WHEN usage_storage_gb_mo \u0026lt;= 150 THEN 'under 150GB-Mo' WHEN usage_storage_gb_mo \u0026gt; 150 AND usage_storage_gb_mo \u0026lt;= 1000 THEN 'between 150-1000GB-Mo' ELSE 'over 1000GB-Mo' END AS storage_summary , CASE WHEN volume_api_name \u0026lt;\u0026gt; 'gp2' THEN 0 WHEN usage_storage_gb_mo*3 \u0026lt; 3000 THEN 3000 - 3000 WHEN usage_storage_gb_mo*3 \u0026gt; 16000 THEN 16000 - 3000 ELSE usage_storage_gb_mo*3 - 3000 END AS gp2_usage_added_iops_mo , CASE WHEN volume_api_name \u0026lt;\u0026gt; 'gp2' THEN 0 WHEN usage_storage_gb_mo \u0026lt;= 150 THEN 0 ELSE 125 END AS gp2_usage_added_throughput_gibps_mo , cost_storage_gb_mo + cost_iops_mo + cost_throughput_gibps_mo AS ebs_all_cost , CASE WHEN volume_api_name = 'sc1' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_sc1_cost\u0026quot;\t, CASE WHEN volume_api_name = 'st1' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_st1_cost\u0026quot;\t, CASE WHEN volume_api_name = 'standard' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_standard_cost\u0026quot;\t, CASE WHEN volume_api_name = 'io1' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_io1_cost\u0026quot; , CASE WHEN volume_api_name = 'io2' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_io2_cost\u0026quot;\t, CASE WHEN volume_api_name = 'gp2' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_gp2_cost\u0026quot; , CASE WHEN volume_api_name = 'gp3' THEN (cost_iops_mo + cost_throughput_gibps_mo + cost_storage_gb_mo) ELSE 0 END \u0026quot;ebs_gp3_cost\u0026quot; , CASE WHEN volume_api_name = 'gp2' THEN cost_storage_gb_mo*0.8/usage_storage_gb_mo ELSE 0 END AS \u0026quot;estimated_gp3_unit_cost\u0026quot; FROM ebs_spend ), ebs_before_map AS\t( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , resource_id , volume_api_name , storage_summary , sum(usage_storage_gb_mo) AS usage_storage_gb_mo , sum(usage_iops_mo) AS usage_iops_mo , sum(usage_throughput_gibps_mo) AS usage_throughput_gibps_mo , sum(gp2_usage_added_iops_mo) gp2_usage_added_iops_mo , sum(gp2_usage_added_throughput_gibps_mo) AS gp2_usage_added_throughput_gibps_mo , sum(ebs_all_cost) AS ebs_all_cost , sum(ebs_sc1_cost) AS ebs_sc1_cost , sum(ebs_st1_cost) AS ebs_st1_cost , sum(ebs_standard_cost) AS ebs_standard_cost , sum(ebs_io1_cost) AS ebs_io1_cost , sum(ebs_io2_cost) AS ebs_io2_cost , sum(ebs_gp2_cost) AS ebs_gp2_cost , sum(ebs_gp3_cost) AS ebs_gp3_cost /* Calculate cost for gp2 gp3 estimate using the following - Storage always 20% cheaper - Additional iops per iops-mo is 6% of the cost of 1 gp3 GB-mo - Additional throughput per gibps-mo is 50% of the cost of 1 gp3 GB-mo */ , sum(CASE /*ignore non gp2' */ WHEN volume_api_name = 'gp2' THEN ebs_gp2_cost - (cost_storage_gb_mo*0.8 + estimated_gp3_unit_cost * 0.5 * gp2_usage_added_throughput_gibps_mo + estimated_gp3_unit_cost * 0.06 * gp2_usage_added_iops_mo) ELSE 0 END) AS ebs_gp3_potential_savings FROM ebs_spend_with_unit_cost GROUP BY 1, 2, 3, 4, 5, 6,7) SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , map.* , resource_id , volume_api_name , usage_storage_gb_mo , usage_iops_mo , usage_throughput_gibps_mo , storage_summary , gp2_usage_added_iops_mo , gp2_usage_added_throughput_gibps_mo , ebs_all_cost , ebs_sc1_cost , ebs_st1_cost , ebs_standard_cost , ebs_io1_cost , ebs_io2_cost , ebs_gp2_cost , ebs_gp3_cost , ebs_gp3_potential_savings FROM ebs_before_map LEFT JOIN map ON map.account_id = linked_account_id Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from kpi_ebs_storage_all limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/kpi_instance_all_view/","title":"","tags":[],"description":"","content":"KPI Instance All View This view will be used to create the KPI Instance All view that is used to analyze all EC2, RDS, ElastiCache, OpenSearch, Sagemaker, DynamoDB, Redshift, Lambda, and Fargate Metrics. metrics and potential savings opportunities. There are fours versions of this view and it is dependent on if you have or do not have Reserved Instances or Savings Plans. Use one of the following queries depending on whether you have Reserved Instances or Savings Plans.\nCreate View This view is dependent on having or historically having an RDS database instance and an ElastiCache cache instance run in your organization. If you get the error that the column \u0026lsquo;product_database_engine\u0026rsquo; or product_deployment_option does not exist, then you do not have any RDS database instances running. To make this column show up in the CUR spin up a database in the RDS service, let it run for a couple of minutes and in the next integration of the crawler the column will appear. If you get the error that the column \u0026lsquo;product_cache_engine\u0026rsquo; does not exist, then you do not have any ElastiCach cache instances running. To make this column show up in the CUR spin up an ElastiCache cache instance in the ElastiCache service, let it run for a couple of minutes and in the next integration of the crawler the column will appear. You can verify this by running the Athena query: SHOW COLUMNS FROM tablename - and replace the tablename accordingly after selecting the correct CUR database in the dropdown on the left side in the Athena view.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View1:\nUpdate line 61 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_instance_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Add instance mapping data\tinstance_map AS (SELECT * FROM kpi_instance_mapping), -- Step 3: Filter CUR to return all usage data\tcur_all AS (SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('month', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonSageMaker','MachineLearningSavingsPlans')) THEN 'Machine Learning' WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonEC2','AmazonECS','AmazonEKS','AWSLambda','ComputeSavingsPlans')) THEN 'Compute' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache')) THEN 'ElastiCache' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES')) THEN\t'OpenSearch' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS')) THEN 'RDS' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift')) THEN 'Redshift' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (line_item_operation = 'CommittedThroughput')) THEN 'DynamoDB' ELSE 'Other' END \u0026quot;commit_service_group\u0026quot;\t, savings_plan_offering_type \u0026quot;savings_plan_offering_type\u0026quot;\t, product_region \u0026quot;region\u0026quot; , line_item_operation \u0026quot;operation\u0026quot; , line_item_usage_type \u0026quot;usage_type\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonRDS','AmazonElastiCache')) THEN \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 2)) ELSE \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 1)) END \u0026quot;instance_type_family\u0026quot; , \u0026quot;product_instance_type\u0026quot; \u0026quot;instance_type\u0026quot; , \u0026quot;product_operating_system\u0026quot; \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , (CASE WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%Graviton%')) THEN 'Graviton' WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%AMD%')) THEN 'AMD' WHEN line_item_product_code IN ('AmazonES','AmazonElastiCache') AND (product_instance_type LIKE '%6g%' OR product_instance_type LIKE '%7g%' OR product_instance_type LIKE '%4g%') THEN 'Graviton' WHEN line_item_product_code IN ('AWSLambda') AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton'\tWHEN line_item_usage_type LIKE '%Fargate%' AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton' ELSE 'Other' END) \u0026quot;adjusted_processor\u0026quot; , product_database_engine \u0026quot;database_engine\u0026quot; , product_deployment_option \u0026quot;deployment_option\u0026quot; , product_license_model \u0026quot;license_model\u0026quot; , product_cache_engine \u0026quot;cache_engine\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN (\u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN ((\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;)) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN (\u0026quot;reservation_effective_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN ((\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%' AND \u0026quot;pricing_public_on_demand_cost\u0026quot; \u0026gt; 0) THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN (\u0026quot;pricing_public_on_demand_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN (\u0026quot;pricing_public_on_demand_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;adjusted_amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; from (database).(tablename) WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH) AND (\u0026quot;bill_payer_account_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;line_item_resource_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')) AND ( ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; ''))\tOR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (\u0026quot;line_item_operation\u0026quot; in ('CommittedThroughput','PayPerRequestThroughput')) AND ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%ReadCapacityUnit-Hrs%') or (\u0026quot;line_item_usage_type\u0026quot; LIKE '%WriteCapacityUnit-Hrs%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Repl%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonSageMaker') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR (\u0026quot;line_item_product_code\u0026quot; = 'ComputeSavingsPlans') OR (\u0026quot;line_item_product_code\u0026quot; = 'MachineLearningSavingsPlans')\t)) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17,18,19,20,21,22,23,24,25 ) SELECT cur_all.* , CASE WHEN (product_code = 'AmazonEC2' AND lower(platform) NOT LIKE '%window%') THEN latest_graviton WHEN (product_code = 'AmazonRDS' AND database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) THEN latest_graviton WHEN (product_code = 'AmazonES') THEN latest_graviton WHEN (product_code = 'AmazonElastiCache') THEN latest_graviton END \u0026quot;latest_graviton\u0026quot; ,\tlatest_amd , latest_intel , generation , instance_processor /*map*/\t, map.* /*SageMaker*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;sagemaker_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;sagemaker_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;sagemaker_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Compute SavingsPlan*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Compute') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;compute_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;compute_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;compute_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*EC2*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonEC2') THEN adjusted_amortized_cost ELSE 0 END ec2_all_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%')) THEN amortized_cost ELSE 0 END ec2_usage_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;ec2_spot_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_previous_generation_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'Graviton') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_cost\u0026quot; , CASE WHEN adjusted_processor = 'Graviton' THEN 0 WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'AMD') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'AMD') AND (latest_amd \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (instance_processor = 'AMD')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (adjusted_amortized_cost * 5.5E-1) ELSE 0 END \u0026quot;ec2_spot_potential_savings\u0026quot; /*Uses 55% savings estimate*/ , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN (adjusted_amortized_cost -amortized_cost) ELSE 0 END \u0026quot;ec2_spot_savings\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;ec2_previous_generation_potential_savings\u0026quot; /*Uses 5% savings estimate*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 2E-1) WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor = 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate for intel and 10% for AMD*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_amd \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_amd_potential_savings\u0026quot; /*Uses 10% savings estimate for intel and 0% for Graviton*/ /*RDS*/\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (adjusted_processor = 'Graviton')) THEN amortized_cost WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; NOT LIKE '%Usage%') THEN 0 WHEN (\u0026quot;product_code\u0026quot; \u0026lt;\u0026gt; 'AmazonRDS') THEN 0 WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN (latest_graviton = '') THEN 0 WHEN ((latest_graviton \u0026lt;\u0026gt; '') AND purchase_option = 'OnDemand' AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL'))) THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;rds_graviton_potential_savings\u0026quot; /*Uses 10% savings estimate*/\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;rds_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;rds_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*ElastiCache*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;elasticache_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;elasticache_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (instance_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_cost\u0026quot; , CASE WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;elasticache_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*opensearch*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonES') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonES')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;opensearch_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;opensearch_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;opensearch_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*Redshift*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;redshift_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;redshift_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;redshift_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*DynamoDB*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_all_cost\u0026quot;\t, CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_committed_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB')) THEN amortized_cost ELSE 0 END \u0026quot;dynamodb_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;dynamodb_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;dynamodb_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Lambda*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AWSLambda') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;lambda_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_usage_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND(\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton')) THEN amortized_cost*.2 ELSE 0 END \u0026quot;lambda_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate*/ FROM cur_all cur_all LEFT JOIN instance_map ON (instance_map.product = product_code AND instance_map.family = instance_type_family) LEFT JOIN map ON map.account_id= linked_account_id Click here - if you have Savings Plans, but do not have Reserved Instances Modify the following SQL query for View1:\nUpdate line 66 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_instance_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Add instance mapping data\tinstance_map AS (SELECT * FROM kpi_instance_mapping), -- Step 3: Filter CUR to return all usage data\tcur_all AS (SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('month', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonSageMaker','MachineLearningSavingsPlans')) THEN 'Machine Learning' WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonEC2','AmazonECS','AmazonEKS','AWSLambda','ComputeSavingsPlans')) THEN 'Compute' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache')) THEN 'ElastiCache' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES')) THEN\t'OpenSearch' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS')) THEN 'RDS' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift')) THEN 'Redshift' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (line_item_operation = 'CommittedThroughput')) THEN 'DynamoDB' ELSE 'Other' END \u0026quot;commit_service_group\u0026quot;\t, savings_plan_offering_type \u0026quot;savings_plan_offering_type\u0026quot;\t, product_region \u0026quot;region\u0026quot; , line_item_operation \u0026quot;operation\u0026quot; , line_item_usage_type \u0026quot;usage_type\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonRDS','AmazonElastiCache')) THEN \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 2)) ELSE \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 1)) END \u0026quot;instance_type_family\u0026quot; , \u0026quot;product_instance_type\u0026quot; \u0026quot;instance_type\u0026quot; , \u0026quot;product_operating_system\u0026quot; \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , (CASE WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%Graviton%')) THEN 'Graviton' WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%AMD%')) THEN 'AMD' WHEN line_item_product_code IN ('AmazonES','AmazonElastiCache') AND (product_instance_type LIKE '%6g%' OR product_instance_type LIKE '%7g%' OR product_instance_type LIKE '%4g%') THEN 'Graviton' WHEN line_item_product_code IN ('AWSLambda') AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton'\tWHEN line_item_usage_type LIKE '%Fargate%' AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton' ELSE 'Other' END) \u0026quot;adjusted_processor\u0026quot; , product_database_engine \u0026quot;database_engine\u0026quot; , product_deployment_option \u0026quot;deployment_option\u0026quot; , product_license_model \u0026quot;license_model\u0026quot; , product_cache_engine \u0026quot;cache_engine\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN (\u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN ((\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;)) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%' AND \u0026quot;pricing_public_on_demand_cost\u0026quot; \u0026gt; 0) THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN (\u0026quot;pricing_public_on_demand_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;adjusted_amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; from (database).(tablename) WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH) AND (\u0026quot;bill_payer_account_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;line_item_resource_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')) AND ( ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; ''))\tOR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (\u0026quot;line_item_operation\u0026quot; in ('CommittedThroughput','PayPerRequestThroughput')) AND ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%ReadCapacityUnit-Hrs%') or (\u0026quot;line_item_usage_type\u0026quot; LIKE '%WriteCapacityUnit-Hrs%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Repl%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonSageMaker') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR (\u0026quot;line_item_product_code\u0026quot; = 'ComputeSavingsPlans') OR (\u0026quot;line_item_product_code\u0026quot; = 'MachineLearningSavingsPlans')\t)) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17,18,19,20,21,22,23,24,25 ) SELECT cur_all.* , CASE WHEN (product_code = 'AmazonEC2' AND lower(platform) NOT LIKE '%window%') THEN latest_graviton WHEN (product_code = 'AmazonRDS' AND database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) THEN latest_graviton WHEN (product_code = 'AmazonES') THEN latest_graviton WHEN (product_code = 'AmazonElastiCache') THEN latest_graviton END \u0026quot;latest_graviton\u0026quot; ,\tlatest_amd , latest_intel , generation , instance_processor /*map*/\t, map.* /*SageMaker*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;sagemaker_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;sagemaker_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;sagemaker_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Compute SavingsPlan*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Compute') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;compute_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;compute_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;compute_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*EC2*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonEC2') THEN adjusted_amortized_cost ELSE 0 END ec2_all_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%')) THEN amortized_cost ELSE 0 END ec2_usage_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;ec2_spot_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_previous_generation_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'Graviton') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_cost\u0026quot; , CASE WHEN adjusted_processor = 'Graviton' THEN 0 WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'AMD') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'AMD') AND (latest_amd \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (instance_processor = 'AMD')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (adjusted_amortized_cost * 5.5E-1) ELSE 0 END \u0026quot;ec2_spot_potential_savings\u0026quot; /*Uses 55% savings estimate*/ , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN (adjusted_amortized_cost -amortized_cost) ELSE 0 END \u0026quot;ec2_spot_savings\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;ec2_previous_generation_potential_savings\u0026quot; /*Uses 5% savings estimate*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 2E-1) WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor = 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate for intel and 10% for AMD*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_amd \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_amd_potential_savings\u0026quot; /*Uses 10% savings estimate for intel and 0% for Graviton*/ /*RDS*/\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (adjusted_processor = 'Graviton')) THEN amortized_cost WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; NOT LIKE '%Usage%') THEN 0 WHEN (\u0026quot;product_code\u0026quot; \u0026lt;\u0026gt; 'AmazonRDS') THEN 0 WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN (latest_graviton = '') THEN 0 WHEN ((latest_graviton \u0026lt;\u0026gt; '') AND purchase_option = 'OnDemand' AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL'))) THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;rds_graviton_potential_savings\u0026quot; /*Uses 10% savings estimate*/\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;rds_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;rds_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*ElastiCache*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;elasticache_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;elasticache_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (instance_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_cost\u0026quot; , CASE WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;elasticache_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*opensearch*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonES') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonES')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;opensearch_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;opensearch_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;opensearch_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*Redshift*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;redshift_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;redshift_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;redshift_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*DynamoDB*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_all_cost\u0026quot;\t, CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_committed_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB')) THEN amortized_cost ELSE 0 END \u0026quot;dynamodb_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;dynamodb_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;dynamodb_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Lambda*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AWSLambda') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;lambda_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_usage_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND(\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton')) THEN amortized_cost*.2 ELSE 0 END \u0026quot;lambda_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate*/ FROM cur_all cur_all LEFT JOIN instance_map ON (instance_map.product = product_code AND instance_map.family = instance_type_family) LEFT JOIN map ON map.account_id= linked_account_id Click here - if you have Reserved Instances, but do not have Savings Plans Modify the following SQL query for View1:\nUpdate line 61 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_instance_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Add instance mapping data\tinstance_map AS (SELECT * FROM kpi_instance_mapping), -- Step 3: Filter CUR to return all usage data\tcur_all AS (SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('month', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , (CASE WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonSageMaker','MachineLearningSavingsPlans')) THEN 'Machine Learning' WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonEC2','AmazonECS','AmazonEKS','AWSLambda','ComputeSavingsPlans')) THEN 'Compute' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache')) THEN 'ElastiCache' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES')) THEN\t'OpenSearch' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS')) THEN 'RDS' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift')) THEN 'Redshift' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (line_item_operation = 'CommittedThroughput')) THEN 'DynamoDB' ELSE 'Other' END \u0026quot;commit_service_group\u0026quot;\t, ' ' \u0026quot;savings_plan_offering_type\u0026quot;\t, product_region \u0026quot;region\u0026quot; , line_item_operation \u0026quot;operation\u0026quot; , line_item_usage_type \u0026quot;usage_type\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonRDS','AmazonElastiCache')) THEN \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 2)) ELSE \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 1)) END \u0026quot;instance_type_family\u0026quot; , \u0026quot;product_instance_type\u0026quot; \u0026quot;instance_type\u0026quot; , \u0026quot;product_operating_system\u0026quot; \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , (CASE WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%Graviton%')) THEN 'Graviton' WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%AMD%')) THEN 'AMD' WHEN line_item_product_code IN ('AmazonES','AmazonElastiCache') AND (product_instance_type LIKE '%6g%' OR product_instance_type LIKE '%7g%' OR product_instance_type LIKE '%4g%') THEN 'Graviton' WHEN line_item_product_code IN ('AWSLambda') AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton'\tWHEN line_item_usage_type LIKE '%Fargate%' AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton' ELSE 'Other' END) \u0026quot;adjusted_processor\u0026quot; , product_database_engine \u0026quot;database_engine\u0026quot; , product_deployment_option \u0026quot;deployment_option\u0026quot; , product_license_model \u0026quot;license_model\u0026quot; , product_cache_engine \u0026quot;cache_engine\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN (\u0026quot;reservation_effective_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN ((\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%' AND \u0026quot;pricing_public_on_demand_cost\u0026quot; \u0026gt; 0) THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN (\u0026quot;pricing_public_on_demand_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;adjusted_amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; from (database).(tablename) WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH) AND (\u0026quot;bill_payer_account_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;line_item_resource_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')) AND ( ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; ''))\tOR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (\u0026quot;line_item_operation\u0026quot; in ('CommittedThroughput','PayPerRequestThroughput')) AND ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%ReadCapacityUnit-Hrs%') or (\u0026quot;line_item_usage_type\u0026quot; LIKE '%WriteCapacityUnit-Hrs%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Repl%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonSageMaker') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR (\u0026quot;line_item_product_code\u0026quot; = 'ComputeSavingsPlans') OR (\u0026quot;line_item_product_code\u0026quot; = 'MachineLearningSavingsPlans')\t)) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17,18,19,20,21,22,23,24,25 ) SELECT cur_all.* , CASE WHEN (product_code = 'AmazonEC2' AND lower(platform) NOT LIKE '%window%') THEN latest_graviton WHEN (product_code = 'AmazonRDS' AND database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) THEN latest_graviton WHEN (product_code = 'AmazonES') THEN latest_graviton WHEN (product_code = 'AmazonElastiCache') THEN latest_graviton END \u0026quot;latest_graviton\u0026quot; ,\tlatest_amd , latest_intel , generation , instance_processor /*map*/\t, map.* /*SageMaker*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;sagemaker_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;sagemaker_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;sagemaker_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Compute SavingsPlan*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Compute') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;compute_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;compute_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;compute_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*EC2*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonEC2') THEN adjusted_amortized_cost ELSE 0 END ec2_all_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%')) THEN amortized_cost ELSE 0 END ec2_usage_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;ec2_spot_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_previous_generation_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'Graviton') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_cost\u0026quot; , CASE WHEN adjusted_processor = 'Graviton' THEN 0 WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'AMD') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'AMD') AND (latest_amd \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (instance_processor = 'AMD')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (adjusted_amortized_cost * 5.5E-1) ELSE 0 END \u0026quot;ec2_spot_potential_savings\u0026quot; /*Uses 55% savings estimate*/ , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN (adjusted_amortized_cost -amortized_cost) ELSE 0 END \u0026quot;ec2_spot_savings\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;ec2_previous_generation_potential_savings\u0026quot; /*Uses 5% savings estimate*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 2E-1) WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor = 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate for intel and 10% for AMD*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_amd \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_amd_potential_savings\u0026quot; /*Uses 10% savings estimate for intel and 0% for Graviton*/ /*RDS*/\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (adjusted_processor = 'Graviton')) THEN amortized_cost WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; NOT LIKE '%Usage%') THEN 0 WHEN (\u0026quot;product_code\u0026quot; \u0026lt;\u0026gt; 'AmazonRDS') THEN 0 WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN (latest_graviton = '') THEN 0 WHEN ((latest_graviton \u0026lt;\u0026gt; '') AND purchase_option = 'OnDemand' AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL'))) THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;rds_graviton_potential_savings\u0026quot; /*Uses 10% savings estimate*/\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;rds_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;rds_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*ElastiCache*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;elasticache_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;elasticache_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (instance_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_cost\u0026quot; , CASE WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;elasticache_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*opensearch*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonES') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonES')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;opensearch_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;opensearch_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;opensearch_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*Redshift*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;redshift_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;redshift_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;redshift_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*DynamoDB*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_all_cost\u0026quot;\t, CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_committed_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB')) THEN amortized_cost ELSE 0 END \u0026quot;dynamodb_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;dynamodb_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;dynamodb_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Lambda*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AWSLambda') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;lambda_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_usage_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND(\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton')) THEN amortized_cost*.2 ELSE 0 END \u0026quot;lambda_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate*/ FROM cur_all cur_all LEFT JOIN instance_map ON (instance_map.product = product_code AND instance_map.family = instance_type_family) LEFT JOIN map ON map.account_id= linked_account_id Click here - if you do not have Reserved Instances, and do not have Savings Plans Modify the following SQL query for View1:\nUpdate line 58 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_instance_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Add instance mapping data\tinstance_map AS (SELECT * FROM kpi_instance_mapping), -- Step 3: Filter CUR to return all usage data\tcur_all AS (SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('month', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , (CASE WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonSageMaker','MachineLearningSavingsPlans')) THEN 'Machine Learning' WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonEC2','AmazonECS','AmazonEKS','AWSLambda','ComputeSavingsPlans')) THEN 'Compute' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache')) THEN 'ElastiCache' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES')) THEN\t'OpenSearch' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS')) THEN 'RDS' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift')) THEN 'Redshift' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (line_item_operation = 'CommittedThroughput')) THEN 'DynamoDB' ELSE 'Other' END \u0026quot;commit_service_group\u0026quot;\t, ' ' \u0026quot;savings_plan_offering_type\u0026quot;\t, product_region \u0026quot;region\u0026quot; , line_item_operation \u0026quot;operation\u0026quot; , line_item_usage_type \u0026quot;usage_type\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonRDS','AmazonElastiCache')) THEN \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 2)) ELSE \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 1)) END \u0026quot;instance_type_family\u0026quot; , \u0026quot;product_instance_type\u0026quot; \u0026quot;instance_type\u0026quot; , \u0026quot;product_operating_system\u0026quot; \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , (CASE WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%Graviton%')) THEN 'Graviton' WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%AMD%')) THEN 'AMD' WHEN line_item_product_code IN ('AmazonES','AmazonElastiCache') AND (product_instance_type LIKE '%6g%' OR product_instance_type LIKE '%7g%' OR product_instance_type LIKE '%4g%') THEN 'Graviton' WHEN line_item_product_code IN ('AWSLambda') AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton'\tWHEN line_item_usage_type LIKE '%Fargate%' AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton' ELSE 'Other' END) \u0026quot;adjusted_processor\u0026quot; , product_database_engine \u0026quot;database_engine\u0026quot; , product_deployment_option \u0026quot;deployment_option\u0026quot; , product_license_model \u0026quot;license_model\u0026quot; , product_cache_engine \u0026quot;cache_engine\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%' AND \u0026quot;pricing_public_on_demand_cost\u0026quot; \u0026gt; 0) THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;adjusted_amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; from (database).(tablename) WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH) AND (\u0026quot;bill_payer_account_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;line_item_resource_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%')) AND ( ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; ''))\tOR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (\u0026quot;line_item_operation\u0026quot; in ('CommittedThroughput','PayPerRequestThroughput')) AND ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%ReadCapacityUnit-Hrs%') or (\u0026quot;line_item_usage_type\u0026quot; LIKE '%WriteCapacityUnit-Hrs%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Repl%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonSageMaker') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR (\u0026quot;line_item_product_code\u0026quot; = 'ComputeSavingsPlans') OR (\u0026quot;line_item_product_code\u0026quot; = 'MachineLearningSavingsPlans')\t)) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17,18,19,20,21,22,23,24,25 ) SELECT cur_all.* , CASE WHEN (product_code = 'AmazonEC2' AND lower(platform) NOT LIKE '%window%') THEN latest_graviton WHEN (product_code = 'AmazonRDS' AND database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) THEN latest_graviton WHEN (product_code = 'AmazonES') THEN latest_graviton WHEN (product_code = 'AmazonElastiCache') THEN latest_graviton END \u0026quot;latest_graviton\u0026quot; ,\tlatest_amd , latest_intel , generation , instance_processor /*map*/\t, map.* /*SageMaker*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;sagemaker_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;sagemaker_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;sagemaker_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Compute SavingsPlan*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Compute') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;compute_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;compute_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;compute_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*EC2*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonEC2') THEN adjusted_amortized_cost ELSE 0 END ec2_all_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%')) THEN amortized_cost ELSE 0 END ec2_usage_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;ec2_spot_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_previous_generation_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'Graviton') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_cost\u0026quot; , CASE WHEN adjusted_processor = 'Graviton' THEN 0 WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'AMD') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'AMD') AND (latest_amd \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (instance_processor = 'AMD')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (adjusted_amortized_cost * 5.5E-1) ELSE 0 END \u0026quot;ec2_spot_potential_savings\u0026quot; /*Uses 55% savings estimate*/ , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN (adjusted_amortized_cost -amortized_cost) ELSE 0 END \u0026quot;ec2_spot_savings\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;ec2_previous_generation_potential_savings\u0026quot; /*Uses 5% savings estimate*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 2E-1) WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor = 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate for intel and 10% for AMD*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_amd \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_amd_potential_savings\u0026quot; /*Uses 10% savings estimate for intel and 0% for Graviton*/ /*RDS*/\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (adjusted_processor = 'Graviton')) THEN amortized_cost WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; NOT LIKE '%Usage%') THEN 0 WHEN (\u0026quot;product_code\u0026quot; \u0026lt;\u0026gt; 'AmazonRDS') THEN 0 WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN (latest_graviton = '') THEN 0 WHEN ((latest_graviton \u0026lt;\u0026gt; '') AND purchase_option = 'OnDemand' AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL'))) THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;rds_graviton_potential_savings\u0026quot; /*Uses 10% savings estimate*/\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;rds_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;rds_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*ElastiCache*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;elasticache_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;elasticache_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (instance_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_cost\u0026quot; , CASE WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;elasticache_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*opensearch*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonES') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonES')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;opensearch_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;opensearch_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;opensearch_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*Redshift*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;redshift_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;redshift_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;redshift_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*DynamoDB*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_all_cost\u0026quot;\t, CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_committed_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB')) THEN amortized_cost ELSE 0 END \u0026quot;dynamodb_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;dynamodb_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;dynamodb_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Lambda*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AWSLambda') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;lambda_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_usage_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND(\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton')) THEN amortized_cost*.2 ELSE 0 END \u0026quot;lambda_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate*/ FROM cur_all cur_all LEFT JOIN instance_map ON (instance_map.product = product_code AND instance_map.family = instance_type_family) LEFT JOIN map ON map.account_id= linked_account_id Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - for an example with a cost allocation tags Example uses the tag resource_tags_user_project\nCREATE OR REPLACE VIEW kpi_instance_all AS WITH -- Step 1: Add mapping view map AS(SELECT * FROM account_map), -- Step 2: Add instance mapping data\tinstance_map AS (SELECT * FROM kpi_instance_mapping), -- Step 3: Filter CUR to return all usage data\tcur_all AS (SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('month', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , resource_tags_user_project , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonSageMaker','MachineLearningSavingsPlans')) THEN 'Machine Learning' WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonEC2','AmazonECS','AmazonEKS','AWSLambda','ComputeSavingsPlans')) THEN 'Compute' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache')) THEN 'ElastiCache' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES')) THEN\t'OpenSearch' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS')) THEN 'RDS' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift')) THEN 'Redshift' WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (line_item_operation = 'CommittedThroughput')) THEN 'DynamoDB' ELSE 'Other' END \u0026quot;commit_service_group\u0026quot;\t, savings_plan_offering_type \u0026quot;savings_plan_offering_type\u0026quot;\t, product_region \u0026quot;region\u0026quot; , line_item_operation \u0026quot;operation\u0026quot; , line_item_usage_type \u0026quot;usage_type\u0026quot; , CASE WHEN (\u0026quot;line_item_product_code\u0026quot; in ('AmazonRDS','AmazonElastiCache')) THEN \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 2)) ELSE \u0026quot;lower\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;product_instance_type\u0026quot;, '.', 1)) END \u0026quot;instance_type_family\u0026quot; , \u0026quot;product_instance_type\u0026quot; \u0026quot;instance_type\u0026quot; , \u0026quot;product_operating_system\u0026quot; \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , (CASE WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%Graviton%')) THEN 'Graviton' WHEN ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_physical_processor\u0026quot; LIKE '%AMD%')) THEN 'AMD' WHEN line_item_product_code IN ('AmazonES','AmazonElastiCache') AND (product_instance_type LIKE '%6g%' OR product_instance_type LIKE '%7g%' OR product_instance_type LIKE '%4g%') THEN 'Graviton' WHEN line_item_product_code IN ('AWSLambda') AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton'\tWHEN line_item_usage_type LIKE '%Fargate%' AND line_item_usage_type LIKE '%ARM%' THEN 'Graviton' ELSE 'Other' END) \u0026quot;adjusted_processor\u0026quot; , product_database_engine \u0026quot;database_engine\u0026quot; , product_deployment_option \u0026quot;deployment_option\u0026quot; , product_license_model \u0026quot;license_model\u0026quot; , product_cache_engine \u0026quot;cache_engine\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN (\u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN ((\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;)) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN (\u0026quot;reservation_effective_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN ((\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%' AND \u0026quot;pricing_public_on_demand_cost\u0026quot; \u0026gt; 0) THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN (\u0026quot;pricing_public_on_demand_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN (\u0026quot;pricing_public_on_demand_cost\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE (\u0026quot;line_item_unblended_cost\u0026quot; ) END)) \u0026quot;adjusted_amortized_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; from (database).(tablename) WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH) AND (\u0026quot;bill_payer_account_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;line_item_resource_id\u0026quot; \u0026lt;\u0026gt;'') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; LIKE '%Usage%') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')) AND ( ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR((\u0026quot;line_item_product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonES') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; ''))\tOR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonDynamoDB') AND (\u0026quot;line_item_operation\u0026quot; in ('CommittedThroughput','PayPerRequestThroughput')) AND ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%ReadCapacityUnit-Hrs%') or (\u0026quot;line_item_usage_type\u0026quot; LIKE '%WriteCapacityUnit-Hrs%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Repl%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') OR ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonSageMaker') AND (\u0026quot;product_instance_type\u0026quot; \u0026lt;\u0026gt; '')) OR (\u0026quot;line_item_product_code\u0026quot; = 'ComputeSavingsPlans') OR (\u0026quot;line_item_product_code\u0026quot; = 'MachineLearningSavingsPlans')\t)) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17,18,19,20,21,22,23,24,25,26 ) SELECT cur_all.* , CASE WHEN (product_code = 'AmazonEC2' AND lower(platform) NOT LIKE '%window%') THEN latest_graviton WHEN (product_code = 'AmazonRDS' AND database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) THEN latest_graviton WHEN (product_code = 'AmazonES') THEN latest_graviton WHEN (product_code = 'AmazonElastiCache') THEN latest_graviton END \u0026quot;latest_graviton\u0026quot; ,\tlatest_amd , latest_intel , generation , instance_processor /*map*/\t, map.* /*SageMaker*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;sagemaker_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;sagemaker_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;sagemaker_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Machine Learning') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;sagemaker_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Compute SavingsPlan*/ , CASE WHEN (\u0026quot;commit_service_group\u0026quot; = 'Compute') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;compute_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (purchase_option = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;compute_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'Compute')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;compute_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'Compute') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN (\u0026quot;amortized_cost\u0026quot; * 2E-1) ELSE 0 END \u0026quot;compute_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*EC2*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonEC2') THEN adjusted_amortized_cost ELSE 0 END ec2_all_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%')) THEN amortized_cost ELSE 0 END ec2_usage_cost\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;ec2_spot_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_previous_generation_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'Graviton') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_graviton_cost\u0026quot; , CASE WHEN adjusted_processor = 'Graviton' THEN 0 WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND ((adjusted_processor = 'AMD') OR (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'AMD') AND (latest_amd \u0026lt;\u0026gt; ''))) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (instance_processor = 'AMD')) THEN amortized_cost ELSE 0 END \u0026quot;ec2_amd_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (adjusted_amortized_cost * 5.5E-1) ELSE 0 END \u0026quot;ec2_spot_potential_savings\u0026quot; /*Uses 55% savings estimate*/ , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (purchase_option = 'Spot')) THEN (adjusted_amortized_cost -amortized_cost) ELSE 0 END \u0026quot;ec2_spot_savings\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (generation IN ('Previous')) AND (purchase_option \u0026lt;\u0026gt; 'Spot') AND (purchase_option \u0026lt;\u0026gt; 'Reserved') AND (savings_plan_offering_type NOT LIKE '%EC2%')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;ec2_previous_generation_potential_savings\u0026quot; /*Uses 5% savings estimate*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 2E-1) WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '') AND adjusted_processor = 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate for intel and 10% for AMD*/ , CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (\u0026quot;operation\u0026quot; LIKE '%RunInstances%') AND (((purchase_option = 'OnDemand') OR (savings_plan_offering_type = 'ComputeSavingsPlans')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_amd \u0026lt;\u0026gt; '') AND adjusted_processor \u0026lt;\u0026gt; 'AMD') THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;ec2_amd_potential_savings\u0026quot; /*Uses 10% savings estimate for intel and 0% for Graviton*/ /*RDS*/\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;rds_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (adjusted_processor = 'Graviton')) THEN amortized_cost WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL')) AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;rds_graviton_cost\u0026quot; , CASE WHEN (\u0026quot;charge_type\u0026quot; NOT LIKE '%Usage%') THEN 0 WHEN (\u0026quot;product_code\u0026quot; \u0026lt;\u0026gt; 'AmazonRDS') THEN 0 WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN (latest_graviton = '') THEN 0 WHEN ((latest_graviton \u0026lt;\u0026gt; '') AND purchase_option = 'OnDemand' AND (database_engine in ('Aurora MySQL','Aurora PostgreSQL','MariaDB','PostgreSQL'))) THEN (amortized_cost * 1E-1) ELSE 0 END \u0026quot;rds_graviton_potential_savings\u0026quot; /*Uses 10% savings estimate*/\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;rds_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRDS') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;rds_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*ElastiCache*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;elasticache_ondemand_cost\u0026quot;\t, CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;elasticache_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;elasticache_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (instance_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;elasticache_graviton_cost\u0026quot; , CASE WHEN (adjusted_processor = 'Graviton') THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonElastiCache') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;elasticache_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*opensearch*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonES') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;opensearch_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonES')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;opensearch_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;opensearch_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ , CASE WHEN ((\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;opensearch_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (adjusted_processor = 'Graviton')) THEN 0 WHEN ((\u0026quot;charge_type\u0026quot; = 'Usage') AND (\u0026quot;product_code\u0026quot; = 'AmazonES') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (latest_graviton \u0026lt;\u0026gt; '')) THEN (amortized_cost * 5E-2) ELSE 0 END \u0026quot;opensearch_graviton_potential_savings\u0026quot; /*Uses 5% savings estimate*/ /*Redshift*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '')) THEN amortized_cost ELSE 0 END \u0026quot;redshift_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN adjusted_amortized_cost ELSE 0 END \u0026quot;redshift_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;redshift_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonRedshift') AND (\u0026quot;instance_type\u0026quot; \u0026lt;\u0026gt; '') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;redshift_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*DynamoDB*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_all_cost\u0026quot;\t, CASE WHEN (\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_committed_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AmazonDynamoDB')) THEN amortized_cost ELSE 0 END \u0026quot;dynamodb_usage_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (\u0026quot;purchase_option\u0026quot; = 'OnDemand')) THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;dynamodb_ondemand_cost\u0026quot; , CASE WHEN ((\u0026quot;purchase_option\u0026quot; in ('Reserved','SavingsPlan')) AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB')) THEN (\u0026quot;adjusted_amortized_cost\u0026quot; - \u0026quot;amortized_cost\u0026quot;) ELSE 0 END \u0026quot;dynamodb_commit_savings\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;commit_service_group\u0026quot; = 'DynamoDB') AND (purchase_option = 'OnDemand')) THEN (amortized_cost * 2E-1) ELSE 0 END \u0026quot;dynamodb_commit_potential_savings\u0026quot; /*Uses 20% savings estimate*/ /*Lambda*/\t, CASE WHEN (\u0026quot;product_code\u0026quot; = 'AWSLambda') THEN \u0026quot;adjusted_amortized_cost\u0026quot; ELSE 0 END \u0026quot;lambda_all_cost\u0026quot;\t, CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_usage_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND(\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost\tWHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_eligible_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor = 'Graviton')) THEN amortized_cost ELSE 0 END \u0026quot;lambda_graviton_cost\u0026quot; , CASE WHEN ((\u0026quot;charge_type\u0026quot; LIKE '%Usage%') AND (\u0026quot;product_code\u0026quot; = 'AWSLambda') AND (adjusted_processor \u0026lt;\u0026gt; 'Graviton')) THEN amortized_cost*.2 ELSE 0 END \u0026quot;lambda_graviton_potential_savings\u0026quot; /*Uses 20% savings estimate*/ FROM cur_all cur_all LEFT JOIN instance_map ON (instance_map.product = product_code AND instance_map.family = instance_type_family) LEFT JOIN map ON map.account_id= linked_account_id Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from kpi_instance_all limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/kpi_s3_storage_all_view/","title":"","tags":[],"description":"","content":"KPI S3 Storage All View This view will be used to create the KPI S3 Storage All view that is used to analyze S3 storage metrics and potential savings opportunities. There is only one version of this view and it is not dependent on if you have or do not have Reserved Instances or Savings Plans.\nCreate View Click here to expand the view Modify the following SQL query for the KPI S3 Storage All view:\nUpdate line 42, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW kpi_s3_storage_all AS -- Step 1: Enter S3 standard savings savings assumption. Default is set to 0.3 for 30% savings WITH inputs AS ( SELECT * FROM (VALUES (0.3)) t(s3_standard_savings)), -- Step: 2 Add mapping view map AS(SELECT * FROM account_map), -- Step 3: Filter CUR to return all storage usage data s3_usage_all_time AS ( SELECT year , month , bill_billing_period_start_date AS billing_period , line_item_usage_start_date AS usage_start_date , bill_payer_account_id AS payer_account_id , line_item_usage_account_id AS linked_account_id , line_item_resource_id AS resource_id , s3_standard_savings , line_item_operation AS operation , line_item_usage_type AS usage_type , CASE WHEN line_item_usage_type LIKE '%EarlyDelete%' THEN 'EarlyDelete' ELSE line_item_operation END \u0026quot;early_delete_adjusted_operation\u0026quot; , CASE WHEN line_item_product_code = 'AmazonGlacier' AND line_item_operation = 'Storage' THEN 'Amazon Glacier' WHEN line_item_product_code = 'AmazonS3' AND product_volume_type LIKE '%Intelligent%' AND line_item_operation LIKE '%IntelligentTiering%' THEN 'Intelligent-Tiering'\tELSE product_volume_type END AS storage_class_type , pricing_unit , sum(line_item_usage_amount) AS usage_quantity , sum(line_item_unblended_cost) unblended_cost , sum(CASE WHEN (pricing_unit = 'GB-Mo' AND line_item_operation like '%Storage%' AND product_volume_type LIKE '%Glacier Deep Archive%') THEN line_item_unblended_cost WHEN (pricing_unit = 'GB-Mo' AND line_item_operation like '%Storage%') THEN line_item_unblended_cost ELSE 0 END) AS s3_all_storage_cost , sum(CASE WHEN (pricing_unit = 'GB-Mo' AND line_item_operation like '%Storage%') THEN line_item_usage_amount ELSE 0 END) AS s3_all_storage_usage_quantity FROM (database).(tablename) , inputs WHERE bill_payer_account_id \u0026lt;\u0026gt; '' AND line_item_resource_id \u0026lt;\u0026gt; '' AND line_item_line_item_type LIKE '%Usage%' AND (line_item_product_code LIKE '%AmazonGlacier%' OR line_item_product_code LIKE '%AmazonS3%') GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12,13 ), -- Step 4: Return most recent request date to understand if bucket is in active use most_recent_request AS ( SELECT DISTINCT resource_id , max(usage_start_date) AS last_request_date FROM s3_usage_all_time WHERE usage_quantity \u0026gt; 0 AND operation IN ('PutObject', 'PutObjectForRepl', 'GetObject', 'CopyObject') AND pricing_unit = 'Requests' GROUP BY 1 ), -- Step 5: Pivot table so storage classes into separate columns and filter for current month month_usage AS ( SELECT DISTINCT billing_period , date_trunc('month', usage_start_date) AS \u0026quot;usage_date\u0026quot; , payer_account_id , linked_account_id , s3.resource_id , most_recent_request.last_request_date AS \u0026quot;last_requests\u0026quot; ,s3_standard_savings , sum(unblended_cost) AS s3_all_cost -- All Storage , sum(s3_all_storage_cost) AS s3_all_storage_cost , sum(s3_all_storage_usage_quantity) AS \u0026quot;s3_all_storage_usage_quantity\u0026quot; -- S3 Standard , sum(CASE WHEN storage_class_type = 'Standard' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_standard_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Standard' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_standard_storage_usage_quantity\u0026quot; -- S3 Standard Infrequent Access , sum(CASE WHEN storage_class_type = 'Standard - Infrequent Access' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Standard - Infrequent Access' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_standard-ia_storage_usage_quantity\u0026quot; , sum(CASE WHEN usage_type LIKE '%Requests-SIA-Tier1%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_tier1_cost\u0026quot; , sum(CASE WHEN usage_type LIKE '%Requests-SIA-Tier2%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_tier2_cost\u0026quot;\t, sum(CASE WHEN usage_type LIKE '%Retrieval-SIA%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_retrieval_cost\u0026quot;\t-- S3 One Zone Infrequent Access , sum(CASE WHEN storage_class_type = 'One Zone - Infrequent Access' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_onezone-ia_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'One Zone - Infrequent Access' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_onezone-ia_storage_usage_quantity\u0026quot; -- S3 Reduced Redundancy , sum(CASE WHEN storage_class_type = 'Reduced Redundancy' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_reduced_redundancy_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Reduced Redundancy' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_reduced_redundancy_storage_usage_quantity\u0026quot; -- S3 Intelligent-Tiering , sum(CASE WHEN storage_class_type LIKE '%Intelligent%' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_intelligent-tiering_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type LIKE '%Intelligent%' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_intelligent-tiering_storage_usage_quantity\u0026quot; -- S3 Glacier Instant Retrieval , sum(CASE WHEN storage_class_type LIKE '%Instant%' AND storage_class_type NOT LIKE '%Intelligent%' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type LIKE '%Instant%' AND storage_class_type NOT LIKE '%Intelligent%' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_storage_usage_quantity\u0026quot;\t, sum(CASE WHEN usage_type LIKE '%Requests-GIR-Tier1%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_tier1_cost\u0026quot; , sum(CASE WHEN usage_type LIKE '%Requests-GIR-Tier2%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_tier2_cost\u0026quot; , sum(CASE WHEN usage_type LIKE '%Retrieval-SIA-GIR%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_retrieval_cost\u0026quot;\t-- S3 Glacier Flexible Retrieval , sum(CASE WHEN storage_class_type = 'Amazon Glacier' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_glacier_flexible_retrieval_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Amazon Glacier' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_glacier_flexible_retrieval_storage_usage_quantity\u0026quot; -- Glacier Deep Archive , sum(CASE WHEN storage_class_type = 'Glacier Deep Archive' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_glacier_deep_archive_storage_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Glacier Deep Archive' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_glacier_deep_archive_storage_usage_quantity\u0026quot;\t-- Operations , sum(CASE WHEN operation = 'PutObject' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_put_object_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'PutObjectForRepl' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_put_object_replication_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'GetObject' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_get_object_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'CopyObject' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_copy_object_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'Inventory' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_inventory_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'S3.STORAGE_CLASS_ANALYSIS.OBJECT' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_analytics_usage_quantity\u0026quot; ,sum(CASE WHEN operation like '%Transition%' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_transition_usage_quantity\u0026quot; ,sum(CASE WHEN early_delete_adjusted_operation = 'EarlyDelete' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_early_delete_cost\u0026quot;\tFROM s3_usage_all_time s3 LEFT JOIN most_recent_request ON most_recent_request.resource_id = s3.resource_id WHERE CAST(concat(s3.year, '-', s3.month, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH) GROUP BY 1, 2, 3, 4, 5, 6,7 ) -- Step 6: Add account mapping \u0026amp; apply KPI logic - Add or Adjust bucket name keywords based on your requirements SELECT DISTINCT billing_period , usage_date , payer_account_id , linked_account_id , map.* , resource_id , CASE WHEN resource_id LIKE '%backup%' THEN 'backup' WHEN resource_id LIKE '%archive%' THEN 'archive' WHEN resource_id LIKE '%historical%' THEN 'historical'\tWHEN resource_id LIKE '%log%' THEN 'log' WHEN resource_id LIKE '%compliance%' THEN 'compliance' ELSE 'Other' END AS bucket_name_keywords , last_requests , CASE WHEN last_requests \u0026gt;= (usage_date - INTERVAL '2' MONTH) THEN 'No Action' WHEN s3_all_storage_cost = s3_standard_storage_cost THEN 'Potential Action' ELSE 'No Action' END AS s3_standard_underutilized_optimization , CASE WHEN ((s3_transition_usage_quantity)\u0026gt; 0 AND (last_requests \u0026gt;= (usage_date - INTERVAL '1' MONTH))) THEN 'No Action' WHEN s3_put_object_replication_usage_quantity \u0026gt; 0 THEN 'Potential Action' ELSE 'No Action' END AS s3_replication_bucket_optimization , CASE WHEN s3_all_storage_cost = s3_standard_storage_cost THEN 'Yes' ELSE 'No' END AS s3_standard_only_bucket , CASE WHEN s3_glacier_deep_archive_storage_storage_cost \u0026gt; 0 THEN 'in use' WHEN s3_glacier_flexible_retrieval_storage_cost \u0026gt; 0 THEN 'in use' WHEN s3_glacier_instant_retrieval_storage_cost \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_archive_in_use , CASE WHEN s3_inventory_usage_quantity \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_inventory_in_use , CASE WHEN s3_analytics_usage_quantity \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_analytics_in_use , CASE WHEN \u0026quot;s3_intelligent-tiering_storage_usage_quantity\u0026quot; \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_int_in_use , s3_standard_storage_cost * s3_standard_savings AS s3_standard_storage_potential_savings , s3_all_cost , s3_all_storage_cost , s3_all_storage_usage_quantity , s3_standard_storage_cost , s3_standard_storage_usage_quantity , \u0026quot;s3_intelligent-tiering_storage_cost\u0026quot; , \u0026quot;s3_intelligent-tiering_storage_usage_quantity\u0026quot;\t, \u0026quot;s3_standard-ia_storage_cost\u0026quot; , \u0026quot;s3_standard-ia_storage_usage_quantity\u0026quot; , \u0026quot;s3_standard-ia_tier1_cost\u0026quot; , \u0026quot;s3_standard-ia_tier2_cost\u0026quot; , \u0026quot;s3_standard-ia_retrieval_cost\u0026quot; , \u0026quot;s3_onezone-ia_storage_cost\u0026quot; , \u0026quot;s3_onezone-ia_storage_usage_quantity\u0026quot; , s3_reduced_redundancy_storage_cost , s3_reduced_redundancy_storage_usage_quantity , s3_glacier_instant_retrieval_storage_cost , s3_glacier_instant_retrieval_storage_usage_quantity , s3_glacier_instant_retrieval_tier1_cost , s3_glacier_instant_retrieval_tier2_cost , s3_glacier_instant_retrieval_retrieval_cost , s3_glacier_flexible_retrieval_storage_cost , s3_glacier_flexible_retrieval_storage_usage_quantity , s3_glacier_deep_archive_storage_storage_cost , s3_glacier_deep_archive_storage_usage_quantity\t, s3_early_delete_cost , s3_transition_usage_quantity , s3_put_object_usage_quantity , s3_put_object_replication_usage_quantity , s3_get_object_usage_quantity , s3_copy_object_usage_quantity FROM month_usage LEFT JOIN map ON map.account_id = linked_account_id Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - for an example with a cost allocation tags Example uses the tag resource_tags_user_project\nCREATE OR REPLACE VIEW kpi_s3_storage_all AS -- Step 1: Enter S3 standard savings savings assumption. Default is set to 0.3 for 30% savings WITH inputs AS ( SELECT * FROM (VALUES (0.3)) t(s3_standard_savings)), -- Step: 2 Add mapping view map AS(SELECT * FROM account_map), -- Step 3: Filter CUR to return all storage usage data s3_usage_all_time AS ( SELECT year , month , bill_billing_period_start_date AS billing_period , line_item_usage_start_date AS usage_start_date , bill_payer_account_id AS payer_account_id , line_item_usage_account_id AS linked_account_id , resource_tags_user_project , line_item_resource_id AS resource_id , s3_standard_savings , line_item_operation AS operation , line_item_usage_type AS usage_type , CASE WHEN line_item_usage_type LIKE '%EarlyDelete%' THEN 'EarlyDelete' ELSE line_item_operation END \u0026quot;early_delete_adjusted_operation\u0026quot; , CASE WHEN line_item_product_code = 'AmazonGlacier' AND line_item_operation = 'Storage' THEN 'Amazon Glacier' WHEN line_item_product_code = 'AmazonS3' AND product_volume_type LIKE '%Intelligent%' AND line_item_operation LIKE '%IntelligentTiering%' THEN 'Intelligent-Tiering'\tELSE product_volume_type END AS storage_class_type , pricing_unit , sum(line_item_usage_amount) AS usage_quantity , sum(line_item_unblended_cost) unblended_cost , sum(CASE WHEN (pricing_unit = 'GB-Mo' AND line_item_operation like '%Storage%' AND product_volume_type LIKE '%Glacier Deep Archive%') THEN line_item_unblended_cost WHEN (pricing_unit = 'GB-Mo' AND line_item_operation like '%Storage%') THEN line_item_unblended_cost ELSE 0 END) AS s3_all_storage_cost , sum(CASE WHEN (pricing_unit = 'GB-Mo' AND line_item_operation like '%Storage%') THEN line_item_usage_amount ELSE 0 END) AS s3_all_storage_usage_quantity FROM (database).(tablename) , inputs WHERE bill_payer_account_id \u0026lt;\u0026gt; '' AND line_item_resource_id \u0026lt;\u0026gt; '' AND line_item_line_item_type LIKE '%Usage%' AND (line_item_product_code LIKE '%AmazonGlacier%' OR line_item_product_code LIKE '%AmazonS3%') GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12,13,14 ), -- Step 4: Return most recent request date to understand if bucket is in active use most_recent_request AS ( SELECT DISTINCT resource_id , max(usage_start_date) AS last_request_date FROM s3_usage_all_time WHERE usage_quantity \u0026gt; 0 AND operation IN ('PutObject', 'PutObjectForRepl', 'GetObject', 'CopyObject') AND pricing_unit = 'Requests' GROUP BY 1 ), -- Step 5: Pivot table so storage classes into separate columns and filter for current month month_usage AS ( SELECT DISTINCT billing_period , date_trunc('month', usage_start_date) AS \u0026quot;usage_date\u0026quot; , payer_account_id , linked_account_id , resource_tags_user_project , s3.resource_id , most_recent_request.last_request_date AS \u0026quot;last_requests\u0026quot; ,s3_standard_savings , sum(unblended_cost) AS s3_all_cost -- All Storage , sum(s3_all_storage_cost) AS s3_all_storage_cost , sum(s3_all_storage_usage_quantity) AS \u0026quot;s3_all_storage_usage_quantity\u0026quot; -- S3 Standard , sum(CASE WHEN storage_class_type = 'Standard' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_standard_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Standard' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_standard_storage_usage_quantity\u0026quot; -- S3 Standard Infrequent Access , sum(CASE WHEN storage_class_type = 'Standard - Infrequent Access' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Standard - Infrequent Access' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_standard-ia_storage_usage_quantity\u0026quot; , sum(CASE WHEN usage_type LIKE '%Requests-SIA-Tier1%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_tier1_cost\u0026quot; , sum(CASE WHEN usage_type LIKE '%Requests-SIA-Tier2%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_tier2_cost\u0026quot;\t, sum(CASE WHEN usage_type LIKE '%Retrieval-SIA%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_standard-ia_retrieval_cost\u0026quot;\t-- S3 One Zone Infrequent Access , sum(CASE WHEN storage_class_type = 'One Zone - Infrequent Access' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_onezone-ia_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'One Zone - Infrequent Access' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_onezone-ia_storage_usage_quantity\u0026quot; -- S3 Reduced Redundancy , sum(CASE WHEN storage_class_type = 'Reduced Redundancy' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_reduced_redundancy_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Reduced Redundancy' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_reduced_redundancy_storage_usage_quantity\u0026quot; -- S3 Intelligent-Tiering , sum(CASE WHEN storage_class_type LIKE '%Intelligent%' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_intelligent-tiering_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type LIKE '%Intelligent%' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_intelligent-tiering_storage_usage_quantity\u0026quot; -- S3 Glacier Instant Retrieval , sum(CASE WHEN storage_class_type LIKE '%Instant%' AND storage_class_type NOT LIKE '%Intelligent%' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type LIKE '%Instant%' AND storage_class_type NOT LIKE '%Intelligent%' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_storage_usage_quantity\u0026quot;\t, sum(CASE WHEN usage_type LIKE '%Requests-GIR-Tier1%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_tier1_cost\u0026quot; , sum(CASE WHEN usage_type LIKE '%Requests-GIR-Tier2%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_tier2_cost\u0026quot; , sum(CASE WHEN usage_type LIKE '%Retrieval-SIA-GIR%' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_glacier_instant_retrieval_retrieval_cost\u0026quot;\t-- S3 Glacier Flexible Retrieval , sum(CASE WHEN storage_class_type = 'Amazon Glacier' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_glacier_flexible_retrieval_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Amazon Glacier' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_glacier_flexible_retrieval_storage_usage_quantity\u0026quot; -- Glacier Deep Archive , sum(CASE WHEN storage_class_type = 'Glacier Deep Archive' THEN s3_all_storage_cost ELSE 0 END) AS \u0026quot;s3_glacier_deep_archive_storage_storage_cost\u0026quot; , sum(CASE WHEN storage_class_type = 'Glacier Deep Archive' THEN s3_all_storage_usage_quantity ELSE 0 END) AS \u0026quot;s3_glacier_deep_archive_storage_usage_quantity\u0026quot;\t-- Operations , sum(CASE WHEN operation = 'PutObject' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_put_object_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'PutObjectForRepl' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_put_object_replication_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'GetObject' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_get_object_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'CopyObject' AND pricing_unit = 'Requests' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_copy_object_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'Inventory' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_inventory_usage_quantity\u0026quot; , sum(CASE WHEN operation = 'S3.STORAGE_CLASS_ANALYSIS.OBJECT' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_analytics_usage_quantity\u0026quot; ,sum(CASE WHEN operation like '%Transition%' THEN usage_quantity ELSE 0 END) AS \u0026quot;s3_transition_usage_quantity\u0026quot; ,sum(CASE WHEN early_delete_adjusted_operation = 'EarlyDelete' THEN unblended_cost ELSE 0 END) AS \u0026quot;s3_early_delete_cost\u0026quot;\tFROM s3_usage_all_time s3 LEFT JOIN most_recent_request ON most_recent_request.resource_id = s3.resource_id WHERE CAST(concat(s3.year, '-', s3.month, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH) GROUP BY 1, 2, 3, 4, 5, 6,7,8 ) -- Step 6: Add account mapping \u0026amp; apply KPI logic - Add or Adjust bucket name keywords based on your requirements SELECT DISTINCT billing_period , usage_date , payer_account_id , linked_account_id , resource_tags_user_project , map.* , resource_id , CASE WHEN resource_id LIKE '%backup%' THEN 'backup' WHEN resource_id LIKE '%archive%' THEN 'archive' WHEN resource_id LIKE '%historical%' THEN 'historical'\tWHEN resource_id LIKE '%log%' THEN 'log' WHEN resource_id LIKE '%compliance%' THEN 'compliance' ELSE 'Other' END AS bucket_name_keywords , last_requests , CASE WHEN last_requests \u0026gt;= (usage_date - INTERVAL '2' MONTH) THEN 'No Action' WHEN s3_all_storage_cost = s3_standard_storage_cost THEN 'Potential Action' ELSE 'No Action' END AS s3_standard_underutilized_optimization , CASE WHEN ((s3_transition_usage_quantity)\u0026gt; 0 AND (last_requests \u0026gt;= (usage_date - INTERVAL '1' MONTH))) THEN 'No Action' WHEN s3_put_object_replication_usage_quantity \u0026gt; 0 THEN 'Potential Action' ELSE 'No Action' END AS s3_replication_bucket_optimization , CASE WHEN s3_all_storage_cost = s3_standard_storage_cost THEN 'Yes' ELSE 'No' END AS s3_standard_only_bucket , CASE WHEN s3_glacier_deep_archive_storage_storage_cost \u0026gt; 0 THEN 'in use' WHEN s3_glacier_flexible_retrieval_storage_cost \u0026gt; 0 THEN 'in use' WHEN s3_glacier_instant_retrieval_storage_cost \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_archive_in_use , CASE WHEN s3_inventory_usage_quantity \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_inventory_in_use , CASE WHEN s3_analytics_usage_quantity \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_analytics_in_use , CASE WHEN \u0026quot;s3_intelligent-tiering_storage_usage_quantity\u0026quot; \u0026gt; 0 THEN 'in use' ELSE 'not in use' END AS s3_int_in_use , s3_standard_storage_cost * s3_standard_savings AS s3_standard_storage_potential_savings , s3_all_cost , s3_all_storage_cost , s3_all_storage_usage_quantity , s3_standard_storage_cost , s3_standard_storage_usage_quantity , \u0026quot;s3_intelligent-tiering_storage_cost\u0026quot; , \u0026quot;s3_intelligent-tiering_storage_usage_quantity\u0026quot;\t, \u0026quot;s3_standard-ia_storage_cost\u0026quot; , \u0026quot;s3_standard-ia_storage_usage_quantity\u0026quot; , \u0026quot;s3_standard-ia_tier1_cost\u0026quot; , \u0026quot;s3_standard-ia_tier2_cost\u0026quot; , \u0026quot;s3_standard-ia_retrieval_cost\u0026quot; , \u0026quot;s3_onezone-ia_storage_cost\u0026quot; , \u0026quot;s3_onezone-ia_storage_usage_quantity\u0026quot; , s3_reduced_redundancy_storage_cost , s3_reduced_redundancy_storage_usage_quantity , s3_glacier_instant_retrieval_storage_cost , s3_glacier_instant_retrieval_storage_usage_quantity , s3_glacier_instant_retrieval_tier1_cost , s3_glacier_instant_retrieval_tier2_cost , s3_glacier_instant_retrieval_retrieval_cost , s3_glacier_flexible_retrieval_storage_cost , s3_glacier_flexible_retrieval_storage_usage_quantity , s3_glacier_deep_archive_storage_storage_cost , s3_glacier_deep_archive_storage_usage_quantity\t, s3_early_delete_cost , s3_transition_usage_quantity , s3_put_object_usage_quantity , s3_put_object_replication_usage_quantity , s3_get_object_usage_quantity , s3_copy_object_usage_quantity FROM month_usage LEFT JOIN map ON map.account_id = linked_account_id Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from kpi_s3_storage_all limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/kpi_tracker_view/","title":"","tags":[],"description":"","content":"KPI Tracker View This view will be used to create the KPI Tracker view that is used to combine the all the different KPI views and the summary view into a single view so that all KPI metrics can be in a single visual. There is only one version of this view and it is not dependent on if you have or do not have Reserved Instances or Savings Plans.\nCreate View Click here to expand the view Modify the following SQL query for the KPI Tracker view:\nNo updates needed before running this view\nCREATE OR REPLACE VIEW kpi_tracker AS SELECT DISTINCT spend_all.billing_period , spend_all.payer_account_id , spend_all.linked_account_id , account_map.* , spend_all.spend_all_cost\t, instance_all.ec2_all_cost , instance_all.ec2_spot_cost , instance_all.ec2_spot_potential_savings , instance_all.ec2_previous_generation_cost , instance_all.ec2_previous_generation_potential_savings , instance_all.ec2_graviton_eligible_cost , instance_all.ec2_graviton_cost , instance_all.ec2_graviton_potential_savings , instance_all.ec2_amd_eligible_cost , instance_all.ec2_amd_cost , instance_all.ec2_amd_potential_savings , instance_all.rds_all_cost , instance_all.rds_ondemand_cost , instance_all.rds_graviton_cost , instance_all.rds_graviton_eligible_cost , instance_all.rds_graviton_potential_savings , instance_all.rds_commit_potential_savings , instance_all.rds_commit_savings , instance_all.elasticache_all_cost , instance_all.elasticache_ondemand_cost , instance_all.elasticache_graviton_cost , instance_all.elasticache_graviton_eligible_cost , instance_all.elasticache_graviton_potential_savings , instance_all.elasticache_commit_potential_savings , instance_all.elasticache_commit_savings , ebs_all.ebs_all_cost , ebs_all.ebs_gp_all_cost , ebs_all.ebs_gp2_cost , ebs_all.ebs_gp3_cost , ebs_all.ebs_gp3_potential_savings , snap.ebs_snapshots_under_1yr_cost , snap.ebs_snapshots_over_1yr_cost , snap.ebs_snapshot_cost , s3_all.s3_all_storage_cost , s3_all.s3_standard_storage_cost , s3_all.s3_standard_storage_potential_savings , instance_all.compute_all_cost , instance_all.compute_ondemand_cost , instance_all.compute_commit_potential_savings , instance_all.compute_commit_savings , instance_all.dynamodb_all_cost , instance_all.dynamodb_committed_cost , instance_all.dynamodb_ondemand_cost , instance_all.dynamodb_commit_potential_savings , instance_all.dynamodb_commit_savings , instance_all.opensearch_all_cost , instance_all.opensearch_ondemand_cost , instance_all.opensearch_graviton_cost , instance_all.opensearch_graviton_eligible_cost , instance_all.opensearch_graviton_potential_savings , instance_all.opensearch_commit_potential_savings , instance_all.opensearch_commit_savings , instance_all.redshift_all_cost , instance_all.redshift_ondemand_cost , instance_all.redshift_commit_potential_savings , instance_all.redshift_commit_savings , instance_all.sagemaker_all_cost , instance_all.sagemaker_ondemand_cost , instance_all.sagemaker_commit_potential_savings , instance_all.sagemaker_commit_savings , instance_all.lambda_all_cost , instance_all.lambda_graviton_cost , instance_all.lambda_graviton_eligible_cost , instance_all.lambda_graviton_potential_savings FROM (((((account_map LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , \u0026quot;sum\u0026quot;(amortized_cost) \u0026quot;spend_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(unblended_cost) \u0026quot;unblended_cost\u0026quot; FROM summary_view WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) GROUP BY 1, 2, 3 ) spend_all ON (spend_all.linked_account_id = account_id)) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , \u0026quot;sum\u0026quot;(\u0026quot;ec2_all_cost\u0026quot;) \u0026quot;ec2_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_spot_cost\u0026quot;) \u0026quot;ec2_spot_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_spot_potential_savings\u0026quot;) \u0026quot;ec2_spot_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_previous_generation_cost\u0026quot;) \u0026quot;ec2_previous_generation_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_previous_generation_potential_savings\u0026quot;) \u0026quot;ec2_previous_generation_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_graviton_eligible_cost\u0026quot;) \u0026quot;ec2_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_graviton_cost\u0026quot;) \u0026quot;ec2_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_graviton_potential_savings\u0026quot;) \u0026quot;ec2_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_amd_eligible_cost\u0026quot;) \u0026quot;ec2_amd_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_amd_cost\u0026quot;) \u0026quot;ec2_amd_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_amd_potential_savings\u0026quot;) \u0026quot;ec2_amd_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_all_cost\u0026quot;) \u0026quot;rds_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_ondemand_cost\u0026quot;) \u0026quot;rds_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_graviton_cost\u0026quot;) \u0026quot;rds_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_graviton_eligible_cost\u0026quot;) \u0026quot;rds_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_graviton_potential_savings\u0026quot;) \u0026quot;rds_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_commit_potential_savings\u0026quot;) \u0026quot;rds_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_commit_savings\u0026quot;) \u0026quot;rds_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_all_cost\u0026quot;) \u0026quot;elasticache_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_ondemand_cost\u0026quot;) \u0026quot;elasticache_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_graviton_cost\u0026quot;) \u0026quot;elasticache_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_graviton_eligible_cost\u0026quot;) \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_graviton_potential_savings\u0026quot;) \u0026quot;elasticache_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_commit_potential_savings\u0026quot;) \u0026quot;elasticache_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_commit_savings\u0026quot;) \u0026quot;elasticache_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_all_cost\u0026quot;) \u0026quot;compute_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_ondemand_cost\u0026quot;) \u0026quot;compute_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_commit_potential_savings\u0026quot;) \u0026quot;compute_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_commit_savings\u0026quot;) \u0026quot;compute_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_all_cost\u0026quot;) \u0026quot;opensearch_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_ondemand_cost\u0026quot;) \u0026quot;opensearch_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_graviton_cost\u0026quot;) \u0026quot;opensearch_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_graviton_eligible_cost\u0026quot;) \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_graviton_potential_savings\u0026quot;) \u0026quot;opensearch_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_commit_potential_savings\u0026quot;) \u0026quot;opensearch_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_commit_savings\u0026quot;) \u0026quot;opensearch_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_all_cost\u0026quot;) \u0026quot;redshift_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_ondemand_cost\u0026quot;) \u0026quot;redshift_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_commit_potential_savings\u0026quot;) \u0026quot;redshift_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_commit_savings\u0026quot;) \u0026quot;redshift_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_all_cost\u0026quot;) \u0026quot;dynamodb_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_committed_cost\u0026quot;) \u0026quot;dynamodb_committed_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_ondemand_cost\u0026quot;) \u0026quot;dynamodb_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_commit_potential_savings\u0026quot;) \u0026quot;dynamodb_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_commit_savings\u0026quot;) \u0026quot;dynamodb_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_all_cost\u0026quot;) \u0026quot;sagemaker_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_ondemand_cost\u0026quot;) \u0026quot;sagemaker_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_commit_potential_savings\u0026quot;) \u0026quot;sagemaker_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_commit_savings\u0026quot;) \u0026quot;sagemaker_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_all_cost\u0026quot;) \u0026quot;lambda_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_graviton_cost\u0026quot;) \u0026quot;lambda_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_graviton_eligible_cost\u0026quot;) \u0026quot;lambda_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_graviton_potential_savings\u0026quot;) \u0026quot;lambda_graviton_potential_savings\u0026quot; FROM kpi_instance_all GROUP BY 1, 2, 3 ) instance_all ON ((instance_all.linked_account_id = account_id) AND (instance_all.billing_period = spend_all.billing_period))) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , \u0026quot;sum\u0026quot;(\u0026quot;ebs_all_cost\u0026quot;) \u0026quot;ebs_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp3_cost\u0026quot;+\u0026quot;ebs_gp2_cost\u0026quot;) \u0026quot;ebs_gp_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp3_cost\u0026quot;) \u0026quot;ebs_gp3_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp2_cost\u0026quot;) \u0026quot;ebs_gp2_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp3_potential_savings\u0026quot;) \u0026quot;ebs_gp3_potential_savings\u0026quot; FROM kpi_ebs_storage_all GROUP BY 1, 2, 3 ) ebs_all ON ((ebs_all.linked_account_id = account_id) AND (ebs_all.billing_period = spend_all.billing_period))) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , \u0026quot;sum\u0026quot;(\u0026quot;ebs_snapshots_under_1yr_cost\u0026quot;) \u0026quot;ebs_snapshots_under_1yr_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_snapshots_over_1yr_cost\u0026quot;) \u0026quot;ebs_snapshots_over_1yr_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_snapshot_cost\u0026quot;) \u0026quot;ebs_snapshot_cost\u0026quot; FROM kpi_ebs_snap GROUP BY 1, 2, 3 ) snap ON ((snap.linked_account_id = account_id) AND (snap.billing_period = spend_all.billing_period))) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , \u0026quot;sum\u0026quot;(\u0026quot;s3_all_storage_cost\u0026quot;) \u0026quot;s3_all_storage_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;s3_standard_storage_cost\u0026quot;) \u0026quot;s3_standard_storage_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;s3_standard_storage_potential_savings\u0026quot;) \u0026quot;s3_standard_storage_potential_savings\u0026quot; FROM kpi_s3_storage_all GROUP BY 1, 2, 3 ) s3_all ON ((s3_all.linked_account_id = account_id) AND (s3_all.billing_period = spend_all.billing_period))) WHERE (spend_all.billing_period \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - for an example with a cost allocation tags Example uses the tag resource_tags_user_project\nCREATE OR REPLACE VIEW kpi_tracker AS SELECT DISTINCT spend_all.billing_period , spend_all.payer_account_id , spend_all.linked_account_id , spend_all.resource_tags_user_project , account_map.* , spend_all.spend_all_cost\t, instance_all.ec2_all_cost , instance_all.ec2_spot_cost , instance_all.ec2_spot_potential_savings , instance_all.ec2_previous_generation_cost , instance_all.ec2_previous_generation_potential_savings , instance_all.ec2_graviton_eligible_cost , instance_all.ec2_graviton_cost , instance_all.ec2_graviton_potential_savings , instance_all.ec2_amd_eligible_cost , instance_all.ec2_amd_cost , instance_all.ec2_amd_potential_savings , instance_all.rds_all_cost , instance_all.rds_ondemand_cost , instance_all.rds_graviton_cost , instance_all.rds_graviton_eligible_cost , instance_all.rds_graviton_potential_savings , instance_all.rds_commit_potential_savings , instance_all.rds_commit_savings , instance_all.elasticache_all_cost , instance_all.elasticache_ondemand_cost , instance_all.elasticache_graviton_cost , instance_all.elasticache_graviton_eligible_cost , instance_all.elasticache_graviton_potential_savings , instance_all.elasticache_commit_potential_savings , instance_all.elasticache_commit_savings , ebs_all.ebs_all_cost , ebs_all.ebs_gp_all_cost , ebs_all.ebs_gp2_cost , ebs_all.ebs_gp3_cost , ebs_all.ebs_gp3_potential_savings , snap.ebs_snapshots_under_1yr_cost , snap.ebs_snapshots_over_1yr_cost , snap.ebs_snapshot_cost , s3_all.s3_all_storage_cost , s3_all.s3_standard_storage_cost , s3_all.s3_standard_storage_potential_savings , instance_all.compute_all_cost , instance_all.compute_ondemand_cost , instance_all.compute_commit_potential_savings , instance_all.compute_commit_savings , instance_all.dynamodb_all_cost , instance_all.dynamodb_committed_cost , instance_all.dynamodb_ondemand_cost , instance_all.dynamodb_commit_potential_savings , instance_all.dynamodb_commit_savings , instance_all.opensearch_all_cost , instance_all.opensearch_ondemand_cost , instance_all.opensearch_graviton_cost , instance_all.opensearch_graviton_eligible_cost , instance_all.opensearch_graviton_potential_savings , instance_all.opensearch_commit_potential_savings , instance_all.opensearch_commit_savings , instance_all.redshift_all_cost , instance_all.redshift_ondemand_cost , instance_all.redshift_commit_potential_savings , instance_all.redshift_commit_savings , instance_all.sagemaker_all_cost , instance_all.sagemaker_ondemand_cost , instance_all.sagemaker_commit_potential_savings , instance_all.sagemaker_commit_savings , instance_all.lambda_all_cost , instance_all.lambda_graviton_cost , instance_all.lambda_graviton_eligible_cost , instance_all.lambda_graviton_potential_savings FROM (((((account_map LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , \u0026quot;sum\u0026quot;(amortized_cost) \u0026quot;spend_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(unblended_cost) \u0026quot;unblended_cost\u0026quot; FROM summary_view WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) GROUP BY 1, 2, 3,4 ) spend_all ON (spend_all.linked_account_id = account_id)) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , \u0026quot;sum\u0026quot;(\u0026quot;ec2_all_cost\u0026quot;) \u0026quot;ec2_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_spot_cost\u0026quot;) \u0026quot;ec2_spot_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_spot_potential_savings\u0026quot;) \u0026quot;ec2_spot_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_previous_generation_cost\u0026quot;) \u0026quot;ec2_previous_generation_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_previous_generation_potential_savings\u0026quot;) \u0026quot;ec2_previous_generation_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_graviton_eligible_cost\u0026quot;) \u0026quot;ec2_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_graviton_cost\u0026quot;) \u0026quot;ec2_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_graviton_potential_savings\u0026quot;) \u0026quot;ec2_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_amd_eligible_cost\u0026quot;) \u0026quot;ec2_amd_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_amd_cost\u0026quot;) \u0026quot;ec2_amd_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ec2_amd_potential_savings\u0026quot;) \u0026quot;ec2_amd_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_all_cost\u0026quot;) \u0026quot;rds_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_ondemand_cost\u0026quot;) \u0026quot;rds_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_graviton_cost\u0026quot;) \u0026quot;rds_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_graviton_eligible_cost\u0026quot;) \u0026quot;rds_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_graviton_potential_savings\u0026quot;) \u0026quot;rds_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_commit_potential_savings\u0026quot;) \u0026quot;rds_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;rds_commit_savings\u0026quot;) \u0026quot;rds_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_all_cost\u0026quot;) \u0026quot;elasticache_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_ondemand_cost\u0026quot;) \u0026quot;elasticache_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_graviton_cost\u0026quot;) \u0026quot;elasticache_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_graviton_eligible_cost\u0026quot;) \u0026quot;elasticache_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_graviton_potential_savings\u0026quot;) \u0026quot;elasticache_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_commit_potential_savings\u0026quot;) \u0026quot;elasticache_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;elasticache_commit_savings\u0026quot;) \u0026quot;elasticache_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_all_cost\u0026quot;) \u0026quot;compute_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_ondemand_cost\u0026quot;) \u0026quot;compute_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_commit_potential_savings\u0026quot;) \u0026quot;compute_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;compute_commit_savings\u0026quot;) \u0026quot;compute_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_all_cost\u0026quot;) \u0026quot;opensearch_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_ondemand_cost\u0026quot;) \u0026quot;opensearch_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_graviton_cost\u0026quot;) \u0026quot;opensearch_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_graviton_eligible_cost\u0026quot;) \u0026quot;opensearch_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_graviton_potential_savings\u0026quot;) \u0026quot;opensearch_graviton_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_commit_potential_savings\u0026quot;) \u0026quot;opensearch_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;opensearch_commit_savings\u0026quot;) \u0026quot;opensearch_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_all_cost\u0026quot;) \u0026quot;redshift_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_ondemand_cost\u0026quot;) \u0026quot;redshift_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_commit_potential_savings\u0026quot;) \u0026quot;redshift_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;redshift_commit_savings\u0026quot;) \u0026quot;redshift_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_all_cost\u0026quot;) \u0026quot;dynamodb_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_committed_cost\u0026quot;) \u0026quot;dynamodb_committed_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_ondemand_cost\u0026quot;) \u0026quot;dynamodb_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_commit_potential_savings\u0026quot;) \u0026quot;dynamodb_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;dynamodb_commit_savings\u0026quot;) \u0026quot;dynamodb_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_all_cost\u0026quot;) \u0026quot;sagemaker_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_ondemand_cost\u0026quot;) \u0026quot;sagemaker_ondemand_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_commit_potential_savings\u0026quot;) \u0026quot;sagemaker_commit_potential_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;sagemaker_commit_savings\u0026quot;) \u0026quot;sagemaker_commit_savings\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_all_cost\u0026quot;) \u0026quot;lambda_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_graviton_cost\u0026quot;) \u0026quot;lambda_graviton_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_graviton_eligible_cost\u0026quot;) \u0026quot;lambda_graviton_eligible_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;lambda_graviton_potential_savings\u0026quot;) \u0026quot;lambda_graviton_potential_savings\u0026quot; FROM kpi_instance_all GROUP BY 1, 2, 3,4 ) instance_all ON ((instance_all.linked_account_id = account_id) AND (instance_all.billing_period = spend_all.billing_period) AND (instance_all.resource_tags_user_project = spend_all.resource_tags_user_project))) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , \u0026quot;sum\u0026quot;(\u0026quot;ebs_all_cost\u0026quot;) \u0026quot;ebs_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp3_cost\u0026quot;+\u0026quot;ebs_gp2_cost\u0026quot;) \u0026quot;ebs_gp_all_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp3_cost\u0026quot;) \u0026quot;ebs_gp3_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp2_cost\u0026quot;) \u0026quot;ebs_gp2_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_gp3_potential_savings\u0026quot;) \u0026quot;ebs_gp3_potential_savings\u0026quot; FROM kpi_ebs_storage_all GROUP BY 1, 2, 3,4 ) ebs_all ON ((ebs_all.linked_account_id = account_id) AND (ebs_all.billing_period = spend_all.billing_period) AND (ebs_all.resource_tags_user_project = spend_all.resource_tags_user_project))) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , \u0026quot;sum\u0026quot;(\u0026quot;ebs_snapshots_under_1yr_cost\u0026quot;) \u0026quot;ebs_snapshots_under_1yr_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_snapshots_over_1yr_cost\u0026quot;) \u0026quot;ebs_snapshots_over_1yr_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;ebs_snapshot_cost\u0026quot;) \u0026quot;ebs_snapshot_cost\u0026quot; FROM kpi_ebs_snap GROUP BY 1, 2, 3,4 ) snap ON ((snap.linked_account_id = account_id) AND (snap.billing_period = spend_all.billing_period) AND (snap.resource_tags_user_project = spend_all.resource_tags_user_project))) LEFT JOIN ( SELECT DISTINCT billing_period , payer_account_id , linked_account_id , resource_tags_user_project , \u0026quot;sum\u0026quot;(\u0026quot;s3_all_storage_cost\u0026quot;) \u0026quot;s3_all_storage_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;s3_standard_storage_cost\u0026quot;) \u0026quot;s3_standard_storage_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;s3_standard_storage_potential_savings\u0026quot;) \u0026quot;s3_standard_storage_potential_savings\u0026quot; FROM kpi_s3_storage_all GROUP BY 1, 2, 3,4 ) s3_all ON ((s3_all.linked_account_id = account_id) AND (s3_all.billing_period = spend_all.billing_period) AND (s3_all.resource_tags_user_project = spend_all.resource_tags_user_project))) WHERE (spend_all.billing_period \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from kpi_tracker limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/6_view6/","title":"","tags":[],"description":"","content":"View 6 - Customer_All The customer_all view is used to direct query a small portion of the full cost and usage report.\nClick here - to create your customer_all view Modify the following SQL query for View0 - Account Map:\nOn line 5, replace (database.table_name) with your Cost \u0026amp; Usage Report database and table name\nCREATE OR REPLACE VIEW customer_all AS SELECT * FROM (database.table_name) WHERE (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)) Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from (database).customer_all limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/7_view7/","title":"","tags":[],"description":"","content":"Data Transfer View This view will be used to create the main Data Transfer Cost Analysis dashboard page.\nWe recommend large customers with over 500 linked accounts, or more than $5M a month in invoiced cost, display 1 or 2 months previous data instead of 3. Modify the INTERVAL in the statements below to less than 3 months for improved performance.\nCreate View Click here - to expand data transfer view query OR Modify the following SQL query for data_transfer_view:\nReplace (database).(tablename) with your CUR database and table name\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame. You can add or remove months from condition between line 41-67\nCREATE OR REPLACE VIEW \u0026quot;data_transfer_view\u0026quot; AS SELECT \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot; , \u0026quot;product_servicecode\u0026quot; , \u0026quot;product_servicename\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;to_location\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , (\u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) / 1024) \u0026quot;TBs\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_blended_cost\u0026quot;) \u0026quot;blended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; , \u0026quot;line_item_blended_rate\u0026quot; \u0026quot;blended_rate\u0026quot; , \u0026quot;line_item_unblended_rate\u0026quot; \u0026quot;unblended_rate\u0026quot; , \u0026quot;pricing_public_on_demand_rate\u0026quot; \u0026quot;public_ondemand_rate\u0026quot; , \u0026quot;product_transfer_type\u0026quot; \u0026quot;data_transfer_type\u0026quot; FROM (database).(tablename) WHERE ( ( ( ( ( (\u0026quot;line_item_usage_type\u0026quot; LIKE '%In_Bytes%') OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Out_Bytes%') ) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Rigional_Bytes%') ) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%DataTransfer%') ) AND ( ( ( ( year = \u0026quot;format_datetime\u0026quot;(current_timestamp, 'YYYY') ) AND ( month = \u0026quot;format_datetime\u0026quot;(current_timestamp, 'MM') ) ) OR ( ( year = \u0026quot;format_datetime\u0026quot;( ( \u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '2' MONTH ), 'YYYY' ) ) AND ( month = \u0026quot;format_datetime\u0026quot;( ( \u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '2' MONTH ), 'MM' ) ) ) ) OR ( ( year = \u0026quot;format_datetime\u0026quot;( ( \u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH ), 'YYYY' ) ) AND ( month = \u0026quot;format_datetime\u0026quot;( ( \u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH ), 'MM' ) ) ) ) ) OR ( ( \u0026quot;product_from_location_type\u0026quot; = 'AWS Edge Location' ) AND ( NOT ( \u0026quot;line_item_line_item_type\u0026quot; IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit') ) ) ) ) GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_line_item_type\u0026quot;, \u0026quot;line_item_operation\u0026quot;, \u0026quot;product_region\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;product_from_location\u0026quot;, \u0026quot;product_to_location\u0026quot;, \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;line_item_blended_rate\u0026quot;, \u0026quot;product_transfer_type\u0026quot;, \u0026quot;product_usagetype\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;product_product_family\u0026quot;, \u0026quot;product_servicecode\u0026quot;, \u0026quot;product_servicename\u0026quot; Click here - to expand Create view in Athena using aws cli Copy the code below to a new file and name it create-data-transfer-view-query.json. Then replace the values as follows-\n\u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; = Your database.table\n\u0026lt;your s3 bucket\u0026gt; = Your s3 bucket\n\u0026lt;your Athena Workgroup\u0026gt; = Your Athena WorkGroup\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in where condition Months parameter\n{ \u0026quot;QueryString\u0026quot;: \u0026quot;CREATE OR REPLACE VIEW data_transfer_view AS SELECT product_product_family product_family , product_servicecode , product_servicename , line_item_product_code product_code , bill_billing_period_start_date billing_period , bill_payer_account_id payer_account_id , line_item_usage_account_id linked_account_id , product_product_name product_name , line_item_line_item_type charge_type , line_item_operation operation , product_region region , line_item_usage_type usage_type , product_from_location from_location , product_to_location to_location , line_item_resource_id resource_id , (sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) / 1024) TBs , sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) usage_quantity , sum(line_item_blended_cost) blended_cost , sum(line_item_unblended_cost) unblended_cost , sum(pricing_public_on_demand_cost) public_cost , line_item_blended_rate blended_rate , line_item_unblended_rate unblended_rate , pricing_public_on_demand_rate public_ondemand_rate , product_transfer_type data_transfer_type FROM \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; WHERE ( ( ( ( ( (line_item_usage_type LIKE '%In_Bytes%') OR (line_item_usage_type LIKE '%Out_Bytes%') ) OR (line_item_usage_type LIKE '%Rigional_Bytes%') ) OR (line_item_usage_type LIKE '%DataTransfer%') ) AND ( ( ( ( year = format_datetime(current_timestamp, 'YYYY') ) AND ( month = format_datetime(current_timestamp, 'MM') ) ) OR ( ( year = format_datetime( ( date_trunc('month', current_timestamp) - INTERVAL '2' MONTH ), 'YYYY' ) ) AND ( month = format_datetime( ( date_trunc('month', current_timestamp) - INTERVAL '2' MONTH ), 'MM' ) ) ) ) OR ( ( year = format_datetime( ( date_trunc('month', current_timestamp) - INTERVAL '1' MONTH ), 'YYYY' ) ) AND ( month = format_datetime( ( date_trunc('month', current_timestamp) - INTERVAL '1' MONTH ), 'MM' ) ) ) ) ) OR ( ( product_from_location_type = 'AWS Edge Location' ) AND ( NOT ( line_item_line_item_type IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit') ) ) ) ) GROUP BY line_item_product_code, bill_billing_period_start_date, line_item_usage_account_id, bill_payer_account_id, product_product_name, line_item_line_item_type, line_item_operation, product_region, line_item_usage_type, product_from_location, product_to_location, line_item_resource_id, line_item_blended_rate, product_transfer_type, product_usagetype, pricing_public_on_demand_rate, line_item_unblended_rate, product_product_family, product_servicecode, product_servicename\u0026quot;, \u0026quot;QueryExecutionContext\u0026quot;: { \u0026quot;Database\u0026quot;: \u0026quot;costmaster\u0026quot;, \u0026quot;Catalog\u0026quot;: \u0026quot;AWSDataCatalog\u0026quot; }, \u0026quot;ResultConfiguration\u0026quot;: { \u0026quot;OutputLocation\u0026quot;: \u0026quot;s3://\u0026lt;your S3 bucket\u0026gt;/tmp\u0026quot; }, \u0026quot;WorkGroup\u0026quot;: \u0026quot;\u0026lt;your Athena Workgroup\u0026gt;\u0026quot; } Run the following command in a terminal window from the folder where you created create-data-transfer-view-query.json\naws athena start-query-execution --cli-input-json file://create-data-transfer-view-query.json To check query execution status\naws athena get-query-execution --query-execution-id \u0026lt;QueryExecutionId returned from previus command\u0026gt; --region us-east-1 Response:\nAdding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - to add your cost allocation tags To add your tags locate the the \u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; in the query you are using and add it after make sure to add a comma between each attribute and then add a group by field for any tags added (i.e. if you add one cost allocation tag you would add ,# to group by in the bottom of your query)\nExample: Add your project tag by first locating the tag in your CUR attributes for project it will show up as resource_tags_user_projects. You will then find the ,\u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; line in your query and add , resource_tags_user_projects then add ,# in at the bottom of your query in the group by section. Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.data_transfer_view limit 10; "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/6_view6/","title":"","tags":[],"description":"","content":"Data Transfer View This view will be used to create the main Data Transfer Cost Analysis dashboard page.\nWe recommend large customers with over 500 linked accounts, or more than $5M a month in invoiced cost, display 1 or 2 months previous data instead of 3. Modify the INTERVAL in the statements below to less than 3 months for improved performance.\nClick here - to expand data transfer view query OR Modify the following SQL query for data_transfer_view:\nReplace (database).(tablename) with your CUR database and table name\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in row 32,33\nCREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;to_location\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , (\u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) / 1024) \u0026quot;TBs\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_blended_cost\u0026quot;) \u0026quot;blended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; , \u0026quot;line_item_blended_rate\u0026quot; \u0026quot;blended_rate\u0026quot; , \u0026quot;line_item_unblended_rate\u0026quot; \u0026quot;unblended_rate\u0026quot; , \u0026quot;pricing_public_on_demand_rate\u0026quot; \u0026quot;public_ondemand_rate\u0026quot; , \u0026quot;product_transfer_type\u0026quot; \u0026quot;data_transfer_type\u0026quot; FROM (database).(tablename) WHERE (((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Bytes%') AND (((\u0026quot;line_item_usage_type\u0026quot; LIKE '%In%') OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Out%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Regional%'))) AND (((\u0026quot;product_from_location\u0026quot; = '') OR (\u0026quot;product_from_location\u0026quot; LIKE '%(%')) OR (\u0026quot;product_from_location_type\u0026quot; = 'AWS Edge Location'))) AND ((\u0026quot;line_item_line_item_type\u0026quot; IN ('PrivateRateDiscount', 'Usage', 'EdpDiscount')))) AND ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH)))) GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_line_item_type\u0026quot;, \u0026quot;line_item_operation\u0026quot;, \u0026quot;product_region\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;product_from_location\u0026quot;, \u0026quot;product_to_location\u0026quot;, \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;line_item_blended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;product_transfer_type\u0026quot;, \u0026quot;product_usagetype\u0026quot;, \u0026quot;pricing_public_on_demand_cost\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_unblended_cost\u0026quot;, \u0026quot;line_item_blended_cost\u0026quot; Click here - to expand Create view in Athena using aws cli Copy the code below to a new file and name it create-data-transfer-view-query.json. Then replace the values as follows-\n\u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; = Your database.table\n\u0026lt;your s3 bucket\u0026gt; = Your s3 bucket\n\u0026lt;your Athena Workgroup\u0026gt; = Your Athena WorkGroup\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in QueryString\n{ \u0026quot;QueryString\u0026quot;: \u0026quot;CREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT line_item_product_code product_code, bill_billing_period_start_date billing_period, bill_payer_account_id payer_account_id, line_item_usage_account_id linked_account_id, product_product_name product_name, line_item_line_item_type charge_type, line_item_operation operation, product_region region, line_item_usage_type usage_type, product_from_location from_location, product_to_location to_location, line_item_resource_id resource_id, (sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) / 1024) TBs, sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) usage_quantity, sum(line_item_blended_cost) blended_cost, sum(line_item_unblended_cost) unblended_cost, sum(pricing_public_on_demand_cost) public_cost, line_item_blended_rate blended_rate, line_item_unblended_rate unblended_rate, pricing_public_on_demand_rate public_ondemand_rate, product_transfer_type data_transfer_type FROM \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; WHERE (((((line_item_usage_type LIKE '%Bytes%') AND (((line_item_usage_type LIKE '%In%') OR (line_item_usage_type LIKE '%Out%')) OR (line_item_usage_type LIKE '%Regional%'))) AND (((product_from_location = '') OR (product_from_location LIKE '%(%')) OR (product_from_location_type = 'AWS Edge Location'))) AND ((line_item_line_item_type IN ('PrivateRateDiscount', 'Usage', 'EdpDiscount')))) AND ((bill_billing_period_start_date \u0026gt;= (date_trunc('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(concat(\\\u0026quot;year\\\u0026quot;, '-', \\\u0026quot;month\\\u0026quot;, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH)))) GROUP BY line_item_product_code, bill_billing_period_start_date, line_item_usage_account_id, bill_payer_account_id, product_product_name, line_item_line_item_type, line_item_operation, product_region, line_item_usage_type, product_from_location, product_to_location, line_item_resource_id, line_item_blended_rate, line_item_line_item_description, product_transfer_type, product_usagetype, pricing_public_on_demand_cost, pricing_public_on_demand_rate, line_item_unblended_rate, line_item_unblended_cost, line_item_blended_cost\u0026quot;, \u0026quot;QueryExecutionContext\u0026quot;: { \u0026quot;Database\u0026quot;: \u0026quot;costmaster\u0026quot;, \u0026quot;Catalog\u0026quot;: \u0026quot;AWSDataCatalog\u0026quot; }, \u0026quot;ResultConfiguration\u0026quot;: { \u0026quot;OutputLocation\u0026quot;: \u0026quot;s3://\u0026lt;your S3 bucket\u0026gt;/tmp\u0026quot; }, \u0026quot;WorkGroup\u0026quot;: \u0026quot;\u0026lt;your Athena Workgroup\u0026gt;\u0026quot; } Run the following command in a terminal window from the folder where you created create-data-transfer-view-query.json\naws athena start-query-execution --cli-input-json file://create-data-transfer-view-query.json To check query execution status\naws athena get-query-execution --query-execution-id \u0026lt;QueryExecutionId returned from previus command\u0026gt; --region us-east-1 Response:\nConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.data_transfer_view limit 10; "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/6_view6/","title":"","tags":[],"description":"","content":"Data Transfer View This view will be used to create the main Data Transfer Cost Analysis dashboard page.\nWe recommend large customers with over 500 linked accounts, or more than $5M a month in invoiced cost, display 1 or 2 months previous data instead of 3. Modify the INTERVAL in the statements below to less than 3 months for improved performance.\nClick here - to expand data transfer view query OR Modify the following SQL query for data_transfer_view:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in row 32,33\nCREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;to_location\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , (\u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) / 1024) \u0026quot;TBs\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_blended_cost\u0026quot;) \u0026quot;blended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; , \u0026quot;line_item_blended_rate\u0026quot; \u0026quot;blended_rate\u0026quot; , \u0026quot;line_item_unblended_rate\u0026quot; \u0026quot;unblended_rate\u0026quot; , \u0026quot;pricing_public_on_demand_rate\u0026quot; \u0026quot;public_ondemand_rate\u0026quot; , \u0026quot;product_transfer_type\u0026quot; \u0026quot;data_transfer_type\u0026quot; FROM (database).(tablename) WHERE (((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Bytes%') AND ((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%In%') OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Out%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Regional%')) AND ((NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Cloudfront%')) AND (\u0026quot;line_item_usage_type\u0026quot; \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((\u0026quot;product_from_location\u0026quot; = '') OR (\u0026quot;product_from_location\u0026quot; LIKE '%(%'))) AND (NOT (\u0026quot;line_item_line_item_type\u0026quot; IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (\u0026quot;line_item_blended_cost\u0026quot; \u0026gt; 0.0)) AND ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH))) GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_line_item_type\u0026quot;, \u0026quot;line_item_operation\u0026quot;, \u0026quot;product_region\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;product_from_location\u0026quot;, \u0026quot;product_to_location\u0026quot;, \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;line_item_blended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;product_transfer_type\u0026quot;, \u0026quot;product_usagetype\u0026quot;, \u0026quot;pricing_public_on_demand_cost\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_unblended_cost\u0026quot;, \u0026quot;line_item_blended_cost\u0026quot; Click here - to expand Create view in Athena using aws cli Copy the code below to a new file and name it create-data-transfer-view-query.json. Then replace the values as follows-\n\u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; = Your database.table\n\u0026lt;your s3 bucket\u0026gt; = Your s3 bucket\n\u0026lt;your Athena Workgroup\u0026gt; = Your Athena WorkGroup\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in QueryString\n{ \u0026quot;QueryString\u0026quot;: \u0026quot;CREATE OR REPLACE VIEW data_transfer_view_cli AS SELECT DISTINCT line_item_product_code product_code, bill_billing_period_start_date billing_period, bill_payer_account_id payer_account_id, line_item_usage_account_id linked_account_id, product_product_name product_name, line_item_line_item_type charge_type, line_item_operation operation, product_region region, line_item_usage_type usage_type, product_from_location from_location , product_to_location to_location , line_item_resource_id resource_id , (sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) / 1024) TBs , sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) usage_quantity, sum(line_item_blended_cost) blended_cost , sum(line_item_unblended_cost) unblended_cost , sum(pricing_public_on_demand_cost) public_cost , line_item_blended_rate blended_rate, line_item_unblended_rate unblended_rate, pricing_public_on_demand_rate public_ondemand_rate, product_transfer_type data_transfer_type FROM \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; WHERE (((((line_item_usage_type LIKE '%Bytes%') AND ((((line_item_usage_type LIKE '%In%') OR (line_item_usage_type LIKE '%Out%')) OR (line_item_usage_type LIKE '%Regional%')) AND ((NOT (line_item_usage_type LIKE '%Cloudfront%')) AND (line_item_usage_type \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((product_from_location = '') OR (product_from_location LIKE '%(%'))) AND (NOT (line_item_line_item_type IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (line_item_blended_cost \u0026gt; 0.0)) AND ((bill_billing_period_start_date \u0026gt;= (date_trunc('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(concat(\\year\\, '-', \\month\\, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH))) GROUP BY line_item_product_code, bill_billing_period_start_date, line_item_usage_account_id, bill_payer_account_id, product_product_name, line_item_line_item_type, line_item_operation, product_region, line_item_usage_type, product_from_location, product_to_location, line_item_resource_id, line_item_blended_rate, line_item_line_item_description, product_transfer_type, product_usagetype, pricing_public_on_demand_cost, pricing_public_on_demand_rate, line_item_unblended_rate, line_item_unblended_cost, line_item_blended_cost\u0026quot;, \u0026quot;QueryExecutionContext\u0026quot;: { \u0026quot;Database\u0026quot;: \u0026quot;costmaster\u0026quot;, \u0026quot;Catalog\u0026quot;: \u0026quot;AWSDataCatalog\u0026quot; }, \u0026quot;ResultConfiguration\u0026quot;: { \u0026quot;OutputLocation\u0026quot;: \u0026quot;s3://\u0026lt;your S3 bucket\u0026gt;/tmp\u0026quot; }, \u0026quot;WorkGroup\u0026quot;: \u0026quot;\u0026lt;your Athena Workgroup\u0026gt;\u0026quot; } Run the following command in a terminal window from the folder where you created create-data-transfer-view-query.json\naws athena start-query-execution --cli-input-json file://create-data-transfer-view-query.json To check query execution status\naws athena get-query-execution --query-execution-id \u0026lt;QueryExecutionId returned from previus command\u0026gt; --profile qs --region us-east-1 Response:\nConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.data_transfer_view limit 10; "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/6_view6/","title":"","tags":[],"description":"","content":"Data Transfer View This view will be used to create the main Data Transfer Cost Analysis dashboard page.\nWe recommend large customers with over 500 linked accounts, or more than $5M a month in invoiced cost, display 1 or 2 months previous data instead of 3. Modify the INTERVAL in the statements below to less than 3 months for improved performance.\nClick here - to expand data transfer view query OR Modify the following SQL query for data_transfer_view:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in row 32,33\nCREATE OR REPLACE VIEW data_transfer_view AS SELECT DISTINCT \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;to_location\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , (\u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) / 1024) \u0026quot;TBs\u0026quot; , \u0026quot;sum\u0026quot;((CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END)) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_blended_cost\u0026quot;) \u0026quot;blended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; , \u0026quot;line_item_blended_rate\u0026quot; \u0026quot;blended_rate\u0026quot; , \u0026quot;line_item_unblended_rate\u0026quot; \u0026quot;unblended_rate\u0026quot; , \u0026quot;pricing_public_on_demand_rate\u0026quot; \u0026quot;public_ondemand_rate\u0026quot; , \u0026quot;product_transfer_type\u0026quot; \u0026quot;data_transfer_type\u0026quot; FROM (database).(tablename) WHERE (((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Bytes%') AND ((((\u0026quot;line_item_usage_type\u0026quot; LIKE '%In%') OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Out%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Regional%')) AND ((NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Cloudfront%')) AND (\u0026quot;line_item_usage_type\u0026quot; \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((\u0026quot;product_from_location\u0026quot; = '') OR (\u0026quot;product_from_location\u0026quot; LIKE '%(%'))) AND (NOT (\u0026quot;line_item_line_item_type\u0026quot; IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (\u0026quot;line_item_blended_cost\u0026quot; \u0026gt; 0.0)) AND ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '3' MONTH))) GROUP BY \u0026quot;line_item_product_code\u0026quot;, \u0026quot;bill_billing_period_start_date\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot;, \u0026quot;bill_payer_account_id\u0026quot;, \u0026quot;product_product_name\u0026quot;, \u0026quot;line_item_line_item_type\u0026quot;, \u0026quot;line_item_operation\u0026quot;, \u0026quot;product_region\u0026quot;, \u0026quot;line_item_usage_type\u0026quot;, \u0026quot;product_from_location\u0026quot;, \u0026quot;product_to_location\u0026quot;, \u0026quot;line_item_resource_id\u0026quot;, \u0026quot;line_item_blended_rate\u0026quot;, \u0026quot;line_item_line_item_description\u0026quot;, \u0026quot;product_transfer_type\u0026quot;, \u0026quot;product_usagetype\u0026quot;, \u0026quot;pricing_public_on_demand_cost\u0026quot;, \u0026quot;pricing_public_on_demand_rate\u0026quot;, \u0026quot;line_item_unblended_rate\u0026quot;, \u0026quot;line_item_unblended_cost\u0026quot;, \u0026quot;line_item_blended_cost\u0026quot; Click here - to expand Create view in Athena using aws cli Copy the code below to a new file and name it create-data-transfer-view-query.json. Then replace the values as follows-\n\u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; = Your database.table\n\u0026lt;your s3 bucket\u0026gt; = Your s3 bucket\n\u0026lt;your Athena Workgroup\u0026gt; = Your Athena WorkGroup\nOptional: Adjust the look back from \u0026lsquo;3\u0026rsquo; months to desired time-frame in QueryString\n{ \u0026quot;QueryString\u0026quot;: \u0026quot;CREATE OR REPLACE VIEW data_transfer_view_cli AS SELECT DISTINCT line_item_product_code product_code, bill_billing_period_start_date billing_period, bill_payer_account_id payer_account_id, line_item_usage_account_id linked_account_id, product_product_name product_name, line_item_line_item_type charge_type, line_item_operation operation, product_region region, line_item_usage_type usage_type, product_from_location from_location , product_to_location to_location , line_item_resource_id resource_id , (sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) / 1024) TBs , sum((CASE WHEN (line_item_line_item_type = 'Usage') THEN line_item_usage_amount ELSE 0 END)) usage_quantity, sum(line_item_blended_cost) blended_cost , sum(line_item_unblended_cost) unblended_cost , sum(pricing_public_on_demand_cost) public_cost , line_item_blended_rate blended_rate, line_item_unblended_rate unblended_rate, pricing_public_on_demand_rate public_ondemand_rate, product_transfer_type data_transfer_type FROM \u0026lt;your database\u0026gt;.\u0026lt;your table\u0026gt; WHERE (((((line_item_usage_type LIKE '%Bytes%') AND ((((line_item_usage_type LIKE '%In%') OR (line_item_usage_type LIKE '%Out%')) OR (line_item_usage_type LIKE '%Regional%')) AND ((NOT (line_item_usage_type LIKE '%Cloudfront%')) AND (line_item_usage_type \u0026lt;\u0026gt; 'DataTransfer-In-Bytes')))) AND ((product_from_location = '') OR (product_from_location LIKE '%(%'))) AND (NOT (line_item_line_item_type IN ('Tax', 'RIFee', 'Fee', 'Refund', 'Credit')))) AND (line_item_blended_cost \u0026gt; 0.0)) AND ((bill_billing_period_start_date \u0026gt;= (date_trunc('month', current_timestamp) - INTERVAL '3' MONTH)) AND (CAST(concat(\\year\\, '-', \\month\\, '-01') AS date) \u0026gt;= (date_trunc('month', current_date) - INTERVAL '3' MONTH))) GROUP BY line_item_product_code, bill_billing_period_start_date, line_item_usage_account_id, bill_payer_account_id, product_product_name, line_item_line_item_type, line_item_operation, product_region, line_item_usage_type, product_from_location, product_to_location, line_item_resource_id, line_item_blended_rate, line_item_line_item_description, product_transfer_type, product_usagetype, pricing_public_on_demand_cost, pricing_public_on_demand_rate, line_item_unblended_rate, line_item_unblended_cost, line_item_blended_cost\u0026quot;, \u0026quot;QueryExecutionContext\u0026quot;: { \u0026quot;Database\u0026quot;: \u0026quot;costmaster\u0026quot;, \u0026quot;Catalog\u0026quot;: \u0026quot;AWSDataCatalog\u0026quot; }, \u0026quot;ResultConfiguration\u0026quot;: { \u0026quot;OutputLocation\u0026quot;: \u0026quot;s3://\u0026lt;your S3 bucket\u0026gt;/tmp\u0026quot; }, \u0026quot;WorkGroup\u0026quot;: \u0026quot;\u0026lt;your Athena Workgroup\u0026gt;\u0026quot; } Run the following command in a terminal window from the folder where you created create-data-transfer-view-query.json\naws athena start-query-execution --cli-input-json file://create-data-transfer-view-query.json To check query execution status\naws athena get-query-execution --query-execution-id \u0026lt;QueryExecutionId returned from previus command\u0026gt; --profile qs --region us-east-1 Response:\nConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.data_transfer_view limit 10; "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/0_view0/","title":"","tags":[],"description":"","content":"View 0 - Account Map The Cost \u0026amp; Usage Report data doesn\u0026rsquo;t contain account names and other business or organization specific mapping so the first view you will create is a view that enhances your CUR data. There are a few options you can leverage to create your account_map view to provide opportunities to leverage your existing mapping tables. organization information, or other business mappings allows for deeper insights and additional This view will be used to create the Account_Map for your dashboards.\nYou can update your account_map view or change options at a future time. If you are unsure of what option to use we suggest starting with Option 1\nOption 1: Placeholder Account Map data The Account Map placeholder option is a quick way to create your view if you do not use AWS Organizations, have an existing account mapping document, or are looking to quickly create the dashboards for a proof of concept.\nClick here - to create using placeholder data Modify the following SQL query for View0 - Account Map:\nOn line 5, replace (database.table_name) with your Cost \u0026amp; Usage Report database and table name\nCREATE OR REPLACE VIEW account_map AS SELECT DISTINCT \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;account_id\u0026quot;, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;account_name\u0026quot; FROM (database.table_name) Option 2: Account Map CSV file using your existing AWS account mapping data Many organizations already maintain their account mapping outside of AWS. You can leverage your existing mapping data by creating a csv file with your account mapping data including any additional organization attributes.\nClick here - to create using an your own account mapping csv and Amazon S3 Create your account_map csv file This example will show you how to create using a sample account_map csv file\nCreate an account_map csv file locally, you can use the sample here and requirements below as a starting point: account_map.csv Update your account_map csv with your account mapping data\nUpload your account_map csv file to Amazon S3 Navigate to Amazon S3\nSelect Create Bucket\nName your bucket, we recommend cost-account-map-\u0026lt;account_id\u0026gt; to easily locate\nScroll to the bottom and select Create Bucket\nNavigate to your newly created s3 bucket\nSelect Create folder\nName your folder account-map and select Create folder\nClick on your newly created account-map folder\nSelect Upload\nIn your newly created folder, drag and drop your account_map.csv file then select Upload\nCopy down the S3 Destination of the account-map.csv. You will need this to create your Athena table\nCreate your account_mapping Athena table Navigate to Amazon Athena\nModify the following SQL query with your account_map.csv information\nReplace the in row 15 with your account-map S3 destination from step 8 of the last section (i.e. s3://cost-account-map-123456789012)\nCREATE EXTERNAL TABLE `account_mapping`( `account_id` string, `account_name` string, `business_unit` string, `team` string, `cost_center` string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '\u0026lt;S3 Destination\u0026gt;' TBLPROPERTIES ( 'has_encrypted_data'='false', 'skip.header.line.count'='1') NOTE: Validate rows 2-5 match your csv columns. If you removed one of the fields in the csv you will need to remove it in the SQL query. If you added any additional fields you will need to add the attribute to the SQL query. Create your account_map Athena view The account_map Athena view ensures any new accounts are not missed in your dashboard by creating a view off of your CUR table and account_mapping Athena table.\nModify the following SQL query with your table names:\nReplace (database).(tablename) in line 13 with your CUR database and table name\nReplace (database).(tablename) in line 23 with your account_mapping database and table name\nCREATE OR REPLACE VIEW account_map AS SELECT DISTINCT a.line_item_usage_account_id \u0026quot;account_id\u0026quot; , b.account_name , b.business_unit , b.team , b.cost_center FROM (( SELECT DISTINCT line_item_usage_account_id FROM (database).(tablename) ) a LEFT JOIN ( SELECT DISTINCT \u0026quot;lpad\u0026quot;(\u0026quot;account_id\u0026quot;, 12, '0') \u0026quot;account_id\u0026quot; , account_name , business_unit , team , cost_center FROM (database).(tablename) ) b ON (b.account_id = a.line_item_usage_account_id)) Option 3: Leverage your existing AWS Organizations account mapping This option allows your to bring in your AWS Organizations data including OU grouping\nClick here - to create using your AWS Organization Data Complete sections 1-3 of the Level 300: Organization Data CUR Connection Lab Click to navigate to Level 300 Organization CUR connection steps NOTE: Stop at section 3. Utilize Organization Data Source when you reach the Join with Cost and Usage Report Create your account_map view by running the following query.\nReplace (database).(tablename) in line 6 with your account_mapping database and organization table name\nCREATE OR REPLACE VIEW account_map AS SELECT DISTINCT \u0026quot;id\u0026quot; \u0026quot;account_id\u0026quot;, \u0026quot;name\u0026quot; \u0026quot;account_name\u0026quot; FROM (database).(tablename) "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/1_view1/","title":"","tags":[],"description":"","content":"View 1 - Summary View This view is the main dataset used in the your dashboards and is the foundation view for your dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nCreate View We recommend large customers with over 500 linked accounts or more than $10M a month in invoiced cost update the usage date field in the query from \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; for improved performance\nThis view is dependent on having or historically having an RDS database instance run in your organization. If you get the error that the column \u0026lsquo;product_database_engine\u0026rsquo; or product_deployment_option does not exist, then you do not have any RDS database instances running. To make this column show up in the CUR spin up a database in the RDS service, let it run for a couple of minutes and in the next integration of the crawler the column will appear. You can verify this by running the Athena query: SHOW COLUMNS FROM tablename - and replace the tablename accordingly after selecting the correct CUR database in the dropdown on the left side in the Athena view.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired time-frame in row 75\nCREATE OR REPLACE VIEW summary_view AS SELECT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot; , \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot; , \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot; , \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot; , \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot; , \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot; , \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot; , \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , approx_distinct(\u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34 Click here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 78 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 79\nCREATE OR REPLACE VIEW summary_view AS SELECT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot; , \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot; , \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot; , \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot; , \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot; , \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot; , \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot; , \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) -- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; -- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34 Click here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 75\nCREATE OR REPLACE VIEW summary_view AS SELECT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot; , \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot; , \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot; , \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot; , \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot; , \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot; , \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot; , \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot; , sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot; , sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34 Click here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 84 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 85\nCREATE OR REPLACE VIEW summary_view AS SELECT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot; ,CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN '' ELSE '' END \u0026quot;ri_sp_arn\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot; , CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot; , \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot; , \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot; , \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot; , CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot; , \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot; , \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot; , \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot; , \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot; , \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot; , \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot; , \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot; , \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot; , \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot; , sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_unblended_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;) -- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot; , CAST(sum(CASE WHEN (line_item_line_item_type = 'Usage') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) AS double) \u0026quot;ri_sp_trueup\u0026quot; , CAST(sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 0 -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot; -- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) AS Double) \u0026quot;ri_sp_upfront_fees\u0026quot; , sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34 Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - to add your cost allocation tags To add your tags locate the the \u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; in the query you are using and add it after make sure to add a comma between each attribute and then add a group by field for any tags added (i.e. if you add one cost allocation tag you would add ,35 to group by in the bottom of your query)\nExample: Add your project tag by first locating the tag in your CUR attributes for project it will show up as resource_tags_user_projects. You will then find the ,\u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; line in your query and add , resource_tags_user_projects then add ,35 in at the bottom of your query in the group by section. Validate View Confirm the view is working, run the following Athena query and substitute (database) for your CUR database and you should receive 10 rows of data:\nselect * from (database).summary_view limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/2_view2/","title":"","tags":[],"description":"","content":"View 2 - EC2 Running Costs This view will be used to create the EC2 Running Costs dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nCreate View Click here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 17 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;amortized_cost\u0026quot; , \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot; FROM (database).(tablename) WHERE ((((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2')) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) AND (((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')) OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage'))) GROUP BY 1, 2, 3, 4,5,6,7 Click here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;amortized_cost\u0026quot; , \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot; FROM (database).(tablename) WHERE ( (\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') )) GROUP BY 1, 2, 3, 4,5,6,7 Click here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;sum\u0026quot;(CASE -- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;amortized_cost\u0026quot; , \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot; FROM (database).(tablename) WHERE ( (\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage'))) GROUP BY 1, 2, 3, 4,5,6,7 Click here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot; , \u0026quot;sum\u0026quot;(CASE -- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;amortized_cost\u0026quot; , \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot; FROM (database).(tablename) WHERE ( (\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') -- OR --(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') )) GROUP BY 1, 2, 3, 4,5,6,7 Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - to add your cost allocation tags To add your tags locate the the \u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; in the query you are using and add it after make sure to add a comma between each attribute and then add a group by field for any tags added (i.e. if you add one cost allocation tag you would add *,8 to group by in the bottom of your query)\nExample: Add your project tag by first locating the tag in your CUR attributes for project it will show up as resource_tags_user_projects. You will then find the ,\u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; line in your query and add , resource_tags_user_projects then add ,8 in at the bottom of your query in the group by section. Validate View Confirm the view is working, run the following Athena query and substitute (database) for your CUR database and you should receive 10 rows of data:\nselect * from (database).ec2_running_cost limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/3_view3/","title":"","tags":[],"description":"","content":"View 3 - Compute Savings Plan Eligible Spend This view will be used to create the Compute Savings Plan Eligible Spend dashboard page.\nCreate View Click here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN CASE WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END ELSE 0 END) \u0026quot;unblended_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6 Click here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN CASE WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END ELSE 0 END) \u0026quot;unblended_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6 Click here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN CASE WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END ELSE 0 END) \u0026quot;unblended_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6 Click here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN CASE WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot; WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END ELSE 0 END) \u0026quot;unblended_cost\u0026quot; FROM (database).(tablename) WHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6 Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - to add your cost allocation tags To add your tags locate the the \u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; in the query you are using and add it after make sure to add a comma between each attribute and then add a group by field for any tags added (i.e. if you add one cost allocation tag you would add ,7 to group by in the bottom of your query)\nExample: Add your project tag by first locating the tag in your CUR attributes for project it will show up as resource_tags_user_projects. You will then find the ,\u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; line in your query and add , resource_tags_user_projects then add ,7 in at the bottom of your query in the group by section. Validate View Confirm the view is working, run the following Athena query and substitute (database) for your CUR database and you should receive 10 rows of data:\nselect * from (database).compute_savings_plan_eligible_spend limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/4_view4/","title":"","tags":[],"description":"","content":"View 4 - S3 This view will be used to create the S3 dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nCreate View Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%'))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12 Click here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%'))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12 Click here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%'))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12 Click here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT \u0026quot;year\u0026quot; , \u0026quot;month\u0026quot; , \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot; , \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot; , \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot; , \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot; , \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot; , \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot; , \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot; , \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot; , \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot; , \u0026quot;sum\u0026quot;(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot; , \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot; FROM (database).(tablename) WHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%'))) GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12 Adding Cost Allocation Tags Cost Allocation tags can be added to any views. We recommend adding while creating the dashboard to eliminate rework.\nClick here - to add your cost allocation tags To add your tags locate the the \u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; in the query you are using and add it after make sure to add a comma between each attribute and then add a group by field for any tags added (i.e. if you add one cost allocation tag you would add ,13 to group by in the bottom of your query)\nExample: Add your project tag by first locating the tag in your CUR attributes for project it will show up as resource_tags_user_projects. You will then find the ,\u0026ldquo;line_item_usage_account_id\u0026rdquo; \u0026ldquo;linked_account_id\u0026rdquo; line in your query and add , resource_tags_user_projects then add ,13 in at the bottom of your query in the group by section. Confirm the view is working, run the following Athena query and substitute (database) for your CUR database and you should receive 10 rows of data:\nselect * from (database).s3_view limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/kpi_instance_mapping_view/","title":"","tags":[],"description":"","content":"KPI Instance Mapping View This view will be used to create the KPI Instance Mapping view that is used to identify instance families with Graviton and/or AMD options as well as create an adjusted generation mapping. There is only one version of this view and it is not dependent on if you have or do not have Reserved Instances or Savings Plans.\nCreate View Click here to expand the view Use the following SQL query for the KPI Instance Mapping View:\nNo updates needed before running this view\nCREATE OR REPLACE VIEW kpi_instance_mapping AS SELECT * FROM ( VALUES ROW ('a1', 'AmazonEC2', 'Current', 'Graviton', '', '', '', '') , ROW ('c1', 'AmazonEC2', 'Previous', 'Intel', 'c5', 'c5a', 'c6g', '') , ROW ('c3', 'AmazonEC2', 'Previous', 'Intel', 'c5', 'c5a', 'c6g', '') , ROW ('c4', 'AmazonEC2', 'Previous', 'Intel', 'c5', 'c5a', 'c6g', '') , ROW ('c5', 'AmazonEC2', 'Current', 'Intel', '', 'c5a', 'c6g', '') , ROW ('c5a', 'AmazonEC2', 'Current', 'AMD', '', '', 'c6g', '') , ROW ('c5ad', 'AmazonEC2', 'Current', 'AMD', '', '', 'c6gd', '') , ROW ('c5d', 'AmazonEC2', 'Current', 'Intel', '', 'c5ad', 'c6gd', '') , ROW ('c5n', 'AmazonEC2', 'Current', 'Intel', '', '', 'c6gn', '') , ROW ('c6g', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'c5') , ROW ('c6gd', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'c5d') , ROW ('c6gn', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'c5n') , ROW ('cc2', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('cr1', 'AmazonEC2', 'Current', 'Intel', 'r5', '', '', '') , ROW ('d2', 'AmazonEC2', 'Previous', 'Intel', 'd3', '', '', '') , ROW ('d3', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('d3en', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('f1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('g2', 'AmazonEC2', 'Previous', 'Intel', 'g3', '', '', '') , ROW ('g3', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('g3s', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('g4ad', 'AmazonEC2', 'Current', 'AMD', '', '', '', '') , ROW ('g4dn', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('h1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('hs1', 'AmazonEC2', 'Current', 'Intel', 'd3', '', '', '') , ROW ('i2', 'AmazonEC2', 'Previous', 'Intel', 'i3', '', '', '') , ROW ('i3', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('i3en', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('i3p', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('inf1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('m1', 'AmazonEC2', 'Previous', 'Intel', 'm5', 'm5a', 'm6g', '') , ROW ('m2', 'AmazonEC2', 'Previous', 'Intel', 'm5', 'm5a', 'm6g', '') , ROW ('m3', 'AmazonEC2', 'Previous', 'Intel', 'm5', 'm5a', 'm6g', '') , ROW ('m4', 'AmazonEC2', 'Previous', 'Intel', 'm5', 'm5a', 'm6g', '') , ROW ('m5', 'AmazonEC2', 'Current', 'Intel', '', 'm5a', 'm6g', '') , ROW ('m5a', 'AmazonEC2', 'Current', 'AMD', '', '', 'm6g', '') , ROW ('m5ad', 'AmazonEC2', 'Current', 'AMD', '', '', 'm6gd', '') , ROW ('m5d', 'AmazonEC2', 'Current', 'Intel', '', 'm5a', 'm6gd', '') , ROW ('m5dn', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('m5n', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('m5zn', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('m6g', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'm5') , ROW ('m6gd', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'm5d') , ROW ('m6i', 'AmazonEC2', 'Current', 'Intel', '', '', 'm6g', '') , ROW ('mac1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('p2', 'AmazonEC2', 'Previous', 'Intel', 'p3', '', '', '') , ROW ('p3', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('p3dn', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('p4d', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('r3', 'AmazonEC2', 'Previous', 'Intel', 'r5', 'r5a', 'r6g', '') , ROW ('r4', 'AmazonEC2', 'Previous', 'Intel', 'r5', 'r5a', 'r6g', '') , ROW ('r5', 'AmazonEC2', 'Current', 'Intel', '', 'r5a', 'r6g', '') , ROW ('r5a', 'AmazonEC2', 'Current', 'AMD', '', '', 'r6g', '') , ROW ('r5ad', 'AmazonEC2', 'Current', 'AMD', '', '', 'r6gd', '') , ROW ('r5b', 'AmazonEC2', 'Current', 'Intel', '', '', 'r6g', '') , ROW ('r5d', 'AmazonEC2', 'Current', 'Intel', '', 'r5ad', 'r6gd', '') , ROW ('r5dn', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('r5n', 'AmazonEC2', 'Current', 'Intel', '', 'r5', 'r6g', '') , ROW ('r6g', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'r5') , ROW ('r6gd', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'r5') , ROW ('t1', 'AmazonEC2', 'Previous', 'Intel', 't3', 't3a', 't4g', '') , ROW ('t2', 'AmazonEC2', 'Previous', 'Intel', 't3', 't3a', 't4g', '') , ROW ('t3', 'AmazonEC2', 'Current', 'Intel', '', '', 't4g', '') , ROW ('t3a', 'AmazonEC2', 'Current', 'AMD', '', '', 't4g', '') , ROW ('t4g', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 't3') , ROW ('x1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('x1e', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('z1d', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('x2gd', 'AmazonEC2', 'Current', 'Graviton', '', '', '', '') , ROW ('z1d', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('c6i', 'AmazonEC2', 'Current', 'Intel', '', 'c5a', 'c6g', '') , ROW ('dl1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('g5', 'AmazonEC2', 'Current', 'Intel', '', '', 'g5g', '') , ROW ('g5g', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'g5') , ROW ('m6a', 'AmazonEC2', 'Current', 'AMD', '', '', 'm6g', '') , ROW ('mac2', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('r6i', 'AmazonEC2', 'Current', 'Intel', '', 'r5a', 'r6g', '') , ROW ('u-12tb1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('u-18tb1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('u-24tb1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('u-6tb1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('u-9tb1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('vt1', 'AmazonEC2', 'Current', 'Intel', '', '', '', '') , ROW ('c7g', 'AmazonEC2', 'Current', 'Graviton', '', '', '', 'c5') , ROW ('c1', 'AmazonElastiCache', 'Previous', 'Intel', '', '', '', '') , ROW ('m1', 'AmazonElastiCache', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m2', 'AmazonElastiCache', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m3', 'AmazonElastiCache', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m4', 'AmazonElastiCache', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m5', 'AmazonElastiCache', 'Current', 'Intel', '', '', 'm6g', '') , ROW ('m6g', 'AmazonElastiCache', 'Current', 'Graviton', '', '', '', 'm5') , ROW ('r3', 'AmazonElastiCache', 'Previous', 'Intel', 'r5', '', 'r6g', '') , ROW ('r4', 'AmazonElastiCache', 'Previous', 'Intel', 'r5', '', 'r6g', '') , ROW ('r5', 'AmazonElastiCache', 'Current', 'Intel', '', '', 'r6g', '') , ROW ('r6g', 'AmazonElastiCache', 'Current', 'Graviton', '', '', '', 'r5') , ROW ('r6gd', 'AmazonElastiCache', 'Current', 'Graviton', '', '', '', '') , ROW ('t1', 'AmazonElastiCache', 'Previous', 'Intel', 't3', '', 't4g', '') , ROW ('t2', 'AmazonElastiCache', 'Previous', 'Intel', 't3', '', 't4g', '') , ROW ('t4g', 'AmazonElastiCache', 'Current', 'Graviton', '', '', '', 't3') , ROW ('t3', 'AmazonElastiCache', 'Current', 'Intel', '', '', 't4g', '') , ROW ('c4', 'AmazonES', 'Previous', 'Intel', 'c5', '', 'c6g', '') , ROW ('c5', 'AmazonES', 'Current', 'Intel', '', '', 'c6g', '') , ROW ('c6g', 'AmazonES', 'Current', 'Graviton', '', '', '', 'c5') , ROW ('i2', 'AmazonES', 'Previous', 'Intel', 'i3', '', '', '') , ROW ('i3', 'AmazonES', 'Current', 'Intel', '', '', '', '') , ROW ('m3', 'AmazonES', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m4', 'AmazonES', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m5', 'AmazonES', 'Current', 'Intel', '', '', 'm6g', '') , ROW ('m6g', 'AmazonES', 'Current', 'Graviton', '', '', '', 'm5') , ROW ('r3', 'AmazonES', 'Previous', 'Intel', 'r5', '', 'r6g', '') , ROW ('r4', 'AmazonES', 'Previous', 'Intel', 'r5', '', 'r6g', '') , ROW ('r5', 'AmazonES', 'Current', 'Intel', '', '', 'r6g', '') , ROW ('r6g', 'AmazonES', 'Current', 'Graviton', '', '', '', 'r5') , ROW ('t2', 'AmazonES', 'Previous', 'Intel', 't3', '', '', '') , ROW ('t3', 'AmazonES', 'Current', 'Intel', '', '', '', '') , ROW ('r6gd', 'AmazonES', 'Current', 'Graviton', '', '', '', 'r5') , ROW ('cv11', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('m1', 'AmazonRDS', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m2', 'AmazonRDS', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m3', 'AmazonRDS', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m4', 'AmazonRDS', 'Previous', 'Intel', 'm5', '', 'm6g', '') , ROW ('m5', 'AmazonRDS', 'Current', 'Intel', '', '', 'm6g', '') , ROW ('m5d', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('m6g', 'AmazonRDS', 'Current', 'Graviton', '', '', '', 'm5') , ROW ('m6gd', 'AmazonRDS', 'Current', 'Graviton', '', '', '', '') , ROW ('mv11', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('r3', 'AmazonRDS', 'Previous', 'Intel', 'r5', '', 'r6g', '') , ROW ('r4', 'AmazonRDS', 'Previous', 'Intel', 'r5', '', 'r6g', '') , ROW ('r5', 'AmazonRDS', 'Current', 'Intel', '', '', 'r6g', '') , ROW ('r5b', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('r5d', 'AmazonRDS', 'Current', 'Intel', '', '', 'r6gd', '') , ROW ('r6g', 'AmazonRDS', 'Current', 'Graviton', '', '', '', 'r5') , ROW ('r6gd', 'AmazonRDS', 'Current', 'Graviton', '', '', '', 'r5d') , ROW ('rv11', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('t1', 'AmazonRDS', 'Previous', 'Intel', 't3', '', 't4g', '') , ROW ('t2', 'AmazonRDS', 'Previous', 'Intel', 't3', '', 't4g', '') , ROW ('t3', 'AmazonRDS', 'Current', 'Intel', '', '', 't4g', '') , ROW ('t4g', 'AmazonRDS', 'Current', 'Graviton', '', '', '', 't3') , ROW ('x1', 'AmazonRDS', 'Current', 'Intel', '', '', 'x2g', '') , ROW ('x1e', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('z1d', 'AmazonRDS', 'Current', 'Intel', '', '', '', '') , ROW ('x2g', 'AmazonRDS', 'Current', 'Graviton', '', '', '', 'x1') ) ignored_table_name (family, product, generation, instance_processor, latest_intel, latest_amd, latest_graviton, previous_intel) Validate View Confirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from kpi_instance_mapping limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/1_view1/","title":"","tags":[],"description":"","content":"View 1 - Summary View This view will be used to create the main Usage Cost Summary dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nWe recommend large customers with over 500 linked accounts or more than $10M a month in invoiced cost update the usage date field in the query from \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; for improved performance\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired time-frame in row 75\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE\rWHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, approx_distinct(\u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34\rClick here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 78 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 79\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rClick here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 75\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot; ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rClick here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 84 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 85\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r,CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN '' ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, CAST(sum(CASE\rWHEN (line_item_line_item_type = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) AS double) \u0026quot;ri_sp_trueup\u0026quot;\r, CAST(sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) AS Double) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.summary_view\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/2_view2/","title":"","tags":[],"description":"","content":"View 2 - EC2 Running Costs This view will be used to create the EC2 Running Costs dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 17 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2')) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) AND (((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')) OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR\r(\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') -- OR --(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.ec2_running_cost\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/3_view3/","title":"","tags":[],"description":"","content":"View 3 - Compute Savings Plan Eligible Spend This view will be used to create the Compute Savings Plan Eligible Spend dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)) AND (((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) OR ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) OR (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%')) AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%Spot%') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) GROUP BY 1, 2, 3, 4,5,6\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.compute_savings_plan_eligible_spend\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/4_view4/","title":"","tags":[],"description":"","content":"View 4 - S3 This view will be used to create the S3 dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.s3_view\rlimit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/1_view1/","title":"","tags":[],"description":"","content":"View 1 - Summary View This view will be used to create the main Usage Cost Summary dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nWe recommend large customers with over 500 linked accounts or more than $10M a month in invoiced cost update the usage date field in the query from \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; for improved performance\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired time-frame in row 75\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE\rWHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, approx_distinct(\u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34\rClick here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 77 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 78\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rClick here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 75\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rClick here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 84 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 85\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r,CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN '' ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r--\tWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (line_item_line_item_type = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.summary_view\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/2_view2/","title":"","tags":[],"description":"","content":"View 2 - EC2 Running Costs This view will be used to create the EC2 Running Costs dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 17 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2')) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) AND (((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')) OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r--\tWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR --(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.ec2_running_cost\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/3_view3/","title":"","tags":[],"description":"","content":"View 3 - Compute Savings Plan Eligible Spend This view will be used to create the Compute Savings Plan Eligible Spend dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.compute_savings_plan_eligible_spend\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/4_view4/","title":"","tags":[],"description":"","content":"View 4 - S3 This view will be used to create the S3 dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.s3_view\rlimit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/1_view1/","title":"","tags":[],"description":"","content":"View 1 - Summary View This view will be used to create the main Usage Cost Summary dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nWe recommend large customers with over 500 linked accounts or more than $10M a month in invoiced cost update the usage date field in the query from \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; for improved performance\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired time-frame in row 75\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE\rWHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, approx_distinct(\u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34\rClick here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 77 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 78\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rClick here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 74 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 75\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;ELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rClick here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View1:\nUpdate line 84 replace (database).(tablename) with your CUR database and table name\nOptional: Adjust the granularity to monthly, by changing \u0026lsquo;day\u0026rsquo; to \u0026lsquo;month\u0026rsquo; in row 6\nOptional: Adjust the look back from \u0026lsquo;7\u0026rsquo; months to desired timeframe in row 85\nCREATE OR REPLACE VIEW summary_view AS\rSELECT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_invoice_id\u0026quot; \u0026quot;invoice_id\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN 'Running_Usage' -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN 'Running_Usage' WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 'Running_Usage' ELSE 'non_usage' END \u0026quot;charge_category\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END \u0026quot;purchase_option\u0026quot;\r,CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN '' ELSE '' END \u0026quot;ri_sp_arn\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;product_product_name\u0026quot; \u0026quot;product_name\u0026quot;\r, CASE WHEN (\u0026quot;bill_billing_entity\u0026quot; = 'AWS Marketplace' AND \u0026quot;line_item_line_item_type\u0026quot; NOT LIKE '%Discount%') THEN \u0026quot;Product_Product_Name\u0026quot; WHEN (\u0026quot;product_servicecode\u0026quot; = '') THEN \u0026quot;line_item_product_code\u0026quot; ELSE \u0026quot;product_servicecode\u0026quot; END \u0026quot;service\u0026quot;\r, \u0026quot;product_product_family\u0026quot; \u0026quot;product_family\u0026quot;\r, \u0026quot;line_item_usage_type\u0026quot; \u0026quot;usage_type\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;line_item_line_item_description\u0026quot; \u0026quot;item_description\u0026quot;\r, \u0026quot;line_item_availability_zone\u0026quot; \u0026quot;availability_zone\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, '.', 1) ELSE \u0026quot;product_instance_type_family\u0026quot; END \u0026quot;instance_type_family\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 1) ELSE \u0026quot;product_instance_type\u0026quot; END \u0026quot;instance_type\u0026quot;\r, CASE WHEN ((\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage')) THEN \u0026quot;split_part\u0026quot;(\u0026quot;split_part\u0026quot;(\u0026quot;line_item_line_item_description\u0026quot;, ' ', 2), '/', 1) ELSE \u0026quot;product_operating_system\u0026quot; END \u0026quot;platform\u0026quot; , \u0026quot;product_tenancy\u0026quot; \u0026quot;tenancy\u0026quot;\r, \u0026quot;product_physical_processor\u0026quot; \u0026quot;processor\u0026quot;\r, \u0026quot;product_processor_features\u0026quot; \u0026quot;processor_features\u0026quot;\r, \u0026quot;product_database_engine\u0026quot; \u0026quot;database_engine\u0026quot;\r, \u0026quot;product_group\u0026quot; \u0026quot;product_group\u0026quot;\r, \u0026quot;product_from_location\u0026quot; \u0026quot;product_from_location\u0026quot;\r, \u0026quot;product_to_location\u0026quot; \u0026quot;product_to_location\u0026quot;\r, \u0026quot;product_current_generation\u0026quot; \u0026quot;current_generation\u0026quot;\r, \u0026quot;line_item_legal_entity\u0026quot; \u0026quot;legal_entity\u0026quot;\r, \u0026quot;bill_billing_entity\u0026quot; \u0026quot;billing_entity\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;count\u0026quot;(DISTINCT \u0026quot;Line_item_resource_id\u0026quot;) \u0026quot;resource_id_count\u0026quot;\r, sum(CASE -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;line_item_usage_amount\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot; ELSE 0 END) \u0026quot;usage_quantity\u0026quot;\r, sum (\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r--\tWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (\u0026quot;savings_plan_total_commitment_to_date\u0026quot; - \u0026quot;savings_plan_used_commitment\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanNegation') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') THEN \u0026quot;reservation_effective_cost\u0026quot; -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (\u0026quot;reservation_unused_amortized_upfront_fee_for_billing_period\u0026quot; + \u0026quot;reservation_unused_recurring_fee\u0026quot;)\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN 0 ELSE \u0026quot;line_item_unblended_cost\u0026quot; END) \u0026quot;amortized_cost\u0026quot;\r, sum(CASE\rWHEN (line_item_line_item_type = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') THEN (-\u0026quot;savings_plan_amortized_upfront_commitment_for_billing_period\u0026quot;) -- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') THEN (-\u0026quot;reservation_amortized_upfront_fee_for_billing_period\u0026quot;) ELSE 0 END) \u0026quot;ri_sp_trueup\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN 0\r-- WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanUpfrontFee') THEN \u0026quot;line_item_unblended_cost\u0026quot;\r-- WHEN ((\u0026quot;line_item_line_item_type\u0026quot; = 'Fee') AND (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;ri_sp_upfront_fees\u0026quot;\r, sum(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'SavingsPlanNegation') THEN \u0026quot;pricing_public_on_demand_cost\u0026quot; ELSE 0 END) \u0026quot;public_cost\u0026quot; FROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '7' MONTH)) AND (CAST(\u0026quot;concat\u0026quot;(\u0026quot;year\u0026quot;, '-', \u0026quot;month\u0026quot;, '-01') AS date) \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_date) - INTERVAL '7' MONTH)))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,33,34\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.summary_view\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/2_view2/","title":"","tags":[],"description":"","content":"View 2 - EC2 Running Costs This view will be used to create the EC2 Running Costs dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 17 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2')) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) AND (((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')) OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you have Savings Plans, but do not have Reserved Instances The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r--\tWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you have Reserved Instances, but do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')))\rGROUP BY 1, 2, 3, 4,5,6,7\rClick here - if you do not have Reserved Instances, and do not have Savings Plans The query is the same as the first query, except some of lines have been commented out. If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View2 - EC2_Running_Cost:\nUpdate line 21 replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ec2_running_cost\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, (CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'SavingsPlan' -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN 'Reserved' WHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%') THEN 'Spot' ELSE 'OnDemand' END) \u0026quot;purchase_option\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage' THEN \u0026quot;savings_plan_savings_plan_effective_cost\u0026quot;\r-- WHEN \u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage' THEN \u0026quot;reservation_effective_cost\u0026quot;\rWHEN \u0026quot;line_item_line_item_type\u0026quot; = 'Usage' THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0 END) \u0026quot;amortized_cost\u0026quot;\r, \u0026quot;round\u0026quot;(\u0026quot;sum\u0026quot;(\u0026quot;line_item_usage_amount\u0026quot;), 2) \u0026quot;usage_quantity\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%') AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%') AND ((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage') OR --(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage')\r))\rGROUP BY 1, 2, 3, 4,5,6,7\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.ec2_running_cost\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/3_view3/","title":"","tags":[],"description":"","content":"View 3 - Compute Savings Plan Eligible Spend This view will be used to create the Compute Savings Plan Eligible Spend dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View3 - Compute Savings PlaneEligible spend:\nUpdate line 23, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;compute_savings_plan_eligible_spend\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('hour', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN ((((\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') AND (NOT (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Spot%'))) AND (\u0026quot;product_servicecode\u0026quot; \u0026lt;\u0026gt; 'AWSDataTransfer')) AND (\u0026quot;line_item_usage_type\u0026quot; NOT LIKE '%DataXfer%')) THEN\rCASE\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AmazonEC2') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'ElasticMapReduce') AND (\u0026quot;line_item_operation\u0026quot; LIKE '%RunInstances%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-GB-Second%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN ((\u0026quot;line_item_product_code\u0026quot; = 'AWSLambda') AND (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Lambda-Provisioned-Concurrency%')) THEN \u0026quot;line_item_unblended_cost\u0026quot;\rWHEN (\u0026quot;line_item_usage_type\u0026quot; LIKE '%Fargate%') THEN \u0026quot;line_item_unblended_cost\u0026quot;\rELSE 0\rEND\rELSE 0 END) \u0026quot;unblended_cost\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '1' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY)))\rGROUP BY 1, 2, 3, 4,5,6\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.compute_savings_plan_eligible_spend\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/4_view4/","title":"","tags":[],"description":"","content":"View 4 - S3 This view will be used to create the S3 dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View4 - S3:\nUpdate line 22, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;s3_view\u0026quot; AS SELECT DISTINCT\r\u0026quot;year\u0026quot;\r, \u0026quot;month\u0026quot;\r, \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period\u0026quot;\r, \u0026quot;date_trunc\u0026quot;('day', \u0026quot;line_item_usage_start_date\u0026quot;) \u0026quot;usage_date\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id\u0026quot;\r, \u0026quot;line_item_usage_account_id\u0026quot; \u0026quot;linked_account_id\u0026quot;\r, \u0026quot;line_item_resource_id\u0026quot; \u0026quot;resource_id\u0026quot;\r, \u0026quot;line_item_product_code\u0026quot; \u0026quot;product_code\u0026quot;\r, \u0026quot;line_item_operation\u0026quot; \u0026quot;operation\u0026quot;\r, \u0026quot;product_region\u0026quot; \u0026quot;region\u0026quot;\r, \u0026quot;line_item_line_item_type\u0026quot; \u0026quot;charge_type\u0026quot;\r, \u0026quot;pricing_unit\u0026quot; \u0026quot;pricing_unit\u0026quot;\r, \u0026quot;sum\u0026quot;(CASE\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN \u0026quot;line_item_usage_amount\u0026quot;\rELSE 0\rEND) \u0026quot;usage_quantity\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;line_item_unblended_cost\u0026quot;) \u0026quot;unblended_cost\u0026quot;\r, \u0026quot;sum\u0026quot;(\u0026quot;pricing_public_on_demand_cost\u0026quot;) \u0026quot;public_cost\u0026quot;\rFROM (ADD YOUR CUR TABLE NAME)\rWHERE ((((\u0026quot;bill_billing_period_start_date\u0026quot; \u0026gt;= (\u0026quot;date_trunc\u0026quot;('month', current_timestamp) - INTERVAL '3' MONTH)) AND (\u0026quot;line_item_usage_start_date\u0026quot; \u0026lt; (\u0026quot;date_trunc\u0026quot;('day', current_timestamp) - INTERVAL '1' DAY))) AND (\u0026quot;line_item_operation\u0026quot; LIKE '%Storage%')) AND ((\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonGlacier%') OR (\u0026quot;line_item_product_code\u0026quot; LIKE '%AmazonS3%')))\rGROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.s3_view\rlimit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/code/iam_policy/","title":"","tags":[],"description":"","content":"Users will require the following access to complete this lab. Edit the policy below before implementation, replace (Account ID) with the required account ID from the account they will work in. Ensure you remove this policy after the lab is completed.\nThis Policy is only required to complete this lab. It must be removed from the users and delted once the lab is complete.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:ListPolicies\u0026quot;, \u0026quot;iam:GetPolicyVersion\u0026quot;, \u0026quot;iam:CreateGroup\u0026quot;, \u0026quot;iam:GetPolicy\u0026quot;, \u0026quot;iam:DeletePolicy\u0026quot;, \u0026quot;iam:DetachGroupPolicy\u0026quot;, \u0026quot;iam:ListGroupPolicies\u0026quot;, \u0026quot;iam:AttachUserPolicy\u0026quot;, \u0026quot;iam:CreateUser\u0026quot;, \u0026quot;iam:GetGroup\u0026quot;, \u0026quot;iam:CreatePolicy\u0026quot;, \u0026quot;iam:CreateLoginProfile\u0026quot;, \u0026quot;iam:AddUserToGroup\u0026quot;, \u0026quot;iam:ListPolicyVersions\u0026quot;, \u0026quot;iam:AttachGroupPolicy\u0026quot;, \u0026quot;iam:ListUsers\u0026quot;, \u0026quot;iam:ListAttachedGroupPolicies\u0026quot;, \u0026quot;iam:ListGroups\u0026quot;, \u0026quot;iam:GetGroupPolicy\u0026quot;, \u0026quot;iam:CreatePolicyVersion\u0026quot;, \u0026quot;iam:DeletePolicyVersion\u0026quot;, \u0026quot;iam:GetLoginProfile\u0026quot;, \u0026quot;iam:GetAccountPasswordPolicy\u0026quot;, \u0026quot;iam:DeleteLoginProfile\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::(Account ID):user/TestUser1\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::(Account ID):group/costtest\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:*\u0026quot;, \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:iam::(Account ID):policy/RegionRestrict\u0026quot;, \u0026quot;arn:aws:iam::(Account ID):policy/EC2EBS_Restrict\u0026quot;, \u0026quot;arn:aws:iam::(Account ID):policy/EC2_FamilyRestrict\u0026quot; ] }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:DeleteSecurityGroup\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:security-group/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ec2:DeleteVolume\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:volume/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:ModifyVolume\u0026quot;, \u0026quot;ec2:ModifyVolumeAttribute\u0026quot;, \u0026quot;ec2:DescribeVolumeStatus\u0026quot;, \u0026quot;ec2:DescribeVolumes\u0026quot;, \u0026quot;ec2:DescribeVolumesModifications\u0026quot;, \u0026quot;ec2:DescribeVolumeAttribute\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/5_view5/","title":"","tags":[],"description":"","content":"View 5 - RI SP Mapping This view will be used to create the RI SP Mapping dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nCreate View Click here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View5 - RI SP Mapping:\nUpdate line 21 to replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;ELSE '' END \u0026quot;ri_sp_term\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\tELSE '' END \u0026quot;ri_sp_payment\u0026quot;\tFROM (database).(tablename) WHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')) Click here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate line 26, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot; ,CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp) -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot; ELSE '' END \u0026quot;ri_sp_term\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot; , CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot; ELSE '' END \u0026quot;ri_sp_payment\u0026quot;\tFROM (database).(tablename) WHERE ( -- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') ) Click here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate line 26, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot; ELSE '' END \u0026quot;ri_sp_term\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot; ELSE '' END \u0026quot;ri_sp_payment\u0026quot;\tFROM (database).(tablename) WHERE ( (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') ) Click here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate line 31, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT \u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot; , \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' ' ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot; , CAST(CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp) -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' ' ELSE NULL END AS timestamp) \u0026quot;ri_sp_end_date\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' ' ELSE '' END \u0026quot;ri_sp_term\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' ' ELSE '' END \u0026quot;ri_sp_offering\u0026quot; , CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' ' ELSE '' END \u0026quot;ri_sp_payment\u0026quot; FROM (database).(tablename) WHERE ( (\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'Usage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee') ) Validate View Confirm the view is working, run the following Athena query and substitute (database) for your CUR database and you should receive 10 rows of data:\nselect * from (database).ri_sp_mapping limit 10 "},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/5_view5/","title":"","tags":[],"description":"","content":"View 5 - RI SP Mapping This view will be used to create the RI SP Mapping dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 23 and 44, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee'))\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;ELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\tELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage'))\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 25 and 54, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r,CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 25 and 54, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 27 and 62, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CAST(CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '' AND \u0026quot;reservation_end_time\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE NULL END AS timestamp) \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ' '\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; \u0026lt;\u0026gt; 'Usage') -- OR\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.ri_sp_mapping\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/5_view5/","title":"","tags":[],"description":"","content":"View 5 - RI SP Mapping This view will be used to create the RI SP Mapping dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 23 and 44, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee'))\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;ELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\tELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage'))\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 25 and 54, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 25 and 54, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 27 and 60, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE --\tWHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r--\tWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN NULL\rELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\r-- WHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r-- )\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\r-- WHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r-- )\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.ri_sp_mapping\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/5_view5/","title":"","tags":[],"description":"","content":"View 5 - RI SP Mapping This view will be used to create the RI SP Mapping dashboard page. Use one of the following queries depending on whether you have Reserved Instances, or Savings Plans.\nClick here - if you have both Savings Plans and Reserved Instances Modify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 23 and 44, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee'))\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;ELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;ELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\tELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE ((\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage'))\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you have Savings Plans, but do not have Reserved Instances If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 25 and 54, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r-- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you have Reserved Instances, but do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 25 and 54, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\rWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) ELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r)\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; ELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\rWHERE (\r(\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r)\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rClick here - if you do not have Reserved Instances, and do not have Savings Plans If your usage changes you can delete and recreate the required view with Savings Plans or Reserved Instance usage.\nModify the following SQL query for View5 - RI SP Mapping:\nUpdate lines 27 and 60, replace (database).(tablename) with your CUR database and table name\nCREATE OR REPLACE VIEW \u0026quot;ri_sp_mapping\u0026quot; AS SELECT DISTINCT\r\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;\r, \u0026quot;a\u0026quot;.\u0026quot;ri_sp_end_date\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_term\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_offering\u0026quot;\r, \u0026quot;b\u0026quot;.\u0026quot;ri_sp_payment\u0026quot;\rFROM\r((\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE --\tWHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;savings_plan_end_time\u0026quot;) AS timestamp)\r--\tWHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN CAST(from_iso8601_timestamp(\u0026quot;reservation_end_time\u0026quot;) AS timestamp) WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN NULL\rELSE NULL END \u0026quot;ri_sp_end_date\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\r-- WHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'RIFee') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanRecurringFee')\r-- )\r) a\rLEFT JOIN (\rSELECT DISTINCT\r\u0026quot;bill_billing_period_start_date\u0026quot; \u0026quot;billing_period_mapping\u0026quot;\r, \u0026quot;bill_payer_account_id\u0026quot; \u0026quot;payer_account_id_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;reservation_reservation_a_r_n\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_arn_mapping\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_purchase_term\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_lease_contract_length\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_term\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_offering_type\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_offering_class\u0026quot; WHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_offering\u0026quot;\r, CASE -- WHEN (\u0026quot;savings_plan_savings_plan_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;savings_plan_payment_option\u0026quot; -- WHEN (\u0026quot;reservation_reservation_a_r_n\u0026quot; \u0026lt;\u0026gt; '') THEN \u0026quot;pricing_purchase_option\u0026quot;\rWHEN (\u0026quot;line_item_line_item_type\u0026quot; = 'Usage') THEN ''\rELSE '' END \u0026quot;ri_sp_Payment\u0026quot;\rFROM\r(ADD YOUR CUR TABLE NAME)\r-- WHERE (\r-- (\u0026quot;line_item_line_item_type\u0026quot; = 'DiscountedUsage') -- OR -- (\u0026quot;line_item_line_item_type\u0026quot; = 'SavingsPlanCoveredUsage')\r-- )\r) b ON ((\u0026quot;a\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;ri_sp_arn_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;billing_period_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;billing_period_mapping\u0026quot;) AND (\u0026quot;a\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot; = \u0026quot;b\u0026quot;.\u0026quot;payer_account_id_mapping\u0026quot;)))\rConfirm the view is working, run the following Athena query and you should receive 10 rows of data:\nselect * from costmaster.ri_sp_mapping\rlimit 10\r"},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/cost-usage-report-dashboards/dashboards/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_cloud_intelligence/quicksight/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_enterprise_dashboards/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_optimization_data_collection/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_organization_data_cur_connection/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/200_labs/200_2_cost_and_usage_governance/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/operational-excellence/","title":"Operational Excellence","tags":[],"description":"","content":"Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.\nLabs 100 Labs 100 - Inventory and Patch Management 100 - Dependency Monitoring 200 Labs 200 - Automating operations with Playbooks and Runbooks "},{"uri":"https://wellarchitectedlabs.com/security/","title":"Security","tags":[],"description":"","content":"Introduction The security labs are documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is foundational, 200 is intermediate, 300 is advanced, and 400 is expert. Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn.\nFor more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper in PDF or online . Also check out https://awssecworkshops.com/ for hands-on workshops, and AWS Training and Certification Learning Library for official security training options.\nLabs \u0026amp; Quests 100 Level Foundational Labs AWS Account Setup and Root User Creating your first Identity and Access Management User, Group, Role CloudFront with S3 Bucket Origin Enable Security Hub Create a Data Bunker Account 200 Level Intermediate Labs Automated Deployment of Detective Controls Automated Deployment of EC2 Web Application Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Automated Deployment of Web Application Firewall Level 200: Automated IAM User Cleanup Level 200: Basic EC2 Web Application Firewall Protection Level 200: AWS Certificate Manager Request Public Certificate Level 200: CloudFront for Web Application Level 200: CloudFront with WAF Protection Level 200: Remote Configuration, Installation, and Viewing of CloudWatch logs 300 Level Advanced Labs Level 300: Multilayered API Security with Cognito and WAF Level 300: Autonomous Monitoring Of Cryptographic Activity With KMS Level 300: Autonomous Patching With EC2 Image Builder And Systems Manager Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response Playbook with Jupyter - AWS IAM Level 300: Incident Response with AWS Console and CLI Level 300: Lambda Cross Account Using Bucket Policy Level 300: Lambda Cross Account IAM Role Assumption Level 300: VPC Flow Logs Analysis Dashboard Quests Quest: Loft - Introduction to Security Quest: Simplest Security Steps Quest: Quick Steps to Security Success Quest: AWS Incident Response - Credential Misuse Quest: Reviewing Security Essential Best Practice - Well-Architected Webinar Quest: AWS Incident Response Day Quest: re:Invent 2020 - Automate The Well-Architected Way With WeInvest Quest: AWS Security Best Practices Workshop Quest: AWS Security Best Practices Day Quest: Managing Credentials \u0026amp; Authentication Quest: Control Human Access Quest: Control Programmatic Access Quest: Detect \u0026amp; Investigate Events Quest: Defend Against New Threats Quest: Protect Networks Quest: Protect Compute Quest: Classify Data Quest: Protect Data at Rest Quest: Protect Data in Transit Quest: Incident Response "},{"uri":"https://wellarchitectedlabs.com/reliability/","title":"Reliability","tags":[],"description":"","content":"\nThese hands-on labs will teach you how to implement reliable workloads using AWS.\nReliability is the ability of a workload to perform its intended function correctly and consistently when it’s expected to. This includes the ability to operate and test the workload through its total lifecycle Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues. Reliability depends on multiple factors, of which resiliency is one the most impactful. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper .\nReliability Labs Reliability Labs by Level 100 Labs Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation 200 Labs Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3) Level 200: Deploy and Update CloudFormation Level 200: Testing Backup and Restore of Data Level 200: Testing for Resiliency of EC2 instances Level 200: Backup and Restore with Failback for Analytics Workload 300 Labs Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability Level 300: Testing for Resiliency of EC2, RDS, and AZ Level 300: Fault Isolation with Shuffle Sharding Disaster Recovery Introduction Module 1: Backup and Restore Module 2: Pilot Light Module 3: Warm Standby Module 4: Hot Standby AWS Elastic Disaster Recovery Reliability Labs by tag (topic) implement_change Level 100: Deploy a Reliable Multi-tier Infrastructure using CloudFormation Level 200: Deploy and Update CloudFormation data_backup Level 200: Testing Backup and Restore of Data Level 200: Implementing Bi-Directional Cross-Region Replication (CRR) for Amazon Simple Storage Service (Amazon S3) test_resiliency Level 300: Testing for Resiliency of EC2, RDS, and AZ Level 200: Testing for Resiliency of EC2 instances mitigate_failure Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability Level 300: Fault Isolation with Shuffle Sharding "},{"uri":"https://wellarchitectedlabs.com/performance-efficiency/","title":"Performance Efficiency","tags":[],"description":"","content":"Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Performance Efficiency on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Performance Efficiency whitepaper.\nLabs 100 Labs Level 100: Monitoring with CloudWatch Dashboards Level 100: Calculating differences in clock source Level 100: Monitoring Windows EC2 instance with CloudWatch Dashboards Level 100: Monitoring an Amazon Linux EC2 instance with CloudWatch Dashboards "},{"uri":"https://wellarchitectedlabs.com/cost/","title":"Cost Optimization","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Cost Optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Cost Optimization whitepaper.\nLabs Fundamentals Expenditure Awareness Cost Effective Resources 100 Labs Level 100: AWS Account Setup: Lab Guide Level 100: Cost and Usage Governance Level 100: Pricing Models Level 100: Cost and Usage Analysis Level 100: Cost Visualization Level 100: Rightsizing Recommendations Level 100: Cost Estimation Level 100: Goals and Targets Level 100: Tag Policies 200 Labs Level 200: Cost and Usage Governance Level 200: Pricing Models Level 200: Cost and Usage Analysis Level 200: Cost Visualization Level 200: Rightsizing with Compute Optimizer Level 200: Pricing Model Analysis Level 200: Cloud Intelligence Dashboards Level 200: Workload Efficiency Level 200: Licensing Level 200: Cost Journey 300 Labs Level 300: Automated Athena CUR Query and E-mail Delivery Level 300: Automated CUR Updates and Ingestion Level 300: AWS CUR Query Library Level 300: Splitting the CUR and Sharing Access Level 300: Optimization Data Collection Level 300: Organization Data CUR Connection "},{"uri":"https://wellarchitectedlabs.com/sustainability/","title":"Sustainability","tags":[],"description":"","content":"Introduction Environmental sustainability is a shared responsibility between customers and AWS. AWS is responsible for sustainability of the cloud; delivering efficient, shared infrastructure, water stewardship and sourcing renewable power. Customers are responsible for sustainability in the cloud, optimizing workloads and resource utilization.\nThe labs in this workshop focus on sustainability in the cloud.\nSustainability in the cloud Sustainability in the cloud is a continuous effort focused primarily on energy reduction and efficiency across all components of a workload by achieving the maximum benefit from the resources provisioned and minimizing the total resources required. This effort can range from the initial selection of an efficient programming language, adoption of modern algorithms, use of efficient data storage techniques, deploying to correctly sized and efficient compute infrastructure, and minimizing requirements for high-powered end-user hardware.\nDesign principles for sustainability in the cloud Apply these design principles when architecting your cloud workloads to maximize sustainability and minimize impact.\nUnderstand your impact Establish your sustainability goals Maximise utilization Anticipate and adopt new, more efficient hardware and software offerings Use managed services Reduce the downstream impact of your cloud workloads For more information about sustainability at AWS, please visit Sustainability in the Cloud and read the AWS Well-Architected Sustainability Pillar whitepaper .\nLabs 200 Level Labs Level 200: Optimize Amazon EC2 using Amazon CloudWatch and AWS Compute Optimizer 300 Level Advanced Labs Level 300: Optimize Data Pattern using Amazon Redshift Data Sharing Level 300: Turning Cost \u0026amp; Usage Reports into Efficiency Reports "},{"uri":"https://wellarchitectedlabs.com/well-architectedtool/","title":"Well-Architected Tool","tags":[],"description":"","content":" Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.\nFor more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation .\nLabs 100 Labs Level 100: Walkthrough of the Well-Architected Tool 200 Labs Level 200: Integration with AWS Compute Optimizer and AWS Trusted Advisor Level 200: Using AWSCLI to Manage WA Reviews Level 200: Manage Workload Risks with OpsCenter 300 Labs Level 300: Using custom resource in AWS CloudFormation to create and update Well-Architected Reviews Level 300: Build custom reports of AWS Well-Architected Reviews Helpful Resources Copy a workload from one account or region to another Generate a custom WellArchitected Framework HTML Report Export Well-Architected content to XLSX Spreadsheet Export and Import Workload Utility "},{"uri":"https://wellarchitectedlabs.com/well-architectedpartners/","title":"Well-Architected Partners","tags":[],"description":"","content":"Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.\nThe Well-Architected Partner Program has enabled our best practice methodology to scale effectively across thousands of worldwide customers. For more information on the partner program, check out the following documentation:\nAWS Well-Architected Partner Program .\nLabs 100 Labs Level 100: Automating Serverless Best Practices with Dashbird "},{"uri":"https://wellarchitectedlabs.com/contributing/","title":"Contributing Guide","tags":[],"description":"","content":"Introduction The Well-Architected labs are open source and we welcome feedback and contributions from the community.\nPlease read through this contributing guide before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.\nThe contribution process for both bugs and new features/labs is:\nMake the team aware bug or new feature via a GitHub issue Fork the GitHub repo Update or Create a Lab Submit a Pull Request with your changes from your Fork Authors Nathan Besh. Cost-Lead, Well-Architected, AWS Alee Whitman, Commercial Architect (AWS OPTICS) Duncan Bell, Geo Solutions Architect, Well-Architected, AWS "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/tools_for_powershell/","title":"Install AWS Tools for Powershell","tags":[],"description":"","content":"The AWS documentation for AWS Tools for Powershell is here: https://aws.amazon.com/powershell/ If you run into trouble using the instructions below, please consult the AWS documentation.\nOpen Windows PowerShell using Run as administrator\nIf you have not done so already, setup your AWS credentials\nConfigure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the SessionToken flag and value as shown below in brackets (omit the brackets when running the command): Set-AWSCredentials -AccessKey \u0026lt;Your access key\u0026gt; -SecretKey \u0026lt;Your secret key\u0026gt; ` [ -SessionToken \u0026lt;your session key\u0026gt; ] -StoreAs \u0026lt;SomeProfileName\u0026gt; Initialize-AWSDefaults -ProfileName \u0026lt;SomeProfileName\u0026gt; -Region us-east-2 Run the following commands to install these respective packages using PowerShellGet. It is recommended you run these commands one by one. They will each require a response - you should respond [A] Yes to All.\nInstall-Module -Name AWS.Tools.Common Install-Module -Name AWS.Tools.EC2 Install-Module -Name AWS.Tools.RDS Install-Module -Name AWS.Tools.AutoScaling Troubleshooting\nIf you get this error: Install-Module : Administrator rights are required to install modules..., then make sure you are running PowerShell as Administrator.\nFor other problems with authorization to access your AWS account, consult AWS Tools for Windows PowerShell - Using AWS Credentials Return to the Lab Guide "},{"uri":"https://wellarchitectedlabs.com/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/test_resiliency/","title":"test_resiliency","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/workload_risk_management/","title":"workload_risk_management","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/data_backup/","title":"data_backup","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/amazon-linux/","title":"Amazon Linux","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/cloudwatch/","title":"CloudWatch","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/cloudwatch-dashboard/","title":"CloudWatch Dashboard","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/ec2/","title":"EC2","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/linux/","title":"Linux","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/windows/","title":"Windows","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/windows-server/","title":"Windows Server","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/tags/implement_change/","title":"implement_change","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/cfn_parameters/","title":"CloudFormation Parameters","tags":[],"description":"","content":"All entries are Case-Sensitive\nsingle region stack Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip multi region stack Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true DMSLambdaKey Reliability/DMSLambda.zip LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip RDSRRLambdaKey Reliability/RDSReadReplicaLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/self_aws_account/","title":"Creating new AWS credentials for your AWS account","tags":[],"description":"","content":"Use these instructions to get a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY which you will need for the workshop\nIf you are using your own AWS account\nThese instructions are for you. Use this guide if you are running the workshop on your own, or with at an event using your own AWS account you have brought with you If you are attending an in-person workshop and were provided with an AWS account by the instructor\nSTOP \u0026ndash; Follow the If you are attending an in-person workshop and were provided with an AWS account by the instructor instructions here instead Create new AWS credentials for an IAM User you already control Sign in to the AWS Management Console as a IAM user who has IAM management permissions and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users.\nChoose the name of the user whose access keys you want to manage.\nSelect the Permissions tab of this user and confirm that they have either PowerUserAccess or AdministratorAccess policy attached. If not, attach the PowerUserAccess policy using the Add permissions button.\nSelect the Security credentials tab.\nChoose Create access key. Then choose Download .csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close.\nCreate a new IAM User for use in the lab Use the instructions only if you cannot Create new AWS credentials for an IAM User you already control. If you have already obtained a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY using the preceding instructions then STOP and Click here to return to the Lab Guide Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users and then choose Add users.\nType the user name for the new user. if you wish you can choose rel300-chaos-workshop-user\nSelect Access key - Programmatic access.\nIncluding access to the AWS Management Console is optional. To do this select Password - AWS Management Console access Choose Next: Permissions\nselect Attach existing policies directly\nIn the search box type PowerUserAccess tick the check box next to PowerUserAccess Choose Next: Tags\nChoose Next: Review\nChoose Create User\nIMPORTANT: Choose Download.csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close.\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/multi_region_stack_deletion/","title":"Delete workshop CloudFormation stacks - Multi region deployment","tags":[],"description":"","content":" Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal The AWS Console does not let you select multiple stacks for deletion. To simultaneously delete stacks, individually select one stack at a time and click the Delete button. Helpful hint: have the AWS CloudFormation console for each region open in separate tabs CloudFormation console for Ohio CloudFormation console for Oregon Order CloudFormation stack Region 1 DMSforResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Ohio 2 WebServersforResiliencyTesting Ohio 2 MySQLforResiliencyTesting Ohio 2 WebServersforResiliencyTesting Oregon 2 MySQLforResiliencyTesting Oregon 3 ResiliencyVPC Ohio 3 ResiliencyVPC Oregon 3 DeployResiliencyWorkshop Ohio "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/s3_with_aws_cli/","title":"Disable All Public Read Access to an S3 Bucket using AWS CLI","tags":[],"description":"","content":"Disable read access to S3 bucket This command will disable public read from an entire bucket. If you want to only disable public read from one object, use the AWS Console instructions\nIf your S3 bucket is in a different aWS account, you will need to provide credentials for that account first.\naws ssm start-automation-execution --document-name AWS-DisableS3BucketPublicReadWrite --parameters \u0026quot;{\\\u0026quot;S3BucketName\\\u0026quot;: [\\\u0026quot;\u0026lt;bucket-name\u0026gt;\\\u0026quot;]}\u0026quot; Return to the Lab Guide , but keep this page open if you want to re-enable public read access to the bucket after testing.\nRe-enable access (after testing) using the S3 console This requires using the S3 console. Go to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the \u0026ldquo;Permissions\u0026rdquo; tab Click Edit (upper-right) Un-check all the boxes Click Save You are asked to type \u0026ldquo;confirm\u0026rdquo; - this is a security feature to ensure you truly intend this bucket to allow public access. Click here to return to the Lab Guide "},{"uri":"https://wellarchitectedlabs.com/security/200_labs/200_remote_configuration_installation_and_viewing_cloudwatch_logs/","title":"Level 200: Remote Configuration, Installation, and Viewing of CloudWatch logs","tags":[],"description":"","content":"Authors Pavan Baloo, Solutions Architect Intern, Well-Architected Introduction Most applications require monitoring services that make up the workload to understand the workload state and performance. One way of collecting this data is through log files generated by the application and underlying services. Collecting and analyzing log files improves your security posture by creating a record of activity or audit trail in your workload, enabling you to detect and investigate potential threats.\nManually configuring logging on each instance is tedious and becomes difficult at scale. It increases your risk of human error and unintended access from accessing instances directly with a protocol like SSH. Manually processing the collected data is difficult to scale with large volumes of data.\nUtilizing AWS services such as AWS Systems Manager, Amazon CloudWatch, Amazon Simple Storage Service (S3), Amazon Athena, and Amazon QuickSight, you can collect and store logs without having to directly access the instance, or accessing data directly. You minimize your threat surface area by removing SSH access on your instance and improve your threat detection by collecting valuable log data.\nThis lab illustrates the following Well-Architected Security Best Practices:\n“Configure service and application logging”: You will configure the CloudWatch agent on an EC2 instance. This enables you to collect logs from the instance used to host your application, such as Apache Web Server logs, SSH logs, boot logs, and more. “Configure services and resources centrally”: You will centrally configure your CloudWatch log agent by storing the configuration file in Systems Manager Parameter Store. Parameter Store enables you to maintain consistent, reusable configuration data. “Analyze logs centrally”: You will analyze logs centrally in this lab in two ways. Using the CloudWatch console, you can view all of your raw log data in one location. Through QuickSight, you can create visualizations from your logs that can be shared with others for central viewing of key data. “Enable people to perform actions at a distance”: You will use Systems Manager Run Command to install and start the CloudWatch agent on your EC2 instance. You will perform these actions “at a distance” through Run Command, as you will not need to SSH directly into the instance to perform these tasks. “Reduce attack surface”: Run Command removes the necessity to directly SSH into the EC2 instance. Because of this, you can close the SSH access port on your instance, reducing the attack surface of the workload. In the lab, you will deploy an EC2 instance with Apache and PHP installed. The web server will host a very simple website. You will configure a CloudWatch Agent on the instance via Amazon Systems Manager (SSM). This agent will collect log files from services running on the EC2 instance, such as Apache access and error logs, yum logs, SSH logs, and CloudWatch agent logs. These logs are exported from the EC2 instance to the CloudWatch logs service for centralized storage. You will export these logs to an S3 bucket for long term storage and archival. These logs will then be queried via Athena, so people are kept away from accessing the log files directly. This data will be visually represented in a QuickSight dashboard.\nPrerequisites I have access to an AWS Account to use for testing, from which I can deploy EC2 instances, create S3 Buckets, access and export CloudWatch Logs, run Athena queries, and use QuickSight. I am operating in a region in which I can use Amazon EC2, Amazon S3, Amazon CloudWatch, Amazon Athena, AWS Systems Manager, and Amazon QuickSight. To see if these services are available in your region, click to view the service availability page . Files Used Lab CloudFormation Template CloudWatch Agent Configuration X Lab complete! Now that you have completed this lab, make sure to update your Well-Architected review if you have implemented these changes in your workload.\nClick here to access the Well-Architected Tool Start Lab Steps Deploy the CloudFormation Stack Install the CloudWatch Agent Store the CloudWatch Config File in Parameter Store Start the CloudWatch Agent Generate Logs View your CloudWatch Logs Export Logs to S3 Query logs from S3 using Athena Create a QuickSight Visualization Lab Recap Lab Teardown "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/multi_region_state_machine/","title":"Multi Region State Machine","tags":[],"description":"","content":"\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/multi_region_event_data/","title":"New Execution Input for **multi region** Deployment","tags":[],"description":"","content":" On the \u0026ldquo;New execution\u0026rdquo; dialog, for \u0026ldquo;Enter an execution name\u0026rdquo; enter BuildResiliency\nThen for \u0026ldquo;Input\u0026rdquo; enter JSON that will be used to supply parameter values to the Lambdas in the workflow.\nmulti region uses the following values\n{ \u0026quot;region1\u0026quot;: { \u0026quot;log_level\u0026quot;: \u0026quot;DEBUG\u0026quot;, \u0026quot;region_name\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;secondary_region_name\u0026quot;: \u0026quot;us-west-2\u0026quot;, \u0026quot;cfn_region\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;folder\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;workshop\u0026quot;: \u0026quot;300-ResiliencyofEC2RDSandS3\u0026quot;, \u0026quot;boot_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;boot_prefix\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;websiteimage\u0026quot; : \u0026quot;https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg\u0026quot; }, \u0026quot;region2\u0026quot;: { \u0026quot;log_level\u0026quot;: \u0026quot;DEBUG\u0026quot;, \u0026quot;region_name\u0026quot;: \u0026quot;us-west-2\u0026quot;, \u0026quot;secondary_region_name\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_region\u0026quot;: \u0026quot;us-east-2\u0026quot;, \u0026quot;cfn_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;folder\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;workshop\u0026quot;: \u0026quot;300-ResiliencyofEC2RDSandS3\u0026quot;, \u0026quot;boot_bucket\u0026quot;: \u0026quot;aws-well-architected-labs-ohio\u0026quot;, \u0026quot;boot_prefix\u0026quot;: \u0026quot;Reliability/\u0026quot;, \u0026quot;websiteimage\u0026quot; : \u0026quot;https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/images/Cirque_of_the_Towers.jpg\u0026quot; } } Note: for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/service_linked_roles/","title":"Service-Linked Roles","tags":[],"description":"","content":"Does AWS account already have service-linked roles AWS requires “service-linked” roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. If your AWS account has been previously been used, then these roles may already exist as they would have been automatically created for you. You will determine if any of the following three IAM service-linked roles already exists in the AWS account you are using for this workshop:\nAWSServiceRoleForElasticLoadBalancing AWSServiceRoleForAutoScaling AWSServiceRoleForRDS Steps to determine if service-linked roles already exist Open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, click Roles.\nIn the filter box, type “Service” to find the service linked roles that exist in your account and look for the three roles. In this screenshot, the service linked role for AutoScaling exists (AWSServiceRoleForAutoScaling), but the roles for Elastic Load Balancing and RDS do not. Note which roles already exist as you will use this information when performing the next step.\nSTOP HERE and return to the Lab Guide Learn more: After the lab see the AWS documentation on Service-Linked Roles Setup CloudFormation for service-linked roles If you are using your own AWS account: Then use these instructions when entering CloudFormation parameters\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor: Skip this step and go to back to the Lab Guide If you already have this role \u0026hellip;then set this parameter false AWSServiceRoleForElasticLoadBalancing CreateTheELBServiceRole AWSServiceRoleForAutoScaling CreateTheAutoScalingServiceRole AWSServiceRoleForRDS CreateTheRDSServiceRole If the service-linked role does not already exist, then leave the parameter value as true Leave all the other parameter values at their default values "},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/aws_credentials/","title":"Setup AWS credentials and configuration","tags":[],"description":"","content":" You will supply configuration and credentials used by the AWS CLI and AWS SDK to access your AWS account. You identified these credentials back in step 1 of the Lab Guide Choose an option Select the appropriate option for configuration of your AWS credentials:\nOption 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3\nYou should have already copied the credentials for your account. If not then:\nClick here for instructions to copy the credentials from your assigned AWS account: Go to https://dashboard.eventengine.run/login Enter the 16 character hashcode you were provided and click \u0026ldquo;Proceed\u0026rdquo; Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. [optional] assign a name to your account (this is referred to as \u0026ldquo;Team name\u0026rdquo;)\nclick \u0026ldquo;Set Team Name\u0026rdquo; Enter a name and click \u0026ldquo;Set Team Name\u0026rdquo; Click \u0026ldquo;AWS Console\u0026rdquo; Get and store your AWS credentials\nIMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.\nAccess the AWS console\nClick \u0026ldquo;Open AWS Console\u0026rdquo;. The AWS Console will open and you can continue the lab. Now continue the steps to setup your AWS credentials\u0026hellip;.\nThe copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below\nexport AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below\nexport AWS_DEFAULT_OUTPUT=json Note that if you end your bash session, or start a new one, you will need to re-execute the export statements\nIf you completed Option 1 then STOP HERE and return to the Lab Guide Option 2 AWS CLI This option uses the AWS CLI. Note that running the bash failure testing scripts requires this software. If you are using another programming environment for failure testing, you can use Option 3 if you do not or cannot install the AWS CLI.\nTo see if the AWS CLI is installed:\n$ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values:\n$ aws configure AWS Access Key ID [*************xxxx]: \u0026lt;Your AWS Access Key ID\u0026gt; AWS Secret Access Key [**************xxxx]: \u0026lt;Your AWS Secret Access Key\u0026gt; Default region name: [us-east-2]: us-east-2 Default output format [None]: json Option 3 Manually creating credential files If you already did Option 2, then skip this\ncreate a .aws directory under your home directory\nmkdir ~/.aws Change directory to there\ncd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials. In this file you should have the following text.\n[default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; Create a text file (no extension) named config. In this file you should have the following text:\n[default] region = us-east-2 output = json Configure a session token as part of your credentials If you used Option 2 or Option 3, please follow these steps:\nDetermine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \u0026ldquo;yes\u0026rdquo;, you need to configure a session token Edit the file ~/.aws/credentials\nThe default profile will already be present. Under it add an entry for aws_session_token\n[default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; aws_session_token = \u0026lt;your session token\u0026gt; Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set:\nAWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux:\n# Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience:\nunset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE Option 4 (PowerShell) You will setup your AWS credentials as part of setting up AWS Tools for Powershell. Continue with this step .\nReturn to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/reliability/300_labs/300_testing_for_resiliency_of_ec2_rds_and_s3/documentation/software_install/","title":"Software Install","tags":[],"description":"","content":"This reference will help you install software necessary to setup your workshop environment\nAWS CLI jq Install AWS CLI v2 The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.\nLinux This includes:\nAWS CloudShell All native Linux installs Windows Subsystem for Linux (WSL) Verify existing version:\nRun the following command aws --version If the version number is less than 2.1.12 or you get \u0026ldquo;command not found\u0026rdquo; You need to install or upgrade. Follow these steps:\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot; unzip awscliv2.zip sudo ./aws/install --update After typing the commands, you should see the following in your console: For additional troubleshooting, see the detailed installation instructions here MacOS See the detailed MacOS installation instructions here Windows See the detailed Windows installation instructions here STOP HERE and return to the Lab Guide jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.\nRun the following command\n$ jq --version jq-1.6 Any version is fine.\nIf you instead got command not found then you need to install jq.\nInstall jq If you are using Amazon Linux (or Red Hat Enterprise Linux) run the following:\nsudo yum install jq If using another Linux distro, then follow the instructions at https://stedolan.github.io/jq/download/ If that succeeded, then you are finished. Return to the Lab Guide\nAlternate instructions for Linux If the steps above did not work, and you are running Linux, then try the following\nDownload the jq executable\n$ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================\u0026gt;] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - ‘jq-linux64’ saved [3953824/3953824] You can find out what your execution path is with the following command.\n$ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable.\n$ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory.\n$ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq STOP HERE and return to the Lab Guide "},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/documentation/tuneinsightsquery/","title":"Troubleshooting Guide to Tuning your Insights Query","tags":[],"description":"","content":"Troubleshooting Guide to Tuning your Insights Query Verify you are in the correct AWS Region You should be in the east region\nIf you used the directions in this lab, then this is Ohio (us-east-2) If your query returned zero results or less than three results: Possibility #1: It was been too recent since you did the bucket operations It usually takes five minutes after an operation for it to show up in CloudTrail and can take as long as 15 minutes or\nPossibility #2: It was been too long ago since you did the bucket operations If you did the previous part of the lab much earlier you should expand the time range for your query, found to the right of Select log group(s) If you see more than three results Then you are seeing bucket activity other than the operations you did with this lab\nYou are looking for three events, one for each of the test objects you uploaded. See the key field to see the test object names\nYou can also update the query to only look at the lab buckets. Add the following to your query\nfilter requestParameters.bucketName like'crrlab' Click here to return to the Lab Guide\n"},{"uri":"https://wellarchitectedlabs.com/reliability/200_labs/200_bidirectional_replication_for_s3/documentation/detachiampolicy/","title":"Troubleshooting: CloudFormation stack deletion for Lab S3 replication lab","tags":[],"description":"","content":"If your CloudFormation stack deletion fails with status DELETE_FAILED\nFrom the CloudFormation console Click on the Events tab and refresh Verify the cause of the failure is the error: Cannot delete entity, must detach all policies first Delete workshop IAM Roles Go the the IAM console Click on Roles\nIn the search box enter S3-Replication-Role. There should be either:\nTwo roles shown (if you setup both replication buckets) Or just one (if you only setup one replication bucket) If you see more than two roles stop here and investigate which roles are associated with your execution of this lab Check the box next to each of the IAM Roles\nClick Delete\nConfirm the deletion by clicking Yes, delete\nDelete workshop CloudFormation stacks Re-initiate deletion of the S3-CRR-lab-east CloudFormation stack in Ohio (us-east-2) Re-initiate deletion of the S3-CRR-lab-west CloudFormation stack in Oregon (us-west-2) CloudFormation will give you an option for Resources to retain - optional Do NOT check anything Click Delete stack Click here to return to the Lab Guide\n"},{"uri":"https://wellarchitectedlabs.com/common/","title":"","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/common/documentation/","title":"Common Documentation","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/common/documentation/createnews3bucketandaddobjects/","title":"Create new S3 bucket and add objects to it","tags":[],"description":"","content":"Create new S3 bucket These steps will guide you to create a bucket\nGo to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket\nFor Bucket name supply a name. This must be unique across all buckets in AWS\nTip: Name the bucket \u0026lt;first_name\u0026gt;\u0026lt;last_initial\u0026gt;_\u0026lt;date in yyyymmdd format\u0026gt; (do NOT include the angle brackets) Click Next three times\nReview screen: click Create bucket\nAdd object(s) to an S3 bucket Use these instructions to add one or more objects to an S3 bucket\nNote: You have the option to make the object(s) publically readable. Do NOT do this for S3 buckets used in production, or containing sensitive data. It is recommended you create a new test S3 bucket if you want to host publically readable objects.\nClick on the name of the bucket you are using (this can be the one you created above) If, and only if, you want to make the uploaded object(s) publically readable then: Click on the Permissions tab Clear both \u0026hellip;access control lists (ACLs) checkboxes (or verify they are already cleared) Click Save Type confirm Click Confirm Click on the Overview tab Drag the file(s) you want to upload to the bucket into the object upload area Click Next If, and only if, you want to make this object(s) publically readable then under Manage public permissions select Grant public read access to this object(s) Click Next two more times Click Upload Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/common/examples/usecreatenewcloudformationstack/","title":"example implementations for CreateNewCloudFormationStack","tags":[],"description":"","content":" Case 1 - all parameters left as default Case 2 - provides directions to update or view one or more parameters Case 1 - all parameters left as default Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next\nFor Stack name use CloudFormationLab\nParameters\nLook over the Parameters and their default values.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\nCase 2 - provides directions to update or view one or more parameters Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack \u0026gt; With new resources Leave Prepare template setting as-is\nFor Template source select Upload a template file Click Choose file and supply the CloudFormation template you downloaded: vpc-alb-app-db.yaml Click Next\nFor Stack name use WebApp1-VPC\nParameters\nLook over the Parameters and their default values.\nEC2InstanceSubnetId – The subnet you wish to deploy the 2 EC2 instances into for testing.\nSet the numberOfAZ parameter to 3\nLeave other parameters as their default values unless you are experimenting.\nClick Next\nFor Configure stack options we recommend configuring tags, which are key-value pairs, that can help you identify your stacks and the resources they create. For example, enter Owner in the left column which is the key, and your email address in the right column which is the value. We will not use additional permissions or advanced options so click Next. For more information, see Setting AWS CloudFormation Stack Options .\nFor Review\nReview the contents of the page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress.\nClick on the Events tab Scroll through the listing. It shows the activities performed by CloudFormation (newest events at top), such as starting to create a resource and then completing the resource creation. Any errors encountered during the creation of the stack will be listed in this tab. When it shows status CREATE_COMPLETE, then you are finished with this step.\n"},{"uri":"https://wellarchitectedlabs.com/tags/mitigate_failure/","title":"mitigate_failure","tags":[],"description":"","content":""},{"uri":"https://wellarchitectedlabs.com/common/documentation/aws_credentials/","title":"Setup AWS credentials and configuration","tags":[],"description":"","content":" You will supply configuration and credentials used by the AWS CLI and AWS SDK to access your AWS account. Choose an option Select the appropriate option for configuration of your AWS credentials:\nOption 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1\nIf you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3\nYou should have already copied the credentials for your account. If not then:\nClick here for instructions to copy the credentials from your assigned AWS account: \u0026lt;!-- noCreds: set this any value to skip the Get and store your AWS credentials step. Otherwise that step will be shown (which was the previous behavior) --\u0026gt; 1. Go to \u0026lt;https://dashboard.eventengine.run/login\u0026gt; 1. Enter the 16 character _hashcode_ you were provided and click \u0026quot;Proceed\u0026quot; ![AWSAccountCodeProceed](/Common/images/AWSAccountCodeProceed.png) 1. Sign-in using either an Amazon.com retail account or a One-Time Password (OTP) that will be emailed to you. ![AWSAccountSignIn](/Common/images/AWSAccountSignIn.png) 1. [optional] assign a name to your account (this is referred to as \u0026quot;Team name\u0026quot;) * click \u0026quot;Set Team Name\u0026quot; * Enter a name and click \u0026quot;Set Team Name\u0026quot; 1. Click \u0026quot;AWS Console\u0026quot; ![AWSAccountEvent](/Common/images/AWSAccountEvent.png) 1. Get and store your AWS credentials * **IMPORTANT** Copy the provided credentials and save them. You wil need these to complete the workshop ![AWSAccountCredsAndConsole](/Common/images/AWSAccountCredsAndConsole.png) * Copy the _whole_ code block corresponding to the system you are using. 1. Access the AWS console * Click \u0026quot;Open AWS Console\u0026quot;. * The AWS Console will open and you can continue the lab. --- Now continue the steps to setup your AWS credentials\u0026hellip;.\nThe copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below\nexport AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below\nexport AWS_DEFAULT_OUTPUT=json Note that if you end your bash session, or start a new one, you will need to re-execute the export statements\nIf you completed Option 1 then STOP HERE and return to the Lab Guide\nOption 2 AWS CLI This option uses the AWS CLI. You should use Option 3 if you do not or cannot install the AWS CLI.\nTo see if the AWS CLI is installed:\n$ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values:\n$ aws configure AWS Access Key ID [*************xxxx]: \u0026lt;Your AWS Access Key ID\u0026gt; AWS Secret Access Key [**************xxxx]: \u0026lt;Your AWS Secret Access Key\u0026gt; Default region name: [us-east-2]: us-east-2 Default output format [None]: json Option 3 Manually creating credential files If you already did Option 2, then skip this\ncreate a .aws directory under your home directory\nmkdir ~/.aws Change directory to there\ncd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials. In this file you should have the following text.\n[default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; Create a text file (no extension) named config. In this file you should have the following text:\n[default] region = us-east-2 output = json Configure a session token as part of your credentials If you used Option 2 or Option 3, please follow these steps:\nDetermine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \u0026ldquo;yes\u0026rdquo;, you need to configure a session token Edit the file ~/.aws/credentials\nThe default profile will already be present. Under it add an entry for aws_session_token\n[default] aws_access_key_id = \u0026lt;Your access key\u0026gt; aws_secret_access_key = \u0026lt;Your secret key\u0026gt; aws_session_token = \u0026lt;your session token\u0026gt; Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set:\nAWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux:\n# Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience:\nunset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE Option 4 (PowerShell) If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ .\nStart a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog.\nConfigure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command):\nSet-AWSCredentials -AccessKey \u0026lt;Your access key\u0026gt; -SecretKey \u0026lt;Your secret key\u0026gt; \\ [ -SessionToken \u0026lt;your session key\u0026gt; ] -StoreAs \u0026lt;SomeProfileName\u0026gt; Initialize-AWSDefaults -ProfileName \u0026lt;SomeProfileName\u0026gt; -Region us-east-2 Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/common/documentation/software_install/","title":"Software Install","tags":[],"description":"","content":"This reference will help you install software necessary to setup your workshop environment\nAWS CLI jq Install AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.\nLinux This includes:\nAWS CloudShell\nAll native Linux installs\nMacOS\nWindows Subsystem for Linux (WSL)\nRun the following command\n$ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine\nIf you instead got command not found then you need to install awscli:\n$ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... If that succeeded, then you are finished. Return to the Lab Guide\nIf that does not work, then do the following:\nSee the detailed installation instructions here Other environments (not Linux) See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html STOP HERE and return to the Lab Guide\njq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.\nRun the following command\n$ jq --version jq-1.6 Any version is fine.\nIf you instead got command not found then you need to install jq.\nInstall jq If you are using Amazon Linux (or Red Hat Enterprise Linux) run the following:\nsudo yum install jq If using another Linux distro, then follow the instructions at https://stedolan.github.io/jq/download/ If that succeeded, then you are finished. Return to the Lab Guide\nAlternate instructions for Linux If the steps above did not work, and you are running Linux, then try the following\nDownload the jq executable\n$ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================\u0026gt;] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - ‘jq-linux64’ saved [3953824/3953824] You can find out what your execution path is with the following command.\n$ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable.\n$ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory.\n$ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Return to the Lab Guide to continue the lab\n"},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/crawler-cfn/","title":"","tags":[],"description":"","content":"Below is a sample crawler config file. It is suggested you modify your existing file, modifications are between \u0026lsquo;***\u0026rsquo; characters.\nVariables that need to be changed in the new code below:\n(region): The region that contains the Lambda function (accountID): The account that contains the Lambda function AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: \u0026#39;AWS::Glue::Database\u0026#39; Properties: DatabaseInput: Name: \u0026#39;(Database Name)\u0026#39; CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: \u0026#39;AWS::IAM::Role\u0026#39; Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; Path: / ManagedPolicyArns: - \u0026#39;arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\u0026#39; Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \u0026#39;logs:CreateLogGroup\u0026#39; - \u0026#39;logs:CreateLogStream\u0026#39; - \u0026#39;logs:PutLogEvents\u0026#39; Resource: \u0026#39;arn:aws:logs:*:*:*\u0026#39; - Effect: Allow Action: - \u0026#39;glue:UpdateDatabase\u0026#39; - \u0026#39;glue:UpdatePartition\u0026#39; - \u0026#39;glue:CreateTable\u0026#39; - \u0026#39;glue:UpdateTable\u0026#39; - \u0026#39;glue:ImportCatalogToGlue\u0026#39; Resource: \u0026#39;*\u0026#39; - Effect: Allow Action: - \u0026#39;s3:GetObject\u0026#39; - \u0026#39;s3:PutObject\u0026#39; Resource: arn:aws:s3:::\u0026lt;bucketname\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/WorkshopCUR* AWSCURCrawlerLambdaExecutor: Type: \u0026#39;AWS::IAM::Role\u0026#39; Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \u0026#39;logs:CreateLogGroup\u0026#39; - \u0026#39;logs:CreateLogStream\u0026#39; - \u0026#39;logs:PutLogEvents\u0026#39; ***** - \u0026#39;lambda:InvokeFunction\u0026#39; ***** Resource: ***** - \u0026#39;arn:aws:logs:*:*:*\u0026#39; - \u0026#39;arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;accountID\u0026gt;:function:SubAcctSplit\u0026#39; ***** - Effect: Allow Action: - \u0026#39;glue:StartCrawler\u0026#39; Resource: \u0026#39;*\u0026#39; AWSCURCrawler: Type: \u0026#39;AWS::Glue::Crawler\u0026#39; DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: \u0026#39;s3://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/WorkshopCUR\u0026#39; Exclusions: - \u0026#39;**.json\u0026#39; - \u0026#39;**.yml\u0026#39; - \u0026#39;**.sql\u0026#39; - \u0026#39;**.csv\u0026#39; - \u0026#39;**.gz\u0026#39; - \u0026#39;**.zip\u0026#39; SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: \u0026#39;AWS::Lambda::Function\u0026#39; DependsOn: AWSCURCrawler Properties: Code: ZipFile: \u0026gt; const AWS = require(\u0026#39;aws-sdk\u0026#39;); const response = require(\u0026#39;cfn-response\u0026#39;); exports.handler = function(event, context, callback) { if (event.RequestType === \u0026#39;Delete\u0026#39;) { response.send(event, context, response.SUCCESS); } else { const glue = new AWS.Glue(); glue.startCrawler({ Name: \u0026#39;AWSCURCrawler-WorkshopCUR\u0026#39; }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData[\u0026#39;__type\u0026#39;] == \u0026#39;CrawlerRunningException\u0026#39;) { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); ***** var lambda = new AWS.Lambda(); var params = { FunctionName: \u0026#39;SubAcctSplit\u0026#39; }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** } }; Handler: \u0026#39;index.handler\u0026#39; Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: \u0026#39;Custom::AWSStartCURCrawler\u0026#39; Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: \u0026#39;lambda:InvokeFunction\u0026#39; FunctionName: !GetAtt AWSCURInitializer.Arn Principal: \u0026#39;s3.amazonaws.com\u0026#39; SourceAccount: !Ref AWS::AccountId SourceArn: \u0026#39;arn:aws:s3:::\u0026lt;bucket\u0026gt;\u0026#39; AWSS3CURLambdaExecutor: Type: \u0026#39;AWS::IAM::Role\u0026#39; Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \u0026#39;logs:CreateLogGroup\u0026#39; - \u0026#39;logs:CreateLogStream\u0026#39; - \u0026#39;logs:PutLogEvents\u0026#39; Resource: \u0026#39;arn:aws:logs:*:*:*\u0026#39; - Effect: Allow Action: - \u0026#39;s3:PutBucketNotification\u0026#39; Resource: \u0026#39;arn:aws:s3:::\u0026lt;bucket\u0026gt;\u0026#39; AWSS3CURNotification: Type: \u0026#39;AWS::Lambda::Function\u0026#39; DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: \u0026gt; const AWS = require(\u0026#39;aws-sdk\u0026#39;); const response = require(\u0026#39;cfn-response\u0026#39;); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== \u0026#39;Delete\u0026#39;) { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ \u0026#39;s3:ObjectCreated:*\u0026#39; ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || \u0026#39;missing arn\u0026#39;, Filter: { Key: { FilterRules: [ { Name: \u0026#39;prefix\u0026#39;, Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: \u0026#39;index.handler\u0026#39; Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: \u0026#39;Custom::AWSPutS3CURNotification\u0026#39; Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: \u0026#39;\u0026lt;bucket\u0026gt;\u0026#39; ReportKey: \u0026#39;\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/WorkshopCUR\u0026#39; AWSCURReportStatusTable: Type: \u0026#39;AWS::Glue::Table\u0026#39; DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: \u0026#39;cost_and_usage_data_status\u0026#39; TableType: \u0026#39;EXTERNAL_TABLE\u0026#39; StorageDescriptor: Columns: - Name: status Type: \u0026#39;string\u0026#39; InputFormat: \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OutputFormat: \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; SerdeInfo: SerializationLibrary: \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; Location: \u0026#39;s3://\u0026lt;bucket\u0026gt;/\u0026lt;prefix\u0026gt;/\u0026lt;folder\u0026gt;/cost_and_usage_data_status/\u0026#39; "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/iam_athena/","title":"","tags":[],"description":"","content":"IAM policy for access to Athena\nNOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;athena:StartQueryExecution\u0026#34;,\r\u0026#34;glue:GetCrawler\u0026#34;,\r\u0026#34;glue:GetDataCatalogEncryptionSettings\u0026#34;,\r\u0026#34;glue:GetTableVersions\u0026#34;,\r\u0026#34;glue:GetPartitions\u0026#34;,\r\u0026#34;athena:GetQueryResults\u0026#34;,\r\u0026#34;athena:ListWorkGroups\u0026#34;,\r\u0026#34;athena:GetNamedQuery\u0026#34;,\r\u0026#34;glue:GetDevEndpoint\u0026#34;,\r\u0026#34;glue:GetSecurityConfiguration\u0026#34;,\r\u0026#34;glue:GetResourcePolicy\u0026#34;,\r\u0026#34;glue:GetTrigger\u0026#34;,\r\u0026#34;glue:GetUserDefinedFunction\u0026#34;,\r\u0026#34;athena:GetExecutionEngine\u0026#34;,\r\u0026#34;glue:GetJobRun\u0026#34;,\r\u0026#34;athena:GetExecutionEngines\u0026#34;,\r\u0026#34;s3:HeadBucket\u0026#34;,\r\u0026#34;glue:GetUserDefinedFunctions\u0026#34;,\r\u0026#34;glue:GetClassifier\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;athena:GetQueryResultsStream\u0026#34;,\r\u0026#34;glue:GetJobs\u0026#34;,\r\u0026#34;glue:GetTables\u0026#34;,\r\u0026#34;glue:GetTriggers\u0026#34;,\r\u0026#34;athena:GetNamespace\u0026#34;,\r\u0026#34;athena:GetQueryExecutions\u0026#34;,\r\u0026#34;athena:GetCatalogs\u0026#34;,\r\u0026#34;athena:ListNamedQueries\u0026#34;,\r\u0026#34;athena:GetNamespaces\u0026#34;,\r\u0026#34;glue:GetPartition\u0026#34;,\r\u0026#34;glue:GetDevEndpoints\u0026#34;,\r\u0026#34;athena:GetTables\u0026#34;,\r\u0026#34;athena:GetTable\u0026#34;,\r\u0026#34;athena:BatchGetNamedQuery\u0026#34;,\r\u0026#34;athena:BatchGetQueryExecution\u0026#34;,\r\u0026#34;glue:GetJob\u0026#34;,\r\u0026#34;glue:GetConnections\u0026#34;,\r\u0026#34;glue:GetCrawlers\u0026#34;,\r\u0026#34;glue:GetClassifiers\u0026#34;,\r\u0026#34;athena:ListQueryExecutions\u0026#34;,\r\u0026#34;glue:GetCatalogImportStatus\u0026#34;,\r\u0026#34;athena:GetWorkGroup\u0026#34;,\r\u0026#34;glue:GetConnection\u0026#34;,\r\u0026#34;glue:BatchGetPartition\u0026#34;,\r\u0026#34;glue:GetSecurityConfigurations\u0026#34;,\r\u0026#34;glue:GetDatabases\u0026#34;,\r\u0026#34;athena:ListTagsForResource\u0026#34;,\r\u0026#34;glue:GetTable\u0026#34;,\r\u0026#34;glue:GetDatabase\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;glue:GetDataflowGraph\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;athena:GetQueryExecution\u0026#34;,\r\u0026#34;glue:GetPlan\u0026#34;,\r\u0026#34;glue:GetCrawlerMetrics\u0026#34;,\r\u0026#34;glue:GetJobRuns\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r},\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:GetBucketLocation\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::aws-athena-query-results-\u0026lt;Account ID\u0026gt;-us-east-1\u0026#34;,\r\u0026#34;arn:aws:s3:::aws-athena-query-results-\u0026lt;Account ID\u0026gt;-us-east-1/*\u0026#34;\r]\r},\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor2\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;s3:ListBucketByTags\u0026#34;,\r\u0026#34;s3:GetLifecycleConfiguration\u0026#34;,\r\u0026#34;s3:GetBucketTagging\u0026#34;,\r\u0026#34;s3:GetInventoryConfiguration\u0026#34;,\r\u0026#34;s3:GetObjectVersionTagging\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:GetBucketLogging\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:GetAccelerateConfiguration\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetObjectVersionTorrent\u0026#34;,\r\u0026#34;s3:GetObjectAcl\u0026#34;,\r\u0026#34;s3:GetEncryptionConfiguration\u0026#34;,\r\u0026#34;s3:GetBucketRequestPayment\u0026#34;,\r\u0026#34;s3:GetObjectVersionAcl\u0026#34;,\r\u0026#34;s3:GetObjectTagging\u0026#34;,\r\u0026#34;s3:GetMetricsConfiguration\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:GetBucketWebsite\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketNotification\u0026#34;,\r\u0026#34;s3:GetReplicationConfiguration\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectTorrent\u0026#34;,\r\u0026#34;s3:GetBucketCORS\u0026#34;,\r\u0026#34;s3:GetAnalyticsConfiguration\u0026#34;,\r\u0026#34;s3:GetObjectVersionForReplication\u0026#34;,\r\u0026#34;s3:GetBucketLocation\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::\u0026lt;S3 CUR Bucket\u0026gt;/*\u0026#34;,\r\u0026#34;arn:aws:s3:::\u0026lt;S3 CUR Bucket\u0026gt;\u0026#34;\r]\r}\r]\r} "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/s3_bucket_policy/","title":"","tags":[],"description":"","content":"Bucket policy for member/linked account access to CUR files\nNOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;Policy1335892530063\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1335892150622\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::386209384616:root\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[S3 Bucket Name]\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1335892526596\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::386209384616:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[S3 Bucket Name]/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1546900919345\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::[Sub-Account ID]:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[S3 Bucket Name]\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1546901049588\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::[Sub-Account ID]:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::[S3 Bucket Name]/*\u0026#34; } ] } "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/s3linkedputacl/","title":"","tags":[],"description":"","content":"Here is the Lambda function to re-write object ACLs. It is triggered by an S3 Event, reads the folder from the object - and then applies the required object ACL: FULL_CONTROL for the owner, READ for the sub account.\nEdit the following fields in the code below:\nfolder1: The name of the folder where new files will be placed Owner Account Name: The owner account name - the account email without the @companyname, they will get FULL_CONTROL permissions Owner Canonical ID: The owner canonical ID, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html Sub Account Name: The sub account name - the account email without the @companyname, they will get READ permissions Sub Acct Canonical ID: The sub account canonical ID const AWS = require(\u0026#39;aws-sdk\u0026#39;); const util = require(\u0026#39;util\u0026#39;); // Permissions for the new objects // Key MUST match the top level folder // Format: \u0026lt;owner account name\u0026gt; - \u0026lt;Canonical ID\u0026gt; - \u0026lt;sub account name\u0026gt; - \u0026lt;canonical ID\u0026gt; // This will give owner full permission \u0026amp; sub account read only permission var permissions = new Array(); var permissions = { \u0026#39;\u0026lt;folder1\u0026gt;\u0026#39;: [\u0026#39;\u0026lt;owner acct name\u0026gt;\u0026#39;,\u0026#39;\u0026lt;Owner Canonical ID\u0026gt;\u0026#39;,\u0026#39;\u0026lt;sub account name\u0026gt;\u0026#39;,\u0026#39;\u0026lt;Sub Acct Canonical ID\u0026gt;\u0026#39;], \u0026#39;\u0026lt;folder2\u0026gt;\u0026#39;: [\u0026#39;\u0026lt;owner acct name\u0026gt;\u0026#39;,\u0026#39;\u0026lt;Owner Canonical ID\u0026gt;\u0026#39;,\u0026#39;\u0026lt;sub account name\u0026gt;\u0026#39;,\u0026#39;\u0026lt;Sub Acct Canonical ID\u0026gt;\u0026#39;] }; // Main Loop exports.handler = function(event, context, callback) { // If its an object delete, do nothing if (event.RequestType === \u0026#39;Delete\u0026#39;) { } else // Its an object put { // Get the source bucket from the S3 event var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters, decode it var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \u0026#34; \u0026#34;)); // Gets the top level folder, which is the key for the permissions array var folderID = srcKey.split(\u0026#34;/\u0026#34;)[0]; // Define the object permissions, using the permissions array var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { \u0026#39;Owner\u0026#39;: { \u0026#39;DisplayName\u0026#39;: permissions[folderID][0], \u0026#39;ID\u0026#39;: permissions[folderID][1] }, \u0026#39;Grants\u0026#39;: [ { \u0026#39;Grantee\u0026#39;: { \u0026#39;Type\u0026#39;: \u0026#39;CanonicalUser\u0026#39;, \u0026#39;DisplayName\u0026#39;: permissions[folderID][0], \u0026#39;ID\u0026#39;: permissions[folderID][1] }, \u0026#39;Permission\u0026#39;: \u0026#39;FULL_CONTROL\u0026#39; }, { \u0026#39;Grantee\u0026#39;: { \u0026#39;Type\u0026#39;: \u0026#39;CanonicalUser\u0026#39;, \u0026#39;DisplayName\u0026#39;: permissions[folderID][2], \u0026#39;ID\u0026#39;: permissions[folderID][3] }, \u0026#39;Permission\u0026#39;: \u0026#39;READ\u0026#39; }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); } }; "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/sub_account_split/","title":"","tags":[],"description":"","content":"Below is the code for the lambda function.\nYou will need to modify the following variable:\nathena_output: This is where Athena puts output data, this is typically the management/payer Account ID, which is the default folder for Athena output queries bucketname: This is the output bucket for the Athena queries You will need to modify the following arrays, the order is important - the first folder in the subfolder array, will be given the permissions of the first element of the S3ObjectPolicies array.\nsubfolders: This contains the list of folders that the queries write to S3ObjectPolicies: This contains the S3 Object permissions ACL that will be written to objects in the corresponding folder. You will need to add the owners details (management/payer account) and the grantee (sub account) details. import boto3 import json import datetime import time from dateutil.relativedelta import relativedelta bucketname = \u0026#39;(output bucket)\u0026#39; #List of Subfolders \u0026amp; ACLs to apply to objects in them #There MUST be a 1:1 between subfolders \u0026amp; policies subfolders = [\u0026#39;\u0026lt;folder1\u0026gt;\u0026#39;] # Arrays to hold the Athena delete \u0026amp; create queries that we need to run delete_query_strings = [] create_query_strings = [] # Athena output folder athena_output = \u0026#39;s3://aws-athena-query-results-us-east-1-\u0026lt;account ID\u0026gt;/\u0026#39; # Main loop def lambda_handler(event, context): #clear the strings for every execution context - useful for troubleshooting after failures create_query_strings.clear() delete_query_strings.clear() # Get the current date, so you know which months folder you\u0026#39;re working on now = datetime.datetime.now() lastmonth = now - relativedelta(months=1) # Variables to construct the s3 folder name # YES! you can do multiple subfolders if you have multiple queries to run, 1 subfolder per query # We need current and previous month because otherwise we can miss data in the last day of the month currentmonth = \u0026#39;/year_1=\u0026#39; + str(now.year) + \u0026#39;/month_1=\u0026#39; + str(now.month) + \u0026#39;/\u0026#39; previousmonth = \u0026#39;/year_1=\u0026#39; + str(lastmonth.year) + \u0026#39;/month_1=\u0026#39; + str(lastmonth.month) + \u0026#39;/\u0026#39; # Clear the current and previous months S3 folder s3_clear_folders(currentmonth) s3_clear_folders(previousmonth) # Get the athena queries to run get_athena_queries(currentmonth,\u0026#39;0\u0026#39;) get_athena_queries(previousmonth,\u0026#39;1\u0026#39;) # Make sure to delete any existing temp tables, so no wobbly\u0026#39;s are thrown run_delete_athena_queries() # Create the athena tables, which will actually output data to S3 folders run_create_athena_queries() # Delete the array in case of another Lambda invocation return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Finished!\u0026#39;) } # Clear the S3 folders for the current month def s3_clear_folders(month): # Get S3 client/object client = boto3.client(\u0026#39;s3\u0026#39;) # For each subfolder - in case you have multilpe subfolders, i.e. multilpe accounts/business units to split data out to for subfolder in subfolders: # List all objects in the current months bucket response = client.list_objects_v2( Bucket=bucketname, Prefix=subfolder + month ) # Get how many objects there are to delete, if any keys = response[\u0026#39;KeyCount\u0026#39;] # Only try to delete if there\u0026#39;s objects if (keys \u0026gt; 0): # Get the ojbects from the response s3objects = response[\u0026#39;Contents\u0026#39;] # For each object, we\u0026#39;re going to delete it # cycle through the list of objects for s3object in s3objects: # Get the object key objectkey = s3object[\u0026#39;Key\u0026#39;] # Delete the object response = client.delete_object( Bucket=bucketname, Key=objectkey ) # Get the Athena saved queries to run # They need to be labelled \u0026#39;create_linked\u0026#39; or \u0026#39;delete_linked\u0026#39; def get_athena_queries(month,interval): # Get Athena client/object client = boto3.client(\u0026#39;athena\u0026#39;) # Get all the saved queries in Athena response = client.list_named_queries() # Get the named query IDs from the response named_query_IDs = response[\u0026#39;NamedQueryIds\u0026#39;] # Go through all the query ID, to find the delete \u0026amp; create queries we need to run for query_ID in named_query_IDs: # Get all the details of a named query using its ID named_query = client.get_named_query( NamedQueryId=query_ID ) # Get the query string \u0026amp; query name of the query querystring = named_query[\u0026#39;NamedQuery\u0026#39;][\u0026#39;QueryString\u0026#39;] queryname = named_query[\u0026#39;NamedQuery\u0026#39;][\u0026#39;Name\u0026#39;] # If its a create query, add it to the list of create queries # We also replace the \u0026#39;/subfolder\u0026#39; string in the query with the folder structure for the current month if \u0026#39;create_linked_\u0026#39; in queryname: # Get a unique ID for the temp table tableID = queryname.split(\u0026#39;_\u0026#39;)[2] + interval # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace(\u0026#39;/subfolder\u0026#39;, month).replace(\u0026#39;__interval__\u0026#39;,interval) new_query2 = new_query1.replace(\u0026#39;temp_table\u0026#39;, \u0026#39;temp_\u0026#39; + tableID) # Add the create query string to the array create_query_strings.append(new_query2) # If its a delete query, add it to the list of delete queries to execute later if \u0026#39;delete_linked_\u0026#39; in queryname: # Get a unique ID for the temp table tableID = queryname.split(\u0026#39;_\u0026#39;)[2] + interval # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace(\u0026#39;temp_table\u0026#39;, \u0026#39;temp_\u0026#39;+tableID) # Add the delete query string to the array delete_query_strings.append(new_query1) # Run the delete Athena queries to remove any temp tables def run_delete_athena_queries(): # Get Athena client/object client = boto3.client(\u0026#39;athena\u0026#39;) # Go through each of the delete query strings in the list for delete_query_string in delete_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=delete_query_string, ResultConfiguration={ \u0026#39;OutputLocation\u0026#39;: athena_output, \u0026#39;EncryptionConfiguration\u0026#39;: { \u0026#39;EncryptionOption\u0026#39;: \u0026#39;SSE_S3\u0026#39;, } } ) # Get the state of the delete execution response = client.get_query_execution( QueryExecutionId=executionID[\u0026#39;QueryExecutionId\u0026#39;] )[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;][\u0026#39;State\u0026#39;] # A busy wait to make sure its finished before moving on # Tables must not exist before creation # If the function runs for a long time ($) you should implement step functions or a cost effective wait # This is a low \u0026#34;cost of complexity\u0026#34; solution while response in [\u0026#39;QUEUED\u0026#39;,\u0026#39;RUNNING\u0026#39;]: # Busy wait to make sure it finishes time.sleep(1) # Get the current state of the query response = client.get_query_execution( QueryExecutionId=executionID[\u0026#39;QueryExecutionId\u0026#39;] )[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;][\u0026#39;State\u0026#39;] # Run the Athena queries to create the table \u0026amp; populate the S3 data def run_create_athena_queries(): # Get Athena client/object client = boto3.client(\u0026#39;athena\u0026#39;) # Go through each of the create query strings in the list for create_query_string in create_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=create_query_string, ResultConfiguration={ \u0026#39;OutputLocation\u0026#39;: athena_output, \u0026#39;EncryptionConfiguration\u0026#39;: { \u0026#39;EncryptionOption\u0026#39;: \u0026#39;SSE_S3\u0026#39;, } } ) "},{"uri":"https://wellarchitectedlabs.com/cost/300_labs/300_splitting_sharing_cur_access/code/subacctsplit_role/","title":"","tags":[],"description":"","content":"Review the policy below, and use it as a starting point to create your policy for the Lambda fuction.\nThe following fields will need to be changed:\nOutput bucket: The S3 bucket that will contain the output from the Athena queries Account ID: the management/payer account ID Source bucket: the location of the original CUR files in the management/payer { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;athena:StartQueryExecution\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34;, \u0026#34;athena:GetQueryResults\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;athena:GetNamedQuery\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;athena:ListQueryExecutions\u0026#34;, \u0026#34;athena:ListNamedQueries\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;athena:GetQueryExecution\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::(output bucket)/*\u0026#34;, \u0026#34;arn:aws:logs:us-east-1:(account ID):log-group:/aws/lambda/SubAcctSplit:*\u0026#34;, \u0026#34;arn:aws:athena:*:*:workgroup/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:CreateTable\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34;, \u0026#34;glue:GetPartition\u0026#34;, \u0026#34;glue:DeleteTable\u0026#34;, \u0026#34;glue:GetTable\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor3\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)/*\u0026#34;, \u0026#34;arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor4\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::(source bucket)/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor5\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:us-east-1:(account ID):*\u0026#34; } ] } "},{"uri":"https://wellarchitectedlabs.com/categories/","title":"Categories","tags":[],"description":"","content":""}]